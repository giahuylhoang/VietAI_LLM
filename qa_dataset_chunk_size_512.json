{
    "queries": {
        "fd210e54-14e8-4bb5-8218-e1bee0d3e996": "What are the key features introduced in LlamaIndex version 0.9?",
        "8f3f66cd-9a9b-47a0-8271-99c79b80900d": "What are the advantages of running a large language model (LLM) locally on your laptop?",
        "55f68166-ec37-43c7-bf6e-846acafc16f7": "How does Agentic RAG handle the addition of new documents to the system?",
        "833031ce-f277-4e30-aeb9-23398af5371b": "What are the key steps involved in deploying a HuggingFace embedding model to Amazon SageMaker and consuming it with LlamaIndex, as demonstrated in Davide Gallitelli's tutorial?",
        "d6548cae-4ed4-48bf-a1ee-f9d56941c9cd": "How can LlamaIndex's ReAct agent and API abstractions be used to build a finance agent that can query public companies, summarize financial news, and plot stock data?",
        "cb7273aa-4b1e-4e64-b64e-74c6aedd8335": "How does LlamaParse handle the challenge of parsing translated manga, specifically addressing the issue of panel interpretation and reading order?",
        "ea662649-09c3-4b47-9b89-d6db9ad8d8ae": "What are the possible relationships that can be associated with a \"Product\" entity in the graph schema?",
        "cc42664a-2abf-4c88-b90f-16c1e86ddefa": "What is the purpose of the Instrumentation module introduced in LlamaIndex v0.10.20?",
        "cabea52f-ae33-4f05-bf2a-6cdc7a75bc9f": "What were the initial challenges Jerry faced that led to the creation of LlamaIndex?",
        "f2b83b45-0573-4a34-bbf4-27cf9f986464": "What is the significance of the rotational symmetry of the Gaussian function in the context of convolution?",
        "e19314cb-00ec-45dd-a8eb-b35bcae08173": "What are the key challenges faced by companies when building production-ready LLM applications, according to the provided text?",
        "dc144ef4-1f28-4c05-bea8-6ec2ea57ece3": "How does the `SQLAutoVectorQueryEngine` utilize both the `sql_tool` and `vector_tool` to process a query?",
        "d9134c70-8ce6-4122-b89d-118e8502f33a": "What are the two traditional retrieval metrics used to evaluate the efficacy of the `NetworkRetriever`?",
        "d868e129-490a-4c8f-9248-964008176054": "How can LlamaParse be used to extract text from a Tesla quarterly filing?",
        "dc627d58-bb1d-46f7-9b3e-24c57c896973": "How does the LlamaIndex DataGenerator class utilize OpenAI ChatGPT to generate questions from a given document?",
        "71ff7f2c-710d-4ca1-ae95-bafa7d6052be": "What are the limitations of using 10M tokens for large document corpuses?",
        "90a8d1a0-6d05-4aaa-86fd-e0f1084c15bf": "What is the purpose of using CohereRerank as a postprocessor in LlamaIndex?",
        "ed7388aa-29b4-4dee-af3d-3af95ee63fde": "How can you configure your environment variables to run the `test_llama_index()` function?",
        "32b39c45-d427-4492-9b06-0f5bba2793ba": "What are some of the emerging problems at the intersection of LLMs and data architecture stacks?",
        "034c867e-5362-4f3e-91d8-142f468d1e72": "What is the name of the upcoming in-person hackathon hosted by LlamaIndex?",
        "1d7f5757-5580-49ec-887e-c618dd597207": "What is the purpose of the `create_chat_engine` function and how does it utilize the provided image description?",
        "bb92f150-3eb2-4a2a-91c2-12eb8d4a3f2f": "Explain how Timescale Vector's time-based partitioning improves the efficiency of ANN queries with time-based filters.",
        "18068179-93c5-4c52-900f-29cd9c429712": "How does the `MyCustomRetriever` class utilize entity detection in its retrieval process?",
        "a05943ce-3a70-4e30-bfcf-a83b66a91f65": "What are the advantages of using asynchronous requests (acomplete) over synchronous requests (complete) in MyMagic AI's API?",
        "9429228e-16e8-4881-8981-e7b92ae714ff": "How can you use the provided code snippet to build an \"advanced RAG\" workflow that includes query rewriting, retrieval, reranking, and synthesis?",
        "b78a568f-5c0c-4c57-bb98-e2d8b5f989d9": "What are the necessary steps to set up a Cohere reranker for fine-tuning?",
        "6413d9aa-52b7-4ce2-861b-c6278b57f517": "What are the potential benefits of providing users with customizable building blocks for interacting with their data, as opposed to a fixed set of pre-defined functions?",
        "1af08f51-af63-4df1-8c09-d8cdcd92c167": "How does the speaker envision the interface between LLM-based data processing systems and data sources evolving, and what are some specific examples of how this evolution might manifest?",
        "3adfa49a-ea28-4d2a-b727-c4cb804c2cdd": "What are the limitations of the observations made in this blog post regarding hallucination and correctness in GPT-4V?",
        "47569011-39fd-4878-9cee-f0e974f23303": "What are the benefits of using the new `CitationQueryEngine` in LlamaIndex applications?",
        "8b41aa15-fd5c-4586-9a95-be6d29286b99": "How does the code snippet utilize the `ImageReader` class to process image files within the receipts directory?",
        "5871f70a-b880-4af5-8691-e96ec30213e7": "What are the benefits of using the new lower-level agent API in LlamaIndex?",
        "fbe0a312-d4e9-4946-8521-42df40bb9cef": "What are the advantages of using Modal for finetuning LLaMa?",
        "6687d5ac-9256-43ab-a2ab-2fa8c37be92b": "What is the email address to submit noteworthy projects, articles, or videos for potential inclusion in the LlamaIndex newsletter?",
        "4ee0120f-5866-4641-a271-d2ad1040efe9": "What are some ways to enhance the functionality of a Slack bot beyond its basic message-listening, remembering, and replying capabilities?",
        "7ee3beee-2d73-443e-9a7b-70339c439f0b": "What is the purpose of the `dotenv.load_dotenv()` function in the provided code snippet?",
        "eaaeb28b-d4f2-45c1-8f71-29b00780a472": "What is the purpose of using the `DiffPrivateSimpleDatasetPack` in the provided code snippet?",
        "949c9ff0-225b-49fb-bf8f-121628fb5413": "What is RAFT and how does it improve domain-specific RAG performance in LLMs?",
        "53a37f16-aea7-409b-8f3e-baa7b3b10c6a": "What are the two demo notebooks provided for experimenting with GPT-4o using LlamaIndex?",
        "84a2e04f-3d73-4ad6-8302-dea02fe19bc0": "What are the key components of a basic RAG system?",
        "db298c6e-b289-4b33-a7f9-3bb09b98958a": "How does LlamaCloud Chat facilitate instant conversational data access?",
        "8e9493d2-97c2-4452-b0f8-319fe810ff4b": "What are the benefits of using LITS for streaming on all endpoints?",
        "dd1a9b12-6404-423d-8bc4-55d827b8bef9": "How does the \"Lost in the Middle\" paper address the limitations of language models in processing long contexts, and how does this relate to the concept of capturing logical structure in visually structured documents as presented in the \"Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser\" paper?",
        "085dad1d-f991-4e7f-b298-8e1dd9eb7969": "What are the two main utility tools provided in the LlamaIndex repository and how do they help address the issue of large data volumes returned by APIs?",
        "ab5968d4-e001-48fa-8e23-9ef868c2992e": "What are the limitations of LayoutPDFReader in terms of PDF parsing?",
        "f743d7d4-7a06-473f-99fe-6593c7fa7b11": "What is the primary goal of Prosper AI as stated in the provided text?",
        "3a361519-2038-4f8e-94b9-4fc888c90383": "How does the `create_chat_engine` function utilize the provided image description in its prompt?",
        "0b87f703-56d5-400c-beb4-82cc3daa5e2b": "What are the key benefits of using JinaAI reranker, LlamaIndex, and MistralAI to enhance the quality of a RAG application?",
        "3eb5fcec-cb3a-4d0c-a597-285a797596df": "What are the key findings of the comparison between GPT-4V, Fuyu-8B, MiniGPT4, CogVLM-4, and LLaVa-13B?",
        "b30364cd-9e31-4d64-ac6e-4481af44c8c0": "What is the significance of the \"novel recursive retrieval RAG technique\" mentioned in the context?",
        "f7804ffb-8af0-4089-8fda-ee9b5dad6c50": "What is the significance of the discovery of LK-99 in terms of its potential applications?",
        "3bf5931a-1775-460b-a705-13d24933ccb2": "How does the code handle the user's speech and response?",
        "b55e6200-1cf7-4bff-bc52-cf0cba3e8906": "What are the limitations of the public preview mode of LlamaParse?",
        "39c67be5-fdaf-4fb7-9310-de7df3cc3f47": "How does LlamaIndex leverage Microsoft Guidance to improve structured outputs from LLMs?",
        "5ce2cc3a-8950-43fd-ac77-272444744c52": "How does Presidio function as a post-processor within LlamaIndex?",
        "c5aa5915-0a46-4571-9311-b23ed17b9236": "What are the tradeoffs associated with injecting long context into LLMs?",
        "c1dab154-8894-44f3-be53-a7ecbaccd720": "What is the purpose of the \"LLMCompiler Implementation\" and where can you find more information about it?",
        "1960eadc-7b01-44e7-96a4-ba0018a41f0f": "What are the limitations of state-of-the-art LLMs like ChatGPT, GPT-4, and Claude 2 in terms of retrieving contextually relevant information?",
        "a523edbd-ae53-448a-83ed-f1b8285e5a4c": "What are the benefits of using standalone modules for response synthesis compared to the previous approach with `ResponseSynthesizer` and `ResponseBuilder`?",
        "38d2b116-f533-46a9-b177-7d8540879fc8": "What are the key tools used by Counselor Copilot to provide context and support during conversations?",
        "8e362a0a-44a7-4e05-8246-2fd63cc8c7da": "How does Glenn Parham's cookbook demonstrate the application of RAG on unclassified DoD policy documents?",
        "f47c84d7-fdb6-4998-b8bb-56e763b3fa50": "What are the two main methods for taking in an input task for each agent?",
        "84460b9e-67ce-49a7-aec5-93cd5e77daa2": "How does Counselor Copilot utilize contact data stored in complex PDFs to enhance counselor interactions?",
        "4c672820-584d-42e2-a920-4f4bbe79b4a1": "What are the key components of the `llama-agents` framework and how do they interact to enable the creation of multi-agent AI systems?",
        "f5891cb4-4d07-4f50-9c7f-9a4e7b1ba4f9": "How does the `DatasetGenerator` help in selecting the appropriate `chunk_size` for document processing?",
        "2623b299-ca37-4503-b3ec-b398f218feb7": "How does metadata play a crucial role in managing multi-tenancy within a document indexing system?",
        "2dd4c96d-b5dd-4da7-8438-45544e965384": "How could Uber's reliance on third parties for services like cloud computing, payment processing, and background checks negatively impact its offerings, growth, and profitability?",
        "5404c6e4-b8b5-40e4-84d0-abdbb27230fb": "Which model in the 7B series has the highest performance on the SAT-en (w/o Ps_8) task?",
        "f7014d5f-ef03-4859-b5c1-0efeb6d1c634": "What were the criteria for the \"Best Knowledge-Intensive LLM App\" prize at the Berkeley Hackathon?",
        "9f6a1be6-71af-4595-a84d-f48c618a22e4": "What is the cost per hour of using the Hugging Face endpoint with an AWS Nvidia A100G GPU?",
        "492025f4-129c-4a70-a62f-c31fcd789222": "How does Prosper AI aim to bridge the resource gap in financial expertise?",
        "54929c0f-730e-4d0d-8a54-f7cb08cea19c": "What are the different types of node parsers available in the provided code snippet?",
        "6032bbb7-e924-4f0a-8f0f-9dbde05a657c": "What are the key technologies used in the project and how do they contribute to the multimodal experience?",
        "be95270f-aa5d-4502-8995-b32810d6a39a": "What are the key differences between ReAct agents and routing agents in terms of their interaction behavior and complexity?",
        "83092fcc-e8d6-485d-b072-58e11e952cbe": "What are the key differences between notebook development and full-stack app development within the LlamaIndex ecosystem?",
        "5a0eec1e-14ed-4709-8d5c-41880c6a0099": "What are the key challenges in ensuring the performance and stability of Retrieval Augmented Generation (RAG) systems in production environments?",
        "ec5ca094-a893-4ad1-9a62-e435e2fec270": "How does the gmail-extractor project leverage LLMs to extract structured data from Gmail?",
        "28761c53-3a70-42fa-b201-37b7191fde67": "What is the purpose of the \"Event Subscriptions\" link in the Slack app configuration?",
        "5cd58ebb-2ecf-427b-a018-7752adb66dc4": "How does the `MultiModalVectorStoreIndex` facilitate the retrieval of both text and image data during a query?",
        "1b24a4ff-9575-4194-9d56-ca2516ef61d9": "What are the limitations of the public preview mode of LlamaParse?",
        "6f6ad0c2-dca9-4903-81c8-4307660c2ba5": "What is the purpose of the new `IngestionPipeline` concept introduced in LlamaIndex v0.9?",
        "8f6d5b8e-c1df-4d78-957f-545df2fb6977": "What are the key topics discussed in the webinar on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph)?",
        "b953daec-6af2-4043-bd54-3cff9627d97b": "How does LlamaIndex facilitate the integration of audio data with AssemblyAI?",
        "82008236-a527-4d06-b66b-42b269beb3ed": "How does the `quantization_config` parameter affect the performance of the model in the provided code snippet?",
        "18c93aff-6132-4c95-a39f-e40f863861ef": "What are the top 10 item categories sold during Christmas time across all years?",
        "145b73df-548c-467d-977b-abb6603ba72b": "What is the purpose of the \"Event Subscriptions\" link in the Slack app setup process?",
        "339468e1-66fd-405c-8d0d-f101f8214200": "How does the provided code snippet connect to a MongoDB index and utilize it for vector search?",
        "969711c8-ebc7-4776-a524-e6db3ee57c28": "What are the potential implications of the observed differences in violation percentages across different digital platforms?",
        "b12ad8e8-a756-4855-8148-638456a2f993": "What are the benefits of using RabbitMQ with llama-agents for multi-agent communication?",
        "8d24ac32-6e3e-48b5-9590-0e0cacf9a260": "What is the purpose of fine-tuning embedding models in the context of Retrieval Augmented Generation (RAG) systems?",
        "9da4835d-2e55-44d9-ab13-037636f3d547": "What are the two main changes made to the system, and how do they address the issue of laboriousness and inconsistent model usage?",
        "b4198b63-b6a7-41cf-a5ed-0e4032412b1c": "What is the purpose of using Qdrant in the provided code snippet?",
        "a6c57119-8311-4d1e-8f48-669374495710": "How does the \"Response + Source Nodes (Context)\" approach differ from simply evaluating the response against the source nodes?",
        "10845c44-67eb-4871-9fc8-783598b505c9": "What are the key differences in the Tool inputs and outputs between the two prompt variations provided?",
        "5924da84-d566-4382-8f1c-032db606e899": "What is the purpose of Prosper AI?",
        "a93e52b7-ba03-4fbf-8e5a-31ab72f21283": "What are the new features added to LlamaCloud?",
        "9969d2ed-de4e-4222-a4af-8518160ad329": "What is the name of the LlamaIndex blog post that celebrates their first anniversary?",
        "ab6e16fa-5f63-4c05-a82c-3cceda3c5389": "How can LlamaParse be used to extract diverse elements from complex research papers and build a RAG pipeline?",
        "a1badba4-25e6-48e4-b5f8-22f1ea7ca818": "What is a distinctive characteristic of Lisp that sets it apart from other programming languages?",
        "f1d78f34-d5c6-4e7a-8b84-56b83df1adea": "How does ColBERT integration improve document reranking in LlamaIndex, and what is the estimated speed improvement compared to BERT-based models?"
    },
    "corpus": {
        "d6b72243-ae05-482e-a9f7-d2baa5d6e6eb": "Hello Llama Fam \ud83e\udd99\n\nWhat an amazing week we\u2019ve had! We\u2019re excited to share that, according to the [Retool State of AI 2023 survey](https://retool.com/reports/state-of-ai-2023), 1 in 12 respondents are now using LlamaIndex. We\u2019re grateful for all your support.\n\nIf you have a fascinating project or video you\u2019d like to share, we\u2019d love to see it! Feel free to send it to us at [news@llamaindex.ai](mailto:news@llamaindex.ai). And remember to subscribe to our newsletter on our [website](https://www.llamaindex.ai/) to stay in the loop. We can\u2019t wait to connect with you there!\n\n\ud83e\udd29 **First, the highlights:**\n\n1. **LlamaIndex 0\\.9 Release:** we introduced LlamaIndex version 0\\.9 featuring streamlined data handling with a new IngestionPipeline, automated caching, improved text processing interfaces, tokenizer updates, PyPi packaging enhancements, consistent import paths, and a beta version of MultiModal RAG Modules. [Blog post](/announcing-llamaindex-0-9-719f03282945), [Tweet](https://twitter.com/llama_index/status/1724836383259582548?s=20).\n2. **Multi\\-Modal Evaluation Tools:** we launched multi\\-modal evaluation with the introduction of MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, plus a guide for their application in multi\\-modal settings.",
        "4c5c5e65-f38c-4f1d-bf60-88cf9b07f91a": "*This is a guest post from our friends at Mozilla about [Llamafile](https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-democratizing-ai/)*\n\n[llamafile](https://github.com/Mozilla-Ocho/llamafile), an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from [HuggingFace](https://huggingface.co/models?sort=trending&search=llamafile) then run the file. That's it. **On most computers, you won't need to install anything.**\n\nThere are a few reasons why you might want to run an LLM on your laptop, including:\n\n1\\. Privacy: Running locally means you won't have to share your data with third parties.\n\n2\\. High availability: Run your LLM\\-based app without an internet connection.\n\n3\\. Bring your own model: You can easily test many different open\\-source LLMs (anything available on HuggingFace) and see which one works best for your task.\n\n4\\. Free debugging/testing: Local LLMs allow you to test many parts of an LLM\\-based system without paying for API calls.\n\nIn this blog post, we'll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we'll show how to use LlamaIndex with your llamafile as the LLM \\& embedding backend for a local RAG\\-based research assistant. You won't have to sign up for any cloud service or send your data to any third party\\-\\-everything will just run on your laptop.\n\nNote: You can also get all of the example code below as a Jupyter notebook from our [GitHub repo](https://github.com/Mozilla-Ocho/llamafile-llamaindex-examples).",
        "e5ec79af-b4f0-4b6e-a082-9caa6725a2e8": "The topic of Agentic RAG explores how agents can be incorporated into existing\n RAG pipelines for enhanced, conversational search and retrieval.\n\n\n\nIntroduction\n============\n\n\n\n**Considering the architecture below, it is evident how Agentic RAG creates\n an implementation which easily scales. New documents can be added with\n each new set being managed by a sub\\-agent.**\n\n\n\n\n![](/blog/images/1*-rJXCN31NIyVQYrQPaqNjA.png)\n\n\n The basic structure of LlamaIndex\u2019s approach called Agentic RAG is shown in\n the diagram below where a large set of documents are ingested, in this case it\n was limited to 100\\.\n\n\n\n\n The large corpus of data is broken up into smaller documents. An agent is\n created for each document, and each of the numerous document agents have the\n power of search via embeddings and to summarise the response.\n\n\n\n\n A top\\-level agent is created over the set of document agents. The meta\\-agent /\n top\\-level agent performs tool retrieval and then uses Chain\\-of\\-Thought to\n answer the user\u2019s question.\n\n\n\n\n The Rerank endpoint computes a relevance score for the query and each\n document, and returns a sorted list from the most to the least relevant\n document.\n\n\n\n\n![](/blog/images/1*MlPWgXmw7u-pqTERKqVAAA.png)\n\nNotebook Example\n================",
        "47648e02-d7bd-4db4-82e3-6cefe5937274": "* [Raghav](https://www.linkedin.com/in/raghav-dixit/) [tutorial](/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e) on MultiModal RAG for Advanced Video Processing with LlamaIndex \\& LanceDB.\n* [Tech With Tim](https://twitter.com/TechWithTimm) [tutorial](https://www.youtube.com/watch?v=ul0QsodYct4) on querying unstructured data, analyzing tabular data with Pandas, and actioning results in a concise, step\\-by\\-step approach.\n* [Florian June](https://medium.com/@florian_algo)\u2019s [tutorial](https://ai.plainenglish.io/advanced-rag-03-using-ragas-llamaindex-for-rag-evaluation-84756b82dca7) on using RAGAs and LlamaIndex for RAG evaluation.\n* [Davide Gallitelli](https://dgallitelli95.medium.com/)\u2019s [tutorial](https://dgallitelli95.medium.com/deploying-an-huggingface-embedding-model-to-amazon-sagemaker-and-consuming-it-with-llama-index-4d4f6dcd2fbc) on Deploying a HuggingFace embedding model to Amazon SageMaker and consuming it with Llama\\-Index.\n\n\ud83c\udfa5 **Webinar:**\n\n* [Webinar](https://www.youtube.com/watch?v=k5Txq5C_AWA) with Henry Heng, co\\-founder of Flowise on building advanced no\\-code RAG apps over your data.",
        "7de5c654-daa4-4138-8185-4af1b48baa85": "* [Hanane Dupouy](https://www.linkedin.com/in/hanane-d-algo-trader/)'s [tutorial](https://www.linkedin.com/posts/hanane-d-algo-trader_react-financial-agent-llamaindex-activity-7186333474256035840-jyQV/?utm_source=share&utm_medium=member_desktop) on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex's ReAct agent and API abstractions.\n* [Andy Singal](https://twitter.com/andysingal)'s [tutorial](https://ai.gopubby.com/enhancing-document-retrieval-with-memory-a-tutorial-for-llamaindex-with-colbert-based-agent-1c3c47461122) on Building a ColBERT\\-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions.\n* Mariboo\u2019s [tutorial](https://medium.com/@diagnosta/lora-fine-tuning-of-embedding-models-using-llamaindex-a60b823a2c94) on Fine\\-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetuning abstractions.",
        "28f358d8-5a94-42df-92c7-4024f409fcf9": "Example 1: rich table support\n\nSince we first released LlamaParse it has featured [industry\\-leading table extraction](https://github.com/run-llama/llama_parse/blob/main/examples/demo_advanced.ipynb) capabilities. Under the hood, this has been using LLM intelligence since the start. It seamlessly integrates with the advanced indexing/retrieval capabilities that the open\\-source framework offers, enabling users to build state\\-of\\-the\\-art document RAG. Now with JSON mode (see below) and parsing instructions, you can take this even further.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb5404df39ca9c68da96a69a72cb877ec6c22ab1a-6426x1688.png%3Ffit%3Dmax%26auto%3Dformat&w=3840&q=75)### Example 2: parsing comic books\n\nParsing translated manga presents a particular challenge for a parser since a regular parser interprets the panels as cells in a table, and the reading order is right\\-to\\-left even though the book is in English, as shown in this extract from \"The manga guide to calculus\", by Hiroyuki Kojima:\n\n!",
        "cf8cfb96-c6d0-4ac8-a5aa-ce8366465794": "```\nentities = Literal[\"PERSON\", \"LOCATION\", \"ORGANIZATION\", \"PRODUCT\", \"EVENT\"]\nrelations = Literal[\n    \"SUPPLIER_OF\",\n    \"COMPETITOR\",\n    \"PARTNERSHIP\",\n    \"ACQUISITION\",\n    \"WORKS_AT\",\n    \"SUBSIDIARY\",\n    \"BOARD_MEMBER\",\n    \"CEO\",\n    \"PROVIDES\",\n    \"HAS_EVENT\",\n    \"IN_LOCATION\",\n]\n```\nAs you can see, we are focusing our graph extraction around people and organizations. Next, we will specify the relationships associated with each node label.\n\n\n```\n# define which entities can have which relations\nvalidation_schema = {\n    \"Person\": [\"WORKS_AT\", \"BOARD_MEMBER\", \"CEO\", \"HAS_EVENT\"],\n    \"Organization\": [\n        \"SUPPLIER_OF\",\n        \"COMPETITOR\",\n        \"PARTNERSHIP\",\n        \"ACQUISITION\",\n        \"WORKS_AT\",\n        \"SUBSIDIARY\",\n        \"BOARD_MEMBER\",\n        \"CEO\",\n        \"PROVIDES\",\n        \"HAS_EVENT\",\n        \"IN_LOCATION\",\n    ],\n    \"Product\": [\"PROVIDES\"],\n    \"Event\": [\"HAS_EVENT\", \"IN_LOCATION\"],\n    \"Location\": [\"HAPPENED_AT\", \"IN_LOCATION\"],\n}\n```\nFor example, a person can have the following relationships:\n\n* WORKS\\_AT\n* BOARD\\_MEMBER\n* CEO\n* HAS\\_EVENT\n\nThe schema is quite specific except for the EVENT node label, which is slightly more ambiguous and allows the LLM to capture various types of information.\n\nNow that we have defined the graph schema, we can input it into the `SchemaLLMPathExtractor` and use it to construct a graph.",
        "f349384f-8dd5-45ee-a804-010ad95dff70": "3. **Financial Data Parsing:** Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb), [Tweet](https://x.com/llama_index/status/1768303288381030408?s=20).\n\n**\u2728 Feature Releases and Enhancements:**\n\n* We introduced LlamaIndex v0\\.10\\.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module\\-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application. [Docs](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html), [Tweet](https://x.com/llama_index/status/1768730443921396220?s=20).\n* We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline. [Blogpost](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform), [Tweet](https://x.com/llama_index/status/1768443551267049492?s=20).\n* We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question\\-answering over complex financial data.",
        "0d5c8b48-bb7e-4787-8c4d-ce597fc1af57": "**What is the origin story of LlamaIndex?**\n\nThe origin story of LlamaIndex is that it was founded in November by Jerry, who was trying to build a sales bot. He was playing around with GPT\\-3 and wanted to use it on his internal company data. He wanted to use it to synthesize a to do list for him for the next customer meeting, as he had to spend 20\u201330 minutes reviewing notes from the previous call transcripts. This led to the idea of stuffing data from Notions, Slack, Salesforce, data lakes, vector databases, and structure databases into language models. This was the impetus for LlamaIndex, which is focused on connecting data to language models and tapping into the capabilities of language models to utilize them on top of private sources of data.\n\n**What is LlamaIndex doing beyond top\\-k retrieval?**\n\nLlamaIndex is offering more advanced primitives on top of basic top\\-k retrieval in order to provide responses to more complicated questions. These primitives include decision\\-making at the top level to route queries to the right sub\\-data structure, synthesizing information from multiple data systems, and providing trade\\-offs between different approaches.\n\nAdditionally, LlamaIndex is working on building tooling to help users create customizable indexes and views of their data to allow them to execute different types of queries. This includes connecting to existing data systems, defining metadata on top of each unit of data, providing the building blocks to create different types of indexes, and abstracting away complexity with an outer agent layer that can automatically reason which structure to use given a query request.",
        "9f4a97a8-31b3-40e7-8ffe-3886751271d7": "response_1 = openai_mm_llm.complete(\n    prompt=qa_tmpl_str.format(\n        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n    ),\n    image_documents=image_documents,\n)\n\npprint(response_1.text)\n```\n\n> *The generated response captures the context pretty well and structures the answer correctly :*\n> \n> The video \u201cA pretty reason why Gaussian \\+ Gaussian \\= Gaussian\u201d by 3Blue1Brown delves into the Gaussian function or normal distribution, highlighting several critical aspects:\n> \n> **Central Limit Theorem:** It starts with the central limit theorem, illustrating how the sum of multiple random variable copies tends toward a normal distribution, improving with more variables.\n> \n> **Convolution of Random Variables:** Explains the addition of two random variables as their distributions\u2019 convolution, focusing on visualizing this through diagonal slices.\n> \n> **Gaussian Function:** Details the Gaussian function, emphasizing the normalization factor for a valid probability distribution, and describes the distribution\u2019s spread and center with standard deviation (\u03c3) and mean (\u03bc).\n> \n> **Convolution of Two Gaussians:** Discusses adding two normally distributed variables, equivalent to convolving two Gaussian functions, and visualizes this using the graph\u2019s rotational symmetry.\n> \n> **Rotational Symmetry and Slices:** Shows the rotational symmetry of e^(\\-x\u00b2) \\* e^(\\-y\u00b2) around the origin, a unique Gaussian function property. It explains computing the area under diagonal slices, equating to the functions\u2019 convolution.",
        "b465c492-d1bc-4b00-a0ce-7b04e7a3bc3a": "RAG is only as Good as your Data\n--------------------------------\n\nBuilding production\\-ready LLM applications is hard. We've been chatting with hundreds of users, ranging from Fortune 500 enterprises to pre\\-seed startups and here's what they tell us they struggle with:\n\n* **Data Quality Issues**: Most companies deal with large sets of complex, heterogeneous documents. Think PDFs with messy formatting, images, tables across multiple pages, different languages \\- the list goes on. Ensuring high\\-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications.\n* **Scalability Hurdles**: Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either.\n* **Accuracy Concerns**: Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs.\n* **Configuration Overload:** Fine\\-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task.\n\nAs developers shift from prototypes towards building production applications \\- complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data.\n\nTo address these difficulties, we soft\\-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production\\-grade context\\-augmentation to your LLM and RAG applications. LlamaParse can already support 50\\+ languages and 100\\+ document formats. The adoption has been incredible \\- we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here\u2019s an example from Dean Barr, Applied AI Lead at Carlyle:",
        "ff9b1746-47e4-4cf2-8bbb-28e99649e42c": "```\nsql_query_engine = sql_index.as_query_engine()\n```\nBoth the SQL query engine and vector query engine can be wrapped as `QueryEngineTool` objects.\n\n\n```\nsql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        'Useful for translating a natural language query into a SQL query over a table containing: '\n        'city_stats, containing the population/country of each city'\n    )\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=query_engine,\n    description=f'Useful for answering semantic questions about different cities',\n)\n```\nFinally, we can define our `SQLAutoVectorQueryEngine`\n\n\n```\nquery_engine = SQLAutoVectorQueryEngine(\n    sql_tool,\n    vector_tool,\n    service_context=service_context\n)\n```\nResults\n-------\n\nWe run some example queries.\n\n**Query 1**\n\n\n```\nquery_engine.query(\n  'Tell me about the arts and culture of the city with the highest population'\n)\n```\nIntermediate steps:\n\n![](/blog/images/1*T4W6P-jNV5FnT-O83KlGAg.png)Final Response:",
        "78e7c79b-b763-40f9-8abf-41c7f8ac9533": "Evaluating the `NetworkRetriever`\n\nTo evaluate the efficacy of the `NetworkRetriever` we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank.\n\n* **hit rate:** a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set.\n* **mean reciprocal rank:** similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0\\. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set.\n\nIn addition to evaluating the `NetworkRetriever` we consider the two baselines that represent Retrieving only over the individual Contributor\u2019s synthetic datasets.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F26b87d159b170a7fa1c6fa8727f7c94b83752f0f-2212x1213.png%3Ffit%3Dmax%26auto%3Dformat&w=3840&q=75)\n\nRetriever evaluations, with sigma equal to 1\\.5\\.\n\nIn the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever\u2019s in the test set. This shouldn\u2019t be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor\u2019s synthetic observations\u2014this is the point after all of a network!\n\nAnother important observation can be made upon inspection of these results.",
        "334ee959-8526-46fa-a9d5-a18eff8dc70f": "[Blogpost](https://devpost.com/software/home-ai), [Code](https://github.com/2sunflower33/homeai), [Tweet](https://x.com/llama_index/status/1767289805719978288?s=20).\n\n**\ud83d\uddfa\ufe0f Guides:**\n\n* [Guide](https://github.com/tensorsense/Retrieval-Framework/blob/main/hierarchical_retrieval.ipynb) to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique.\n\n**\u270d\ufe0f Tutorials:**\n\n* [Thomas Reid](https://twitter.com/taupirho)\u2019s [tutorial](https://ai.gopubby.com/llamaparse-rag-beats-all-comers-60948c6cc0e4) on using LlamaParse can help properly extract text from a Tesla quarterly filings.\n* [Sudarshan Koirala](https://twitter.com/mesudarshan) [video tutorial](https://www.youtube.com/watch?v=w7Ap6gZFXl0) on RAG with LlamaParse, Qdrant, and Groq.\n* Kyosuke Morita [tutorial](https://pub.towardsai.net/rag-based-job-search-assistant-98dd72c98fbd) showing how to match a candidate to jobs based on their CV with LlamaParse \\+ LlamaIndex.",
        "dda19e05-78f1-4b7c-9fa3-35bed5ae004e": "3. Evaluate if the question (query), answer, and source nodes are matching/inline\n\n![](/blog/images/1*wzCS9ysf0Up1C5KwipzwiQ.png)**1\\. Question Generation**\n===========================\n\nIt should be noted that this approach does not require ground\\-truth labels. The purpose of question generation is to generate an initial dataset of inputs over context that can be used to evaluate the question\\-answering system.\n\nLlamaIndex offers the DataGenerator class, which generates questions from a given document using ListIndex. By default, it uses OpenAI ChatGPT (get\\-3\\.5\\-turbo) for question generation.\n\n\n```\nfrom llama_index.evaluation import DatasetGenerator\nfrom llama_index import SimpleDirectoryReader\n\n# Load documents\nreader = SimpleDirectoryReader(\"./data\")\ndocuments = reader.load_data()\n\n# Generate Question\ndata_generator = DatasetGenerator.from_documents(documents)\nquestion = data_generator.generate_questions_from_nodes()\n```\n![](/blog/images/1*JRRi91-cPqV_31ui4cWKvg.png)**2\\. Generate Answers/Source Nodes (Context)**\n===============================================\n\nUsing List Index, we generate answers and source nodes for the generated questions in the response object.",
        "5d560f43-2c76-4fac-9024-cb97446fba4c": "Long\\-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages.\n2. **Developers will need to spend less time tuning retrieval and chain\\-of\\-thought over single documents**. An issue with small\\-chunk top\\-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain\\-of\\-thought agent to do two retrievals against a weak retriever; instead, they can just one\\-shot prompt the LLM to obtain the answer.\n3. **Summarization will be easier.** This is related to the above statement. A lot of summarization strategies over big documents involve \u201chacks\u201d such as sequential refinement or hierarchical summarization (see our [response synthesis modules](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html) as a reference guide). This can now be alleviated with a single LLM call.\n4. **Personalized memory will be better and easier to build:** A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents \\- if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M\\-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction).\n\nThere are, however, some lingering challenges:\n\n1. **10M tokens is not enough for large document corpuses \\- kilodoc retrieval is still a challenge.** 1M tokens is around \\~7 Uber SEC 10K filings.",
        "bbf78d7f-e9d7-4466-a9f2-f2bd2e909ec3": "**LlamaIndex Re\\-Ranking For Better Generation Recipe** ([notebook guide](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html))**:**\n\n\n```\nimport os\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.postprocessor import LongLLMLinguaPostprocessor\n\n### Recipe\n### Define a Postprocessor object, here CohereRerank\n### Build QueryEngine that uses this Postprocessor on retrieved docs\n\n# Build CohereRerank post retrieval processor\napi_key = os.environ[\"COHERE_API_KEY\"]\ncohere_rerank = CohereRerank(api_key=api_key, top_n=2)\n\n# Build QueryEngine (RAG) using the post processor\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=[cohere_rerank],\n)\n\n# Use your advanced RAG\nresponse = query_engine.query(\n    \"What did Sam Altman do in this essay?\"\n)\n```\nAdvanced techniques for simultaneously addressing Retrieval and Generation success requirements\n===============================================================================================\n\nIn this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries).\n\n1. **Generator\\-Enhanced Retrieval:** These techniques make use of the LLM\u2019s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response.\n\n**LlamaIndex Generator\\-Enhanced Retrieval Recipe** ([notebook guide](https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html))**:**",
        "73550e84-1add-47fa-bd66-f36cde76c2d2": "```\ndef test_llama_index():\n    questions, reference_answers = get_q_and_a()\n    llm_answers, context_lists = get_responses(questions)\n    run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)\n    # Upload results to web ui\n    validate_api = ValidateApi()\n    # Get project id from env\n    project_id = os.getenv(\"PROJECT_ID\")\n    validate_api.upload_run(project_id, run)\n```\nThis runs all the code we\u2019ve written to get the scores and then sends them to Tonic Validate\u2019s API to visualize in the UI. In order to send the metrics for each run to the UI, you need to sign up for a free account, which you can do [here](https://validate.tonic.ai/). I highly recommend utilizing the UI to make visualizing and monitoring performance changes a breeze. Once you sign up, you will be taken through a short onboarding process where you create an API key and a project. The API key should be stored in an environment variable called `TONIC_VALIDATE_API_KEY` and the project ID in an environment variable called `PROJECT_ID`.\n\nOnce you have set up your account and configured your environment variables, you can run the test via the following commands:\n\n\n```\npoetry shell\npytest\n```\nYou can also make the test fail if the metrics score too low. This would be a pertinent step to add in if you want to avoid introducing breaking changes to a production RAG system; for example, if you update the model version and the answer similarity score suddenly drop below a certain threshold, you could have the test fail and issue a warning to debug the issue.",
        "26675a9e-43a5-47d5-af4c-b9dcf5c8ad87": "If you\u2019re building LLM apps in the enterprise setting, we\u2019d love to chat and learn more about pain points \\+ desired features! Check out our [form here](https://docs.google.com/forms/d/1lzXIE9G07D9eoK7MBUpRuN-PdBYGA7nWVqcIvNtN9mQ/edit#responses).\n\n**Join the Llama Gang! \ud83e\udd99**\n==========================\n\nJoin the Llama(Index) gang as we embark on this journey to solve problems at the intersection of LLMs and data. We are not just building tools for ML practitioners/researchers; the emerging LLM \\+ data architecture stacks have implications for *all* of software development. As a result, we are operating at the intersection of incredibly fun and challenging problems from a variety of different fields:\n\n* Foundation Model Development\n* Information Retrieval \\+ Recommendation Systems\n* Data Systems\n* MLOps\n* DevOps\n\nInterested in checking out the project?\n\n* Find our project on [Github](https://github.com/jerryjliu/llama_index) and check out our [Docs](https://gpt-index.readthedocs.io/en/latest/)\n* Check out our brand new landing page: <https://llamaindex.ai>\n* Join our [Discord](https://discord.gg/dGcwcsnxhU) or Follow our [Twitter](https://twitter.com/llama_index)\n\nAlso, we\u2019re hiring!\n\n* We\u2019re looking for founding engineers \u2014 experience in one or more of AI, data systems, and full\\-stack/front\\-end is nice to have but not a requirement.",
        "e76de264-33d6-40aa-8d30-d42f885fb265": "Hello LlamaIndex Adventurers \ud83e\udd99,\n\nWelcome to another thrilling week at LlamaIndex! It\u2019s brimming with community contributions and a wealth of educational content that will take your LlamaIndex experience to new heights. Dive into our latest features, comprehensive tutorials, insightful guides, and interactive demos, all designed to supercharge your journey with LlamaIndex.\n\nBut first, let\u2019s ignite your excitement with a reminder about our upcoming [first\\-ever in\\-person hackathon](https://rag-a-thon.devpost.com/), happening February 2nd\\-4th. Don\u2019t miss this incredible chance to mingle with fellow RAG aficionados, collaborate on exciting projects, and vie for a share of over $16,000 in prizes!\n\nYour creations inspire us! Whether it\u2019s a project, article, or video that you\u2019re proud of, we\u2019d love to see it. Share your brilliance with us at [news@llamaindex.ai](mailto:news@llamaindex.ai). And for those who haven\u2019t yet, make sure to subscribe to our newsletter on our [website](https://www.llamaindex.ai/) \u2014 it\u2019s your gateway to all the latest and greatest from LlamaIndex, delivered directly to your inbox.\n\n\ud83e\udd29 **The highlights:**\n\n1. **RAG CLI**: Easy\\-to\\-use tool for local file indexing and search, with advanced integration and customization features. [Docs](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html), [Tweet](https://x.com/llama_index/status/1750950516925079777?s=20).\n2. **JSONalyze**: Efficiently summarizes large JSON datasets, transforming them into SQLite for detailed SQL queries.",
        "9836f5a6-c1a0-4a2d-8a6b-5b8f6f78f1be": "\"You always answer in great detail and are polite. Your responses always descriptive. \"\n            \"Your job is to talk about an image the user has uploaded. Image description: {img_desc}.\"\n        ),\n        verbose=True,\n        memory=chatmemory\n    )\n    return chat_engine\n```\nThe `create_chat_engine` function builds the infrastructure for our app's conversation capabilities. It starts by instantiating a PaLM object with the provided API key, setting up the service context, and creating a document with the image description. This document is then indexed to prepare it for Llamaindex\u2019s context chat engine. Finally, the chat engine is configured with a prompt that instructs the AI on how to engage in the conversation, referencing the image description and defining the chatbot's behavior.\n\n**5\\. User Interaction and Message Handling**\n\nThe application ensures an engaging and controlled user experience by limiting the number of messages to 20 per session in the demo version. If this limit is reached, it gracefully notifies the user and disables further input to manage resources effectively.",
        "daf094d9-74a4-4a49-806c-beeec5238112": "11.2 This release contains bug fixes since the 2.11.1 release\u2026\n```\nSuccess! Notice how only vectors with timestamps within the specified start and end date ranges of 1 August, 2023, and 30 August, 2023, are included in the results.\n\nHere\u2019s some intuition for why Timescale Vector\u2019s time\\-based partitioning speeds up ANN queries with time\\-based filters.\n\nTimescale Vector partitions the data by time and creates ANN indexes on each partition individually. Then, during search, we perform a three\\-step process:\n\n* Step 1: filter our partitions that don\u2019t match the time predicate.\n* Step 2: perform the similarity search on all matching partitions.\n* Step 3: combine all the results from each partition in step 2, rerank, and filter out results by time.\n\nTimescale Vector leverages [TimescaleDB\u2019s hypertables](https://docs.timescale.com/use-timescale/latest/hypertables/about-hypertables/), which automatically partition vectors and associated metadata by a timestamp. This enables efficient querying on vectors by both similarity to a query vector and time, as partitions not in the time window of the query are ignored, making the search a lot more efficient by filtering out whole swaths of data in one go.\n\nWhen performing a vector similarity search on \\`TimescaleVectorStore\\`, rather than specifying the start and end dates for our search, we can also specify a time filter with a provided start date and time delta later:",
        "1d6a476c-34ee-482c-8b66-0a6fedd7cd6e": "Our custom retriever will first identify entities in the input query and then execute the VectorContextRetriever for each identified entity separately.\n\nFirst, we will define the entity extraction model and prompt.\n\n\n```\nfrom pydantic import BaseModel\nfrom typing import Optional, List\n\n\nclass Entities(BaseModel):\n    \"\"\"List of named entities in the text such as names of people, organizations, concepts, and locations\"\"\"\n    names: Optional[List[str]]\n\n\nprompt_template_entities = \"\"\"\nExtract all named entities such as names of people, organizations, concepts, and locations\nfrom the following text:\n{text}\n\"\"\"\n```\nNow we can progress to the custom retriever implementation.\n\n\n```\nfrom typing import Any, Optional\n\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.core.retrievers import CustomPGRetriever, VectorContextRetriever\nfrom llama_index.core.vector_stores.types import VectorStore\nfrom llama_index.program.openai import OpenAIPydanticProgram\n\n\nclass MyCustomRetriever(CustomPGRetriever):\n    \"\"\"Custom retriever with entity detection.\"\"\"\n    def init(\n        self,\n        ## vector context retriever params\n        embed_model: Optional[BaseEmbedding] = None,\n        vector_store: Optional[VectorStore] = None,\n        similarity_top_k: int = 4,\n        path_depth: int = 1,\n        include_text: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Uses any kwargs passed in from class constructor.\"\"\"",
        "670dca29-b705-45d6-854a-19dfa267ffd8": "```\nresp = llm.complete(\n    question=\"Summarise this in one sentence.\",\n    model=\"mixtral8x7\", \n    max_tokens=20,  # default is 10\n)\nprint(resp)\nasync def main():\n    aresp = await llm.acomplete(\n        question=\"Summarize this in one sentence.\",\n        model=\"llama7b\",\n        max_tokens=20,\n    )\n    print(aresp)\n\nasyncio.run(main())\n```\nThis dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI\u2019s backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data).\n\nCurrently, we do not support chat or achat methods as our API is not designed for real\\-time interactive experience. However, we are planning to add those methods in the future that will function in a \u201cbatch way\u201d. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once.",
        "b9641ce0-da8e-4477-8ddb-bfa82c9483a6": "```\n# try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\n```\nSetting up a DAG for an Advanced RAG Workflow\n---------------------------------------------\n\nGenerally setting up a query workflow will require using our lower\\-level functions to build a DAG.\n\nFor instance, to build an \u201cadvanced RAG\u201d consisting of query rewriting/retrieval/reranking/synthesis, you\u2019d do something like the following.",
        "e92059a1-2fac-419d-97da-4b7974b45eaf": "> NOTE: This is a guide for fine\\-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. We suggest experimenting with your dataset and various parameters before deciding to incorporate it into your RAG pipeline.\n\nSetting Up the Environment\n==========================\n\n\n```\n!pip install llama-index cohere pypdf\n```\nSetting Up the Keys\n===================\n\n\n```\nopenai_api_key = 'YOUR OPENAI API KEY'\ncohere_api_key = 'YOUR COHEREAI API KEY'\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\nos.environ[\"COHERE_API_KEY\"] = cohere_api_key\n```\nDownload the Data\n=================\n\nWe will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC Filings for evaluation.\n\n\n```\n!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\n```\nLoad the Data\n=============",
        "fa4ab7e9-21fc-4990-8801-68ebd6f0073b": "This allows users to get the best results for a query, while also providing an alternative to something like a langchain or using it as part of building a broader solution.\n\n**\\[Sam] It sounds like we\u2019re starting to identify a higher level of abstraction that different use cases will fall under. Is it more the case that there\u2019s some manageable number of these primitives, like 10, 20, or is it that every use case is going to be a little bit different, and there are hundreds of thousands of kind of fundamental ways that people want to work with their documents, and so you need to just give them a very open capability?**\n\nJerry\u2019s response is that there are probably a few different use cases that people tend to want to get answers from over their data, and it is possible there is a giant long tail of different tasks. He believes that the complexity of the task scales with the number of steps it requires to execute, and that users need to be given customizable building blocks in order to get the results they want. He also believes that the next natural step is to automate the process and unify everything under a single query interface, so that users don\u2019t have to specify different parameters for every use case.\n\nHe also believes that this paradigm is displacing more static paradigms like ETL, and that it is applicable to a wide range of applications. He sees this agent type environment becoming fundamental infrastructure that reimagines the entire existing enterprise data stack, and that it can be used to parse unstructured data into structured data, as well as to automatically reason how to best transform data from one place to another.",
        "2efada7e-1d9e-4a86-8c06-ddd7390d28be": "He also believes that this will make the job of the data engineer and data scientist much more efficient, and that it will enable the creation of natural language query interfaces that have a view of the data within the data system.\n\n**\\[Sam] When you think about the interface between LLM\\-based data processing system and the data sources of record, what does that interface evolve to look like? For example, does it evolve to look like the chat GPT plugin model, where we\u2019re going to teach our Oracle databases how to spit out natural language prompt responses, that kind of thing, or do you think that there\u2019s some more efficient way of doing that or is that more efficient? Like, what\u2019s your view of the way these things evolve?**\n\nI think the way this interface will evolve is that it will become more automated and efficient. We will be able to use language models to understand raw text and extract the relevant information from it, without having to manually enter data into a structured format. We will also be able to use agents to automate decision making and provide a unified query interface, so that users don\u2019t have to specify different parameters for every use case.\n\nAdditionally, we can use LlamaIndex to structure data in a way that allows us to make use of the limited prompt size of GPT\\-3, while still being able to achieve the task. We can also use this data stack to infer the right schemas and further write structured data from unstructured data, as well as automatically build a natural language query interface that has a view of the data within the data system.",
        "b5654a79-ebbf-4578-8fce-15dd5ca65edf": "Final Observations:\n===================\n\nObservations made based on experiments on Hallucination and correctness. (Please note that these observations are specific to the images used and cannot be generalized, as they vary depending on the images.)\n\n![](/blog/images/1*sMCkM4DYmFC9S_EkATNM4Q.png)Summary\n=======\n\nIn this blog post, we have showcased experiments ranging from general inquiries to systematic questions and chain of thought prompting techniques and observed Hallucination and correctness metrics.\n\nHowever, it should be noted that the outputs from GPT\\-4V can be somewhat inconsistent, and the levels of hallucination are slightly elevated. Therefore, repeating the same experiment could result in different answers, particularly with generalized questions.",
        "67087298-e5c4-4322-b864-82a968e35647": "Greetings, LlamaIndex community!\n\nWe\u2019re excited to introduce our new blog series, the LlamaIndex Update. Recognizing the fast pace of our open\\-source project, this series will serve as your continual guide, tracking the latest advancements in features, webinars, hackathons, and community events.\n\nOur goal is simple: to keep you updated, engaged, and inspired. Whether you\u2019re a long\\-time contributor or a new joiner, these updates will help you stay in sync with our progress.\n\nSo, let\u2019s explore the recent happenings in our premier edition of the LlamaIndex Update.\n\n**Features And Integrations:**\n------------------------------\n\n1. LLMs with Knowledge Graphs, supported by NebulaGraph. This new stack enables unique retrieval\\-augmented generation techniques. Our Knowledge Graph index introduces a GraphStore abstraction, complementing our existing data store types.\n[Docs](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html), [Tweet](https://twitter.com/jerryjliu0/status/1667196231863656448)\n2. Better LLM app UX supports in\\-line citations of its sources, enhancing interpretability and traceability. Our new `CitationQueryEngine` enables these citations and ensures they correspond with retrieved documents. This feature marks a leap towards improving transparency in LlamaIndex applications.\n[Docs](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/citation_query_engine.html), [Tweet](https://twitter.com/jerryjliu0/status/1667563694472175616?s=20)\n3.",
        "c84c9775-b5b6-4b2c-a9fe-11b51c6219d1": "**Step 5: Lets read the** [**receipts**](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)\n\n\n\n\n```\nfrom llama_index.readers.file.base import DEFAULT_FILE_READER_CLS  \nfrom llama_index.readers.file.image_reader import ImageReader  \n  \nimage_parser =ImageReader(  \n    keep_image=True,  \n    parse_text=True  \n    )  \nfile_extractor = DEFAULT_FILE_READER_CLS  \nfile_extractor.update({  \n    \".jpg\": image_parser,  \n    \".png\": image_parser,  \n    \".jpeg\": image_parser,  \n    })  \n  \nreceipt_reader = SimpleDirectoryReader(  \n    input_dir=\"/content/data\",  \n    file_metadata=filename_fn,  \n    file_extractor=file_extractor,  \n)  \nreceipt_documents = receipt_reader.load_data()  \nprint(len(receipt_documents))  \n  \n#Output  \n3\n```\n\n\n```\nreceipts_index = VectorStoreIndex.from_documents(receipt_documents)  \n  \nfrom llama_index.query_engine import TransformQueryEngine  \nquery_engine = receipts_index.as_query_engine()  \n  \nreceipts_response = query_engine.query(  \n    \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\",  \n)  \n  \ndisplay_response(receipts_response)  \n  \n# Output   \nFinal Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.\n```\n\nConclusion\n----------",
        "ad2fdb22-ea03-4351-b38c-6fc904ee32cb": "3. **Structured Hierarchical RAG:** New RAG technique for optimized retrieval over multiple documents, ensuring precise, relevant responses. [Docs](https://docs.llamaindex.ai/en/latest/examples/query_engine/multi_doc_auto_retrieval/multi_doc_auto_retrieval.html), [Tweet](https://x.com/llama_index/status/1737515390664872040?s=20).\n4. **Custom Agents:** A simple abstraction for custom agent reasoning loops, enabling easy integration with RAG, SQL, and other systems, and enhancing response refinement for complex queries. [Docs](https://docs.llamaindex.ai/en/latest/examples/agent/custom_agent.html), [Tweet](https://x.com/llama_index/status/1741141394558001414?s=20).\n5. **New lower\\-level agent API:** For enhanced transparency, debuggability, and control, supporting step\\-wise execution and task modification. [Docs](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner.html), [Tweet](https://x.com/llama_index/status/1736809248947155076?s=20).\n\n**\u2728 Feature Releases and Enhancements:**\n\n* We have introduced a simple abstraction for building custom agent reasoning loops, surpassing prepackaged frameworks like ReAct. This tool allows for easy integration with RAG, SQL, or other systems, and we demonstrated how to build an agent with retry logic for routers, enhancing its ability to manage complex, multi\\-part questions and refine query responses.",
        "ca6c28ce-aa0d-4160-a0d6-17ccaa9a04bc": "All of our materials can be found in our Github repo: [https://github.com/run\\-llama/modal\\_finetune\\_sql](https://github.com/run-llama/modal_finetune_sql) (again emphasizing that this is adapted from [doppel\\-bot](https://github.com/modal-labs/doppel-bot)). Also, the full tutorial can be found in our [Jupyter notebook guide](https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb). Make sure to check it out!\n\nAs mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don\u2019t cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.\n\nThere are also certainly higher\\-level APIs that we could\u2019ve used (e.g. OpenAI, Lamini) in order to achieve this task. There\u2019s plenty of room for followup tutorials to cover these topics!\n\nStep 1: Loading Training Data for Finetuning LLaMa\n--------------------------------------------------\n\nThe first step here is to open up the [Jupyter notebook](https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb). The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.\n\nOur code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don\u2019t contain Python blocks of their own.",
        "7724721e-b199-4483-b4e9-a0b461f0fc97": "Greetings Llama Enthusiasts \ud83e\udd99!\n\nAnother week has zoomed past, and here we are with our latest roundup of updates, features, tutorials, and so much more. Have a noteworthy project, article, or video to share? We\u2019d love to feature it! Reach out to us at [news@llamaindex.ai](mailto:news@llamaindex.ai).\n\nWant these updates straight to your inbox? Simply subscribe to our newsletter on our [homepage](https://www.llamaindex.ai/).\n\n\ud83e\udd29 **First, the highlights:**\n\n1. **Revamped Documentation:** Overhauled [docs](https://docs.llamaindex.ai/en/stable/) for smoother LLM/RAG app development.\n2. **Contribution Board:** Our new [board](https://github.com/orgs/run-llama/projects/2) welcomes community\\-driven LlamaIndex enhancements.\n3. **Zephyr\\-7b\\-beta Insights:** [Tested and verified](https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing) for unmatched ReAct agent task efficiency on LlamaIndex.\n4. **Image Captioning Boost For RAG:** LLaVa\u2019s outputs are now supercharged with knowledge\\-based augmentation.",
        "23acaa41-358e-446c-9758-9c68b8856ff5": "![](/blog/images/1*kogMt0pPsSyHv_M0D4wBDw.png)You now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations!\n\nWhat next?\n==========\n\nThere\u2019s a whole bunch of features you could add to this bot, roughly in increasing order of difficulty:\n\n* Join every channel instead of just one, clearly!\n* Add a way to tell the bot to forget things (delete nodes)\n* Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar\n* Give the bot \u201ctags\u201d so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way\n* Add multi\\-modal abilities, so the bot can read images and even reply with generated images\n* And tons more!\n\nThis bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!",
        "4d29dc15-da2d-43f3-b5e5-d025432d530b": "Basic System Setup\n\nHere's a simple example of how to set up a basic multi\\-agent system using llama\\-agents. First we\u2019ll bring in our dependencies and set up our control plane, which contains our LLM\\-powered orchestrator\n\n\n```\nimport dotenv\ndotenv.load_dotenv() # our .env file defines OPENAI_API_KEY\nfrom llama_agents import (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    AgentOrchestrator,\n)\nfrom llama_index.core.agent import FunctionCallingAgentWorker\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nimport logging\n\n# turn on logging so we can see the system working\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n\n# Set up the message queue and control plane\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)\n```\nNext we create our tools using LlamaIndex\u2019s existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:",
        "ee2d50df-7561-415c-b0c4-144aa1b4d636": "Generate privacy\\-safe synthetic observations of Symptom2Disease\n\nWe use the training subset and apply the `DiffPrivateSimpleDatasetPack` on it in order to generate privacy\\-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a `LabelledSimpleDataset` object.\n\n\n```\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom llama_index.core.llama_dataset.simple import (\n    LabelledSimpleDataExample,\n    LabelledSimpleDataset,\n)\nfrom llama_index.core.llama_dataset.base import CreatedBy, CreatedByType\n\n# load the Symptom2Disease.csv file\ndf = pd.read_csv(\"Symptom2Disease.csv\")\ntrain, test = train_test_split(df, test_size=0.2)\n\n# create a LabelledSimpleDataset (which is what the pack works with)\nexamples = []\nfor index, row in df.iterrows():\n    example = LabelledSimpleDataExample(\n        reference_label=row[\"label\"],\n        text=row[\"text\"],\n        text_by=CreatedBy(type=CreatedByType.HUMAN),\n    )\n    examples.append(example)\n\nsimple_dataset = LabelledSimpleDataset(examples=examples)\n```\nNow we can use the llama\\-pack to create our synthetic observations.",
        "ff794620-3e03-4e31-8564-adbbedb81b3c": "**\u2728 Feature Releases and Enhancements:**\n\n1. We introduced RAFT \\- Retrieval Augmented Fine Tuning, a method from [Tianjun Zhang](https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D)\u00a0and [Shishir Patil](https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D)\u00a0to enhance domain\\-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation. [LlamaPack](https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-raft-dataset), [Tweet](https://x.com/llama_index/status/1772662480210198809?s=20).\n2. We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full\\-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook.",
        "31c6946b-66a2-437f-9e58-4cd4b88945ac": "**\n\n**\u2728 Feature Releases and Enhancements:**\n----------------------------------------\n\n1. We have introduced day 0 support for GPT\\-4o in both Python and TypeScript. Additionally, we've created demo notebooks ([demo1](https://colab.research.google.com/drive/1CTIs9C6HLqKLqU_PpG1NXaZaH0Ja5pzg#scrollTo=FB55THvgVWXs) and [demo2](https://colab.research.google.com/drive/1CTIs9C6HLqKLqU_PpG1NXaZaH0Ja5pzg#scrollTo=0ZTqYzIl2DU1)) to help you easily experiment with GPT\\-4o using LlamaIndex. [Tweet](https://x.com/llama_index/status/1790081409039872070).\n2. We have launched Llama3 cookbooks showcasing interesting use cases for Llama 3, from basic chat functionalities to advanced agent development. Ideal for anyone building with local models, either on your laptop or through an API. [Notebook,](https://github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/llamaindex_cookbook.ipynb) [Tweet](https://x.com/llama_index/status/1790047097024348444).\n\n**\ud83d\uddfa\ufe0f Guides:**\n--------------\n\n* [Guide](https://github.com/run-llama/ts-agents) to building agents in TypeScript: Dive into our comprehensive, open\\-source guide developed by Laurie that walks you through every step of agent development, from setting up with basic functions to integrating advanced features like local and remote LLMs, and data querying with vectorDB.",
        "18144f76-05c7-45af-a38b-a77f074eeeb7": "It\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe you\u2019ve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user\u2019s queries and data structures.\n\nIn either case, knowing where or how to begin may be a challenge in and of itself! If that\u2019s true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems.\n\nThe RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper ([\u201cRetrieval\\-Augmented Generation for Large Language Models: A Survey\u201d Gao, Yunfan, et al. 2023](https://arxiv.org/pdf/2312.10997.pdf)).\n\nBasic RAG\n=========\n\nMainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user\u2019s query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component.\n\n**LlamaIndex Basic RAG Recipe:**",
        "44f05295-133e-47f8-99d2-a118c9bf2e24": "5. **Human in the Loop with LlamaAgents:** Implementation includes HumanService for math queries and agent handling for other inquiries, managed via Gradio app and RabbitMQ. [Code](https://github.com/run-llama/llama-agents/tree/main/examples/human-in-the-loop).\n\n**\u2728 Feature Releases and Enhancements:**\n----------------------------------------\n\n1. We have released new features on LlamaCloud like LlamaCloud Chat for instant conversational data access, enhanced Teams functionality for collaboration, and expanded data integration with connectors for Notion, Slack, Jira, and improved SharePoint support. [Blogpost](https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud), [Tweet](https://x.com/llama_index/status/1814363518726222119).\n2. We integrated Claude Sonnet\\-3\\.5 with LlamaParse to enhance document parsing capabilities, offering advanced chart understanding and structured data extraction with improved validation and scalability. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/claude_parse.ipynb), [Tweet](https://x.com/llama_index/status/1813249175817232782).\n3. We have released a cookbook on Multimodal RAG for processing slide decks rich in text, diagrams, charts, and tables using LlamaParse, LlamaIndex, and GPT\\-4o, blending text and image data for comprehensive document analysis. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_slide_deck.ipynb), [Tweet](https://x.com/llama_index/status/1812963306032013586).\n4. We have implemented Human in the Loop with LlamaAgents in our new example that integrates a HumanService object for handling math queries and an agent for other queries, all managed through a Gradio app and RabbitMQ messaging. [Code](https://github.com/run-llama/llama-agents/tree/main/examples/human-in-the-loop).\n5.",
        "930862e5-44e9-40d2-b1fd-a76b2bbb4b48": "* LlamaIndex.TS (LITS) supports streaming on all endpoints. [Tweet](https://x.com/llama_index/status/1747746779058290800?s=20).\n* We announced a new integration with Tonic Validate to allow simple access to LLM\\-powered evaluations. [Blog post](https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems)\n\n\ud83c\udfa5 **Demo:**\n\n* **RAG\\-Maestro for ArXiv Research:** Developed by Aymen Kallala, this web app utilizes RAG to efficiently search scientific concepts in ArXiv papers. It extracts keywords using RAKE, queries ArXiv for relevant papers, and offers on\\-the\\-fly indexing with in\\-line citations \u2014 a valuable tool for ML researchers navigating through ArXiv\u2019s extensive library. [Demo](https://rag-maestro-o2wbip4gla-uc.a.run.app/), [GitHub Repo](https://github.com/AymenKallala/RAG_Maestro).\n\n**\ud83d\uddfa\ufe0f Guides:**\n\n* Guide to Advanced QA over Tabular Data which provides a comprehensive tutorial on creating sophisticated query pipelines over tabular data using Pandas or SQL, constructing a query DAG using our Query Pipelines, integrating few\\-shot examples, linked prompts, LLMs, custom functions, retrievers, and more. [Text\\-to\\-SQL](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_sql.html), [Text\\-to\\-Pandas](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas.html).",
        "8fceebc9-752f-4873-9b25-6ae251df3329": "[Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser](https://arxiv.org/abs/2105.00150)\n\n\n\n\n[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)",
        "bed7c1c0-bfee-4e7f-89fd-9be94eeddc2b": "We are actively working on contributing more tools to this repository, and we\u2019re also opening this up to community contributions. If you\u2019re interested in contributing a Tool to LlamaHub, please feel free to open a PR in this repo.\n\nUtility Tools\n-------------\n\nOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).\n\nTo tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in the core LlamaIndex repo \u2014 utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.\n\nLet\u2019s walk through our two main utility tools below.\n\n**OnDemandLoaderTool**\n\nThis tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con\\-demand\u201d. All three of these steps happen in a single tool call.\n\n![](/blog/images/1*gtJHjmMl9kVcjmbL2wx8Pw.png)Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad\\-hoc index to abstract away prompt window limitations for any API call.\n\nA usage example is given below:",
        "7d229610-a21a-4187-8591-b103acf2fb24": "```\nBART works well for text generation, comprehension tasks, abstractive dialogue, question answering, and summarization tasks.\n```\n\nKey Considerations:\n\n\n1. LLMSherpa leverages a cost\\-free and open API server. Your PDFs are not\n retained beyond temporary storage during the parsing process.\n2. LayoutPDFReader has undergone extensive testing with a diverse range of\n PDFs. However, achieving flawless parsing for every PDF remains a\n challenging task.\n3. Please note that OCR (Optical Character Recognition) functionality is\n presently unavailable. The tool exclusively supports PDFs equipped with a\n text layer.\n4. For inquiries regarding private hosting options, OCR support, or tailored\n assistance with particular PDF\\-related concerns, feel free to reach out to\n [contact@nlmatics.com](mailto:contact@nlmatics.com)\n or to\n [me](mailto: kiran@nlmatics.com)\n directly.\n\n\n\n If you have any questions, please leave them in the comments section, and I\n will try to respond ASAP.\n\n\n\n\n***Connect?***\n\n\n\n\n If you want to get in touch, feel free to shoot me a message on\n [LinkedIn](https://www.linkedin.com/in/kirannpanicker/) or via\n [email](mailto: kirankurup@gmail.com).\n\n\n\nReferences\n==========\n\n\n\n[https://github.com/nlmatics/llmsherpa](https://github.com/nlmatics/llmsherpa#layoutpdfreader)\n\n\n\n\n[Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser](https://arxiv.org/abs/2105.00150)",
        "23d0308c-8de2-4936-a0bf-5463ff1c6954": "As we continue to harness cutting\\-edge technology and data analytics, Prosper AI aims to democratize access to financial knowledge and tools that can empower individuals to unlock their financial potential.\n\nStay tuned as Prosper AI embarks on this exciting journey towards transforming the landscape of personal finance, making it more accessible, intelligent, and personalized for all.\n\nTogether with Prosper AI, let\u2019s cultivate the seeds of financial growth and harvest the fruits of prosperity.\n\nVideo/screenshots/links to material.\n------------------------------------\n\nLearn more and join our waitlist for a chance to win a $50 Amazon voucher:\n\n[Unleash The Power of Comparison\n-------------------------------",
        "e11a5645-cdf0-4cee-bbfb-b009223d21d0": "* [Tutorial](https://blog.streamlit.io/build-a-real-time-rag-chatbot-google-drive-sharepoint/) on building a real\\-time RAG chatbot using Google Drive and Sharepoint by\u00a0[Anup Surendran](https://blog.streamlit.io/author/anup/)\u00a0and\u00a0[Berke Can Rizai](https://blog.streamlit.io/author/berke/).\n* Step\\-by\\-step [tutorial](https://jina.ai/news/precise-rag-with-jina-reranker-and-llamaindex/) to improve the quality of your RAG application using JinaAI reranker, LlamaIndex, and MistralAI.\n\n\ud83c\udfa5\u00a0**Webinars:**\n\n* [Webinar](https://www.youtube.com/watch?v=Bhnq8grQm5Y) with [Parth Sarthi](https://twitter.com/parthsarthi03), lead author of RAPTOR \\- Tree\\-Structured Indexing and Retrieval.\n\n\ud83d\udcc5\u00a0Events:\n\n* We are hosting a RAG [meetup](https://www.meetup.com/paris-retrieval-augmented-generation-group/events/299374545/) in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open\\-source RAG in business.",
        "8bfac9a6-aa3a-422e-9243-d2536daea37f": "The models compared include GPT\\-4V, Fuyu\\-8B, MiniGPT4, CogVLM\\-4, and LLaVa\\-13B. Key findings reveal that all models incorrectly identified the number of reviews (correct answer: 5685\\), only GPT\\-4V and Fuyu accurately determined the price, each model\u2019s product description varied from the original, and Mini\\-GPT4 incorrectly assessed the product rating.\n\n**\u270d\ufe0f Tutorials:**\n\n* [Jo Kristian Bergum](https://www.linkedin.com/in/jo-bergum/) [blog post](https://blog.vespa.ai/scaling-personal-ai-assistants-with-streaming-mode/) on Hands\\-On RAG guide for personal data with Vespa and LLamaIndex.\n* [Wenqi Glantz](https://twitter.com/wenqi_glantz) made a [tutorial](https://levelup.gitconnected.com/llama-packs-the-low-code-solution-to-building-your-llm-apps-269eec05557b) on Llama Packs: The Low\\-Code Solution to Building Your LLM Apps.\n* [Liza Shulyayeva](https://twitter.com/Lazer)\u2019s in\\-depth [tutorial](https://www.daily.co/blog/search-your-video-content-library-with-llamaindex-and-chroma/) on building and deploying a retrieval\\-augmented generation (RAG) app to conversationally query the contents of your video library\n\n\ud83c\udfa5 **Webinars:**\n\n* [Webinar](https://www.youtube.com/watch?v=0zGHrcE-Zy4) on PrivateGPT \u2014 Production RAG with Local Models.",
        "fe81af90-14e7-4da6-b055-067babd1c63e": "This simply wasn\u2019t possible before with other approaches, and we\u2019re incredibly excited about this technology.\n\n\n\nLlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document.\n\n![](/blog/images/1*MKmgF62Blz45SCpzzCm58g.png)For the past few months we\u2019ve been obsessed with this problem. This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXiv papers to 10K filings to medical reports.\n\nNaive chunking and retrieval algorithms do terribly. We were the first to propose a [novel recursive retrieval RAG technique](https://docs.llamaindex.ai/en/stable/examples/query_engine/pdf_tables/recursive_retriever.html) for being able to hierarchically index and query over tables and text in a document. The only challenge that remained was how to properly parse out tables and text in the first place.",
        "9b107e00-0b77-4104-ba87-1ff273e13346": "On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.\n```\nThe agent used the `get_date` tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling `search`. It then loaded the documents using `retrieve_documents` and read them using `read_retrieve_documents`.\n\nConclusion\n==========\n\nAs shown above, the integration between LlamaIndex data agents \\+ Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We\u2019re excited to continue exploring this further in future blog posts.\n\nWe encourage you to play around with the notebook \u2014 make sure to check it out!",
        "d5dfcfad-6a79-4222-be54-5ec0a2dd43dd": "transcript\n          .trim()\n          .toLowerCase();\n        if (transcript.includes(\"c3\")) {\n          toggleRecording(); // Start the main speech recognition process\n        }\n      };\n\n      wakeWordRecognitionRef.current.start();\n    }\n\n    // Main speech recognition setup\n    const SpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition &amp;&amp; !recognitionRef.current) {\n      recognitionRef.current = new SpeechRecognition();\n      recognitionRef.current.continuous = false;\n      recognitionRef.current.interimResults = false;\n\n      recognitionRef.current.onresult = (event) =&gt; {\n        const lastResultIndex = event.results.length - 1;\n        const transcriptResult = event.results[lastResultIndex][0].transcript;\n        setTranscript(transcriptResult);\n        setAppState(\"playing\");\n        setShowSpeechBubble(true);\n        setTimeout(() =&gt; setShowSpeechBubble(false), speechBubbleTimeout);\n        fetchResponseFromLLM(transcriptResult);\n      };\n\n      recognitionRef.current.onend = () =&gt; {\n        setShowSpinner(true);\n      };\n    }\n  }, []);\n```\n3\\. Handling User Speech and Response\n-------------------------------------\n\n`toggleRecording` controls the speech recognition process, while `fetchResponseFromLLM` sends the user's speech to the LLM backend and handles the response.",
        "c90c2e59-dd80-48f5-b680-27957083a5e6": "Comparison of baseline PDF approach (top) vs. LlamaParse \\+ advanced retrieval (bottom)\n\n![](/blog/images/1*v_kwUZaZ765SpuJPp37_Qw.png)\n\nResults over the [Uber 10K Dataset](https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_datasets/10k/uber_2021). For more information on our evaluation metrics check out our [evaluation page](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html) here.\n\n![](/blog/images/1*9g72gSs3lVwzkYZHfkG1aw.png)This service is available in a **public preview mode:** available to everyone, but with a usage limit (1k pages per day). It operates as a standalone service that also plugs into our managed ingestion and retrieval API (see below). Check out our [LlamaParse onboarding here](https://docs.cloud.llamaindex.ai/llamaparse/) for more details.\n\n\n```\nfrom llama_parse import LlamaParse\n\nparser = LlamaParse(\n    api_key=\"llx-...\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n    verbose=True\n)\n```\nFor unlimited commercial use of LlamaParse, [get in touch](https://llamaindex.ai/contact) with us.\n\n**Next Steps**\n\nOur early users have already given us important feedback on what they\u2019d like to see next.",
        "02a2d5fd-5381-49ed-ba74-0a582dc562bb": "LlamaIndex integrates with Microsoft Guidance to ensure structured outputs from LLMs. It allows direct prompting of JSON keys and facilitates the conversion of Pydantic objects into the Guidance format, enhancing structured interactions. It can be used independently or with the SubQuestionQueryEngine.\n[Docs](https://gpt-index.readthedocs.io/en/latest/how_to/integrations/guidance.html), [Tweet](https://twitter.com/llama_index/status/1668281830347530242)\n4. The GuidelineEvaluator module allows users to set text guidelines, thereby aiding in the evaluation of LLM\\-generated text responses. This paves the way toward automated error correction capabilities.\n[Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/evaluation/RetryQuery.ipynb), [Tweet](https://twitter.com/llama_index/status/1667920234500751361?s=20)\n5. We now include a simple `OpenAIAgent`, offering an agent interface capable of sequential tool use and async callbacks. This integration was made possible with the help of the OpenAI function API and the LangChain abstractions.\n[Tweet](https://twitter.com/llama_index/status/1668995630356725762)\n6. `OpenAIPydanticProgram` in LlamaIndex enhances structured output extraction. This standalone module allows any LLM input to be converted into a Pydantic object, providing a streamlined approach to data structuring.\n[Docs](https://gpt-index.readthedocs.io/en/latest/examples/output_parsing/openai_pydantic_program.html), [Tweet](https://twitter.com/llama_index/status/1668995632873234435)\n7.",
        "981ac84e-e1de-4980-a684-eaf10705712d": "Update\n\nPresidio is now fully integrated into LlamaIndex as a post processor, follow this [notebook](https://github.com/run-llama/llama_index/blob/ff51e2cbf9815a44ebade9a1382216435d7f4bc8/docs/examples/node_postprocessor/PII.ipynb#L289) to learn how to use Presidio for PII masking. The next steps would be to add more customization and anonymization options.",
        "41c82305-7279-458f-a60b-621a77cf3c2f": "2\\. Intelligent Routing for Latency/Cost Tradeoffs\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F691e821921191e0dff4e28028d9bc0f68d3ddd3f-3006x1522.png%3Ffit%3Dmax%26auto%3Dformat&w=3840&q=75)The arrival of long\\-context LLMs will inevitably raise questions on the amount of context that is suitable for each use case. Injecting LLMs with long context comes with real cost and latency tradeoffs and isn\u2019t suitable for every use case or even every question. Although cost and latency will decrease in the future, we anticipate users will need to think carefully about this tradeoff for the next year or two.\n\nCertain questions that are asking about specific details are well suited for existing RAG techniques of top\\-k retrieval and synthesis.\n\nMore complex questions require more context from disparate pieces of different documents, and in those settings it is less clear how to correctly answer these questions while optimizing for latency and cost:\n\n* Summarization questions require going over entire documents.\n* Multi\\-part questions can be solved by doing chain\\-of\\-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt.\n\nWe imagine an intelligent routing layer that operates on top of multiple RAG and LLM synthesis pipelines over a knowledge base. Given a question, the router can ideally choose an optimal strategy in terms of cost and latency in terms of retrieving context to answer the question. This ensures that a single interface can handle different types of questions while not becoming prohibitively expensive.",
        "41b94dfa-c366-4f67-a436-b0b91ff0a730": "Hello, Llama Lovers \ud83e\udd99,\n\nHappy New Year! As we step into 2024, we\u2019re thrilled to bring you a special edition of our newsletter, packed with updates from the last two weeks of 2023\\. This edition is brimming with the latest features, community demos, courses, insightful tutorials, guides, and webinars that we\u2019ve curated for you.\n\nHave you been working on an interesting project, written an engaging article, or created a video? We can\u2019t wait to hear about it! Please share your work with us at [news@llamaindex.ai](mailto:news@llamaindex.ai). Don\u2019t forget to subscribe to our newsletter via our [website](https://www.llamaindex.ai/) to receive all these exciting updates directly in your inbox.\n\n\ud83e\udd29 **First, the highlights:**\n\n1. **LLMCompiler Implementation:** A SOTA agent implementation for faster, efficient handling of complex queries. [Notebook](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/agents/llm_compiler/llm_compiler.ipynb), [Tweet](https://x.com/llama_index/status/1740778394856648843?s=20).\n2. **MultiDocAutoRetrieverPack:** A RAG template for structured retrieval and dynamic responses to large documents and metadata. [Tweet](https://x.com/llama_index/status/1739307699773518201?s=20), [LlamaPack](https://llamahub.ai/l/llama_packs-multidoc_autoretrieval?from=llama_packs).\n3.",
        "2cf7b3fc-36de-4e4c-ae26-9b36f659de6b": "(co\\-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co\\-founder at Metaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs)\n\nWe\u2019re incredibly excited to launch an [integration](https://llamahub.ai/l/tools-metaphor) between LlamaIndex and [Metaphor](https://platform.metaphor.systems/): combine the capabilities of LlamaIndex data agents with Metaphor as a *native* LLM search tool to enable knowledge workers capable of answering any question over any data, no matter how recent or complex.\n\nWe provide a deeper overview of Metaphor and the LlamaIndex integration below. We also walk through our [example notebook](https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynb) to showcase how they can be combined.\n\nBackground/Context\n==================\n\nState\\-of\\-the art large language models (LLMs) such as ChatGPT, GPT\\-4, Claude 2 have incredible reasoning capabilities that unlock a wide variety of use cases \u2014 from insight extraction to question\\-answering to general workflow automation. Yet they are limited in their abilities to retrieve contextually relevant information. A popular stack that has emerged is to setup a retrieval\\-augmented generation (RAG) system, which combines LLMs with external storage solutions over a static knowledge source. Frameworks such as LlamaIndex provide a variety of tools to setup both simple and complex RAG systems.\n\nYet even this is not the complete picture.",
        "4eda6d3b-be44-4008-905f-122738265904": "This is essentially \u201cboilerplate\u201d that an LLM developer / [\u201cAI engineer\u201d](https://www.latent.space/p/ai-engineer) must write.\n\nWe had this as an internal abstraction in LlamaIndex before (as a `ResponseSynthesizer`), but the external\\-facing UX was unfriendly to users. The actual piece that gathered responses (the `ResponseBuilder` ) was hard to customize, and the `ResponseSynthesizer` itself was adding an extra unnecessary layer.\n\nNow we have a set of standalone modules that you can easily import. Previously, when you set the `response_mode` in the query engine, these were being setup for you. Now they are more directly available and user\\-facing.\n\nHere\u2019s a list of all the new `Response Synthesiszer` modules available from `llama_index.response_synthesizer`:\n\n* `Refine` \\- Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk.",
        "4541e65c-651a-41d2-b89c-0981a5c68fb9": "Below is a demo of our solution:\n\nHow we built it\n---------------\n\nWhen the chat is initiated, the Counselor Copilot gets the contact\u2019s data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real\\-time and then provide a summary of that data to counselors as context at the beginning of each conversation.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F0d224551401600550e9a5cac36177616e4104dcb-1384x1038.webp%3Ffit%3Dmax%26auto%3Dformat&w=1920&q=75)Further, we used a LlamaIndex ReAct Agent to monitor the conversation and \u2014 based on the chat history and contact context \u2014 deploy the right tool. Tools at the ReAct Agent\u2019s disposal include:\n\n1. Escalating the conversation to a supervisor\n2. Suggesting a response and related resources based on The Trevor Project\u2019s guidelines\n3. Searching the web for location\\-specific resources and sending the resources to the contact\n\nFor tool \\#2, we created a vector database that contains The Trevor Project\u2019s documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation.",
        "595e43e1-7925-475b-93a5-784a6f6d411a": "GPT\\-4V Amidst Snow Geese Migration.\n* [Glenn Parham\u2019s](https://twitter.com/glenn__parham) [cookbook](https://github.com/deptofdefense/LLMs-at-DoD/blob/main/tutorials/Chatting%20with%20your%20Docs.ipynb) on LlamaIndex, hosted in the Department of Defense\u2019s official repository, showcases methods for applying RAG on unclassified DoD policy documents.\n* [Sudarshan Koirala](https://www.youtube.com/watch?v=PHEZ6AHR57w) made a tutorial on Using Perplexity API with LlamaIndex.\n* [Ravi Theja](https://twitter.com/ravithejads) [analysis](/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9) on GPT4\\-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques\n\n\ud83c\udfa5 **Webinars:**\n\n* Check out our CEO \u2014 [Jerry Liu\u2019s](https://twitter.com/jerryjliu0) talk on Building Production\\-Ready RAG Applications at [AI.engineer](https://t.co/46VrFt8GCV) Summit.",
        "ca412662-0a1d-4f7f-bb5b-c8e2fe5f55bd": "Given an input task, the data agent uses a **reasoning loop** to decide which tools to use, in which sequence, and the parameters to call each tool. The \u201cloop\u201d can conceptually be very simple (a one\\-step tool selection process), or complex (a multi\\-step selection process, where a multitude of tools are picked at each step).\n\n![](/blog/images/1*WPOS7tiljXCrd3IkJy84CQ.png)These components are described in more detail below.\n\nAgent Abstraction \\+ Reasoning Loop\n-----------------------------------\n\nWe have support for the following agents:\n\n* OpenAI Function agent (built on top of the OpenAI Function API)\n* a ReAct agent (which works across any chat/text completion endpoint).\n\nYou can use them as the following:\n\n\n```\nfrom llama_index.agent import OpenAIAgent, ReActAgent\nfrom llama_index.llms import OpenAI\n\n# import and define tools\n...\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n# initialize ReAct agent\nagent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n# use agent\nresponse = agent.chat(\"What is (121 * 3) + 42?\")\n```\nEach agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task \u2014 `chat` and `query`. Note that these are the core methods used in our `ChatEngine` and `QueryEngine` respectively. In fact that our base agent class (`BaseAgent`) simply inherits from `BaseChatEngine` and `BaseQueryEngine`.",
        "a9aaecfb-1a4d-4159-8eca-16cdf2e1edc5": "Searching for location\\-specific resources for contacts, and quickly sharing those resources via email.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6eb187a5ba134f34d4275d8ae2a81197b2f0defd-1489x232.webp%3Ffit%3Dmax%26auto%3Dformat&w=1920&q=75)5\\. Completing case forms in a CRM for contacts, including summarizing the interaction.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fff5a01b3be56bbff38170e42f2091c83c6254358-1600x803.webp%3Ffit%3Dmax%26auto%3Dformat&w=1920&q=75)While these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time.\n\nWith Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher\\-quality conversations with patients.\n\nBelow is a demo of our solution:\n\nHow we built it\n---------------\n\nWhen the chat is initiated, the Counselor Copilot gets the contact\u2019s data from the CRM, which is stored in complex PDFs.",
        "8be6782b-5389-4f2e-84d9-45f7ba7ba1f7": "orchestrator=orchestrator,\n)\n\n# Set up the launcher\nlauncher = LocalLauncher(\n    [query_rewrite_service, rag_service],\n    control_plane,\n    message_queue,\n)\n\n# Run a query\nresult = launcher.launch_single(\"Tell me about rabbits\")\nprint(result)\n```\nPublic roadmap\n--------------\n\nThis is an alpha release, meaning that we\u2019d love your feedback on features to better help you build multi\\-agent systems in production! We\u2019ve created a [public roadmap](https://github.com/run-llama/llama-agents/discussions/49) showing where we plan to go from here. We\u2019re actively seeking public feedback on what works for you and what doesn\u2019t.\n\nDive in!\n--------\n\n`llama-agents` provides a powerful, flexible framework for building complex multi\\-agent AI systems. Whether you're prototyping a new idea or scaling to production, `llama-agents` offers the tools you need to bring your AI vision to life. Check out [the repo](https://github.com/run-llama/llama-agents) to learn more, especially our library of [examples](https://github.com/run-llama/llama-agents/tree/main/examples).\n\nWe're excited to see what the community builds with `llama-agents`. Happy coding!",
        "65cabb91-e2c4-4934-acdd-df23c2226968": "Setup\n=====\n\nBefore embarking on the experiment, we need to ensure all requisite modules are imported:\n\n\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.evaluation import (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\nfrom llama_index.llms import OpenAI\n\nimport openai\nimport time\nopenai.api_key = 'OPENAI-API-KEY'\n```\nDownload Data\n=============\n\nWe\u2019ll be using the Uber 10K SEC Filings for 2021 for this experiment.\n\n\n```\n!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n```\nLoad Data\n=========\n\nLet\u2019s load our document.\n\n\n```\ndocuments = SimpleDirectoryReader(\"./data/10k/\").load_data()\n```\nQuestion Generation\n===================\n\nTo select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents.",
        "4ac9fdd4-fc25-4c43-907d-75334d11ec1b": "Solving Multi\\-Tenancy Challenges\n---------------------------------\n\nThe key to managing Multi\\-Tenancy lies within the metadata. When indexing documents, we incorporate user\\-specific information into the metadata before adding it to the index. This ensures that each document is uniquely tied to an individual user.\n\nDuring the query phase, the retriever uses this metadata to filter and only access documents associated with the querying user. Subsequently, it performs a semantic search to retrieve the most relevant information segments, or \u2018top\\_k chunks\u2019, for that user. By implementing this approach, we effectively prevent the unauthorized cross\\-referencing of private information between different users, upholding the integrity and confidentiality of each user\u2019s data.\n\nNow that we\u2019ve discussed the concept, let\u2019s dive into the construction of a Multi\\-Tenancy RAG system. For an in\\-depth step\\-by\\-step guide, feel free to follow along with the subsequent instructions in our [Google Colab Notebook](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_tenancy/multi_tenancy_rag.ipynb).\n\nDownload Data:\n--------------\n\nWe will use `An LLM Compiler for Parallel Function Calling` and `Dense X Retrieval: What Retrieval Granularity Should We Use?` papers for the demonstrations.",
        "abb41273-efc2-4e2a-9e74-12b51d3714d2": "Any failure to ensure safety could significantly damage Uber's reputation and business.\n\n\u2022 Financial performance and profitability. Uber continued to experience substantial losses in 2019 due to investments in growth, and there was no guarantee of future profitability. Uber's path to profitability was uncertain, and failure to become profitable could threaten its business model and access to capital.  \n\n\u2022 Reliance on third parties. Uber relied on third parties for services like cloud computing, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could negatively impact Uber's offerings, growth, and profitability.  \n\n\u2022 Macroeconomic conditions. Uber's business was sensitive to the economy and consumer discretionary spending. An economic downturn could reduce demand for Uber's offerings and threaten its growth and financial performance.\n\n\u2022 Reliance on Drivers and Restaurants. Uber's success depended on attracting and retaining enough Drivers and Restaurants to support growth and meet consumer demand. Failure to attract or retain Drivers and Restaurants could significantly impact Uber's business.  \n\n\u2022 Intellectual property. Failure to protect intellectual property or infringement of others' intellectual property could damage Uber's competitive position and business. Uber relied on laws and policies to protect intellectual property, but monitoring infringement and unauthorized use was difficult.\n```\nAs shown, the answer contains the prompt within it; the point of the refine prompt was to produce just the refined answer, not produce both the original and refined answers.\n\nFor some reason, using the Tree Summarize strategy works much better.",
        "8daac2dd-882c-4413-9813-2a2ca10e08d3": "> The table you\u2019ve provided shows performance benchmark scores for different model sizes across various AI models such as MPT, Falcon, and LLaMA on different tasks, such as Aqua\\-RAT, LogiQA, LastAR, SAT\\-en, and SAT\\-math. For the SAT\\-en task specifically, you asked which model in the 7B series has the highest performance. To find out, we need to look at the column labeled \u201cSAT\\-en (w/o Ps\\_8\\).\u201d In the 7B series of models, here are the scores: \u2014 MPT 7B: 37\\.1 \u2014 Falcon 7B: 37\\.3 \u2014 LLaMA 7B: 63\\.9 \u2014 Model2 7B: 37\\.4 The LLaMA 7B model outperforms the other 7B models on the SAT\\-en (w/o Ps\\_8\\) task with a score of 63\\.9\\.\n\n**Observation:**\n----------------\n\nIncorrect answer and recognized Llama series of models and values incorrectly.\n\nChain of thought prompting:\n===========================\n\n**Question:** Based on the image provided. Follow the steps and answer the query \u2014 which model has higher performance in SAT\\-en in 7B series models?\n\nExamine the Image: Look at the mentioned category in the query in the Image.\n\nIdentify Relevant Data: Note the respective percentages.\n\nEvaluate: Compare if there is any comparison required as per the query.\n\nDraw a Conclusion: Now draw the conclusion based on the whole data.\n\n**Answer:**",
        "68848c1c-0fbd-4bbc-adef-936ab5c866ae": "We had an awesome time at the Berkeley Hackathon two weeks ago (6/17\u20136/18\\). The attendance stats were impressive:\n\n* 1200 hackers\n* 262 submitted projects\n* 2 real\\-life llamas \ud83e\udd99\n\nLlamaIndex sponsored a \u201cBest Knowledge\\-Intensive LLM App\u201d prize series at the hackathon. The criteria was an app that leveraged a knowledge base of custom data to build innovative new application experiences.\n\nWe announced three prize winners along with an honorable mention. We are excited to feature each project in a special highlight below. In each highlight, the creators describe the project mission and what it solves, the implementation\\+tech stack, challenges, and future directions. Check it out! \ud83d\udc47\n\nFirst Prize Winner: Helmet AI\n=============================\n\nCreators: Jaiveer Singh, Devin Mui, Ethan Mehta, Manav Rathod\n\nDevpost: [https://devpost.com/software/helmet\\-ai](https://devpost.com/software/helmet-ai)\n\n![](/blog/images/0*GfFIf2fjROahG0NR)Introduction\n------------\n\nIn today\u2019s rapidly evolving business landscape, staying ahead of the competition is paramount for success. However, the deluge of information and the ever\\-changing market dynamics can make it challenging for business leaders to make informed decisions. In this blog post, we introduce Helmet AI, a cutting\\-edge market intelligence tool designed to empower leadership teams with real\\-time insights and a competitive edge. Join us as we explore the capabilities, technology stack, and future prospects of Helmet AI.\n\nUnveiling Helmet AI\n-------------------\n\n!",
        "b643619d-bdc5-4784-876a-78a77475a3a6": "* The endpoint on HF is served on AWS Nvidia A100G \u00b7 1x GPU \u00b7 80 GB which costs $6\\.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.)\n* We used the [Prometheus model](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) for the analysis here. We also made a similar analysis with the [GPTQ Quantized version](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ) of the [Prometheus model](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open\\-sourcing the model and [Tom Jobbins](https://twitter.com/TheBlokeAI) for the quantized version of the model.\n\nReferences:\n===========\n\n* [Prometheus paper](https://arxiv.org/abs/2310.08491).\n* [Prometheus model on HuggingFace.](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)",
        "8112a259-d590-46fc-8242-1b2a14bed8f3": "What\u2019s Next\n-----------\n\nWe hope to completely integrate the code and workflow into a Google plugin or extension so users can easily implement it into their daily emailing. We want to ensure the privacy and security of the user\u2019s data, so we want to experiment with methods to reduce how much data is\nsent to third\\-party services like OpenAI. We also want to dedicate further development to the emotion training, as this could boost the effectiveness of our product and add to our main value proposition of personalized, user\\-specific email generation.\n\nWinner: Prosper AI\n==================\n\nCreators: Alan Yang, Ashay Changwani, Punit Sai Arani, Vedant Tapadia\n\nDevpost: [https://devpost.com/software/prosper\\-ai](https://devpost.com/software/prosper-ai)\n\n[Vercel Demo](https://prosperai.vercel.app/) / [YouTube Video](https://www.youtube.com/watch?v=_-v0BhFPjAQ)\n\nOverview\n--------\n\nProsper AI is a trailblazer in utilizing Artificial Intelligence to unlock your full financial potential. It serves as an accessible and smart virtual financial advisor, armed with precise insights and personalized advice. Our mission is to democratize financial expertise. By bridging the resource gap, Prosper AI aims to level the playing field for all.\n\nThe Genesis of Prosper AI\n-------------------------\n\nThe spark that ignited Prosper AI was a simple observation of the wealth disparity among different social classes. The rich have always had access to knowledge and resources that help in growing and safeguarding their wealth. In contrast, those from modest backgrounds often lack the necessary knowledge and tools to utilize what they earn effectively.",
        "49be2e54-448a-46de-86f3-ccf25ea12db8": "get('chunk_lines_overlap', 15)}, max_chars={node_parser_params.get('max_chars', 1500)})\",\n        \"SemanticSplitterNodeParser\": f\"SemanticSplitterNodeParser(buffer_size={node_parser_params.get('buffer_size', 1)}, breakpoint_percentile_threshold={node_parser_params.get('breakpoint_percentile_threshold', 95)}, embed_model=embed_model)\",\n        \"TokenTextSplitter\": f\"TokenTextSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n        \"HTMLNodeParser\": f\"HTMLNodeParser(tags={node_parser_params.get('tags', ['p', 'h1'])})\",  \n        \"JSONNodeParser\": \"JSONNodeParser()\",\n        \"MarkdownNodeParser\": \"MarkdownNodeParser()\"\n    }\n    code_snippet += f\"node_parser = {node_parsers[node_parser_choice]}\\n\\n\"\n\n    # Response mode\n    code_snippet += f\"response_mode = '{response_mode}'\\n\\n\"\n\n    # Vector store initialization\n    if vector_store_choice == \"Pinecone\":\n        code_snippet += \"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\"\n        code_snippet += \"index = pc.Index('test')\\n\"\n        code_snippet += \"vector_store = PineconeVectorStore(pinecone_index=index)\\n\"\n    elif vector_store_choice == \"Qdrant\":\n        code_snippet += \"client = qdrant_client.",
        "f710a4e0-9e48-4726-91a7-11b1ad03cc9a": "* **Conversational Depth with PaLM:** Google\u2019s language model ensures the chat is as rich and nuanced as the images themselves.\n* **User\\-Friendly Interface:** Streamlit powers an intuitive and clean UI, making it easy for anyone to navigate and interact.\n\nDeep Dive into the Tech Stack\n=============================\n\nThe project is a symphony of technologies, each playing a crucial role:\n\n* Microsoft AI **KOSMOS\\-2** via Replicate breathes life into images by providing them a narrative.\n* Google **PaLM API** adds the layer of linguistic intelligence, making the conversation about the images insightful and engaging.\n* **LlamaIndex** acts as the maestro, coordinating the models to work in harmony.\n\nUnveiling `app.py`: The Core of the Application\n===============================================\n\nThe `app.py` script is the heart of the app, where we bring together KOSMOS\\-2 and PaLM with Llamaindex to create a seamless multimodal experience. Let\u2019s walk through it, from start to finish.\n\n**1\\. Initial Setup**\n\nWe start by importing the necessary libraries and setting up our Streamlit page. Here, we lay the groundwork for image processing and conversation management.",
        "ad8c9ab5-e220-48ba-a933-0f70230312b3": "Summary\n=======\n\nIn this article, we compare how well LLM\\-powered agents with different degrees of complexity perform over practical data tasks (financial analysis). We compare the performance of agents with more *complex, unrestrained* interaction behavior (ReAct) with agents that contain *simpler, more constrained* interactions (routing). We specifically analyze how much complexity can be added to the agent layer vs. the tool layer.\n\nWe find that the choice of the language model matters a lot. ReAct agents that are powered by \u201cdumber\u201d models (in a tongue\\-in\\-cheek fashion we are referring to any non GPT\\-4 model as \u201cdumb\u201d) struggle to return relevant results over data. We find that constraining agent interaction behavior, and giving them access to more tools that can more explicitly perform complex actions, can help improve query performance over these less sophisticated LLMs. In contrast, more sophisticated models (GPT\\-4\\) can more reliably utilize the ReAct loop to execute a variety of complex data queries.\n\nThis blog post is quite detailed; we provide a *lot* of experiments and results below. Best of all, you can run this all yourself with our [example notebook](https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing)!\n\nOverview of Agents\n==================\n\nBuilding LLM\\-powered agents have gotten increasingly popular in the past few months. Frameworks like [LangChain](https://github.com/hwchase17/langchain) have made it much easier to create these agents according to a set of common abstractions.",
        "d1f50639-cdda-4853-b649-e013d2f87a5c": "**\ud83d\uddfa\ufe0f Guides:**\n\n* We introduce a core [guide](https://docs.llamaindex.ai/en/latest/community/full_stack_projects.html#) within the LlamaIndex ecosystem, designed to simplify \u201cfull\\-stack\u201d app development, which is notably more complex than notebook development. This includes \u2018create\\-llama\u2019 for building full\\-stack apps with advanced templates, \u2018SEC Insights\u2019 for multi\\-document handling of over 10,000 filings, and \u2018LlamaIndex Chat\u2019 for a customizable chatbot experience. All tools are open\\-source with full guides and tutorials available.\n* [Guide](https://t.co/2Ygxs6bPoX) on using the Table Transformer model with GPT\\-4V for advanced RAG applications in parsing tables from PDFs: Our method involves CLIP for page retrieval, Table Transforms for table image extraction, and GPT\\-4V for answer synthesis. This approach is compared with three other multi\\-modal table understanding techniques, including using CLIP for whole page retrieval, text extraction and indexing with GPT\\-4V, and OCR on table images for context.\n* [Guide](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pydantic.ipynb) on analyzing various multi\\-modal models for their ability to extract structured data from complex product images on an Amazon page. The models compared include GPT\\-4V, Fuyu\\-8B, MiniGPT4, CogVLM\\-4, and LLaVa\\-13B.",
        "99ef97a9-3884-408c-84dd-1c1bb6424c7c": "*In this technical walkthrough, we\u2019ll highlight the functionality of Tonic Validate and its integration with LlamaIndex. Sign up for a free account* [*here*](https://www.tonic.ai/validate) *before you start.*\n\nIntroduction\n============\n\nAs enterprise adoption of generative AI technologies continues, companies are turning to Retrieval Augmented Generation (RAG) systems to extend the application of large\\-language models (LLMs) to their private data (e.g., a chatbot that can answer questions based on internal technical documentation). Traditionally in software engineering, companies have placed a high emphasis on implementing continuous integration tests to ensure systems remain performant when updates are made. More recently, these same principles have been applied to machine learning models in production.\n\nHowever, as a young technology, RAG currently lacks best practices for integration tests to ensure breaking changes aren\u2019t introduced to the production system. In this article, we will demonstrate how you can use Tonic Validate\u2019s RAG performance monitoring capabilities, LlamaIndex, and GitHub Actions to create novel integration tests that alert you to changes in RAG system performance. To make things easy, Tonic Validate is available natively within LlamaIndex\u2019s core library \u2014 you can read more about that [here](https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems).\n\nWhat is Tonic Validate?\n=======================\n\nTonic Validate is a RAG benchmarking and evaluation platform that monitors performance of RAG systems in production.",
        "559d0812-19c4-4a82-9830-f1d4b22fd922": "* [gmail\\-extractor](https://github.com/run-llama/gmail-extractor), a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision.\n\n**\u270d\ufe0f Tutorials:**\n-----------------\n\n* Sherlock Xu\u2019s [tutorial](https://www.bentoml.com/blog/serving-a-llamaindex-rag-app-as-rest-apis) from BentoML on Serving A LlamaIndex RAG App as REST APIs.\n\n**\ud83d\udcd1\u00a0Papers:**\n-------------\n\n* FinTextQA, a new benchmark dataset for long\\-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto\\-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question\\-answering system for financial text.\n\n**\ud83d\udcf9\u00a0Webinar:**\n--------------\n\n* [Webinar](https://www.youtube.com/watch?v=o0DPxvgML5c) with authors of memary \\- Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open\\-source reference implementation for long\\-term memory in autonomous agents\n\n\ud83d\udcc5\u00a0**Events:**\n-------------\n\n* [Join](https://www.meetup.com/nlp_london/events/301171675/) Pierre from LlamaIndex along with speakers from Weaviate, and Weights \\& Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.",
        "70697641-2f7f-4196-9f4e-e040f40d4486": "[](/blog/images/1*FWueDTZxYh8-kNXYU9PX_Q.png)Once you\u2019ve saved those scopes, scroll up to \u201cInstall to workspace\u201d to install your app.\n\nYou now need to tell Slack where your app is so you can receive messages from it. Click the \u201cEvent Subscriptions\u201d link in the left nav and fill it out so it looks something like this, specifically:\n\n* Set your Request URL to that URL that ngrok gave you earlier\n* Subscribe to the `message.channels` event\n\n![](/blog/images/1*D8GcOA9uoSqNz6t9uX6dog.png)If your app is running and ngrok is correctly tunneling, your Request URL should be Verified.\n\nPhew! That was a lot. Your Slack app is now registered and Slack will send it messages. But to get those messages, you have to tell it to join a channel.\n\nStep 2: Join a channel, and reply to messages\n=============================================\n\nTo do this we\u2019ll need to extend our app. You can see the final result of this step in `2_join_and_reply.py`. Let's walk through what we've added:",
        "3432d1bd-87d4-4b1c-88ed-91cd6bcacfe9": "ada or sbert).\n\nDuring retrieval\\-time, we do the following:\n\n1. Retrieve text via vector search on the text embeddings\n2. Retrieve images via vector search on the image embeddings\n\nBoth text and images are returned as Nodes in the result list. We can then synthesize over these results.\n\n**What\u2019s still to Come**\n\n* More native ways to store images in a vector store (beyond base64 encoding)\n* More flexible multi\\-modal retrieval abstractions (e.g. combining image retrieval with any text retrieval method)\n* Multi\\-modal response synthesis abstractions. Currently the way to deal with long text context is to do \u201ccreate\\-and\\-refine\u201d or \u201ctree\\-summarize\u201d over it. It\u2019s unclear what generic response synthesis over multiple images and text looks like.\n\nNotebook Walkthrough\n====================\n\nLet\u2019s walk through a notebook example. Here we go over a use case of querying Tesla given screenshots of its website/vehicles, SEC fillings, and Wikipedia pages.\n\nWe load the documents as a mix of text docs and images:\n\n\n```\ndocuments = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()\n```\nWe then define two separate vector database collections in Qdrant: a collection for text docs, and a collection for images. We then define a `MultiModalVectorStoreIndex` .",
        "c974a44a-4599-4337-8c8a-770bf2763753": "Our hard\\-working team is delighted to announce our latest major release, LlamaIndex 0\\.9! You can get it right now:\n\n`pip install --upgrade llama_index`\n\nIn LlamaIndex v0\\.9, we are taking the time to refine several key aspects of the user experience, including token counting, text splitting, and more!\n\nAs part of this, there are some new features and minor changes to current usage that developers should be aware of:\n\n* New `IngestionPipline` concept for ingesting and transforming data\n* Data ingestion and transforms are now automatically cached\n* Updated interface for node parsing/text splitting/metadata extraction modules\n* Changes to the default tokenizer, as well as customizing the tokenizer\n* Packaging/Installation changes with PyPi (reduced bloat, new install options)\n* More predictable and consistent import paths\n* Plus, in beta: MultiModal RAG Modules for handling text and images!\n\nHave questions or concerns? You can [report an issue](https://github.com/run-llama/llama_index/issues) on GitHub or [ask a question on our Discord](https://discord.com/invite/eN6D2HQ4aX)!\n\nRead on for more details on our new features and changes.\n\nIngestionPipeline \u2014 New abstraction for purely ingesting data\n=============================================================\n\nSometimes, all you want is to ingest and embed nodes from data sources, for instance if your application allows users to upload new data. New in LlamaIndex V0\\.9 is the concept of an `IngestionPipepline` .\n\nAn `IngestionPipeline` uses a new concept of `Transformations` that are applied to input data.\n\nWhat is a `Transformation` though?",
        "58924869-2ddd-424c-92ae-969178d42cbc": "Webinars And Podcasts:\n----------------------\n\n1. [Webinar](https://www.youtube.com/watch?v=bPoNCkjDmco) on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph).\n2. [Webinar](https://www.youtube.com/watch?v=gbyfXRxU0Gw) with Albus \u2014 a comprehensive Slackbot for enterprise search, [xpress.ai](http://Xpress.ai) \u2014 a low\\-code solution for building LLM workflows \\+ agents and [ImmigrantFirst.ai](https://t.co/QAJyGqZPcB) \u2014 assistant to help immigrants complete their EB\\-1A/O1 apps more efficiently.\n3. [Data Exchange Podcast](https://www.youtube.com/watch?v=NAoqOJrE8rQ&list=PLnTmH22EvTFTtWJRPTNzosDIDblnSg0PD&t=1s) with Ben Lorica on LlamaIndex\n\nEvents:\n-------\n\nRavi Theja gave talks on \u201cLlamaIndex: Basics To Production\u201d at Accel Partners and Together VC Fund in India.",
        "2c46711b-f241-4de2-ac88-adddabddbda2": "**Cross\\-Encoder Fine\\-Tuning:** Cross\\-encoders enhance RAG by refining post\\-embedding search results. With LlamaIndex, you can now fine\\-tune cross\\-encoders on any document, boosting performance. [Docs](https://t.co/vAyv94dFk2), [Tweet](https://x.com/jerryjliu0/status/1712856457413370110?s=20).\n\n\u2699\ufe0f Integrations \\& Collaborations:\n==================================\n\n1. **Assembly AI:** We introduced a new data reader for audio data integration with AssemblyAI. This integration allows effortless audio loading and facilitates building vector store indices and query engines for inquiries. [Docs](https://llamahub.ai/l/assemblyai), [Tweet](https://x.com/llama_index/status/1711156989106299249?s=20).\n2. **Nougat \u2014 MetaAI:** We integrated Nougat, an exceptional OCR tool from Meta, that excels in interpreting scientific papers, notably mathematical notations, and LaTeX as a loader in LlamaHub, allowing streamlined processing of ArXiv papers within the RAG pipeline. [Docs](https://llamahub.ai/l/nougat_ocr), [Tweet](https://x.com/llama_index/status/1711896904928292976?s=20).\n3. **Hugging Face\\-Text Embeddings Inference:** We integrated with the new text\\-embeddings\\-inference server from HuggingFace offering production\\-scale serving with distributed tracing for all BERT models at impressive speeds.",
        "447bede0-09ff-4f26-9c60-476655780b22": "context_window=3900,  \n    max_new_tokens=2000,  \n    model_kwargs={\"quantization_config\": quantization_config},  \n    # tokenizer_kwargs={},  \n    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},  \n    messages_to_prompt=messages_to_prompt,  \n    device_map=\"auto\",  \n)\n```\n\n\n```\nfrom llama_index import ServiceContext, set_global_service_context  \n  \nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\n```\n\n\n```\nset_global_service_context(service_context)\n```\n\n\n**Step 3: Storing your index**\n\n\n\n\n```\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex  \nfrom llama_index.readers.file.base import (  \n    DEFAULT_FILE_READER_CLS,  \n    ImageReader,  \n)  \nfrom llama_index.response.notebook_utils import (  \n    display_response,  \n    display_image,  \n)  \nfrom llama_index.indices.query.query_transform.base import (  \n    ImageOutputQueryTransform,  \n)  \n  \nfilename_fn = lambda filename: {\"file_name\": filename}  \n  \nllama_reader = SimpleDirectoryReader(  \n    input_dir=\"/content/llama\",  \n    file_metadata=filename_fn,  \n)  \nllama_documents = llama_reader.load_data()  \n  \nllama_index = VectorStoreIndex.from_documents(llama_documents)\n```\n\n\n**Step 4: Query** [**Transformations**](https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha)",
        "d35c616b-b780-469f-9b6d-b0816863848d": "I\u2019m interested in the\n `RETAIL_DATA` schema, so let me ask more of the schema \\-\n\n\n\n\n```\nagent.chat(\"What can i do with the retail_data schema\")\n```\n\nAnd I get this:\n\n\n\n```\nThe RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed   \nto support a wide range of analyses related to retail operations. Here   \nare some of the capabilities and types of analyses you can perform with   \nthis schema:  \n  \n1. **Call Center Analysis**: Evaluate the performance of call centers,   \n   understand call volumes, and assess customer service efficiency.  \n  \n2. **Customer Demographics**: Analyze customer profiles, including   \n   demographics, purchasing behaviors, and preferences. This can help   \n   in targeted marketing and customer segmentation.  \n  \nSpecific questions that can be addressed using the RETAIL_DATA schema include:  \n- What is the total number of call centers?  \n- How many customers have a preferred customer flag?  \n- What is the average price of items?\n```\n\nLet me do some more data analysis.\n\n\nGenerate an SQL query and run it\n================================\n\n\n\n Let\u2019s generate a SQL query (asking top 10 item categories sold during\n Christmas time):\n\n\n\n\n```\nagent.chat(\"Top 10 item category sold during christmas time across all years\")\n```\n\nNow it calls `get_answer` function from Waii tool:\n\n\n\n```\n=== Calling Function ===  \nCalling function: get_answer with args:   \n  {\"ask\":\"What are the top 10 item categories sold during   \n          Christmas time across all years?\"}\n```\n\nAnd got a SQL like:",
        "01d9c739-46d4-4b21-8b4f-d658f346810f": "They effortlessly manage conversation history, let you perform lightning\\-fast vector searches, and unlock a whole suite of powerful tools.\n\nWe built our ChatEngine directly from our trusty MongoDB index. This integration was surprisingly simple:\n\n\n```\ndef get_index():\n   logger.info(\"Connecting to index from MongoDB...\")\n   store = MongoDBAtlasVectorSearch(\n       db_name=os.environ[\"MONGODB_DATABASE\"],\n       collection_name=os.environ[\"MONGODB_VECTORS\"],\n       index_name=os.environ[\"MONGODB_VECTOR_INDEX\"],\n   )\n   index = VectorStoreIndex.from_vector_store(store)\n   logger.info(\"Finished connecting to index from MongoDB.\")\n   return index\n\nindex = get_index()\nindex.as_chat_engine(\n    llm = Fireworks(\n             api_key=env_vars['FIREWORKS_API_KEY'],\n             model=\"accounts/fireworks/models/mixtral-8x22b-instruct\" #Can be changed out for Llama3\n             )\n    chat_mode=\"best\", \n    context_prompt=(\n           \"\"\" You are a software developer bot that is an expert at reading over documentation to answer questions.\n           Use the relevant documents for context:\n           {context_str}\n           \\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\n           \"\"\"\n           ),\n    verbose=True\n    )\n```\n`create-llama`: from idea to app in record time\n-----------------------------------------------\n\nWe were seriously impressed by Create\\-Llama. Usually, building a full\\-stack app takes time, but Create\\-Llama had us up and running in under 15 minutes! All we did was point it towards our vector database and give a few basic details. Honestly, it made development a joy! [This blog post goes into more detail about how to use create\\-llama.](https://www.llamaindex.ai/blog/create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191)",
        "96fe23f7-12e6-4762-8313-5145729561be": "\u2014 \u201cCommercial Websites\u201d seem to have lower percentages of reported hateful and harmful activities and unqualified advice but higher percentages of illicit and criminal activities when compared to other platforms like \u201cWiki\u201d or \u201cP2P/File sharing\u201d. \u2014 \u201cGaming\u201d appears to have a moderate percentage of hateful and harmful violations, lower levels of illicit and criminal activity, and relatively higher levels of unqualified advice. Overall, this chart is a visual tool that likely aims to inform about the prevalence of certain types of online violations across different digital platforms.\n\nObservation:\n------------\n\nAs you can see though the categories hateful and harmful, illicit and criminal activity, and unqualified advice but it hallucinated with x\\-axis values with \u2014 \u201cVideo sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums \\& boards\u201d, \u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and \u201cOther\u201d.\n\nSpecific Questions:\n===================\n\n**Question:** Which model among llama2 and vicuna models does better in terms of violation percentages in Hateful and harmful category.\n\n**Answer:**\n\n\n> From the provided bar chart, we can analyze the violation percentage for both the Llama2 and Vicuna models in the \u201cHateful and harmful\u201d category. To determine which model does better (i.e., has a lower violation percentage), you would look at the height of the bars corresponding to each model within that category. In the \u201cHateful and harmful\u201d category (which is the first group of bars on the left), you can compare the blue bar (representing Llama2\\) to the light blue bar (representing Vicuna).",
        "21905bb7-13a7-40f4-b107-5497ce0a186b": "* **Corrective RAG as a Service Guide:** Create a self\\-correcting RAG that ensures context relevance and integrates search fallbacks before generation. [Notebook](https://github.com/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb), [Tweet](https://x.com/llama_index/status/1809282069606068486).\n* **Tutorial series on Property Graphs:** 6\\-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j, and Ollama. [Videos](https://www.youtube.com/playlist?list=PLTZkGHtR085ZYstpcTFWqP27D-SPZe6EZ), [Tweet](https://x.com/llama_index/status/1810410943215710510).\n\n**\u2728 Feature Releases and Enhancements:**\n----------------------------------------\n\n1. We have launched a Multi\\-Agent on Kubernetes Starter Kit to build and deploy a multi\\-agent system using Docker Compose and Kubernetes using llama\\-agents. This kit includes prebuilt agent loops and tools, as well as Dockerfiles and Kubernetes manifests for easy production deployment. [Notebook](https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes), [Tweet](https://x.com/llama_index/status/1807801281324765469).\n2. We have integrated RabbitMQ with llama\\-agents to enhance multi\\-agent communication, offering scalability and reliability for handling large request volumes in production. [Notebook](https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq), [Tweet](https://x.com/llama_index/status/1810342085171855753).\n3. \\[[Yi\\-01\\.AI](http://Yi-01.AI)]([http://Yi\\-01\\.AIhttps](http://Yi-01.AIhttps)://x.com/01AI\\_Yi) is integrated with LlamaIndex for enhanced retrieval and indexing, streamlining the development of smarter, faster RAG applications.",
        "897fe4a9-088d-43bd-9107-a4a5750b27a4": "**UPDATE 9/10/2023:** We\u2019ve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our [embedding fine\\-tuning guides](https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/finetuning.html#finetuning-embeddings-for-better-retrieval-performance) in the core documentation.\n\nWe\u2019ve created a [comprehensive, end\\-to\\-end guide](https://github.com/run-llama/finetune-embedding) showing you how to fine\\-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!).\n\nThe result is a [**5\u201310% performance increase in retrieval evaluation metrics**](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb) \u2014 our finetuned `bge` model almost reaches `text-embedding-ada-002` levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole.\n\nThis tutorial is helpful to *anyone* building RAG systems:\n\n* If you\u2019re new to finetuning, no problem! We have [step by step notebooks](https://github.com/run-llama/finetune-embedding#steps-for-running) walking through the key steps. Simply substitute the file links for your own data, and just run every cell.\n* Finetuning embedding models is lightweight and doesn\u2019t require a GPU. These notebooks were tested on an M2 Macbook Pro.",
        "2ab03df7-eb8c-4569-98ed-eec0ae902b7e": "This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.\n\nTherefore we\u2019ve made the following changes:\n\n1. **ServiceContext is now deprecated:** You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.\n2. **You can now define global settings:** Define this once, and don\u2019t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.\n\nAll references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.\n\nUsage Example\n=============\n\nTo build a `VectorStoreIndex` and then query it, you can now pass in the embedding model and LLM directly\n\n\n```\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.callbacks import CallbackManager\n\nembed_model = OpenAIEmbedding()\nllm = OpenAI()\ncallback_manager = CallbackManager()\nindex = VectorStoreIndex.from_documents(\n documents, embed_model=embed_model, callback_manager=callback_manager\n)\nquery_engine = index.as_query_engine(llm=llm)\n```\nOr you can define a global settings object",
        "910607a0-1710-413c-b06f-17cdc94c9d1a": "```\nindex.insert(Document(text=message.get('text')))\n```\nThe result is a Slackbot that can answer questions about what it\u2019s been told:\n\n![](/blog/images/1*Zk-kD7S8nNZNSzTzi-_MSQ.png)Amazing! You can easily imagine a bot that listens to everybody\u2019s conversations and is able to answer questions about things people said weeks or months ago, saving everybody time and effort searching through old messages.\n\nStep 6: persist our memory\n==========================\n\nOur bot has a critical flaw though: the index is stored only in memory. If we restart the bot, it forgets everything:\n\n![](/blog/images/1*2q-qOH4dVV0ehOJG2h6ycg.png)In `6_qdrant.py` we bring in [Qdrant](https://qdrant.tech/), an open\\-source, local vector database that stores these facts on disk instead. That way if we restart our bot it remembers what was said before. `pip install qdrant-client` and bring in some new deps:\n\n\n```\nimport qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n```\nNow we\u2019ll initialize the Qdrant client, attach it to a storage context, and give that storage context to our index when we initialize it:",
        "24cc01c1-3ddf-44ee-8364-736f908a589c": "The result is a binary response \u2014 either \u201cYES/NO\u201d.\n\n* YES \u2014 Response and Source Nodes (Context) are matching.\n* NO \u2014 Response and Source Nodes (Context) are not matching.\n\n\n```\nfrom llama_index.evaluation import ResponseEvaluator\n\n# build service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n# Build index and get response object\n...\n\n# define evaluator\nevaluator = ResponseEvaluator(service_context=service_context)\n\n# evaluate using the response object\neval_result = evaluator.evaluate(response)\n```\n![](/blog/images/1*6H0F9yAXuIMxoIReDN9eKw.png)**Query \\+ Response \\+ Source Nodes (Context)**\n-----------------------------------------------\n\nThis function answers the question: Are response generated, source nodes (context), and query matching?\n\nOften with the \u201cResponse \\+ Source Nodes (Context)\u201d approach, the response generated is in line with the source nodes but may not be the answer to the query. Therefore, considering the query along with the response and source nodes is a good approach for a more accurate analysis.\n\nThe goal is to determine if the response \\+ source context answers the query. The result is a binary response \u2014 either \u201cYES/NO\u201d.\n\n* YES \u2014 Query, Response, and Source Nodes (Context) are matching.\n* NO \u2014 Query, Response, and Source Nodes (Context) are not matching.",
        "67f77c37-3c36-4b6e-af39-a43e1fad63e0": "```\n# Prompt variation 1 \nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over time\")\n```\nResponse:\n\n![](/blog/images/1*AbOFy6vaAQ3vijwhV5V1ug.png)\n```\n# Prompt variation 2\nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over quarters\")\n```\n![](/blog/images/1*LgCbvrqwEGROw_vma0l4QA.png)The main difference between these two queries is \u201cover time\u201d versus \u201cover quarters.\u201d As we can see, not only are the selected Tools different between the two variations, but the inputs are different as well \u2014 in the first it\u2019s \u201cfinancials\u201d, and in the second it\u2019s \u201cRevenue growth and risk factors.\u201d\n\nSince the Tool input in the first variant is unrelated to the question, the answer is similarly vague: \u201cUber\u2019s revenue growth and risk factors can be analyzed by comparing the financials\u2026\u201d\n\n**Query 4:**\n\nHere instead of asking a compare/contrast question let\u2019s just ask a question about a given statement.",
        "760eba99-6c4b-452b-b5ba-47e00e45fa96": "Tap into Prosper AI, your intelligent sidekick for personalized \\& optimal financial advice.\n\nprosperai.vercel.app](https://prosperai.vercel.app/?source=post_page-----c135681bb6f0--------------------------------)[https://www.youtube.com/watch?v\\=\\_\\-v0BhFPjAQ](https://www.youtube.com/watch?v=_-v0BhFPjAQ)",
        "dbb72fef-bfc2-4f3f-b76e-cff337059a36": "Hello, Llama Followers! \ud83e\udd99\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to share some exciting updates about our products, including LlamaCloud, LlamaParse, and LlamaAgents. You\u2019ll also find success stories with LlamaCloud, extensive guides, in\\-depth tutorials, and information about upcoming hackathons.\n\n\ud83e\udd29\u00a0**The highlights:**\n---------------------\n\n1. **LlamaCloud Updates:** New features including LlamaCloud Chat, enhanced Teams collaboration, and expanded integrations with Notion, Slack, Jira, and SharePoint. [Blogpost](https://www.llamaindex.ai/blog/the-latest-updates-to-llamacloud), [Tweet](https://x.com/llama_index/status/1814363518726222119).\n2. **Scaleport AI\u2019s Accelerated Development with LlamaCloud:** Scaleport AI boosts development speed and sales with LlamaCloud and LlamaIndex, improving data handling and OCR accuracy across multiple industries. [Blogpost](https://www.llamaindex.ai/blog/case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud).\n3. **Claude Sonnet\\-3\\.5 Integration with LlamaParse:** Integration of Claude Sonnet\\-3\\.5 with LlamaParse improves chart understanding and data extraction capabilities. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/claude_parse.ipynb), [Tweet](https://x.com/llama_index/status/1813249175817232782).\n4. **Multimodal RAG Cookbook:** A new guide for processing text, diagrams, charts, and tables in slide decks using LlamaParse, LlamaIndex, and GPT\\-4o. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/multimodal/multimodal_rag_slide_deck.ipynb), [Tweet](https://x.com/llama_index/status/1812963306032013586).\n5.",
        "d12ef9d6-e0ed-4bf9-a0b3-823e0b0c45fc": "Hello Llama Friends \ud83e\udd99\n\nLlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we\u2019re taking a stroll down memory lane on our [blog](/llamaindex-turns-1-f69dcdd45fe3) with twelve milestones from our first year. Be sure to check it out.\n\nLast week we had a blast with all the new things from OpenAI Dev day to learn and explore at LlamaIndex. There was a [special edition newsletter](/llamaindex-news-special-edition-openai-developer-day-e955f16db4e2) with the things we released the same day as the conference, but this week\u2019s newsletter is full of follow\\-up releases and explorations \u2014 don\u2019t miss our slide deck summing up all the new features!\n\nAs always, if you\u2019ve got a cool project or a video to share we\u2019d love to see it! Just drop us a line at [news@llamaindex.ai](mailto:news@llamaindex.ai).\n\n\ud83e\udd29 **First, the highlights:**\n\n1. **Multi\\-Modal RAG Stack:** we unveiled Multi\\-Modal RAG \\*\\*\\*\\*for complex Q\\&A on documents and images, with new text/image queries and retrieval solutions. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb), [Tweet](https://x.com/jerryjliu0/status/1723076174698672417?s=20), [Blog post](/multi-modal-rag-621de7525fea).\n2.",
        "15be087c-300c-47e6-a35b-648fd1dbea0a": "* [Guide](https://x.com/llama_index/status/1789694066382537079) to using RAG for content moderation: [CloudRaft](https://twitter.com/cloudraftio) shows how to set up a RAG pipeline to moderate user\\-generated images effectively, ensuring compliance with predefined rules through techniques like semantic search and efficient inferencing with small LLMs.\n\n**\u270d\ufe0f Tutorials:**\n-----------------\n\n* [Kxsystems](https://twitter.com/kxsystems) advanced workshop on \"Building Advanced RAG over Complex PDFs with LlamaParse\" to demonstrate how LlamaParse can tackle the challenge of extracting diverse elements like text, tables, images, and graphs from complex research papers. [Video Tutorial](https://www.youtube.com/watch?v=EL9lCOgLR58), [BlogPost](https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b), [Notebook](https://colab.research.google.com/github/KxSystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb).\n* [Arslan Shahid](https://twitter.com/naivebaesian) [tutorial](https://medium.com/firebird-technologies/generate-powerpoints-using-llama-3-a-first-step-in-automating-slide-decks-536f5fcb6e0e) on Generating PowerPoints with Llama 3, using LlamaIndex to create a Llama3 RAG pipeline. The approach not only answers questions but also generates PowerPoint slide decks by utilizing the python\\-pptx library to write code programmatically for slide creation.\n* [Hanane Dupouy](https://www.linkedin.com/in/hanane-d-algo-trader/) [demonstrates](https://www.linkedin.com/posts/hanane-d-algo-trader_introspective-agent-llamaindex-financial-activity-7193317247342243841-kqor/?utm_source=share&utm_medium=member_desktop) Building a Financial Agent that can Perform Reflection.",
        "649722b6-5562-4e6c-9764-fe7e20e61b02": "```\nchat.chat(\"what was a distinctive characteristic of that programming language?\").response\n```\n\n> \u201cA distinctive characteristic of Lisp is that its core is a language defined by writing an interpreter in itself. It was originally designed as a formal model of computation and an alternative to the Turing machine. This self\\-referential nature of Lisp sets it apart from other programming languages.\u201d\n\nFor more information on how to use chat\\-engines, check out the [documentation](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html), and for more information on other query capabilities with LlamaIndex, check out the full documentation [here](https://gpt-index.readthedocs.io/en/latest/index.html).\n\nSummary\n=======\n\nLlamaIndex makes it super easy to populate VectaraIndex with content from any document or data source, while utilizing the Vectara service for managing the document processing, chunking, embedding and making all of this data available for advanced retrieval in query time using the LlamaIndex library.\n\nVectaraIndex is based on the new LlamaIndex Managed Index abstraction, which better supports GenAI platforms like Vectara, and enables additional vendors who also provide end\\-to\\-end platforms to join in.\n\nTo get started with Vectara and LlamaIndex you can follow the Vectara quickstart [guide](https://docs.vectara.com/docs/quickstart) to setup your account, and the examples above with your own data.",
        "b4e47904-6886-4f04-b660-60b1c13071e8": "4. **ColBERT Integration:** Document reranking with ColBERT via LlamaIndex, delivering a solution that is about 100x faster than BERT\\-based models for more efficient data processing. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb), [Tweet](https://x.com/llama_index/status/1760830777179471933?s=20).\n5. **Counselor Copilot \u2014 Social Impact Through RAG:** Spotlight on Counselor Copilot, an innovative RAG project supporting the Trevor Project\u2019s crisis counselors, providing real\\-time assistance with context, suggestions, and actions to aid LGBTQ\\+ youth effectively. [BlogPost](https://blog.llamaindex.ai/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3), [Tweet](https://x.com/llama_index/status/1761433854458614075?s=20).\n\n**\u2728 Feature Releases and Enhancements:**\n\n* We have launched a new chunking strategy to enhance RAG retrieval: Sub\\-Document Summaries. This approach overcomes the limitations of naive chunking by injecting hierarchical metadata, offering a nuanced balance of global context awareness and precision through subdocument summaries for improved performance. [Notebook](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb), [Tweet](https://x.com/llama_index/status/1761793821422264757?s=20)."
    },
    "relevant_docs": {
        "fd210e54-14e8-4bb5-8218-e1bee0d3e996": [
            "d6b72243-ae05-482e-a9f7-d2baa5d6e6eb"
        ],
        "8f3f66cd-9a9b-47a0-8271-99c79b80900d": [
            "4c5c5e65-f38c-4f1d-bf60-88cf9b07f91a"
        ],
        "55f68166-ec37-43c7-bf6e-846acafc16f7": [
            "e5ec79af-b4f0-4b6e-a082-9caa6725a2e8"
        ],
        "833031ce-f277-4e30-aeb9-23398af5371b": [
            "47648e02-d7bd-4db4-82e3-6cefe5937274"
        ],
        "d6548cae-4ed4-48bf-a1ee-f9d56941c9cd": [
            "7de5c654-daa4-4138-8185-4af1b48baa85"
        ],
        "cb7273aa-4b1e-4e64-b64e-74c6aedd8335": [
            "28f358d8-5a94-42df-92c7-4024f409fcf9"
        ],
        "ea662649-09c3-4b47-9b89-d6db9ad8d8ae": [
            "cf8cfb96-c6d0-4ac8-a5aa-ce8366465794"
        ],
        "cc42664a-2abf-4c88-b90f-16c1e86ddefa": [
            "f349384f-8dd5-45ee-a804-010ad95dff70"
        ],
        "cabea52f-ae33-4f05-bf2a-6cdc7a75bc9f": [
            "0d5c8b48-bb7e-4787-8c4d-ce597fc1af57"
        ],
        "f2b83b45-0573-4a34-bbf4-27cf9f986464": [
            "9f4a97a8-31b3-40e7-8ffe-3886751271d7"
        ],
        "e19314cb-00ec-45dd-a8eb-b35bcae08173": [
            "b465c492-d1bc-4b00-a0ce-7b04e7a3bc3a"
        ],
        "dc144ef4-1f28-4c05-bea8-6ec2ea57ece3": [
            "ff9b1746-47e4-4cf2-8bbb-28e99649e42c"
        ],
        "d9134c70-8ce6-4122-b89d-118e8502f33a": [
            "78e7c79b-b763-40f9-8abf-41c7f8ac9533"
        ],
        "d868e129-490a-4c8f-9248-964008176054": [
            "334ee959-8526-46fa-a9d5-a18eff8dc70f"
        ],
        "dc627d58-bb1d-46f7-9b3e-24c57c896973": [
            "dda19e05-78f1-4b7c-9fa3-35bed5ae004e"
        ],
        "71ff7f2c-710d-4ca1-ae95-bafa7d6052be": [
            "5d560f43-2c76-4fac-9024-cb97446fba4c"
        ],
        "90a8d1a0-6d05-4aaa-86fd-e0f1084c15bf": [
            "bbf78d7f-e9d7-4466-a9f2-f2bd2e909ec3"
        ],
        "ed7388aa-29b4-4dee-af3d-3af95ee63fde": [
            "73550e84-1add-47fa-bd66-f36cde76c2d2"
        ],
        "32b39c45-d427-4492-9b06-0f5bba2793ba": [
            "26675a9e-43a5-47d5-af4c-b9dcf5c8ad87"
        ],
        "034c867e-5362-4f3e-91d8-142f468d1e72": [
            "e76de264-33d6-40aa-8d30-d42f885fb265"
        ],
        "1d7f5757-5580-49ec-887e-c618dd597207": [
            "9836f5a6-c1a0-4a2d-8a6b-5b8f6f78f1be"
        ],
        "bb92f150-3eb2-4a2a-91c2-12eb8d4a3f2f": [
            "daf094d9-74a4-4a49-806c-beeec5238112"
        ],
        "18068179-93c5-4c52-900f-29cd9c429712": [
            "1d6a476c-34ee-482c-8b66-0a6fedd7cd6e"
        ],
        "a05943ce-3a70-4e30-bfcf-a83b66a91f65": [
            "670dca29-b705-45d6-854a-19dfa267ffd8"
        ],
        "9429228e-16e8-4881-8981-e7b92ae714ff": [
            "b9641ce0-da8e-4477-8ddb-bfa82c9483a6"
        ],
        "b78a568f-5c0c-4c57-bb98-e2d8b5f989d9": [
            "e92059a1-2fac-419d-97da-4b7974b45eaf"
        ],
        "6413d9aa-52b7-4ce2-861b-c6278b57f517": [
            "fa4ab7e9-21fc-4990-8801-68ebd6f0073b"
        ],
        "1af08f51-af63-4df1-8c09-d8cdcd92c167": [
            "2efada7e-1d9e-4a86-8c06-ddd7390d28be"
        ],
        "3adfa49a-ea28-4d2a-b727-c4cb804c2cdd": [
            "b5654a79-ebbf-4578-8fce-15dd5ca65edf"
        ],
        "47569011-39fd-4878-9cee-f0e974f23303": [
            "67087298-e5c4-4322-b864-82a968e35647"
        ],
        "8b41aa15-fd5c-4586-9a95-be6d29286b99": [
            "c84c9775-b5b6-4b2c-a9fe-11b51c6219d1"
        ],
        "5871f70a-b880-4af5-8691-e96ec30213e7": [
            "ad2fdb22-ea03-4351-b38c-6fc904ee32cb"
        ],
        "fbe0a312-d4e9-4946-8521-42df40bb9cef": [
            "ca6c28ce-aa0d-4160-a0d6-17ccaa9a04bc"
        ],
        "6687d5ac-9256-43ab-a2ab-2fa8c37be92b": [
            "7724721e-b199-4483-b4e9-a0b461f0fc97"
        ],
        "4ee0120f-5866-4641-a271-d2ad1040efe9": [
            "23acaa41-358e-446c-9758-9c68b8856ff5"
        ],
        "7ee3beee-2d73-443e-9a7b-70339c439f0b": [
            "4d29dc15-da2d-43f3-b5e5-d025432d530b"
        ],
        "eaaeb28b-d4f2-45c1-8f71-29b00780a472": [
            "ee2d50df-7561-415c-b0c4-144aa1b4d636"
        ],
        "949c9ff0-225b-49fb-bf8f-121628fb5413": [
            "ff794620-3e03-4e31-8564-adbbedb81b3c"
        ],
        "53a37f16-aea7-409b-8f3e-baa7b3b10c6a": [
            "31c6946b-66a2-437f-9e58-4cd4b88945ac"
        ],
        "84a2e04f-3d73-4ad6-8302-dea02fe19bc0": [
            "18144f76-05c7-45af-a38b-a77f074eeeb7"
        ],
        "db298c6e-b289-4b33-a7f9-3bb09b98958a": [
            "44f05295-133e-47f8-99d2-a118c9bf2e24"
        ],
        "8e9493d2-97c2-4452-b0f8-319fe810ff4b": [
            "930862e5-44e9-40d2-b1fd-a76b2bbb4b48"
        ],
        "dd1a9b12-6404-423d-8bc4-55d827b8bef9": [
            "8fceebc9-752f-4873-9b25-6ae251df3329"
        ],
        "085dad1d-f991-4e7f-b298-8e1dd9eb7969": [
            "bed7c1c0-bfee-4e7f-89fd-9be94eeddc2b"
        ],
        "ab5968d4-e001-48fa-8e23-9ef868c2992e": [
            "7d229610-a21a-4187-8591-b103acf2fb24"
        ],
        "f743d7d4-7a06-473f-99fe-6593c7fa7b11": [
            "23d0308c-8de2-4936-a0bf-5463ff1c6954"
        ],
        "3a361519-2038-4f8e-94b9-4fc888c90383": [
            "9836f5a6-c1a0-4a2d-8a6b-5b8f6f78f1be"
        ],
        "0b87f703-56d5-400c-beb4-82cc3daa5e2b": [
            "e11a5645-cdf0-4cee-bbfb-b009223d21d0"
        ],
        "3eb5fcec-cb3a-4d0c-a597-285a797596df": [
            "8bfac9a6-aa3a-422e-9243-d2536daea37f"
        ],
        "b30364cd-9e31-4d64-ac6e-4481af44c8c0": [
            "fe81af90-14e7-4da6-b055-067babd1c63e"
        ],
        "f7804ffb-8af0-4089-8fda-ee9b5dad6c50": [
            "9b107e00-0b77-4104-ba87-1ff273e13346"
        ],
        "3bf5931a-1775-460b-a705-13d24933ccb2": [
            "d5dfcfad-6a79-4222-be54-5ec0a2dd43dd"
        ],
        "b55e6200-1cf7-4bff-bc52-cf0cba3e8906": [
            "c90c2e59-dd80-48f5-b680-27957083a5e6"
        ],
        "39c67be5-fdaf-4fb7-9310-de7df3cc3f47": [
            "02a2d5fd-5381-49ed-ba74-0a582dc562bb"
        ],
        "5ce2cc3a-8950-43fd-ac77-272444744c52": [
            "981ac84e-e1de-4980-a684-eaf10705712d"
        ],
        "c5aa5915-0a46-4571-9311-b23ed17b9236": [
            "41c82305-7279-458f-a60b-621a77cf3c2f"
        ],
        "c1dab154-8894-44f3-be53-a7ecbaccd720": [
            "41b94dfa-c366-4f67-a436-b0b91ff0a730"
        ],
        "1960eadc-7b01-44e7-96a4-ba0018a41f0f": [
            "2cf7b3fc-36de-4e4c-ae26-9b36f659de6b"
        ],
        "a523edbd-ae53-448a-83ed-f1b8285e5a4c": [
            "4eda6d3b-be44-4008-905f-122738265904"
        ],
        "38d2b116-f533-46a9-b177-7d8540879fc8": [
            "4541e65c-651a-41d2-b89c-0981a5c68fb9"
        ],
        "8e362a0a-44a7-4e05-8246-2fd63cc8c7da": [
            "595e43e1-7925-475b-93a5-784a6f6d411a"
        ],
        "f47c84d7-fdb6-4998-b8bb-56e763b3fa50": [
            "ca412662-0a1d-4f7f-bb5b-c8e2fe5f55bd"
        ],
        "84460b9e-67ce-49a7-aec5-93cd5e77daa2": [
            "a9aaecfb-1a4d-4159-8eca-16cdf2e1edc5"
        ],
        "4c672820-584d-42e2-a920-4f4bbe79b4a1": [
            "8be6782b-5389-4f2e-84d9-45f7ba7ba1f7"
        ],
        "f5891cb4-4d07-4f50-9c7f-9a4e7b1ba4f9": [
            "65cabb91-e2c4-4934-acdd-df23c2226968"
        ],
        "2623b299-ca37-4503-b3ec-b398f218feb7": [
            "4ac9fdd4-fc25-4c43-907d-75334d11ec1b"
        ],
        "2dd4c96d-b5dd-4da7-8438-45544e965384": [
            "abb41273-efc2-4e2a-9e74-12b51d3714d2"
        ],
        "5404c6e4-b8b5-40e4-84d0-abdbb27230fb": [
            "8daac2dd-882c-4413-9813-2a2ca10e08d3"
        ],
        "f7014d5f-ef03-4859-b5c1-0efeb6d1c634": [
            "68848c1c-0fbd-4bbc-adef-936ab5c866ae"
        ],
        "9f6a1be6-71af-4595-a84d-f48c618a22e4": [
            "b643619d-bdc5-4784-876a-78a77475a3a6"
        ],
        "492025f4-129c-4a70-a62f-c31fcd789222": [
            "8112a259-d590-46fc-8242-1b2a14bed8f3"
        ],
        "54929c0f-730e-4d0d-8a54-f7cb08cea19c": [
            "49be2e54-448a-46de-86f3-ccf25ea12db8"
        ],
        "6032bbb7-e924-4f0a-8f0f-9dbde05a657c": [
            "f710a4e0-9e48-4726-91a7-11b1ad03cc9a"
        ],
        "be95270f-aa5d-4502-8995-b32810d6a39a": [
            "ad8c9ab5-e220-48ba-a933-0f70230312b3"
        ],
        "83092fcc-e8d6-485d-b072-58e11e952cbe": [
            "d1f50639-cdda-4853-b649-e013d2f87a5c"
        ],
        "5a0eec1e-14ed-4709-8d5c-41880c6a0099": [
            "99ef97a9-3884-408c-84dd-1c1bb6424c7c"
        ],
        "ec5ca094-a893-4ad1-9a62-e435e2fec270": [
            "559d0812-19c4-4a82-9830-f1d4b22fd922"
        ],
        "28761c53-3a70-42fa-b201-37b7191fde67": [
            "70697641-2f7f-4196-9f4e-e040f40d4486"
        ],
        "5cd58ebb-2ecf-427b-a018-7752adb66dc4": [
            "3432d1bd-87d4-4b1c-88ed-91cd6bcacfe9"
        ],
        "1b24a4ff-9575-4194-9d56-ca2516ef61d9": [
            "c90c2e59-dd80-48f5-b680-27957083a5e6"
        ],
        "6f6ad0c2-dca9-4903-81c8-4307660c2ba5": [
            "c974a44a-4599-4337-8c8a-770bf2763753"
        ],
        "8f6d5b8e-c1df-4d78-957f-545df2fb6977": [
            "58924869-2ddd-424c-92ae-969178d42cbc"
        ],
        "b953daec-6af2-4043-bd54-3cff9627d97b": [
            "2c46711b-f241-4de2-ac88-adddabddbda2"
        ],
        "82008236-a527-4d06-b66b-42b269beb3ed": [
            "447bede0-09ff-4f26-9c60-476655780b22"
        ],
        "18c93aff-6132-4c95-a39f-e40f863861ef": [
            "d35c616b-b780-469f-9b6d-b0816863848d"
        ],
        "145b73df-548c-467d-977b-abb6603ba72b": [
            "70697641-2f7f-4196-9f4e-e040f40d4486"
        ],
        "339468e1-66fd-405c-8d0d-f101f8214200": [
            "01d9c739-46d4-4b21-8b4f-d658f346810f"
        ],
        "969711c8-ebc7-4776-a524-e6db3ee57c28": [
            "96fe23f7-12e6-4762-8313-5145729561be"
        ],
        "b12ad8e8-a756-4855-8148-638456a2f993": [
            "21905bb7-13a7-40f4-b107-5497ce0a186b"
        ],
        "8d24ac32-6e3e-48b5-9590-0e0cacf9a260": [
            "897fe4a9-088d-43bd-9107-a4a5750b27a4"
        ],
        "9da4835d-2e55-44d9-ab13-037636f3d547": [
            "2ab03df7-eb8c-4569-98ed-eec0ae902b7e"
        ],
        "b4198b63-b6a7-41cf-a5ed-0e4032412b1c": [
            "910607a0-1710-413c-b06f-17cdc94c9d1a"
        ],
        "a6c57119-8311-4d1e-8f48-669374495710": [
            "24cc01c1-3ddf-44ee-8364-736f908a589c"
        ],
        "10845c44-67eb-4871-9fc8-783598b505c9": [
            "67f77c37-3c36-4b6e-af39-a43e1fad63e0"
        ],
        "5924da84-d566-4382-8f1c-032db606e899": [
            "760eba99-6c4b-452b-b5ba-47e00e45fa96"
        ],
        "a93e52b7-ba03-4fbf-8e5a-31ab72f21283": [
            "dbb72fef-bfc2-4f3f-b76e-cff337059a36"
        ],
        "9969d2ed-de4e-4222-a4af-8518160ad329": [
            "d12ef9d6-e0ed-4bf9-a0b3-823e0b0c45fc"
        ],
        "ab6e16fa-5f63-4c05-a82c-3cceda3c5389": [
            "15be087c-300c-47e6-a35b-648fd1dbea0a"
        ],
        "a1badba4-25e6-48e4-b5f8-22f1ea7ca818": [
            "649722b6-5562-4e6c-9764-fe7e20e61b02"
        ],
        "f1d78f34-d5c6-4e7a-8b84-56b83df1adea": [
            "b4e47904-6886-4f04-b660-60b1c13071e8"
        ]
    },
    "mode": "text"
}