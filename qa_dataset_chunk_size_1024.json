{
    "queries": {
        "4bd4a58b-eff2-442d-b0b9-6b0c0b60bb9e": "How does the `LabelledPairwiseEvaluatorDataset` differ from the `LabelledEvaluatorDataset` in terms of the evaluation task?",
        "8b5ff9c1-fa4e-4bbf-aecd-c1a5804d597e": "What are the advantages and disadvantages of using a more complex and unconstrained agent interaction technique like ReAct with simple tools compared to a simpler and more constrained interaction technique with complex tools?",
        "e08dd455-4ab1-4d58-b365-0d7f9cd67c56": "How does the `Alpha` parameter in hybrid search affect the balance between keyword-based and vector search?",
        "94a717cd-877d-4abf-9180-b23b597b19d9": "What are the potential risks associated with using LLMs in enterprise applications, and how does LLM Guard address these risks?",
        "57f14311-55e0-4367-aece-62a4a91d1627": "What is the primary benefit of using this classification service for businesses?",
        "ac5194ba-01c9-4491-9dfb-e8b58967bbe6": "What are the key features of LlamaParse that make it a GenAI-native document parsing solution?",
        "9943db79-349f-475e-a594-18ff93483229": "How does the `MyCustomRetriever` class handle entity detection in the `custom_retrieve` method?",
        "fa90f859-0fa4-4f76-8c0a-b8bd7bc2c3e6": "Explain the limitations of traditional OCR technology in extracting information from images.",
        "c78fe20d-181a-4578-a698-df8ebb2f9eb9": "What are the key functions of the `app.py` script in RAGArch and what are their roles in the overall functionality of the tool?",
        "17fcfb4e-365b-4704-8129-bee59a2fe65d": "What are the key considerations for evaluating the performance of a Retrieval Augmented Generation (RAG) application, and how can these considerations be addressed using tools like Arize AI Phoenix and Llama Index?",
        "777638bc-05fe-4a76-b4ad-378756934e0d": "How does the `select_llm` function handle the selection of different Large Language Models (LLMs) and initialize them for use?",
        "5d96263c-2a55-44f3-935c-3492bf5a8b9e": "What is the purpose of the \"browse\" function call in the provided code snippet?",
        "9b327dab-7d37-47f2-bc02-c6cfa95b6127": "What is the name of the AI.engineer Summit where Jerry Liu spoke about building production-ready RAG applications?",
        "5822b3b1-5ab3-4e84-b577-e27ca20a780e": "How does the choice of `chunk_size` impact the efficiency and accuracy of a Retrieval-Augmented Generation (RAG) system?",
        "5c2c8e68-5e43-475e-bbcf-5dc834d18307": "How does the `create_uuid` function ensure that each node has a unique ID based on its date?",
        "76eb6ff9-a484-4239-acac-f311403e5c4d": "How does the continuation agent determine if the user has completed their task?",
        "f7d320fa-ea1a-4164-a5ad-7224e783eb7a": "How does the `LocalLauncher` class facilitate testing a multi-agent system locally?",
        "3d26082b-d82d-49ca-a71d-96fee2959a57": "How does the code handle the situation where the user's speech contains the wake word \"c3\" while the app is already in the \"listening\" state?",
        "91e3303f-9354-451f-bd98-70049fc2a158": "What are the steps involved in using Airbyte sources within a LlamaIndex-based application?",
        "d7999cb0-c5cf-4304-b443-80fdccae78f6": "How can LLMs answer questions using private knowledge sources like your company\u2019s data and unlock its true transformative power?",
        "f04c7b56-1771-4bb8-a623-0e9f7580e2b0": "How does the Waii Service utilize semantic rules to enhance query generation?",
        "86872f28-51c7-44f3-9a39-acaa36cf77fd": "What is RAFT and how does it improve domain-specific RAG performance in LLMs?",
        "104ab255-146d-4b28-9d98-e57405458e55": "What are the two parameters used in the provided code to identify potential duplicates in a graph, and how do they contribute to the process?",
        "60a77268-68ff-42d7-a8ed-f089e14e19ba": "What are the limitations of using the `ILIKE` operator in the provided SQL query for identifying Christmas sales?",
        "eff68700-74fa-435d-87bf-8ce234fecdb3": "How does the `FixedRecencyPostprocessor` class help in sorting the results from the vector store?",
        "7cd9fa97-4cc0-4058-b0f6-e34714bf2f08": "What is the purpose of the `prometheus_relevancy_eval_prompt_template` and `prometheus_relevancy_refine_prompt_template` variables?",
        "80e25c83-e23c-4ae5-95d4-d7fd553a8d8a": "How can you ensure that documents ingested by different users are kept separate and only accessible to their respective users?",
        "0fa38b4b-37b5-42cf-a67a-bbd44db8290e": "What are some common pain points encountered when building a RAG pipeline, and how can these challenges be addressed?",
        "6b302f47-e9a7-4710-b2c1-206210b3ed50": "What are the advantages of using a message broker like RabbitMQ for document ingestion compared to a REST API?",
        "f0a5ed7b-c7f4-439a-b04f-ddcb2a3e50b2": "How does the use of cross-encoders in reranking systems address the limitations of traditional reranking methods that rely on historical user interaction data?",
        "84c35a1b-980a-49a1-8cd7-76f0b80f904d": "What are the key features of LlamaIndex's integration with Elastic?",
        "bcea88d5-7ac3-425a-821c-07aea857d09b": "Explain the limitations of traditional OCR technology in extracting information from images.",
        "7ad50fec-3974-48cd-abf3-e5077d094957": "What are the potential benefits for data consumers in a RAG marketplace compared to traditional data marketplaces?",
        "5de3ddff-0b14-4a7d-acb0-d7be738ca849": "How does LlamaIndex integrate with Azure Container Apps to ensure secure code execution?",
        "d8ea6c4c-da1f-4cae-baa8-25c2604f755d": "What are the benefits of using LLMs for re-ranking retrieved documents?",
        "a2350d6a-a006-432d-b73e-542d2e7bb8fc": "How does the Text2Image Prompt Assistant tool use LlamaIndex to rewrite prompts for generating more beautiful images?",
        "c135f1d6-d497-4e47-9bf7-ce264b962c35": "What are the key differences between text-only RAG and multi-modal RAG in terms of RAG build considerations, query-time pipeline, and the role of the LLM?",
        "5a82424f-c02b-476a-ae19-ae6b706182df": "What are the key features of the new V1 agents in LlamaIndex?",
        "59fb5241-f066-4119-917c-19b1cede18a0": "How does the agent use the Azure Container Apps dynamic sessions to execute Python code and return the answer?",
        "2d3e469e-0d94-4494-aa74-43a9e705f64c": "What are the benefits of using the QueryPipeline in LlamaIndex?",
        "862add0e-847c-4fb6-8dd7-b6e03af74326": "Describe the process of training ChatGPT on a website's content using LlamaIndex and EmbedAI.",
        "cc3e9d8d-f114-46ae-bc0c-848718151c7d": "What are the results of adding 5 and 5, and then multiplying the sum by 2?",
        "19b12786-b35b-4fd6-a64a-724ca4a32cc9": "Which model in the 7B series has the highest performance on the SAT-en (w/o Ps_8) task?",
        "d312110c-4c7a-4a54-9ba7-66383cc62806": "How does the code snippet using `streaming_write` enable streaming capability in the Streamlit chat component?",
        "a266e0e2-7e0a-4dbe-8fc0-decf23f29622": "How does the orchestration agent determine which sub-agent to run next based on the user's request and the current state?",
        "81cc8880-4162-4751-985e-d4d6abb6a5ce": "What are the key challenges faced by crisis counselors at The Trevor Project?",
        "eaa1d1a1-b09e-40a0-9113-1444f6d4801a": "How does the `TextCleaner` class work to remove special characters from text before embedding?",
        "62cc5888-d4d5-437c-a236-43c2b3be2700": "What are the benefits of using LlamaCloud's managed ingestion and retrieval API for enterprise developers?",
        "8e083de4-fb44-4aea-b3be-7e2af35682d5": "What is the primary benefit of using this classification service for businesses?",
        "dd7d895f-737d-485f-8037-910ea121bab0": "What are the names of the individuals presenting the webinar on retrieval-augmented fine-tuning (RAFT)?",
        "2171ec17-9dc0-41c0-b134-81ca4b2fd033": "What are the two main features of LlamaCloud and what are their respective purposes?",
        "0e488651-a4f4-487a-a0dc-3a2bb7375782": "What are the different categories of safety violations shown in the bar chart?",
        "7e1cb00d-7fc5-4a40-bc39-94f83af28f64": "What are the key features and enhancements introduced in the fifth edition of the LlamaIndex Update series?",
        "96d6cda2-7d63-41f3-bf32-38f4dbdbbc7b": "How do the `PydanticOutputParser` and `PromptTemplate` modules in `langchain` contribute to the quality of output generated from retrieved context?",
        "d6b63a3d-a5fe-41ba-a737-4d56068645f0": "What are the key challenges faced by companies when building production-ready LLM applications, according to the provided context?",
        "ecbd28af-c60e-412f-acde-366d397eb240": "What are the advantages of running a large language model (LLM) locally on your laptop?",
        "a9f94d73-a8ec-40a6-9722-fa26b02c2490": "What are the key features of the Arize AI Phoenix integration with LlamaIndex?",
        "eaee1bb6-ee82-48be-b06b-1a9641bb48d1": "What are the challenges in evaluating production RAG systems, and how do Llama Datasets address these challenges?",
        "0dc35933-8b54-48d7-9d55-34794d0065b5": "How does the code handle the situation where the user's speech contains the wake word \"c3\" while the app is already in the \"listening\" state?",
        "23dfcc9c-72ac-4ddf-be47-920138e59a00": "What are the key challenges associated with naive RAG approaches and how does the course address them?",
        "dacf0a21-f279-44bf-aa4a-a1bde8eaac5b": "How can LlamaIndex be used to build a knowledge graph using local models and Neo4j?",
        "810cab1e-f08a-46a8-9a84-31eb77bc9be8": "How does the `LocalLauncher` class facilitate testing a multi-agent system locally?",
        "5cfc28db-573b-4b94-bb0c-d468d1891533": "How does Laurie Voss's file organizer project leverage LLMs to categorize files without renaming them?",
        "c1023d7c-b258-43b1-850e-140016ee3d00": "What are the common challenges associated with naive RAG systems and how can these challenges be addressed using advanced RAG techniques with LlamaIndex?",
        "f7ce5589-eff1-480b-ae00-d93ba2256165": "How does the continuation agent determine if the user has completed their task?",
        "5f0a9f9d-7638-48be-989e-a797d95c8523": "How does the `OpenAIAssistantAgent` abstraction facilitate the integration of OpenAI Assistants API with vector databases?",
        "e956ac15-af8e-4f2f-823b-4f4dcf17ddc4": "What are the key components needed for agentic RAG, as discussed by Jason Zhou?",
        "8013b35f-6b87-456d-8e56-34e8e8f96b5c": "What are the key features of the HoneyHiveAI integration with LlamaIndex?",
        "6f719b48-9bb2-42b0-b3c0-2a38e6a89cd7": "What are the benefits of using a message broker system like RabbitMQ for document ingestion, compared to a REST API?",
        "2db52693-bcbb-4889-8a41-31632cbefbd4": "What is the purpose of the `concat_rows` parameter in the `SimpleCSVReader` class?",
        "1366bae5-879d-4d8a-bee8-703269b838c2": "What are the different rerankers used in the provided code snippet?",
        "220591f1-de35-40f1-96bf-f076bcb2ed8b": "Explain the challenges associated with parsing PDFs and how these challenges impact the extraction of structured data.",
        "98183bc4-6907-4fee-9885-487fb6b087b2": "What are the benefits of using RabbitMQ with LlamaIndex for multi-agent communication?",
        "62f286c1-07fc-4779-9e31-1c9740d6b76c": "How does Azure AI Search combine vector search with traditional retrieval and ranking capabilities to enhance search performance?",
        "ea9a87ca-5107-4ca2-9307-e117dbabde72": "What are the key benefits of using the \"return_direct\" feature in tools within LlamaIndex?",
        "875e9ad8-f241-44f5-a7c8-87893012110a": "How does the `temperature` variable in the `prompt_assistant` tool affect the generated image prompts?",
        "5f729726-df9c-4480-9d3e-a33887f0c5e0": "What is the name of the repository used for finetuning the embedding model?",
        "c04069b6-4464-42e2-8f63-d5d5c1f7ce7a": "What are LlamaPacks and how do they relate to Privacy-Preserving In-Context Learning?",
        "d9dca60e-15d2-4895-b397-80fff430962b": "How does sandboxing help address the potential risks associated with executing code generated by LLMs?",
        "1ad1c46c-9e6a-4a91-8ff6-5c0ed56e2d90": "What is the name of the agent that can navigate the web in a Jupyter/Colab notebook, as mentioned in the provided context?",
        "a1826592-a23d-43cc-8ae1-5cc70c9c27dc": "What is the purpose of LlamaParse JSON Mode and how does it simplify RAG pipeline development for complex documents?",
        "59b3ea63-64f0-46ef-8b04-f29474598015": "What are the key drawbacks of typical Retrieval-Augmented Generation (RAG) workflows, particularly for users?",
        "3f989140-ed0a-497c-ad10-c96802cc1d5e": "How does the complexity of an LLM-powered agent's interaction behavior affect its performance on practical data tasks?",
        "e4347893-5355-40c6-93d8-8316944dedc4": "What are the key technical challenges that many teams and enterprises face in getting modern LLM systems ready for real-world applications?",
        "c1550823-1882-4490-89ed-7eb8170840c6": "How does the `GeminiMultiModal` class handle multiple images when used with the `complete` method?",
        "e755e31a-08d9-44b5-9234-2bc852cfabbc": "How does TruLens help identify and address issues related to the relevance of retrieved chunks in retrieval-augmented generative AI apps?",
        "7f28f416-d6f2-4e57-b49b-77d3ed0adf3d": "How does LlamaIndex's Retrieval-Augmented Generation (RAG) system contribute to the effectiveness of Lyzr's AI agents?",
        "56d53e48-c23c-4e2e-afbf-ccd44e7950a4": "How can you evaluate whether the response generated by a query engine matches the retrieved source nodes?",
        "4b987376-4e62-4841-925c-2313d653ef05": "What are the key components involved in building a Question Answering (QA) system, as discussed in Ravi Theja's tutorial?",
        "08e59e6d-e379-417c-9821-95627cbd55f6": "What are the key features of the Chain-of-Table framework in LlamaPack and how does it enhance LLM tabular data understanding?",
        "f0a20c8d-69cd-463b-a674-fa14d7ffa0d1": "How does VectaraIndex simplify the document processing flow compared to the traditional LlamaIndex approach?",
        "e51d528f-189d-4c61-abfc-4b10252a7828": "How does LlamaIndex optimize chunk size for RAG systems?",
        "6d25e3a7-f564-4fef-919a-d4106a329e91": "What are the key features of the new V1 agents in LlamaIndex?",
        "f4bfab0c-18bc-47f1-b590-af02fed742fb": "How does LlamaIndex's Retrieval-Augmented Generation (RAG) system contribute to the effectiveness of Lyzr's AI agents?",
        "82ffd23b-5e5f-4bbc-a36c-7c18bbd863ac": "How do LlamaIndex connectors facilitate natural language access and retrieval of data?",
        "03dad514-4d1a-43f3-8334-5b6eec978fbe": "What are the key features of LLaVa that make it competitive with GPT-4V?",
        "300eab46-6ba2-4f13-a7ef-650e2739f8da": "How would you design an API to extract information from millions of files stored in a bucket with a single call, instead of requiring a million individual calls?",
        "31e15687-6890-43e2-86ff-92168f7995c7": "How can you ensure privacy when using a local LLM and a local vector store?",
        "4688c652-f222-47c4-ba47-1a87e0ce000e": "How does LlamaIndex leverage LLMs to enhance information retrieval, and what are the five key MetadataExtractor modules involved in this process?",
        "ebf4e3a2-7b8b-4489-a2b7-fa7326a351c7": "How does the `NLSQLTableQueryEngine` convert natural language queries into SQL queries?"
    },
    "corpus": {
        "8e1c05ab-ea2c-44c7-b8eb-7e60c2fc73d1": "Benchmarking flow with LabelledEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of the answer to the query and optional contexts and ground truth answer. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama\\-pack called EvaluatorBenchmarkerPack makes benchmarking a one\\-liner.\n\n![](/blog/images/1*YW5jhukFrlchYpG7PgucVg.png)The second llama\\-dataset we\u2019re introducing today can be considered an extension of the first one. The `LabelledPairwiseEvaluatorDataset` is similarly used for benchmarking an LLM evaluator. However, there is a subtle difference in the evaluation task as here the LLM evaluator compares two generated answers from two separate LLMs. Outside of this difference, the flow for using this llama\\-dataset to benchmark an evaluator remains the same.\n\n\n\nBenchmarking flow with LabelledPairwiseEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of two answers to the query and optional contexts and ground truth answer. That is, the LLM evaluator ranks the two answers to determine the superior one. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama\\-pack called EvaluatorBenchmarkerPack makes benchmarking a one\\-liner.\n\n![](/blog/images/1*AH6xWQz34vbC-kpn4fDgKA.png)Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves GPT\\-3\\.5 performance!\n============================================================================================\n\nIn this section, we will put our new llama\\-dataset types to use in order to pit Gemini Pro against GPT models. For this, we\u2019re going to use slightly adapted versions of the MT\\-Bench dataset. These adapted versions have been made available for download and use through [LlamaHub](https://llamahub.ai) along with today\u2019s release!\n\nMini MT\\-Bench Single Grading Dataset\n=====================================\n\nThis llama\\-dataset is a `LabelledEvaluatorDataset` and is a miniature version of the MT\\-Bench single\\-grading dataset. In particular, we consider all of the 160 original questions (i.e., 80 x 2, since MT Bench is a two\\-turn question dataset), but only the responses produced by Llama2\\-70b. For the reference evaluations, we use GPT\\-4\\. As with the original `LabelledRagDataset`, we\u2019ve produced a new llama\\-pack `EvaluatorBenchmarkerPack` (of course, also made available in today\u2019s release!) to make benchmarking an LLM evaluator on the new llama\\-datasets relatively effortless. The below snippet of code is how you can replicate the results of this benchmark\n\n\n```\nfrom llama_index.llama_dataset import download_llama_dataset\nfrom llama_index.llama_pack import download_llama_pack\nfrom llama_index.evaluation import CorrectnessEvaluator\nfrom llama_index.llms import Gemini\nfrom llama_index import ServiceContext\n```\n\n```\n# download dataset\nevaluator_dataset, _ = download_llama_dataset(\n    \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n)# define evaluator\ngemini_pro_context = ServiceContext.from_defaults(\n    llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n)\nevaluator = CorrectnessEvaluator(service_context=gemini_pro_context)# download EvaluatorBenchmarkerPack and define the benchmarker\nEvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\nevaluator_benchmarker = EvaluatorBenchmarkerPack(\n    evaluator=evaluators[\"gpt-3.5\"],\n    eval_dataset=evaluator_dataset,\n    show_progress=True,\n)# produce the benchmark result\nbenchmark_df = await evaluator_benchmarker.arun(\n\t\tbatch_size=5,\n\t\tsleep_time_in_seconds=0.5\n)\n```\nBenchmark Results\n=================",
        "80efa253-8be1-409a-b576-a0ce6197e4eb": "The pro here is that the model will likely make fewer errors. The con here is that the interaction technique allows for less freedom and imposes more constraints on behavior.\n\nInvestigating Agent Interaction Techniques for Data Querying\n------------------------------------------------------------\n\nWe at LlamaIndex are interested in how agents can help augment data tasks. More specifically, we are interested in how agents can help perform complex user queries over a diverse range of data sources. This includes not only asking questions over a single document, but being able to synthesize insights across multiple documents and return that to the user.\n\nLlamaIndex query engines can be used as Tools within an agent construct to query your data (we provide [seamless integrations with LangChain](https://gpt-index.readthedocs.io/en/latest/how_to/integrations/using_with_langchain.html)). These Tools can vary in complexity. For instance, a *simple* Tool could be our [vector store query engine](https://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html), which does top\\-k embedding retrieval from a vector store. A more *advanced* tool could be a query engine over our graph data structure, which can be setup to [explicitly provide compare/contrast capabilities](https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html#compare-contrast-queries) over any subset of documents. The tool itself can contain \u201cagent\\-like\u201d decision\\-making capabilities under the hood. LlamaIndex provides a variety of modules around [routing](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/RouterQueryEngine.html), [query decomposition](https://gpt-index.readthedocs.io/en/latest/how_to/query/query_transformations.html#single-step-query-decomposition), and [multi\\-part query planning](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html).\n\nIn this blog post, we are interested in comparing the following approaches to designing agents and tools to see which approach can provide good answers to different user queries in a robust fashion:\n\n* more *complex and unconstrained* agent interaction (ReAct) over a set of *simple* Tools\n* more *simple and constrained* agent interaction (simple routing) that uses more *complex* Tools",
        "64e80c06-774a-4f9e-bc2b-792462024824": "Introduction\n============\n\nRetrieving the appropriate chunks, nodes, or context is a critical aspect of building an efficient Retrieval\\-Augmented Generation (RAG) application. However, a vector or embedding\\-based search may not be effective for all types of user queries.\n\nTo address this, [Hybrid search](https://weaviate.io/blog/hybrid-search-explained) combines both keyword\\-based methods (BM25\\) and vector (embedding) search techniques. Hybrid search has a specific parameter, `Alpha` to balance the weightage between keyword (BM25\\) and vector search in retrieving the right context for your RAG application. (alpha\\=0\\.0 \\- keyword search (BM25\\) and alpha\\=1\\.0 \\- vector search)\n\nBut here\u2019s where it gets interesting: fine\\-tuning Alpha isn\u2019t just a task; it\u2019s an art form. Achieving the ideal balance is crucial for unlocking the full potential of hybrid search. This involves adjusting different Alpha values for various types of user queries in your RAG system.\n\nIn this blog post, we will look into tuning Alpha within the Weaviate vector database using the `[Retrieval Evaluation](https://docs.llamaindex.ai/en/stable/examples/evaluation/retrieval/retriever_eval.html)` module of LlamaIndex with and without rerankers with the help of Hit Rate and MRR metrics.\n\nBefore diving into the implementation, let\u2019s first understand the different query types and metrics we will be using in this article.\n\nDifferent User Query Types:\n===========================\n\nUser queries in an RAG application vary based on individual intent. For these diverse query types, it\u2019s essential to fine\\-tune the `Alpha` parameter. This process involves routing each user query to a specific `Alpha` value for effective retrieval and response synthesis. [Microsoft](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167) has identified various user query categories, and we have selected a few for tuning our hybrid search. Below are the different user query types we considered:\n\n1. **Web Search Queries:** Brief queries similar to those typically inputted into search engines.\n2. **Concept Seeking Queries:** Abstract questions that necessitate detailed, multi\\-sentence answers.\n3. **Fact Seeking Queries:** Queries that have a single, definitive answer.\n4. **Keyword Queries:** Concise queries composed solely of crucial identifier words.\n5. **Queries With Misspellings:** Queries containing typos, transpositions, and common misspellings.\n6. **Exact Sub\\-string Searches:** Queries that exactly match sub\\-strings from the original context.\n\nLet\u2019s look at sample examples in each of these different user query types:\n\n1. **Web Search Queries**\n\n\n> `Transfer capabilities of LLaMA language model to non-English languages`\n\n**2\\. Concept Seeking Queries**\n\n\n> `What is the dual-encoder architecture used in recent works on dense retrievers?`\n\n**3\\. Fact Seeking Queries**\n\n\n> `What is the total number of propositions the English Wikipedia dump is segmented into in FACTOID WIKI?`\n\n**4\\. Keyword Queries**\n\n\n> `GTR retriever recall rate`\n\n**5\\. Queries With Misspellings**\n\n\n> `What is the advntage of prposition retrieval over sentnce or passage retrieval?`\n\n**6\\. Exact Sub\\-string Searches**",
        "8267d662-6c10-4f11-a901-033a4cb96c7f": "```\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, output_parser=output_parser)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    transformations=transformations,\n    callback_manager=callback_manager,\n)\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\ninput_scanners = [\n    Anonymize(vault, entity_types=[\"PERSON\", \"EMAIL_ADDRESS\", \"EMAIL_ADDRESS_RE\", \"PHONE_NUMBER\"]), \n    Toxicity(), \n    PromptInjection(),\n    Secrets()\n]\n\nllm_guard_postprocessor = LLMGuardNodePostProcessor(\n    scanners=input_scanners,\n    fail_fast=False,\n    skip_scanners=[\"Anonymize\"],\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=3,\n    node_postprocessors=[llm_guard_postprocessor]\n)\nresponse = query_engine.query(\"I am screening candidates for adult caregiving opportunity. Please recommend me an experienced person. Return just a name\")\nprint(str(response))\n\n```\n**LLM Guard protects your LLM applications**\n\nAs demonstrated in the practical example of securing an HR screening application with [LLM Guard](https://llm-guard.com/), the significance of mitigating potential attacks, cannot be overstated. Besides that, as LLMs evolve rapidly and embed advanced capabilities like agency and multi\\-modality, the complexity and impact of potential breaches escalate significantly. Thus, prioritizing RAG security becomes not just a necessity but rather fundamental in safeguarding against increasingly sophisticated threats and ensuring the integrity of critical enterprise LLM applications.\n\nTry out LLM Guard by going to our [library](https://github.com/protectai/llm-guard) or [documentation](https://llm-guard.com/). Also, [join our Slack](https://join.slack.com/t/laiyerai/shared_invite/zt-28jv3ci39-sVxXrLs3rQdaN3mIl9IT~w) channel for any questions!",
        "da5f5482-180f-414c-9708-388eabbe48d4": "2\\. Classification\n\nFor businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning.",
        "8f1b4319-f725-4ee6-a043-4a2179cd6584": "Greetings, LlamaIndex enthusiasts! \ud83e\udd99\n\nWelcome to another exciting weekly update from the world of LlamaVerse!\n\nWe have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI\\-native document parsing solution. With state\\-of\\-the\\-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our [launch post](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform) for all the details!\n\n\ud83e\udd29\u00a0**The highlights:**\n\n1. **New observability with Instrumentation:** Enhanced developer workflow with a new Instrumentation module for improved observability. [Docs](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html), [Tweet](https://x.com/llama_index/status/1768730443921396220?s=20).\n2. **LlamaParse accepts natural language parsing instructions**: Easily extract math snippets from PDFs into LaTeX with LlamaParse. [Blogpost](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform), [Tweet](https://x.com/llama_index/status/1768443551267049492?s=20).\n3. **Financial Data Parsing:** Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb), [Tweet](https://x.com/llama_index/status/1768303288381030408?s=20).\n\n**\u2728 Feature Releases and Enhancements:**\n\n* We introduced LlamaIndex v0\\.10\\.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module\\-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application. [Docs](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation.html), [Tweet](https://x.com/llama_index/status/1768730443921396220?s=20).\n* We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline. [Blogpost](https://www.llamaindex.ai/blog/launching-the-first-genai-native-document-parsing-platform), [Tweet](https://x.com/llama_index/status/1768443551267049492?s=20).\n* We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question\\-answering over complex financial data. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/other_files/demo_ppt_financial.ipynb), [Tweet](https://x.com/llama_index/status/1768303288381030408?s=20).\n* We collaborated with langfuse to launch open\\-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code. [Blogpost](https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse), [Docs](https://docs.llamaindex.ai/en/stable/examples/callbacks/LangfuseCallbackHandler.html), [Tweet](https://x.com/llama_index/status/1769790083564208218?s=20).\n* Search\\-in\\-the\\-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question\\-answering with an advanced system that interleaves retrieval and planning.",
        "5369c749-7dbf-47a7-b77c-58b1525dfb12": "class MyCustomRetriever(CustomPGRetriever):\n    \"\"\"Custom retriever with entity detection.\"\"\"\n    def init(\n        self,\n        ## vector context retriever params\n        embed_model: Optional[BaseEmbedding] = None,\n        vector_store: Optional[VectorStore] = None,\n        similarity_top_k: int = 4,\n        path_depth: int = 1,\n        include_text: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Uses any kwargs passed in from class constructor.\"\"\"\n        self.entity_extraction = OpenAIPydanticProgram.from_defaults(\n            output_cls=Entities, prompt_template_str=prompt_template_entities\n        )\n        self.vector_retriever = VectorContextRetriever(\n            self.graph_store,\n            include_text=self.include_text,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            path_depth=path_depth,\n        )\n\n    def custom_retrieve(self, query_str: str) -> str:\n        \"\"\"Define custom retriever with entity detection.\n\n        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n        \"\"\"\n        entities = self.entity_extraction(text=query_str).names\n        result_nodes = []\n        if entities:\n            print(f\"Detected entities: {entities}\")\n            for entity in entities:\n                result_nodes.extend(self.vector_retriever.retrieve(entity))\n        else:\n            result_nodes.extend(self.vector_retriever.retrieve(query_str))\n        final_text = \"\\n\\n\".join(\n            [n.get_content(metadata_mode=\"llm\") for n in result_nodes]\n        )\n        return final_text\n```\nThe `MyCustomRetriever` class has only two methods. You can use the `init` method to instantiate any functions or classes you will be using in the retriever. In this example, we instantiate the entity detection OpenAI program along with the vector context retriever.\n\nThe `custom_retrieve` method is called during retrieval. In our custom retriever implementation, we first identify any relevant entities in the text. If any entities are found, we iterate and execute the vector context retriever for each entity. On the other hand, if no entities are identified we pass the entire input to the vector context retriever.\n\nAs you can observe, you can easily customize the retriever for your use\\-case by incorporating existing retrievers or starting from scratch as you can easily execute Cypher statements by using the `structured_query` method of the graph store.\n\nQuestion\\-answering flow\n------------------------\n\nLet\u2019s wrap it up by using the custom retriever to answer an example question. We need to pass the retriever to the `RetrieverQueryEngine` .\n\n\n```\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\ncustom_sub_retriever = MyCustomRetriever(\n    index.property_graph_store,\n    include_text=True,\n    vector_store=index.vector_store,\n    embed_model=embed_model\n)\n\nquery_engine = RetrieverQueryEngine.from_args(\n    index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n)\n```\nLet\u2019s test it out!",
        "12f81353-deb9-4d8f-8e57-a29e458bea6f": "**Introduction**\n----------------\n\n\n\n In the domain of document handling, accurately extracting crucial information\n from images has posed an enduring obstacle. Despite Optical Character\n Recognition (OCR) advancements in converting images to editable text, it faces\n numerous intricacies with diverse document formats and quality. Here enters\n Zephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\n addresses these hurdles, heralding a transformative era in image\\-based\n document extraction.\n\n\n\n\n![](/blog/images/1*JFum54ZY63bo7QfJOKi1WQ.png)\n\n\n\n Source:\n [Zephyr\\-llama\\-index](https://corca.substack.com/p/top-llm-papers-of-the-week-95e?utm_source=profile&utm_medium=reader2)\n\n\n\n\n**The OCR Dilemma: Obstacles and Constraints Optical Character**\n----------------------------------------------------------------\n\n\nRecognition (OCR), though potent, faces impediments such as:\n\n\n1. **Diverse Document Formats**: Documents exhibit intricate\n layouts, fonts, and structures, posing challenges for traditional OCR to\n precisely interpret and extract information.\n2. **Quality and Clarity**: Images with low resolution,\n blurriness, or skewed angles hinder OCR\u2019s accuracy in deciphering text.\n3. **Handwritten and Cursive Content**: OCR often struggles with\n handwritten text or cursive fonts, resulting in errors or incomplete\n extraction.\n4. **Multilingual Complexity**: Processing documents in multiple\n languages poses a challenge for OCR systems lacking proficiency in\n recognizing and extracting varied linguistic content.\n\n\n\n![](/blog/images/1*Wjgk1UrzmpXz83skj2TxpQ.png)\n\n\nSource: Created by Author using MidJourney\n\n\n\n**Zephyr 7b LLM: Narrowing the Divide**\n---------------------------------------\n\n\n\n Zephyr 7b LLM revolutionizes the landscape by tackling these inherent\n constraints of OCR technology:\n\n\n\n1. **Advanced Machine Learning Algorithms:**\n\n\n\n Employing state\\-of\\-the\\-art machine learning algorithms, Zephyr 7b LLM\n undergoes extensive training with diverse document formats and languages. This\n equips it to adapt and learn from various document structures, resulting in\n heightened accuracy and robust extraction capabilities.\n\n\n\n\n**2\\. Contextual Comprehension:**\n\n\n\n\n Diverging from conventional OCR, Zephyr 7b LLM doesn\u2019t merely identify\n individual characters; it comprehends the context in which these characters\n exist. This contextual understanding significantly reduces errors, ensuring\n precise extraction even from intricate document layouts.\n\n\n\n\n**3\\. Adaptive Image Processing:**\n\n\n\n\n The fusion with LlamaIndex amplifies Zephyr 7b LLM\u2019s ability to handle images\n of varying resolutions or qualities. Leveraging adaptive image processing\n techniques, it rectifies distortions, enhances clarity, and optimizes images\n for meticulous OCR analysis.\n\n\n\n\n**4\\. Multilingual Proficiency:**\n\n\n\n\n Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency\n facilitates seamless content extraction from documents in various languages,\n extending global accessibility for businesses dealing with multilingual\n documentation.\n\n\n\n\n![](/blog/images/1*1DVGFV2TzRGzsbKfb54zwQ.png)\n\n\nSource: Created by Author using MidJourney\n\n\n\n**Implementation of Code**\n--------------------------\n\n\n\n The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\n transformation in document extraction. By merging Zephyr\u2019s advanced OCR\n capabilities with LlamaIndex\u2019s image enhancement and data organization\n features, this integration presents a comprehensive solution:\n\n\n\n1. **Augmented Precision**: The fusion of Zephyr\u2019s machine\n learning expertise and LlamaIndex\u2019s image enhancement markedly heightens the\n accuracy of extracted data, diminishing errors and enhancing overall\n efficiency.\n2. **Efficient Workflow**: Users experience an optimized workflow,\n enabling swift extraction and conversion of image\\-based documents into\n structured, actionable data, facilitating expedited decision\\-making\n processes.\n3. **Adaptability Across Document Varieties**: This integration\n empowers users to handle diverse document formats and languages\n effortlessly, granting access to previously challenging document types for\n extraction and analysis.",
        "1c634647-7d6f-4a45-8fd1-95a51f97ccd0": "Unlocking the power of AI should be as intuitive as using your favorite apps. That\u2019s the philosophy behind RAGArch, my latest creation designed to demystify and streamline the process of setting up Retrieval\\-Augmented Generation (RAG) pipelines. This tool is born from a simple vision: to provide a straightforward, no\\-code platform that empowers both seasoned developers and curious explorers in the world of AI to craft, test, and implement RAG pipelines with confidence and ease.\n\nFeatures\n========\n\nRAGArch leverages LlamaIndex\u2019s powerful LLM orchestration capabilities, to provide a seamless experience and granular control over your RAG pipeline.\n\n* **Intuitive Interface:** RAGArch\u2019s user\\-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively.\n* **Custom Configuration**: The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project\u2019s needs.\n* **Live Testing:** Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome.\n* **One\\-Click Code Generation**: Once you\u2019re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application.\n\nTools and Technologies\n======================\n\nThe creation of RAGArch was made possible by integrating a variety of powerful tools and technologies:\n\n* **UI:** Streamlit\n* **Hosting:** Hugging Face Spaces\n* **LLMs:** OpenAI GPT 3\\.5 and 4, Cohere API, Gemini Pro\n* **LLM Orchestration:** Llamaindex\n* **Embedding Models:** \u201cBAAI/bge\\-small\\-en\\-v1\\.5\u201d, \u201cWhereIsAI/UAE\\-Large\\-V1\u201d, \u201cBAAI/bge\\-large\\-en\\-v1\\.5\u201d, \u201ckhoa\\-klaytn/bge\\-small\\-en\\-v1\\.5\\-angle\u201d, \u201cBAAI/bge\\-base\\-en\\-v1\\.5\u201d, \u201cllmrails/ember\\-v1\u201d, \u201cjamesgpt1/sf\\_model\\_e5\u201d, \u201cthenlper/gte\\-large\u201d, \u201cinfgrad/stella\\-base\\-en\\-v2\u201d and \u201cthenlper/gte\\-base\u201d\n* **Vector Stores:** Simple (Llamaindex default), Pinecone and Qdrant\n\nDeep Dive into the Code\n=======================\n\nThe `app.py` script is the backbone of RAGArch, integrating various components to provide a cohesive experience. The following are the key functions of app.py\n\n`upload_file`\n-------------\n\nThis function manages file uploads and uses Llamaindex's `SimpleDirectoryReader` to load documents into the system. It supports a wide array of document types, including PDFs, text files, HTML, JSON files, and more, making it versatile for processing diverse data sources.\n\n![](/blog/images/1*hrekcMeqyyEIQbiO1GB8NQ.png)\n```\ndef upload_file():\n    file = st.file_uploader(\"Upload a file\", on_change=reset_pipeline_generated)\n    if file is not None:\n        file_path = save_uploaded_file(file)\n        \n        if file_path:\n            loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n            print(f\"Total documents: {len(loaded_file)}\")\n\n            st.success(f\"File uploaded successfully. Total documents loaded: {len(loaded_file)}\")\n        return loaded_file\n    return None\n```\nsave\\_uploaded\\_file\n--------------------\n\nThis utility function saves the uploaded file to a temporary location on the server, making it accessible for further processing. It\u2019s a crucial part of the file handling process, ensuring data integrity and availability.",
        "0e0c1592-0d7d-4cc9-ac0a-d83a0174294d": "[Stick with Fit](https://docs.google.com/presentation/d/1pOa8AppiKpuF-aQvsD5vKebeL6Gf9lmP505FavrFOm4/edit#slide=id.g2899cea0752_0_15), [SafeQuery](https://github.com/chisler/safequery), Cherry\n\n**Break:**\n\n* [Fuzzy Access](https://github.com/jeremy-brouillet/agi-hackathon)\n\n**Test:**\n\n* X\\-Ray Insight\n\n**Honorable Mentions:**\n\n* [KindleGPT](https://glasp.co/know-thyself/)\n* PenTest\n\n\ud83c\udfa4 LlamaIndex at [AI.Engineer Summit](https://twitter.com/aiDotEngineer):\n========================================================================\n\n1. [Jerry Liu](https://twitter.com/jerryjliu0) gave a talk on Building production\\-ready RAG applications. [Slides](https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit#slide=id.p).\n2. [Simon](https://twitter.com/disiok) conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Production with LlamaIndex. [Slides](https://github.com/run-llama/ai-engineer-workshop/blob/main/presentation.pdf), [Code](https://github.com/run-llama/ai-engineer-workshop/tree/main).\n\n\ud83d\uddfa\ufe0f Guides:\n==========\n\n1. **LLM Compatibility Tracking:** We\u2019ve charted LLM performances on various tasks, revealing zephyr\\-7b\\-alpha as the only current 7B model excelling in advanced RAG/ Agentic tasks. [Docs](https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking).\n2. **Evaluations:** Adjusting chunk size is essential for RAG apps. Having more chunks isn\u2019t necessarily better, and re\\-ranking might be counterproductive. To fine\\-tune, experiment with different chunk sizes and top\\-k values. The Arize AI team has provided a guide to help you evaluate using Arize AI Phoenix and Llama Index. [Slides](https://docs.google.com/presentation/d/18Z7H3WSncPzLOTHKZAj36w0E7HSGY78VkDooSzvvySE/edit), [Notebook](https://colab.research.google.com/drive/1Siufl13rLI-kII1liaNfvf-NniBdwUpS?usp=sharing).\n\n\u270d\ufe0f Tutorials:\n=============\n\n1. [Shahul\u2019s](https://twitter.com/Shahules786) [tutorial](https://t.co/oTA2O8sE21) demonstrates how to choose the best embeddings for your data, emphasizing that retriever performance and embedding quality are crucial for a RAG system\u2019s efficacy using the LlamaIndex and RAGAS libraries.\n2. [Wenqi Glantz](https://twitter.com/wenqi_glantz)\u2019s [tutorial](https://levelup.gitconnected.com/evaluation-driven-development-the-swiss-army-knife-for-rag-pipelines-dba24218d47e) on Evaluation Driven Development for RAG Pipelines.\n3. [Wenqi Glantz](https://twitter.com/wenqi_glantz)\u2019s [tutorial](https://betterprogramming.pub/masking-pii-data-in-rag-pipeline-326d2d330336) on Masking PII Data in the RAG Pipeline.\n4. Ofer Mendelevitch\u2019s from [Vectara](https://twitter.com/vectara) has a [tutorial](https://vectara.com/retrieval-augmented-generation-rag-done-right-retrieval/) on Retrieval Augmented Generation with LlamaIndex on comparing Vectara\u2019s new Boomerang model to OpenAI and Cohere.\n5.",
        "24c49aae-48ee-4fcb-a0fd-0286689628fe": "```\ndef save_uploaded_file(uploaded_file):\n    try:\n        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n            tmp_file.write(uploaded_file.getvalue())\n            return tmp_file.name\n    except Exception as e:\n        st.error(f\"Error saving file: {e}\")\n        return None\n```\n`select_llm`\n------------\n\nAllows users to select a Large Language Model and initializes it for use. You can choose from Google\u2019s Gemini Pro, Cohere, OpenAI\u2019s GPT 3\\.5 and GPT 4\\.\n\n![](/blog/images/1*O2HndW3Ow_OgBHOABLOCLA.png)\n```\ndef select_llm():\n    st.header(\"Choose LLM\")\n    llm_choice = st.selectbox(\"Select LLM\", [\"Gemini\", \"Cohere\", \"GPT-3.5\", \"GPT-4\"], on_change=reset_pipeline_generated)\n    \n    if llm_choice == \"GPT-3.5\":\n        llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-1106\")\n        st.write(f\"{llm_choice} selected\")\n    elif llm_choice == \"GPT-4\":\n        llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n        st.write(f\"{llm_choice} selected\")\n    elif llm_choice == \"Gemini\":\n        llm = Gemini(model=\"models/gemini-pro\")\n        st.write(f\"{llm_choice} selected\")\n    elif llm_choice == \"Cohere\":\n        llm = Cohere(model=\"command\", api_key=os.environ['COHERE_API_TOKEN'])\n        st.write(f\"{llm_choice} selected\")\n    return llm, llm_choice\n```\n`select_embedding_model`\n------------------------\n\nOffers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face\u2019s MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models.\n\n![](/blog/images/1*1HZdInelOskY3EWY1SmWkw.png)\n```\ndef select_embedding_model():\n    st.header(\"Choose Embedding Model\")\n    col1, col2 = st.columns([2,1])\n    with col2:\n        st.markdown(\"\"\"\n                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                    \"\"\")\n    model_names = [\n        \"BAAI/bge-small-en-v1.5\",\n        \"WhereIsAI/UAE-Large-V1\",\n        \"BAAI/bge-large-en-v1.5\",\n        \"khoa-klaytn/bge-small-en-v1.5-angle\",\n        \"BAAI/bge-base-en-v1.5\",\n        \"llmrails/ember-v1\",\n        \"jamesgpt1/sf_model_e5\",\n        \"thenlper/gte-large\",\n        \"infgrad/stella-base-en-v2\",\n        \"thenlper/gte-base\"\n    ]\n    selected_model = st.selectbox(\"Select Embedding Model\", model_names,  on_change=reset_pipeline_generated)\n    with st.spinner(\"Please wait\") as status:\n        embed_model = HuggingFaceEmbedding(model_name=selected_model)\n        st.session_state['embed_model'] = embed_model\n        st.markdown(F\"Embedding Model: {embed_model.model_name}\")\n        st.markdown(F\"Embed Batch Size: {embed_model.embed_batch_size}\")\n        st.markdown(F\"Embed Batch Size: {embed_model.max_length}\")\n\n\n    return embed_model, selected_model\n```\nselect\\_node\\_parser Function\n-----------------------------\n\nThis function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing.",
        "276e9a9d-50e9-47c5-8591-6f1ad72ba996": "```\nprint(agent.chat(\n\t\"pass the entire generated email to the browser and have it send the email as a reply to the chain\"\n))\n```\n\n```\nAdded user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\n=== Calling Function ===\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\nGot output: Email response sent to Julian\n========================\n```\nNext Steps\n----------\n\nMultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the [LlamaHub page](https://llamahub.ai/l/tools/llama-index-tools-multion?from=) here.\n\nIf you\u2019re interested in running through this tutorial on building a browser \\+ Gmail\\-powered agent yourself, check out our [notebook](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-multion/examples/multion.ipynb).\n\nThe integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.",
        "0a5b1a68-40ac-4cb6-8437-d1921e92d4df": "* [Ravi Theja](https://twitter.com/ravithejads) [analysis](/gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9) on GPT4\\-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques\n\n\ud83c\udfa5 **Webinars:**\n\n* Check out our CEO \u2014 [Jerry Liu\u2019s](https://twitter.com/jerryjliu0) talk on Building Production\\-Ready RAG Applications at [AI.engineer](https://t.co/46VrFt8GCV) Summit.",
        "7b98eb30-6bbc-4eae-9f64-90359534a687": "Introduction\n============\n\nRetrieval\\-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system\u2019s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes in handy. In this blog post, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex\u2019s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding.\n\nWhy Chunk Size Matters\n======================\n\nChoosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n\n1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2\\. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of \u2018hallucinations\u2019 and the \u2018relevancy\u2019 of responses based on the query and the retrieved contexts respectively.\n2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n\nIn essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\n\nFor a practical evaluation in choosing the right `chunk_size`, you can access and run the following setup on this [**Google Colab Notebook**](https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing).\n\nSetup\n=====\n\nBefore embarking on the experiment, we need to ensure all requisite modules are imported:\n\n\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.evaluation import (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\nfrom llama_index.llms import OpenAI\n\nimport openai\nimport time\nopenai.api_key = 'OPENAI-API-KEY'\n```\nDownload Data\n=============\n\nWe\u2019ll be using the Uber 10K SEC Filings for 2021 for this experiment.\n\n\n```\n!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n```\nLoad Data\n=========\n\nLet\u2019s load our document.\n\n\n```\ndocuments = SimpleDirectoryReader(\"./data/10k/\").load_data()\n```\nQuestion Generation\n===================\n\nTo select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents.",
        "5c1a8087-7108-45d1-adb3-a23ec33cc5ef": "```\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n# Create a Node object from a single row of data\ndef create_node(row):\n   record = row.to_dict()\n   record_name = split_name(record[\"author\"])\n   record_content = str(record[\"date\"]) + \" \" + record_name + \" \" + str(record[\"change summary\"]) + \" \" + str(record[\"change details\"])\n   node = TextNode(\n       id_=create_uuid(record[\"date\"]),\n       text= record_content,\n       metadata={\n           'commit': record[\"commit\"],\n           'author': record_name,\n           'date': create_date(record[\"date\"]),\n       }\n   )\n   return node\n\nnodes = [create_node(row) for _, row in df.iterrows()]\n```\n**Note:** The code above references two helper functions to get things in the right format (\\`split\\_name()\\` and \\`create\\_date()\\`), which we\u2019ve omitted for brevity. The full code is included in the tutorial linked in the Resources section at the end of this post.\n\nCreate UUIDs for each node based on the date of each git commit\n===============================================================\n\nWe will take a closer look at a helper function we use to create each node\u2019s `id_`. For time\\-based search in LlamaIndex, Timescale Vector uses the \u2018datetime\u2019 portion of a UUID v1 to place vectors in the correct time partition. [Timescale Vector\u2019s Python client library](https://github.com/timescale/python-vector) provides a simple\\-to\\-use function named \\`uuid\\_from\\_time\\` to create a UUID v1 from a Python DateTime object, which we\u2019ll then use as our \\`ids\\` for the TextNodes.\n\n\n```\nfrom timescale_vector import client\n# Function to take in a date string in the past and return a uuid v1\ndef create_uuid(date_string: str):\n   if date_string is None:\n       return None\n   time_format = '%a %b %d %H:%M:%S %Y %z'\n   datetime_obj = datetime.strptime(date_string, time_format)\n   uuid = client.uuid_from_time(datetime_obj)\n   return str(uuid)\n```\nSince we are dealing with timestamps in the past, we take advantage of the \\`uuid\\_from\\_time\\` function to help generate the correct UUIDs for each node. If you want the current date and time associated with your Nodes (or Documents) for time\\-based search, you can skip this step. A UUID associated with the current date and time will be automatically generated as the nodes are added to the table in Timescale Vector by default.\n\nLet\u2019s take a look at the contents of a node:\n\n\n```\nprint(nodes[0].get_content(metadata_mode=\"all\"))\n```\n\n```\ncommit: 44e41c12ab25e36c202f58e068ced262eadc8d16\nauthor: Lakshmi Narayanan Sreethar\ndate: 2023-09-5 21:03:21+0850\n\nTue Sep 5 21:03:21 2023 +0530 Lakshmi Narayanan Sreethar Fix segfault in set_integer_now_func When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037\n```\nCreate vector embeddings for the text of each node\n==================================================\n\nNext, we\u2019ll create vector embeddings of the content of each node so that we can perform similarity searches on the text associated with each node. We\u2019ll use the \\`OpenAIEmbedding\\` model to create the embeddings.",
        "03335930-50a7-4008-975f-90cce453fc3b": "```\n        elif state[\"just_finished\"] == True:\n            print(\"Asking the continuation agent to decide what to do next\")\n            user_msg_str = str(continuation_agent_factory(state).chat(\"\"\"\n                Look at the chat history to date and figure out what the user was originally trying to do.\n                They might have had to do some sub-tasks to complete that task, but what we want is the original thing they started out trying to do.                                                                      \n                Formulate a sentence as if written by the user that asks to continue that task.\n                If it seems like the user really completed their task, output \"no_further_task\" only.\n            \"\"\", chat_history=current_history))\n            print(f\"Continuation agent said {user_msg_str}\")\n            if user_msg_str == \"no_further_task\":\n                user_msg_str = input(\">> \").strip()\n            state[\"just_finished\"] = False\n```\nThe continuation agent's prompt instructs it to reply as if it were the user asking to perform a task, or to output `no_further_task` if there's no more to do. If there's a new task, the output of the continuation agent becomes the input to the orchestrator, which selects a new speaker. If there's no further task, the loop pauses for more user input.\n\nAnd that's the full system! The sub\\-agents can be arbitrarily complicated, multi\\-turn systems in themselves, and the outer loop doesn't need to know how they work, just how they depend on each other.\n\nWhat's next\n-----------\n\nWe think there's some novel stuff in here: coordinating multiple agents \"speaking\" simultaneously, creating implicit \"chains\" of agents through natural language instructions, using a \"continuation\" agent to manage those chains, and using a global state this way. We're excited to see what you do with the patterns we've laid out here. Don't forget to [check out the open\\-source repo](https://github.com/run-llama/multi-agent-concierge/)!",
        "81f909a2-0c7b-4f11-b4d3-88e292fb1466": "Basic System Setup\n\nHere's a simple example of how to set up a basic multi\\-agent system using llama\\-agents. First we\u2019ll bring in our dependencies and set up our control plane, which contains our LLM\\-powered orchestrator\n\n\n```\nimport dotenv\ndotenv.load_dotenv() # our .env file defines OPENAI_API_KEY\nfrom llama_agents import (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    AgentOrchestrator,\n)\nfrom llama_index.core.agent import FunctionCallingAgentWorker\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nimport logging\n\n# turn on logging so we can see the system working\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n\n# Set up the message queue and control plane\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)\n```\nNext we create our tools using LlamaIndex\u2019s existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:\n\n\n```\n# create a tool\ndef get_the_secret_fact() -> str:\n    \"\"\"Returns the secret fact.\"\"\"\n    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n# Define an agent\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nagent = worker.as_agent()\n\n# Create an agent service\nagent_service = AgentService(\n    agent=agent,\n    message_queue=message_queue,\n    description=\"General purpose assistant\",\n    service_name=\"assistant\",\n)\n```\nFinally we launch the service and the control plane. Note that here we\u2019re using a helper function to run a single query through the system and then exit; next we\u2019ll show how to deploy this to production.\n\n\n```\n# Set up the launcher for local testing\nfrom llama_agents import LocalLauncher\n\nlauncher = LocalLauncher(\n    [agent_service],\n    control_plane,\n    message_queue,\n)\n\n# Run a single query through the system\nresult = launcher.launch_single(\"What's the secret fact?\")\nprint(result)\n```\nDeploying Your Multi\\-Agent System\n----------------------------------\n\nOnce you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but we\u2019ve added a second agent service and we\u2019re using a different launcher. Let\u2019s bring in our dependencies and set up our control plane again:\n\n\n```\nimport dotenv\ndotenv.load_dotenv()\nfrom llama_agents import (\n    AgentService,\n    AgentOrchestrator,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n)\n\nfrom llama_index.core.agent import FunctionCallingAgentWorker\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nimport logging\n\n# change logging level to enable or disable more verbose logging\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n\n# create our multi-agent framework components\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)\n```\nThen as before we create a tool and an agent, though this time we\u2019ll add a second agent:\n\n\n```\n# create a tool\ndef get_the_secret_fact() -> str:\n    \"\"\"Returns the secret fact.\"\"\"\n    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n# create our agents\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\nagent1 = worker1.as_agent()\nagent2 = worker2.as_agent()\n```\nWe turn those agents into services:",
        "d2bb959e-2b94-4db4-ad82-324c9aa080f1": "```\n  useEffect(() =&gt; {\n    // Wake word listener setup\n    const WakeWordSpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (WakeWordSpeechRecognition &amp;&amp; !wakeWordRecognitionRef.current) {\n      wakeWordRecognitionRef.current = new WakeWordSpeechRecognition();\n      wakeWordRecognitionRef.current.continuous = true;\n      wakeWordRecognitionRef.current.interimResults = false;\n\n      wakeWordRecognitionRef.current.onresult = (event) =&gt; {\n        const transcript = event.results[event.results.length - 1][0].transcript\n          .trim()\n          .toLowerCase();\n        if (transcript.includes(\"c3\")) {\n          toggleRecording(); // Start the main speech recognition process\n        }\n      };\n\n      wakeWordRecognitionRef.current.start();\n    }\n\n    // Main speech recognition setup\n    const SpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition &amp;&amp; !recognitionRef.current) {\n      recognitionRef.current = new SpeechRecognition();\n      recognitionRef.current.continuous = false;\n      recognitionRef.current.interimResults = false;\n\n      recognitionRef.current.onresult = (event) =&gt; {\n        const lastResultIndex = event.results.length - 1;\n        const transcriptResult = event.results[lastResultIndex][0].transcript;\n        setTranscript(transcriptResult);\n        setAppState(\"playing\");\n        setShowSpeechBubble(true);\n        setTimeout(() =&gt; setShowSpeechBubble(false), speechBubbleTimeout);\n        fetchResponseFromLLM(transcriptResult);\n      };\n\n      recognitionRef.current.onend = () =&gt; {\n        setShowSpinner(true);\n      };\n    }\n  }, []);\n```\n3\\. Handling User Speech and Response\n-------------------------------------\n\n`toggleRecording` controls the speech recognition process, while `fetchResponseFromLLM` sends the user's speech to the LLM backend and handles the response. This response is then spoken out via speech synthesis and also used to update the chat history displayed on the UI.\n\n\n```\n const toggleRecording = () =&gt; {\n    try {\n      if (appState === \"idle\") {\n        recognitionRef.current.start();\n        setAppState(\"listening\");\n      } else if (appState === \"listening\") {\n        recognitionRef.current.stop();\n      }\n    } catch (error) {\n    }\n  };\n```\n\n```\n  const fetchResponseFromLLM = async (text) =&gt; {\n    try {\n      const response = await fetch(\n        `https://c3-python-nostream.onrender.com/api/chat`,\n        {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\" },\n          body: JSON.stringify({\n            messages: [\n              {\n                role: \"user\",\n                content:\n                  \"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\" +\n                  text,\n              },\n            ],\n          }),\n        }\n      );\n      const data = await response.json();\n\n      setChatHistory((prevHistory) =&gt; [\n        ...prevHistory,\n        { query: text, response: data.result.content },\n      ]);\n      speak(data.result.content);\n    } catch (error) {\n      console.error(\"Error communicating with LLM:\", error);\n    }\n  };\n```\n4\\. Speech Synthesis\n--------------------\n\nThe `speak` function takes the text response from the LLM and uses the SpeechSynthesis API to read it aloud, providing an interactive experience for the user.",
        "ce0e4e82-5307-4f35-b146-7e78c1f84d6e": "Authored by Joe Reuter, Software Engineer at Airbyte\n\n(cross\\-posted from the Airbyte blog; check it out [here](https://airbyte.com/blog/introducing-airbyte-sources-within-llamaindex)!)\n\nContent\n=======\n\nIt\u2019s now possible to utilize the Airbyte sources for [Gong](https://llamahub.ai/l/airbyte_gong), [Hubspot](https://llamahub.ai/l/airbyte_hubspot), [Salesforce](https://llamahub.ai/l/airbyte_salesforce), [Shopify](https://llamahub.ai/l/airbyte_shopify), [Stripe](https://llamahub.ai/l/airbyte_stripe), [Typeform](https://llamahub.ai/l/airbyte_typeform) and [Zendesk Support](https://llamahub.ai/l/airbyte_zendesk_support) directly within your LlamaIndex\\-based application, implemented as [data loaders](https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/connector/usage_pattern.html).\n\nFor example, to load the Stripe invoices for a user, you can use the AirbyteStripeLoader. Installing it is super simple, when you have LlamaIndex installed locally you only need to install the source you are interested in, and you are ready to go:\n\n\n```\npip install airbyte-source-stripe\npip install llama-hub\n```\nAfter that, simply download the loader and pass in configuration and the stream you want to load:",
        "5524dc7d-00a0-4e5f-a1f4-abd1c25bd434": "**Co\\-authors:**\n\n* Prakul Agarwal \u2014 Senior Product Manager, Machine Learning at MongoDB\n* Jerry Liu \u2014 co\\-founder at LlamaIndex\n\n**Update (6/22/2023\\):** The preferred way to use LlamaIndex \\+ MongoDB is now with our MongoDBAtlasVectorSearch class. Take a look at our guide here: [https://gpt\\-index.readthedocs.io/en/latest/examples/vector\\_stores/MongoDBAtlasVectorSearch.html](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.html)\n\n**Summary**\n===========\n\nLarge Language Models (LLMs) like ChatGPT have revolutionized the way users can get answers to their questions. However, the \u201cknowledge\u201d of LLMs is restricted by what they were trained on, which for ChatGPT means publicly available information on the internet till September 2021\\. How can LLMs answer questions using private knowledge sources like your company\u2019s data and unlock its true transformative power?\n\nThis blog will discuss how LlamaIndex and MongoDB can enable you to achieve this outcome quickly. The [attached notebook](https://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing) provides a code walkthrough on how to query any PDF document using English queries.\n\n**Background**\n==============\n\nTraditionally, AI has been used to analyze data, identify patterns and make predictions based on existing data. The recent advancements have led to AI becoming better at generating new things (rather than just analyzing existing things). This is referred to as Generative AI. Generative AI is powered mainly by machine learning models called Large Language Models (LLM). LLMs are pre\\-trained on large quantities of publicly available text. There are various proprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of emerging open\\-source LLMs like Llama, Dolly, etc.\n\nThere are 2 main scenarios where the knowledge of LLMs falls short:\n\n* Private data such as your company\u2019s internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and Slack\n* Newer data than when the LLMs were last trained. Example question: Who is the most recent UK prime minister?\n\nThere are 2 main paradigms currently for extending the amazing reasoning and knowledge generation capabilities of LLMs: Model finetuning and in\\-context learning.\n\nModel Finetuning can be more complex and expensive to operationalize. There are also some open questions like how to delete information from a fine\\-tuned model to ensure you comply with local laws (ex. GDPR in Europe), and for changing data you need to fine\\-tune again constantly.\n\nIn\\-context learning requires inserting the new data as part of the input prompts to the LLM. To perform this data augmentation in a secure, high performance and cost\\-effective manner is where tools like LlamaIndex and MongoDB Developer Data Platform can help.\n\n**Introduction to LlamaIndex**\n==============================\n\nLlamaIndex provides a simple, flexible interface to connect LLMs with external data.\n\n* Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc).\n* Provides indices over the unstructured and structured data for use with LLMs.\n* Structures external information so that it can be used with the prompt window limitations of any LLM.\n* Exposes a query interface which takes in an input prompt and returns a knowledge\\-augmented output.\n\n**MongoDB as the Datastore**\n============================\n\nIt is effortless to store the ingested documents (i.e. Node objects), index metadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There is an option to store the \u201cdocuments\u201d as an actual collection in MongoDB using `MongoDocumentStore`. There is an option to persist the \u201cIndexes\u201d using the `MongoIndexStore` .\n\nStoring LlamaIndex\u2019s documents and indexes in a database becomes necessary in a couple of scenarios:\n\n1.",
        "22256500-ee96-4691-8f9d-ffedf642af07": "The LlamaIndex agent operates on the client side, accompanied by a number of\n tools: Each tool provides function specifications and allows functions to be\n selected based on context and the user\u2019s input to\n `chat(\"\u2026\")`. For example, if the question indicates information needs to be retrieved\n from the \u201cinternet\u201d, the Google search tool will be chosen. Internally it uses\n LLM which returns selected functions with parameters for a given context.\n\n\n\n\n When the Waii tool is chosen, whether for describing a dataset, generating a\n query, or running a query, it sends the API request to the Waii Service.\n\n\n\n\n The Waii Service can be deployed as a hosted SaaS or as Docker containers\n running in your on\\-premises environment. The components of the Waii Service\n include:\n\n\n\n* **The Query Generator:** coordinates the entire workflow of\n query generation and communicates with the LLM for this purpose.\n* **Knowledge Graph / Metadata Management**: connects to\n databases, extracting metadata and query history as a knowledge graph to\n assist the Query Generator in choosing the right tables and schemas.\n* **Semantic Rules**: These aid the Query Generator in producing\n semantically correct queries.\n* **Waii Compiler**: After a query is generated by the LLM, the\n Waii Compiler patches identified issues in the query. If a compilation issue\n is not fixable, it regenerates the query with an articulated error message.\n\n\nCreate LlamaIndex agent with Waii \\+ PDF Loader\n===============================================\n\n\n\n Let\u2019s first create two LlamaHub tools \u2014 Waii and PDF Loader. LlamaHub tools\n include specs to identify available functions along with their parameters, the\n agent will select and execute which function to use based on available\n functions and context.\n\n\n\nLet\u2019s start with creating an agent which includes the Waii tool:\n\n\n\n```\nfrom llama_hub.tools.google_search import GoogleSearchToolSpec  \nfrom llama_hub.tools.waii import WaiiToolSpec  \nfrom llama_index.agent import OpenAIAgent  \nfrom llama_index.llms import OpenAI  \n  \nwaii_tool = WaiiToolSpec(  \n    api_key='waii_api_key',  \n    # Connection key of WAII connected database, see   \n    # https://github.com/waii-ai/waii-sdk-py#get-connections  \n    database_key='database_to_use',  \n    verbose=True  \n)\n```\n\nAnd then create a PDF tool:\n\n\n\n```\nfrom pathlib import Path  \nfrom llama_index import download_loader  \nfrom llama_index import VectorStoreIndex  \n  \nPDFReader = download_loader(\"PDFReader\")  \nloader = PDFReader()  \ndocuments = loader.load_data(file=Path('DI_2023-Deloitte-holiday-retail-survey.pdf'))  \nindex = VectorStoreIndex.from_documents(documents)  \nengine = index.as_query_engine(similarity_top_k=5)  \ndeloitte_retail_survey_tool = QueryEngineTool(  \n        query_engine=engine,  \n        metadata=ToolMetadata(  \n            name=\"deloitte_retail_survey\",  \n            description=(  \n                \"Provides retail survey report for holiday sales based on Deloitte's data\"  \n                \"Use a detailed plain text question as input to the tool, and output using plain text based on pdf data\"  \n            ),  \n        ),  \n    )\n```\n\nAnd at last, create an agent which combines Waii and PDF tools:\n\n\n\n```\nagent = OpenAIAgent.from_tools(  \n  [deloitte_retail_survey_tool] + waii_tool.to_tool_list(),   \n  llm=OpenAI(model='gpt-4-1106-preview', temperature=0),   \n  verbose=True)\n```\n\n\n`[deloitte_retail_survey_tool] + waii_tool.to_tool_list()`\n indicate using all functions (such as getting answers from the database,\n generating a query, executing a query, describing datasets, etc.) provided by\n Waii and PDF Search.\n\n\n\nUnderstand your dataset\n=======================",
        "171b0fff-daa5-4a9f-9511-7e46f93b7d4d": "**\u2728 Feature Releases and Enhancements:**\n\n1. We introduced RAFT \\- Retrieval Augmented Fine Tuning, a method from [Tianjun Zhang](https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D)\u00a0and [Shishir Patil](https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D)\u00a0to enhance domain\\-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation. [LlamaPack](https://github.com/run-llama/llama_index/tree/main/llama-index-packs/llama-index-packs-raft-dataset), [Tweet](https://x.com/llama_index/status/1772662480210198809?s=20).\n2. We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full\\-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook. [Course](https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/), [Tweet](https://x.com/AndrewYNg/status/1773006786058219889?s=20).\n3. We integrated with Cohere's Int8 and Binary Embeddings for a memory\\-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/cohere_retriever_eval.ipynb), [Tweet](https://x.com/llama_index/status/1773402379016138955?s=20)\n4. We launched revamped Python docs with top\\-level example notebooks, improved search with previews, and overhauled API documentation. [API Ref](https://docs.llamaindex.ai/en/stable/api_reference/), [Tweet](https://x.com/llama_index/status/1772355240299520083?s=20), [Docs](https://t.co/BS7oDqZ7qW)",
        "e614427b-66a2-4cef-995e-ae8169335e0d": "```\nsimilarity_threshold = 0.9\nword_edit_distance = 5\ndata = graph_store.structured_query(\"\"\"\nMATCH (e:__Entity__)\nCALL {\n  WITH e\n  CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n  YIELD node, score\n  WITH node, score\n  WHERE score > toFLoat($cutoff)\n      AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n           OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n      AND labels(e) = labels(node)\n  WITH node, score\n  ORDER BY node.name\n  RETURN collect(node) AS nodes\n}\nWITH distinct nodes\nWHERE size(nodes) > 1\nWITH collect([n in nodes | n.name]) AS results\nUNWIND range(0, size(results)-1, 1) as index\nWITH results, index, results[index] as result\nWITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n        CASE WHEN index <> index2 AND\n            size(apoc.coll.intersection(acc, results[index2])) > 0\n            THEN apoc.coll.union(acc, results[index2])\n            ELSE acc\n        END\n)) as combinedResult\nWITH distinct(combinedResult) as combinedResult\n// extra filtering\nWITH collect(combinedResult) as allCombinedResults\nUNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\nWITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\nWHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n    WHERE x <> combinedResultIndex\n    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n)\nRETURN combinedResult  \n\"\"\", param_map={'cutoff': similarity_threshold, 'distance': word_edit_distance})\nfor row in data:\n    print(row)\n```\nWithout getting into too many details, we use a combination of text embeddings and word distance to find potential duplicates in our graph. You can tune `similarity_threshold` and `word_distance` to find the best combination that detects as many duplicates without too much false positives. Unfortunately, entity disambiguation is a hard problem and there are no perfect solutions. With this approach, we get quite good results, but there are some false positives in there as well:",
        "12987fcc-6e16-4347-87b0-a50a62bdf97b": "Understand your dataset\n=======================\n\n\n\n The first step in doing data analysis is to get a better understanding of your\n dataset.\n\n\n\nYou can start asking questions to your agent:\n\n\n\n```\nagent.chat(\"Describe my database\")\n```\n\nThe output of which is:\n\n\n\n```\nSTARTING TURN 1  \n---------------  \n  \n=== Calling Function ===  \nCalling function: describe_dataset with args:   \n  {\"ask\":\"Can you describe the whole database?\"}  \n...  \n  \n========================  \n  \nSTARTING TURN 2  \n---------------  \n  \nThe database I have access to consists of several schemas, each with its   \nown set of tables and domains:  \n  \n1. **CRUNCHBASE_2016**: This schema includes tables related to acquisitions,   \n   companies, investments, and funding rounds in the Crunchbase database for  \n   the year 2016. It's useful for analyzing acquisition trends, company   \n   information, investment activity, and funding trends.  \n  \n2. **RETAIL_DATA**: This schema contains tables related to retail operations,   \n   such as call centers, customers, addresses, demographics, inventory,   \n   items, promotions, stores, returns, sales, and warehouses. It can be used   \n   to analyze call center performance, customer demographics, inventory   \n   management, sales performance, and other retail operations.  \n  \n3. ...\n```\n\n\n As you can see, the agent understood the request, called\n `describe_dataset` function provided by Waii, and generated a\n summary of the dataset.\n\n\n\n\n I\u2019m interested in the\n `RETAIL_DATA` schema, so let me ask more of the schema \\-\n\n\n\n\n```\nagent.chat(\"What can i do with the retail_data schema\")\n```\n\nAnd I get this:\n\n\n\n```\nThe RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed   \nto support a wide range of analyses related to retail operations. Here   \nare some of the capabilities and types of analyses you can perform with   \nthis schema:  \n  \n1. **Call Center Analysis**: Evaluate the performance of call centers,   \n   understand call volumes, and assess customer service efficiency.  \n  \n2. **Customer Demographics**: Analyze customer profiles, including   \n   demographics, purchasing behaviors, and preferences. This can help   \n   in targeted marketing and customer segmentation.  \n  \nSpecific questions that can be addressed using the RETAIL_DATA schema include:  \n- What is the total number of call centers?  \n- How many customers have a preferred customer flag?  \n- What is the average price of items?\n```\n\nLet me do some more data analysis.\n\n\nGenerate an SQL query and run it\n================================\n\n\n\n Let\u2019s generate a SQL query (asking top 10 item categories sold during\n Christmas time):\n\n\n\n\n```\nagent.chat(\"Top 10 item category sold during christmas time across all years\")\n```\n\nNow it calls `get_answer` function from Waii tool:\n\n\n\n```\n=== Calling Function ===  \nCalling function: get_answer with args:   \n  {\"ask\":\"What are the top 10 item categories sold during   \n          Christmas time across all years?\"}\n```\n\nAnd got a SQL like:\n\n\n\n```\nWITH christmas_sales AS (  \n    SELECT  \n        ss_item_sk,  \n        SUM(ss_quantity) AS total_quantity  \n    FROM tweakit_playground.retail_data.store_sales AS ss  \n    INNER JOIN tweakit_playground.retail_data.date_dim AS dd  \n        ON ss_sold_date_sk = d_date_sk  \n    WHERE  \n        d_holiday = 'Y' AND d_date ILIKE '%-12-25'  \n    GROUP BY  \n        ss_item_sk  \n),  \nranked_categories AS (  \n    SELECT  \n        i_category,  \n        SUM(total_quantity) AS category_quantity  \n    FROM christmas_sales AS cs  \n    INNER JOIN tweakit_playground.retail_data.item AS i  \n        ON ss_item_sk = i_item_sk  \n    GROUP BY  \n        i_category  \n    ORDER BY  \n        category_quantity DESC  \n    LIMIT 10  \n)  \nSELECT  \n    i_category AS category,  \n    category_quantity  \nFROM ranked_categories\n```\n\nLet me ask it to run the query:",
        "0a606338-63ce-4ecf-a5f0-4caf97f62f3d": "```\ndt_object = datetime.datetime.fromtimestamp(float(message.get('ts')))\nformatted_time = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n\n# get the message text\ntext = message.get('text')\n# create a node with metadata\nnode = TextNode(\n    text=text,\n    id_=str(uuid.uuid4()),\n    metadata={\n        \"when\": formatted_time\n    }\n)\nindex.insert_nodes([node])\n```\nI\u2019ve also factored out the reply logic from message handling into its own function, `answer_question`, just to make things a little easier to read. The first thing we're going to change is the prompt that we give to our LLM: we have to tell it that more recent messages are important. To do this we create a prompt template:\n\n\n```\ntemplate = (\n    \"Your context is a series of chat messages. Each one is tagged with 'who:' \\n\"\n    \"indicating who was speaking and 'when:' indicating when they said it, \\n\"\n    \"followed by a line break and then what they said. There can be up to 20 chat messages.\\n\"\n    \"The messages are sorted by recency, so the most recent one is first in the list.\\n\"\n    \"The most recent messages should take precedence over older ones.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"You are a helpful AI assistant who has been listening to everything everyone has been saying. \\n\"\n    \"Given the most relevant chat messages above, please answer this question: {query_str}\\n\"\n)\nqa_template = PromptTemplate(template)\n```\nThe fun thing about working with LLMs is how often you end up just describing what you\u2019re doing in English and that being what you send to the LLM. A prompt template will automatically get the `context_str` and `query_str` from the query engine. But we have to set this template on our query engine, like so:\n\n\n```\nquery_engine.update_prompts(\n    {\"response_synthesizer:text_qa_template\": qa_template}\n)\n```\nNow there\u2019s two more things we\u2019re going to change. We\u2019re going to take the results we get from the vector store and sort them by recency, something LlamaIndex has a built\\-in class for. It\u2019s called the `FixedRecencyPostprocessor`. We tell it the key that holds the timestamp (which we defined earlier on the nodes, above) and how many results it should return:\n\n\n```\npostprocessor = FixedRecencyPostprocessor(\n    top_k=20, \n    date_key=\"when\", # the key in the metadata to find the date\n    service_context=ServiceContext.from_defaults()\n)\n```\nThen we need to create our query engine with the postprocessor attached:\n\n\n```\nquery_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor])\n```\nWhile we were at it we did our final thing, which was pass `similarity_top_k=20`, which means the vector store will give us 20 Slack messages as context (the default is just 2, because usually the chunks of text in a Node are a lot bigger).\n\nTada! Now the bot knows to take more recent statements as the truth.\n\n![](/blog/images/1*ZFpJSOdrjdb-zvYFo22wLg.png)Step 8: draw the rest of the owl\n================================\n\nThis bot is working pretty well now, but I was having such fun when building it I got carried away and added two more features:\n\n* I attached metadata about *who* was speaking, not just when, so the bot can answer questions like \u201cWhat did Logan say about the project?\u201d\n* My colleagues interacting with the bot tried to ask follow\\-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it\u2019s in a thread, and treat replies in a thread as follow\\-up questions, even if the user doesn\u2019t mention the bot directly:\n\n!",
        "a14fe588-26e3-4ef0-b410-26861d7224df": "```\nprometheus_relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n       1. You are provided with evaluation task with the help of a query with response and context.\n       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n       4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\u201d \n       5. Please do not generate any other opening, closing, and explanations. \n\n        ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n\n        ###Query and Response: {query_str} \n\n        ###Context: {context_str}\n            \n        ###Score Rubrics: \n        Score YES: If the response for the query is in line with the context information provided.\n        Score NO: If the response for the query is not in line with the context information provided.\n    \n        ###Feedback: \"\"\"\n\nprometheus_relevancy_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n   1. You are provided with evaluation task with the help of a query with response and context and an existing answer.\n   2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n   3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n   4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n   5. Please do not generate any other opening, closing, and explanations. \n\n   ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n\n   ###Query and Response: {query_str} \n\n   ###Context: {context_str}\n            \n   ###Score Rubrics: \n   Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n   Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n    \n   ###Feedback: \"\"\"\n```\nDefine Correctness, FaithFulness, Relevancy Evaluators\n------------------------------------------------------",
        "b1dd1519-3d94-429d-8787-56fbb921040c": "```\nfrom llama_index.indices import ZillizCloudPipelineIndex\n\nzcp_index = ZillizCloudPipelineIndex(\n    project_id=\"&lt;YOUR_ZILLIZ_PROJECT_ID&gt;\",\n    cluster_id=\"&lt;YOUR_ZILLIZ_CLUSTER_ID&gt;\",\n    token=\"&lt;YOUR_ZILLIZ_API_KEY&gt;\",\n)\nzcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"})\n```\nYou can copy the Project ID, Cluster ID and API Key from your Zilliz account as follows:\n\n![](/blog/images/1*kZo97PIiOIoXALGXn6TRJg.jpeg)![](/blog/images/1*2WKRAav1WaSHIyV2nvn4DA.jpeg)Ingest Documents\n================\n\nSuppose your application has multiple users, and you would like to tag each user\u2019s document to provide isolation. Your application logic can be implemented as follows. For simplicity, here we demo ingesting public documents. Currently, Zilliz Cloud Pipelines [supports](https://docs.zilliz.com/docs/run-pipelines#run-an-ingestion-pipeline?utm_source=referral&utm_medium=partner&utm_campaign=2024-01-18_blog_zcp-llamaindex_llamaindex) documents stored and managed in AWS S3 and Google Cloud Storage. Local document upload will also be supported soon.\n\n\n```\n# user1 ingests a document, it is technical documentation for v2.3 version. \nzcp_index.insert_doc_url(\n    url=\"https://publicdataset.zillizcloud.com/milvus_doc.md\",\n    metadata={\"user_id\": \"user1\", \"version\": \"2.3\"},\n)\n# user2 ingests a document, it is technical documentation for v2.2 version. \nzcp_index.insert_doc_url(\n    url=\"https://publicdataset.zillizcloud.com/milvus_doc_22.md\",\n    metadata={\"user_id\": \"user2\", \"version\": \"2.2\"},\n)\n```\nQuery\n=====\n\nTo conduct semantic search with *ZillizCloudPipelineIndex*, you can use it *as\\_query\\_engine()* by specifying a few parameters:\n\n* search\\_top\\_k: How many text nodes/chunks to retrieve. Optional, defaults to DEFAULT\\_SIMILARITY\\_TOP\\_K (2\\).\n* filters: Metadata filters. Optional, defaults to None. In this example, we set the filter to only retrieve docs of a specific user, to provide user\\-level data isolation.\n* output\\_metadata: What metadata fields to return with the retrieved text node. Optional, defaults to \\[].",
        "27c77623-ea2b-4036-9969-bef7dea561c9": "* [Wenqi Glantz](https://twitter.com/wenqi_glantz)\u00a0[tutorial](https://towardsdatascience.com/the-journey-of-rag-development-from-notebook-to-microservices-cc065d0210ef) on The Journey of RAG Development: From Notebook to Microservices.\n* [Wenqi Glantz](https://twitter.com/wenqi_glantz)\u00a0[tutorial video](https://www.youtube.com/watch?v=EBpT_cscTis) on 12 RAG Pain Points and Solutions in the RAG pipeline.\n\n\ud83c\udfa5 **Webinar:**\n\n* [Webinar](https://www.youtube.com/watch?v=ZP1F9z-S7T0) with Sisil from JasperAI on Practical Tips and Tricks for Productionizing RAG.",
        "5b1de1a3-9756-4c7b-ad69-318918fa493a": "```\n# get the state of pods/deployments\nkubectl get pods\nkubectl get deployments\n\n# useful for seeing logs/events of pods + full yaml config\nkubectl describe pod <pod name>\nkubectl logs <pod name>\n\n# list clusters kubectl knows about\nkubectl config get-contexts\n\n# switch kubectl to another cluster\nkubectl config use-context <context name>\n\n# delete things\nkubectl delete <pod/deployment/service> <name>\n```\nStep 2: Deploying Text Embeddings Interface\n===========================================\n\nIn order to run embeddings fast, we will deploy an embeddings server using HuggingFace\u2019s [Text Embedding Interface](https://github.com/huggingface/text-embeddings-inference) (TEI). This server has production\\-level features and optimizations out\\-of\\-the\\-box, including continuous batching, flash\\-attention, rust implementation, and more. HuggingFace provides prebuilt docker images to simplify deployment.\n\nHowever, the first step to running embeddings fast is to have a GPU. If you just signed up for AWS, you will have to request a quota increase. For me, I requested a few times for G5 instances (which run an Nvidia A10G GPU), and after a few days of testing on CPU, AWS gave me access to use up to 4 G5 instances.\n\nOnce you have a quota for GPU instances (like G5 nodes), you can create your cluster and deploy\n\n\n```\neksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\nsleep 5\nkubectl create -f ./tei-deployment.yaml\nsleep 5\nkubectl create -f ./tei-service.yaml\nsleep 5\necho \"Embeddings URL is: &lt;http://$&gt;(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n```\nThe code above will create a cluster, a deployment (i.e. our TEI server) and a load balancer server.\n\nYou can see the yaml configs in [the repo](https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/tei), and you can edit them as needed.\n\n**NOTE:** Make sure to write down the URL printed at the end! If you forget, you can get the URL in the `EKS` page on AWS. You\u2019ll want the external IP for the load balancer.\n\nStep 3: Deploying RabbitMQ\n==========================\n\nRabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each `Document` object is processed independently by a consumer.\n\nDeploying RabbitMQ on `EKS` was a little tricky, but using the RabbitMQ operator installed with `krew`, many things are abstracted away.\n\nFirst, you need to create your cluster. For whatever reason, this didn\u2019t work unless I also specified the zones\n\n\n```\neksctl create cluster \\\n  --name mqCluster \\\n  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\n```\nSince RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use `EBS` for storage. This was a frustrating step to figure out since most existing guides skip this detail!",
        "2b7b210b-8006-4f4c-8cb3-184f45950295": "Search and Reranking: Improving Result Relevance\n------------------------------------------------\n\nSearch systems typically employ two main methods: keyword and semantic. Keyword search matches exact query terms to indexed database content, while semantic search uses NLP and machine learning to understand query context and intent. Many effective systems combine both approaches for optimal results.\n\nAfter initial retrieval, reranking can further improve result relevance. Traditional reranking relies on historical user interaction data, but this approach struggles with new content and requires substantial data to train effectively. An advanced alternative is using cross\\-encoders, which directly compare query\\-result pairs for similarity.\n\nCross\\-encoders directly compare two pieces of text and compute a similarity score. Unlike traditional semantic search methods, we cannot precompute embeddings for cross\\-encoders and reuse them later. Instead, we must run the cross\\-encoder for every pair of texts we want to compare, making this method computationally expensive and impractical for large\\-scale searches. However, it is highly effective for reranking a subset of our dataset because it excels at evaluating new, unseen data without the need for extensive user interaction data for fine\\-tuning.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ff2d7e23be53304eec28622c65b9b56f05b904bd7-1999x1296.png%3Ffit%3Dmax%26auto%3Dformat&w=2048&q=75)Cross\\-encoders complement and enhance traditional reranking systems by addressing their limitations in deep text analysis, particularly for novel or highly specific content. They do not rely on large datasets of user interactions for training (though such data can still be beneficial) and are adept at handling new and previously unseen data. This makes cross\\-encoders an excellent choice for enhancing the relevance of search results in a reranking context.\n\nImplementing Reranking\n----------------------\n\nWe are going to implement a simple reranking example using LlamaIndex and the PostgresML managed index. For more info on the PostgresML managed index. Check out our announcement with LlamaIndex: [Simplify your RAG application architecture with LlamaIndex \\+ PostgresML](https://www.llamaindex.ai/blog/simplify-your-rag-application-architecture-with-llamaindex-postgresml).\n\nInstall the required dependencies to get started:\n\n\n```\npip install llama_index llama-index-indices-managed-postgresml\n```\nWe will be using the Paul Graham dataset which can be downloaded with curl:\n\n\n```\nmkdir data\n\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n```\nThe PostgresML Managed Index will handle storing, splitting, embedding, and querying our documents. All we need is a database connection string. If you haven\u2019t already, [create your PostgresML account](https://postgresml.org/signup). You\u2019ll get $100 in free credits when you complete your profile.\n\nSet the PGML\\_DATABASE\\_URL environment variable:\n\n\n```\nexport PGML_DATABASE_URL=\"{YOUR_CONNCECTION_STRING}\"\n```\nLet\u2019s create our index:\n\n\n```\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom llama_index.indices.managed.postgresml import PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name=\"llama-index-rerank-example\"\n)\n```\nNote the collection\\_name is used to uniquely identify the index you are working with.\n\nHere we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents.\n\nThis workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. This is a unique quality of using the PostgresML managed index.\n\nNow let\u2019s search! We can perform semantic search and get the top 2 results by creating a retriever from our index.",
        "3bd04756-2ec7-4806-8cc1-2a4ff8c7af0e": "* [Logan](https://twitter.com/LoganMarkewich) [tutorial](https://www.youtube.com/watch?v=mIyZ_9gqakE) on retrieval/reranking, covering Node Parsing, AutoMergingRetriever, HierarchicalNodeParser, node post\\-processors, and the setup of a RouterQueryEngine.\n\n**Integrations with External Platforms**\n========================================\n\n1. **Integration with PortkeyAI**: LlamaIndex integrates with PortkeyAI, boosting LLM providers like OpenAI with features like auto fallbacks and load balancing. [Tweet,](https://x.com/llama_index/status/1699087716183638256?s=20) [Documentation](https://gpt-index.readthedocs.io/en/latest/examples/llm/portkey.html)\n2. **Collaboration with Anyscale**: LlamaIndex collaborates with anyscalecompute, enabling easy tuning of open\\-source LLMs using Ray Serve/Train. [Tweet,](https://twitter.com/llama_index/status/1699444987627466986?s=20) [Documentation](https://gpt-index.readthedocs.io/en/latest/examples/llm/anyscale.html)\n3. **Integration with Elastic**: LlamaIndex integrates with Elastic, enhancing capabilities such as vector search, text search, hybrid search models, enhanced metadata handling, and es\\_filters. [Tweet,](https://twitter.com/llama_index/status/1700195709041954929?s=20) [Documentation](https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/ElasticsearchIndexDemo.html)\n4. **Integration with MultiOn**: LlamaIndex integrates with MultiOn, enabling data agents to navigate the web and handle tasks via an LLM\\-designed browser. [Tweet,](https://twitter.com/llama_index/status/1700221470427754610?s=20) [Documentation](https://llamahub.ai/l/tools-multion)\n5. **Integration with Vectara**: LlamaIndex collaborates with Vectara to streamline RAG processes from loaders to databases. [Tweet,](https://twitter.com/llama_index/status/1701673229675552876?s=20) [Blog Post](https://medium.com/llamaindex-blog/llamaindex-vectara-7a3889cd34cb)\n6. **Integration with LiteLLM**: LlamaIndex integrates with LiteLLM, offering access to over 100 LLM APIs and features like chat, streaming, and async operations. [Tweet,](https://x.com/llama_index/status/1703188185323561432?s=20) [Documentation](https://gpt-index.readthedocs.io/en/stable/examples/llm/litellm.html)\n7. **Integration with MonsterAPI**: LlamaIndex integrates with MonsterAPI, allowing users to query data using LLMs like Llama 2 and Falcon. [Tweet,](https://x.com/monsterapis/status/1702252516061372595?s=20) [Blog Post](https://blog.monsterapi.ai/llama-index-monsterapi-integration-llm-rag/)\n\n**Events:**\n===========\n\n1. [Jerry Liu](https://twitter.com/jerryjliu0) spoke on [Production Ready LLM Applications](https://docs.google.com/presentation/d/1uzhz1aFWbyXSrWBzQ1FPQWtVjMgJqAYGoGoVzEnNmAg/edit#slide=id.p) at the Arize AI event.\n2. [Ravi Theja](https://twitter.com/ravithejads) conducted a [workshop](https://x.com/ravithejads/status/1699644440002826350?s=20) at LlamaIndex \\+ Replit Pune Generative AI meetup.\n3.",
        "1b24ad1e-a7af-4544-a6cb-4f4c57f48a30": "[](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F7ff5242c5683702cdad91a13ac0f7212a5f0505b-3193x2280.webp%3Ffit%3Dmax%26auto%3Dformat&w=3840&q=75)\n\nRAG marketplaces is a use case that can be made possible with llama\\-index\\[networks].\n\nOne possible world that could easily be powered by `llama-index[networks]` are marketplaces for RAG. A place where data suppliers package their data in the form of RAGs to data consumers look to expand their own query system\u2019s knowledge. Potential RAG (data) suppliers could be your local newspaper, book publishing companies, etc.\n\nIn this new data supply and consumption model, there is more onus on the data supplier to prepare the data in a fashion that makes it easier to consume \u2014 specifically, the suppliers own the building of the query engine. This should benefit the data consumer greatly since, with a standard interface that is provided by the likes of a `ContributorService` (that encapsulates a `QueryEngine`), they can get access to the knowledge they seek from the data easier than ever before (i.e., in relation to traditional data marketplaces that exchange raw data).\n\nIt is with this kind of vision that we\u2019ve built `llama-index[networks]` to make it: (i) easier for data suppliers to supply the knowledge contained in their data in new and arguably more effective ways, and (ii) easier for data consumers to connect to these new forms of external knowledge.\n\nInternal networks: another potential use case\n---------------------------------------------\n\nIn addition to powering RAG marketplaces, we foresee the need of connecting RAGs that an overarching company may own, but not necessarily manage. More concretely, a franchise may have the rights to the data across all of its operations. And, while they could build a \u201ccentral\u201d, monolithic RAG over the entire database, it may still be more efficient and effective to build RAGs over the individual operators and query these instead.\n\nThe idea of exchanging information to build better, more knowledgeable systems is not new. However, the idea of using RAGs to facilitate that exchange may be (to our knowledge, it is), and we believe that both existing and new use cases requiring such collaboration can benefit from concept.\n\nA quick note on privacy\n-----------------------\n\nAn important consideration in the collaboration of data is privacy and security. It bears mentioning that the examples above assume that the data that is being shared across the network is in compliance with data privacy and security laws and regulations. It is our belief that as this technology grows, that the necessary features and capabilities will be developed and incorporated to facilitate in\\-compliance RAG networks.\n\nCheck out the demo to learn more!\n\nTo see an actual demo of a network connecting to a set of contributors, check out the Github repository [code](https://github.com/run-llama/llama_index/tree/main/llama-index-networks) for `llama-index-networks` and navigate to the `examples/demo` subfolder.",
        "d8f36680-df79-4869-8376-7423f67bae3e": "Greetings, LlamaIndex Family! \ud83e\udd99\n\nWelcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week.\n\n\ud83e\udd29\u00a0**The highlights:**\n---------------------\n\n* **Secure Code Execution with AzureCodeInterpreterTool:** Securely run LLM\\-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution.\n* **Build Automated Email Agents:** Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails.\n* **LlamaFS for Organized Files:** Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API.\n* **RAGApp's No\\-Code Chatbots:** Deploy RAG chatbots easily with RAGApp's no\\-code interface, fully open\\-source and cloud\\-compatible.\n\n**\u2728 Feature Releases and Enhancements:**\n----------------------------------------\n\n1. We have launched Azure Container Apps dynamic sessions to securely run LLM\\-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you\u2019re ready to go. [Blogpost](https://www.llamaindex.ai/blog/secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions), [Tweet](https://x.com/llama_index/status/1792958928357335115).\n2. We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance. [Docs](https://docs.llamaindex.ai/en/stable/examples/embeddings/nomic/), [Tweet](https://x.com/llama_index/status/1793677965978673598).\n3. We have integrated the Vespa vector store, supporting hybrid search with BM25\\. [Docs](https://docs.llamaindex.ai/en/stable/examples/vector_stores/VespaIndexDemo/), [Tweet](https://x.com/llama_index/status/1794106979213869413).\n4. We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre\\-process large datasets with an LLM, enabling advanced analysis and querying capabilities. [Docs](https://www.llamaindex.ai/blog/batch-inference-with-mymagic-ai-and-llamaindex), [Tweet](https://x.com/llama_index/status/1793385512386150856).\n\n**\ud83d\uddfa\ufe0f Guides:**\n--------------\n\n* [Guide](https://www.llamaindex.ai/blog/automate-online-tasks-with-multion-and-llamaindex) to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities.\n* [Guide](https://www.koyeb.com/tutorials/using-llamaindex-and-mongodb-to-build-a-job-search-assistant#create-a-job-indexing-api-endpoint) to building Full\\-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real\\-time response streaming and continuous job updates.\n\n**\ud83d\udda5\ufe0f\u00a0Demos:**\n-------------\n\n* LlamaFS, a project developed by [Alex Reibman](https://x.com/AlexReibman) and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency. [Code](https://github.com/iyaja/llama-fs), [Tweet](https://x.com/llama_index/status/1794762651769430381).\n* RAGApp, a project developed by [Marcus Schiesser](https://x.com/MarcusSchiesser), offers a no\\-code interface for configuring RAG chatbots as simply as GPTs by OpenAI.",
        "1fcfb0dd-6082-4eca-9d9b-a9ba79682c0c": "```\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.postprocessor import LongLLMLinguaPostprocessor\n\n### Recipe\n### Define a Postprocessor object, here LongLLMLinguaPostprocessor\n### Build QueryEngine that uses this Postprocessor on retrieved docs\n\n# Define Postprocessor\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=\"Given the context, please answer the final question\",\n    target_token=300,\n    rank_method=\"longllmlingua\",\n    additional_compress_kwargs={\n        \"condition_compare\": True,\n        \"condition_in_question\": \"after\",\n        \"context_budget\": \"+100\",\n        \"reorder_context\": \"sort\",  # enable document reorder\n    },\n)\n\n# Define VectorStoreIndex\ndocuments = SimpleDirectoryReader(input_dir=\"...\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n# Define QueryEngine\nretriever = index.as_retriever(similarity_top_k=2)\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=[node_postprocessor]\n)\n\n# Used your advanced RAG\nresponse = retriever_query_engine.query(\"A user query\")\n```\n**2\\. Result Re\\-Rank:** LLMs suffer from the so\\-called \u201cLost in the Middle\u201d phenomena which stipulates that LLMs focus on the extreme ends of the prompts. In light of this, it is beneficial to re\\-rank retrieved documents before passing them off to the Generation component.\n\n**LlamaIndex Re\\-Ranking For Better Generation Recipe** ([notebook guide](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html))**:**\n\n\n```\nimport os\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.postprocessor import LongLLMLinguaPostprocessor\n\n### Recipe\n### Define a Postprocessor object, here CohereRerank\n### Build QueryEngine that uses this Postprocessor on retrieved docs\n\n# Build CohereRerank post retrieval processor\napi_key = os.environ[\"COHERE_API_KEY\"]\ncohere_rerank = CohereRerank(api_key=api_key, top_n=2)\n\n# Build QueryEngine (RAG) using the post processor\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=[cohere_rerank],\n)\n\n# Use your advanced RAG\nresponse = query_engine.query(\n    \"What did Sam Altman do in this essay?\"\n)\n```\nAdvanced techniques for simultaneously addressing Retrieval and Generation success requirements\n===============================================================================================\n\nIn this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries).\n\n1. **Generator\\-Enhanced Retrieval:** These techniques make use of the LLM\u2019s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response.\n\n**LlamaIndex Generator\\-Enhanced Retrieval Recipe** ([notebook guide](https://docs.llamaindex.ai/en/stable/examples/query_engine/flare_query_engine.html))**:**",
        "c0bc7f63-a9c2-417c-8398-372a0afedbc0": "Summary\n=======\n\nAgents are a popular use\\-case for Large Language Models (LLMs), typically provide a structure that enables LLMs to make decisions, use tools, and accomplish tasks. These agents can take many forms, like the fully\\-autonomous versions seen with [Auto\\-GPT](https://github.com/Significant-Gravitas/Auto-GPT), to more controlled implementations like [Langchain](https://python.langchain.com/en/latest/) Agents. With the recent release of [Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents), we showcase how [LlamaIndex](https://www.llamaindex.ai/) continues to be a useful tool for agents, by augmenting their existing image\\-generator tool. Using an vector index created from 10K [DiffusionDB](https://huggingface.co/datasets/poloclub/diffusiondb) prompts, the Text2Image Prompt Assistant tool we created can re\\-write prompts to generate more beautiful images. Full source code is available in the [Hugging Face Space for the tool](https://huggingface.co/spaces/llamaindex/text2image_prompt_assistant/tree/main), and a [colab notebook](https://colab.research.google.com/drive/1r0t423LTkCYi5fGLrSfTdtC0DzKuU-zq?usp=sharing) is available as a usage walkthrough.\n\nCreating the Tool\n=================\n\nTransformers Agents come with a variety of per\\-configured tools that leverage the vast amounts of open\\-source models hosted on Hugging Face\\-Hub. Furthermore, additional tools can be created and shared by simply publishing a new Hugging Face Space with the proper tool setup.\n\nTo create a tool, your code simply needs a `tool_config.json` file that describes the tool, as well as a file containing the implementation of your tool. While the documentation was a little fuzzy for this part, we eventually were able to use [the implementation of existing custom tools](https://huggingface.co/huggingface-tools) as the framework for our own.\n\nTo enable LlamaIndex to write text\\-to\\-image prompts, we need a way to show the LLM what examples of good prompts look like. To do this, we indexed 10K random text\\-to\\-image prompts from DiffusionDB.\n\n\n```\nfrom datasets import load_dataset\nfrom llama_index import VectorStoreIndex, Document\n\n# downloads a LOT of data\ndataset = load_dataset('poloclub/diffusiondb', '2m_random_10k')\n\ndocuments = []\nfor sample in dataset['train']:\n    documents.append(Document(sample['prompt']))\n\n# create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# store index\nindex.storage_context.persist(persist_dir=\"./storage\")\n```\nTo get LlamaIndex to write prompts using examples, we need to customize the prompt templates a bit. You can see the final prompt templates and how to use them below:\n\n\n```\ntext_qa_template = Prompt(\n    \"Examples of text-to-image prompts are below: \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the existing examples of text-to-image prompts, \"\n    \"write a new text-to-image prompt in the style of the examples, \"\n    \"by re-wording the following prompt to match the style of the above examples: {query_str}\\n\"\n)",
        "46d110ae-ccac-47ca-a90d-e250cb1fd1fd": "A few days ago, we published a blog on [Multi\\-Modal RAG](/multi-modal-rag-621de7525fea) (Retrieval\\-Augmented Generation) and our latest (still in beta) abstractions to help enable and simplify building them. In this post, we now go over the important topic of how one can sensibly evaluate Multi\\-Modal RAG systems.\n\nA natural starting point is to consider how evaluation was done in traditional, text\\-only RAG and then ask ourselves how this ought to be modified to suit the multi\\-modal scenario (e.g., in text\\-only RAG, we use an LLM, but in multi\\-modal RAG we require a Large Multi\\-Modal Model or LMM for short). This is exactly what we\u2019ll do next and as you\u2019ll see, the overarching evaluation framework stays the same as it was in the text\\-only RAG, requiring only a few additions and modifications in order to make it more multi\\-modal appropriate.\n\nPrimer: Multi\\-Modal RAG vs Text\\-Only RAG\n==========================================\n\n\n\nIllustration of text\\-only RAG versus multi\\-modal RAG. In multi\\-modal RAG, images modality can show up in the user query, the retrieved context, as well as the final answer.\n\n![](/blog/images/1*QXZqkYL0hjW-mxvre21GGw.png)Let\u2019s consider the main differences between multi\\-modal and text\\-only RAG. Below are two tables that describe the RAG build considerations as well as query\\-time pipeline and compares and contrasts multi\\-modal and text\\-only cases against them.",
        "9fc3c729-cef5-4be8-a63f-95034ca4ca9b": "Hello Llama Enthusiasts \ud83e\udd99!\n\nAnother week has flown by, and we\u2019re back with a jam\\-packed newsletter filled with updates on hackathons, guides, integrations, features, webinars, tutorials, blogs, and demos. If you have a project, blog post, or video that deserves a spotlight, we\u2019d love to feature it! Just reach out to us at [news@llamaindex.ai](mailto:news@llamaindex.ai).\n\nBonus: You can now get all these updates straight to your inbox! Simply visit our [homepage](https://www.llamaindex.ai/) and sign up for our email updates.\n\n\ud83e\udd29 **First, the highlights:**\n\n1. **AI.Engineer Summit**: At the AI.Engineer Summit, Jerry Liu discussed RAG applications, while Simon led a workshop on RAG app optimization (Jerry\u2019s [slides](https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit#slide=id.p), Simon\u2019s [slides](https://github.com/run-llama/ai-engineer-workshop/blob/main/presentation.pdf))\n2. **Text to pgVector**: we launched PGVectorSQLQueryEngine for combined SQL and vector queries on PostgreSQL. ([Docs](https://t.co/4h3nTzzJ5E), [Tweet](https://x.com/jerryjliu0/status/1712496323742851188?s=20))\n3. **Hugging Face Integration**: Integrated with HuggingFace\u2019s text\\-embeddings\\-inference server for high\\-speed, large\\-scale BERT model serving. ([Docs](https://docs.llamaindex.ai/en/latest/examples/embeddings/text_embedding_inference.html), [Tweet](https://x.com/jerryjliu0/status/1712943016590381554?s=20))\n4. **Multi\\-Document Agents**: New V1 agents support advanced multi\\-document retrieval and async query planning. ([Docs](https://t.co/bWYv0R7J2B), [Tweet](https://x.com/llama_index/status/1712129914386993295?s=20))\n5. **Unstructured Parsing**: Unveiled UnstructuredElementNodeParser, a hierarchical parser for embedded tables/text using UnstructuredIO. ([Docs](https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html), [Tweet](https://x.com/llama_index/status/1711768906866864403?s=20))\n6. **LLM Compatibility**: We have charted LLM performances on various tasks and found that the Zephyr\\-7b\\-alpha model stands out as the top\\-performing 7B model in advanced RAG tasks. ([Docs](https://docs.llamaindex.ai/en/latest/core_modules/model_modules/llms/root.html#llm-compatibility-tracking))\n\n\ud83c\udfc6 Congratulations to our AGI House Hackathon Winners!\n=====================================================\n\nWe love seeing people build amazing things with LlamaIndex!\n\n**Build:**\n\n1. [Demostify](https://drive.google.com/file/d/18Ru1FCchVpMi8jzjr2dHdDuZtCG83zOJ/view)\n2.",
        "983d9c71-0370-496f-8951-eabd7e3c7fd9": "```\n# Test the agent with simple answers that could leverage Python codes\nprint(agent.chat(\"Tell me the current time in Seattle.\"))\n```\nThe agent generates python code to determine the time and convert it to the correct time zone. It passes this code to Azure Container Apps dynamic sessions, which execute the code and return the answer:\n\n\n```\nThought: To provide the current time in Seattle, I need to calculate it based on the current UTC time and adjust for Seattle's time zone, which is Pacific Daylight Time (PDT) during daylight saving time and Pacific Standard Time (PST) outside of daylight saving time. PDT is UTC-7, and PST is UTC-8. I can use the code interpreter tool to get the current UTC time and adjust it accordingly.\nAction: code_interpreter\nAction Input: {'python_code': \"from datetime import datetime, timedelta; import pytz; utc_now = datetime.now(pytz.utc); seattle_time = utc_now.astimezone(pytz.timezone('America/Los_Angeles')); seattle_time.strftime('%Y-%m-%d %H:%M:%S %Z%z')\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '2024-05-04 13:54:09 PDT-0700', 'executionTimeInMilliseconds': 120}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The current time in Seattle is 2024-05-04 13:54:09 PDT.\nThe current time in Seattle is 2024-05-04 13:54:09 PDT.\n\n```\nYou can also use the tool to safely inspect and manipulate data, as in this example where we ask it to open a CSV file and answer questions about it:\n\n\n```\n# Upload a sample temperature file of a day in Redmond Washington and ask a question about it\nres = azure_code_interpreter_spec.upload_file(\n    local_file_path=\"./TemperatureData.csv\"\n)\nif len(res) != 0:\n    print(\n        agent.chat(\"Find the highest temperature in the file that I uploaded.\")\n    )\n```\nIt doesn\u2019t just read data from the CSV, it performs math on it to determine the highest temperature:\n\n\n```\nThought: I need to use the list_files tool to get the metadata for the uploaded file, and then use python to read the file and find the highest temperature.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: I have the metadata for the file. I need to use python to read the file and find the highest temperature.\nAction: code_interpreter\nAction Input: {'python_code': \"import csv\\n\\nwith open('/mnt/data/TemperatureData.csv', 'r') as f:\\n    reader = csv.reader(f)\\n    next(reader)\\n    highest_temp = float('-inf')\\n    for row in reader:\\n        temp = float(row[1])\\n        if temp > highest_temp:\\n            highest_temp = temp\\nprint(highest_temp)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '12.4\\n', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 26}\nThought: I have the highest temperature. I can answer the question.\nAnswer: The highest temperature in the file is 12.4 degrees.\nThe highest temperature in the file is 12.4 degrees.\n```\nThe tool can also safely modify your uploaded files, as in this example where we get it to sort the data in the CSV:\n\n\n```\n# Ask the LLM to perform modifications on the sample temperature file\nprint(\n    agent.chat(\n        \"Rearrange the temperature data in a descending order and save it back to the original csv file.\"\n    )\n)\n```\nIn this example, you\u2019ll see that the LLM initially produces code with a syntax error, but the agent is smart enough to figure out what went wrong and correct itself:",
        "54ac02cd-8612-4ba2-8729-f26e285d8703": "[Source: \u201cAdvanced RAG Techniques: an Illustrated Overview\u201d by Ivan Ilin](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n\n![](/blog/images/1*pMzip79Hk1qFjNbhvw59Lw.png)**RAG has become more modular:** Instead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the [RAG Survey paper by Gao et al.](https://arxiv.org/pdf/2312.10997.pdf)\n\nThis leads to creative new patterns like [DSP](https://github.com/stanfordnlp/dspy), [Rewrite\\-Retrieve\\-Read](https://arxiv.org/abs/2305.14283), or [interleaving retrieval\\+generation multiple times](https://arxiv.org/abs/2305.15294).\n\nPrevious State of LlamaIndex\n----------------------------\n\nLlamaIndex itself has hundreds of RAG guides and 16\\+ Llama Pack recipes letting users setup [different RAG pipelines](/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b), and has been at the forefront of establishing advanced RAG patterns.\n\nWe\u2019ve also exposed low\\-level modules such as [LLMs](https://docs.llamaindex.ai/en/latest/module_guides/models/llms.html), [prompts](https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html#prompts), [embeddings](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html), [postprocessors](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html) and easy subclassability of core components like [retrievers](https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers.html) and [query engines](https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html) so that users can define their own workflows.\n\nBut up until now, we didn\u2019t explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively.\n\nQuery Pipeline\n==============\n\nAs a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity.\n\nYou can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code.\n\nIt has the following benefits:\n\n* **Express common query workflows with fewer lines of code/boilerplate:** Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module!\n* **Greater readability:** Reduced boilerplate leads to greater readability.\n* **End\\-to\\-end observability:** Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations.\n* **\\[In the future] Easy Serializability:** A declarative interface allows the core components to be serialized/redeployed on other systems much more easily.\n* **\\[In the future] Caching:** This interface also allows us to build a caching layer under the hood, allowing input re\\-use.",
        "10647862-eacd-4a78-a72f-a86a8007add5": "ChatGPT, developed by OpenAI, has changed the way we interact online. Being a general purpose chatbot, ChatGPT is limited to answering generic queries. But it becomes even more useful if you can get it to answer your questions specific to your business. To do that, you need to train ChatGPT on your data.\n\n[EmbedAI](https://www.thesamur.ai/) is a no\\-code platform for creating AI chatbots trained on your business data. This includes data sourced from web pages, PDFs, Notion documents, or YouTube videos, allowing EmbedAI to adapt to a wide range of information sources.\n\nIn this blog post, we\u2019ll show you how we used [LlamaIndex](https://www.llamaindex.ai/) with [EmbedAI](https://thesamur.ai/) to enable us to train ChatGPT on your own data, helping you create a customized and effective AI chatbot tailored for your business needs.\n\nChat with your data use\\-cases\n==============================\n\nThere\u2019s a variety of ways that a chatbot trained on your data could be helpful, including:\n\n1. **Customer Support Bot**: Manages frequently asked questions about a product, addressing customer support inquiries efficiently.\n2. **Company Search Engine**: Finds internal company documents and information fast, boosting workplace efficiency.\n3. **Personalized Learning Assistant**: Offers tailored educational support and study guidance based on specific course content.\n4. **Technical Support assistant**: Provides in\\-depth help for complex software issues, from troubleshooting to usage tips.\n5. **Healthcare Assistant**: Gives general health advice and information, based on medical literature and FAQs.\n6. **Finance Chatbot**: Assists with financial queries, offering advice on products, market trends, and investment strategies by training on financial data\n\nLet\u2019s delve into creating our own chat apps that integrate with various data sources like PDFs, Notion documents, videos, webpages, and more.\n\nCase 1: Custom ChatGPT for your site\n====================================\n\nTo train ChatGPT on your website content, we need to scrape the content from all the relevant webpages. The steps to do this are:\n\n* Extract all the URLs from your website, such as from your sitemap\n* Include only relevant URLs which you need to train on\n* Use SimpleWebPageReader from [LlamaIndex](https://www.llamaindex.ai/) to download the content from these URLs\n\nHere\u2019s some sample code to do that:\n\n![](/blog/images/1*CJ-nuYuvaqLKlFntUlQucw.png)Once the data is ready, an AI chatbot can be trained on these documents by using LlamaIndex\u2019s VectorStoreIndex class.\n\nTo create a ChatGPT chatbot on your website without coding you can use EmbedAI as outlined below which uses LlamaIndex internally:\n\nCase 2: Custom ChatGPT for your PDF documents\n=============================================\n\nIf your business specific data is stored in PDF documents and you wish to create a chatbot that can surface the information in them we can do that with LlamaIndex using the PDFMiner library. This time the steps are:\n\n* Upload your PDFs and store them in the cloud\n* Install the PDFMiner library\n* Fetch the uploaded PDFs and extract the document text using LlamaIndex loader\n\nHere\u2019s the code for creating an AI chatbot trained on PDF documents with LlamaIndex\n\n![](/blog/images/1*AonaZTh1O3srF5V5JBKC9w.png)If you want to create a ChatGPT chatbot on your PDF content without coding you can use EmbedAI as in the demo below which uses LlamaIndex internally\n\nCase 3: Custom ChatGPT for your videos\n======================================\n\nOften, valuable information is embedded in videos, which isn\u2019t as accessible for users searching for information. However, by training an AI chatbot with this content, it can become an incredibly rich resource for your users, significantly enhancing their experience.",
        "d507d681-f22e-4313-8fea-f1551a242792": "```\n=== Calling Function ===\nCalling function: sum with args: {\n  \"a\": 5,\n  \"b\": 5\n}\nGot output 10\n==========================\n=== Calling Function ===\nCalling function: multiply with args: {\n  \"a\": 10,\n  \"b\": 2\n}\nGot output 20\n==========================\nThe result of adding 5 and 5 is 10. When you multiply 10 by 2, the result is 20.\n```\nUsing Agents with your documents\n================================\n\nThe second agent will be responsible for going through a set of Dan Abramov essays and answering questions based on the available data.\n\nFirstly, we will import the necessary classes and functions\n\nImport the classes and functions\n--------------------------------\n\n\n```\nimport {\n  OpenAIAgent,\n  SimpleDirectoryReader,\n  VectorStoreIndex,\n  SummaryIndex,\n  QueryEngineTool,\n} from \"llamaindex\";\n```\nLoading Documents\n-----------------\n\nNow we will load the documents and insert them into a local vector store index which will be responsible for storing the documents and allowing the agent to query the most relevant data for the task and a summarize vector index which can better help on tasks summary related.\n\n\n```\n// Load the documents\nconst documents = await new SimpleDirectoryReader().loadData({\n  directoryPath: \"node_modules/llamaindex/examples\",\n});\n\n// Create a vector index from the documents\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\nconst summaryIndex = await SummaryIndex.fromDocuments(documents)\n```\nCreating the Query Engine Tool\n------------------------------\n\nNow we will create the tooling that allows the Agents to access the Vector Index:\n\n\n```\n// Create a query engine from the vector index\nconst abramovQueryEngine = vectorIndex.asQueryEngine();\nconst abramovSummaryEngine = summaryIndex.asQueryEngine();\n\n// Create a QueryEngineTool with the vector engine\nconst vectorEngineTool = new QueryEngineTool({\n  queryEngine: abramovQueryEngine,\n  metadata: {\n    name: \"abramov_query_engine\",\n    description: \"Use this engine to answer specific questions about Abramov\",\n  },\n});\n\n// Create a QueryEngineTool with the summary engine\nconst summaryEngineTool = new QueryEngineTool({\n  queryEngine: abramovSummaryEngine,\n  metadata: {\n    name: \"abramov_summary_engine\",\n    description: \"Use this engine to generate summaries about Abramov\",\n  },\n});\n```\nCreating the OpenAI Agent\n-------------------------\n\nNow we can create and provide the necessary tools for the agent\n\n\n```\n// Setup the agent \nconst agent = new OpenAIAgent({\n  tools: [vectorEngineTool, summaryEngineTool],\n  verbose: true,\n});\n```\nChat with Agent\n---------------\n\nNow you can chat with the agent about Dan Abramov and it will select the right tools to achieve the goal.\n\n\n```\n\n// Chat with Agent\nconst response = await agent.chat({\n  message: \"Where he worked in his 20s?\",\n});\n\n// Log output\nconsole.log(String(response));\n```\nThe tool output",
        "b86d28ee-9a93-472f-90fa-71f83b88cbcd": "> The table you\u2019ve provided shows performance benchmark scores for different model sizes across various AI models such as MPT, Falcon, and LLaMA on different tasks, such as Aqua\\-RAT, LogiQA, LastAR, SAT\\-en, and SAT\\-math. For the SAT\\-en task specifically, you asked which model in the 7B series has the highest performance. To find out, we need to look at the column labeled \u201cSAT\\-en (w/o Ps\\_8\\).\u201d In the 7B series of models, here are the scores: \u2014 MPT 7B: 37\\.1 \u2014 Falcon 7B: 37\\.3 \u2014 LLaMA 7B: 63\\.9 \u2014 Model2 7B: 37\\.4 The LLaMA 7B model outperforms the other 7B models on the SAT\\-en (w/o Ps\\_8\\) task with a score of 63\\.9\\.\n\n**Observation:**\n----------------\n\nIncorrect answer and recognized Llama series of models and values incorrectly.\n\nChain of thought prompting:\n===========================\n\n**Question:** Based on the image provided. Follow the steps and answer the query \u2014 which model has higher performance in SAT\\-en in 7B series models?\n\nExamine the Image: Look at the mentioned category in the query in the Image.\n\nIdentify Relevant Data: Note the respective percentages.\n\nEvaluate: Compare if there is any comparison required as per the query.\n\nDraw a Conclusion: Now draw the conclusion based on the whole data.\n\n**Answer:**\n\n\n> To answer which model has higher performance in SAT\\-en in the 7B series models, I will follow the provided steps: Examine the Image: The categories in the image include model names and sizes, and performance scores for various tasks, including the SAT\\-en category. Identify Relevant Data: The relevant data for the SAT\\-en category are the percentages listed under it for each 7B series model. Evaluate: I will compare the SAT\\-en percentages of each 7B series model. Draw a Conclusion: The SAT\\-en scores for the 7B series models are as follows: \u2014 MPT 7B: 63\\.1% \u2014 Falcon 7B: 73\\.4% \u2014 LLama 1 7B: No data present for this category. \u2014 LLama 2 7B: 76\\.6% Based on the data, the LLama 2 7B model has the highest SAT\\-en performance among the 7B series models with a score of 76\\.6%.\n\nObservation:\n------------\n\nWith chain of thought prompting we are able to get the right conclusion though it should be noted that it picked up wrong values.\n\nFinal Observations:\n===================\n\nObservations made based on experiments on Hallucination and correctness. (Please note that these observations are specific to the images used and cannot be generalized, as they vary depending on the images.)\n\n![](/blog/images/1*sMCkM4DYmFC9S_EkATNM4Q.png)Summary\n=======\n\nIn this blog post, we have showcased experiments ranging from general inquiries to systematic questions and chain of thought prompting techniques and observed Hallucination and correctness metrics.\n\nHowever, it should be noted that the outputs from GPT\\-4V can be somewhat inconsistent, and the levels of hallucination are slightly elevated. Therefore, repeating the same experiment could result in different answers, particularly with generalized questions.",
        "1e30b299-06d9-4dc7-aa60-1b15a5be8e95": "```\nfrom llama_index import ServiceContext\n\nst.session_state[\"service_context\"] = ServiceContext.from_defaults(\n            llm=OpenAI(\n                model=\"gpt-3.5-turbo\",\n                temperature=0.2,\n                chunk_size=1024,\n                chunk_overlap=100,\n                system_prompt=\"As an expert current affairs commentator and analyst,\\\n                               your task is to summarize the articles and answer the questions from the user related to the news articles\",\n            ),\n            chunk_size=256, \n            chunk_overlap=20\n        )\n```\nNext, we create a text splitter using **TokenTextSplitter** and a node parser using **SimpleNodeParser** to parse multiple articles.\n\n\n```\nfrom llama_index.text_splitter import TokenTextSplitter\nfrom llama_index.node_parser import SimpleNodeParser\n\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=256, chunk_overlap=20)\n#create node parser to parse nodes from document\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\nnodes = node_parser.get_nodes_from_documents(documents)\n```\nIn the third step, we create an index using **VectorStoreIndex**. To enable streaming capability, ensure to set **streaming\\=True** while setting up the **query\\_engine**.\n\n\n```\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex(\n        nodes=nodes,\n        service_context=st.session_state[\"service_context\"]\n    )\nst.session_state[\"chat_engine\"] = index.as_query_engine(streaming=True)\n```\nTo add streaming capability to Streamlit chat components, we took inspiration from this [code](https://arnaudmiribel.github.io/streamlit-extras/extras/streaming_write/). Instead of using **st.write()** from the regular chat implementation, we replaced it with the **write** function from **streaming\\_write**.\n\n\n```\nresponse = st.session_state[\"chat_engine\"].query(prompt)\ndef stream_example():\n    for word in response.response_gen:\n        yield word\nwrite(stream_example)\n```\n![](/blog/images/1*OALHjBauROdpVTCC-IYUuA.gif)\n\nStreaming Demo\n\nPredefined Prompts\n------------------\n\nThree different prompts are predefined in the article chat page, allowing users to select from a drop\\-down menu to ask questions without typing. The prompts are 5W1H, Similar Viewpoints, and Discrepancy Viewpoints.\n\n\n```\nprompt_content = {\n    \"5W1H\": 'Summarize the content details in the \"5W1H\" approach (Who, What, When, Where, Why, and How) in bullet points',\n    \"Similar Viewpoints\": \"Compare between the articles and provide the similar viewpoints in bullet points\",\n    \"Discrepency Viewpoints\": \"Compare between the articles and provide the discrepency viewpoints in bullet points\"\n}\n```\n![](/blog/images/1*vu0eKcG1gwe4n9nLXgdrrw.png)What\u2019s Next\n===========\n\nHuge thanks to LlamaIndex and Streamlit for generously providing a massive platform that allows more people to gain awareness of the organic news digest and save valuable time through [**NewsGPT**](https://newsgpt-clickbait-buster.streamlit.app). If you enjoyed reading the article and agree with our concept, please do not hesitate to leave a clap for the article and join [**Neotice**](https://www.neotice.app), the production app of NewsGPT, to support us. We are confident in our mission and look forward to having you on board with us. Thank you!\n\nYou can also connect us on LinkedIn:[**Kang\\-Chi Ho**](http://www.linkedin.com/in/kangchi-ho)**,** [**Jian\\-An Wang**](https://www.linkedin.com/in/chien-an-wang-6054b9110/)\n\n\ud83c\udf89 Click Here to Join Neotice \ud83d\udc49 [**Neotice**](https://www.neotice.app)",
        "2b32c68c-54a2-45fc-9a45-a6e704831231": "```\n    system_prompt = (f\"\"\"\n        You are on orchestration agent.\n        Your job is to decide which agent to run based on the current state of the user and what they've asked to do. Agents are identified by short strings.\n        What you do is return the name of the agent to run next. You do not do anything else.\n        \n        The current state of the user is:\n        {pprint.pformat(state, indent=4)}\n\n        If a current_speaker is already selected in the state, simply output that value.\n\n        If there is no current_speaker value, look at the chat history and the current state and you MUST return one of these strings identifying an agent to run:\n        * \"{Speaker.STOCK_LOOKUP.value}\" - if they user wants to look up a stock price (does not require authentication)\n        * \"{Speaker.AUTHENTICATE.value}\" - if the user needs to authenticate\n        * \"{Speaker.ACCOUNT_BALANCE.value}\" - if the user wants to look up an account balance\n            * If they want to look up an account balance, but they haven't authenticated yet, return \"{Speaker.AUTHENTICATE.value}\" instead\n        * \"{Speaker.TRANSFER_MONEY.value}\" - if the user wants to transfer money between accounts (requires authentication and checking an account balance first)\n            * If they want to transfer money, but is_authenticated returns false, return \"{Speaker.AUTHENTICATE.value}\" instead\n            * If they want to transfer money, but has_balance returns false, return \"{Speaker.ACCOUNT_BALANCE.value}\" instead\n        * \"{Speaker.CONCIERGE.value}\" - if the user wants to do something else, or hasn't said what they want to do, or you can't figure out what they want to do. Choose this by default.\n\n        Output one of these strings and ONLY these strings, without quotes.\n        NEVER respond with anything other than one of the above five strings. DO NOT be helpful or conversational.\n    \"\"\")\n```\nA simple if\\-else block takes the output of the orchestration agent and uses it to instantiate the sub\\-agent to run next. This is when the state object gets passed to each sub\\-agent:\n\n\n```\n        if next_speaker == Speaker.STOCK_LOOKUP:\n            print(\"Stock lookup agent selected\")\n            current_speaker = stock_lookup_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.AUTHENTICATE:\n            print(\"Auth agent selected\")\n            current_speaker = auth_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.ACCOUNT_BALANCE:\n            print(\"Account balance agent selected\")\n            current_speaker = account_balance_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.TRANSFER_MONEY:\n            print(\"Transfer money agent selected\")\n            current_speaker = transfer_money_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.CONCIERGE:\n            print(\"Concierge agent selected\")\n            current_speaker = concierge_agent_factory(state)\n        else:\n            print(\"Orchestration agent failed to return a valid speaker; ask it to try again\")\n            is_retry = True\n            continue\n```\nAnd then the full chat history is passed as part of a regular chat message to the newly\\-instantiated agent:\n\n\n```\nresponse = current_speaker.chat(user_msg_str, chat_history=current_history)\n```\nThe agent reads its prompt and the user input and decides what to say. As we saw in our very first block of code, if the speaker is already selected, then the loop will keep talking to the current sub\\-agent. This continues until the sub\\-agent has completed its task, at which point its prompt instructs it to call the `done()` function:\n\n\n```\n    def done() -> None:\n        \"\"\"When you complete your task, call this tool.\"\"\"\n        print(\"Money transfer is complete\")\n        state[\"current_speaker\"] = None\n        state[\"just_finished\"] = True\n```\nThis modifies the state, setting the current speaker to none. This triggers the outer loop to run the continuation agent, to see if there's anything else to do:",
        "6974f17f-17ab-46a7-9adf-c98647097290": "*Co\\-authored by: [Riya Jagetia](https://medium.com/u/e84f937083e3?source=post_page-----db42e26ab4f3--------------------------------),*\u00a0*[Tarun Malik](https://medium.com/u/7d279643046e?source=post_page-----db42e26ab4f3--------------------------------),*\u00a0*[Divija N](https://medium.com/u/6fe4060f2a53?source=post_page-----db42e26ab4f3--------------------------------),*\u00a0*[Sharon Tan](https://medium.com/u/361137e928c3?source=post_page-----db42e26ab4f3--------------------------------),*\u00a0[*Zehra Rizvi*](https://medium.com/u/7bdf7b817eec?source=post_page-----db42e26ab4f3--------------------------------), [*Amanda Piyapanee*](https://medium.com/u/b75312a598d6?source=post_page-----db42e26ab4f3--------------------------------)\n\nAt the recent LlamaIndex RAG\\-a\\-thon \\[1], our team\u2019s **\u201cCounselor Copilot\u201d** won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our [DevPost](https://devpost.com/software/counselor-copilot) \\[2] writeup.\n\nIntroduction\n------------\n\nAgainst the backdrop of growing strain on mental health services \\[3, 4], non\\-profit organizations like The Trevor Project \\[5] are a critical part of the care ecosystem. Focusing on helping LGBTQ\\+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors.\n\nProblem: The Dual Challenge Faced by Crisis Counselors\n------------------------------------------------------\n\nHowever, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care.\n\nIn light of these challenges, there\u2019s a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services.\n\nWhile our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well.\n\nThe Winning Solution: An AI Copilot for Crisis Counselors\n---------------------------------------------------------\n\nCounselor Copilot is a real\\-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care. **There is no prompting that is needed from counselors; the copilot works seamlessly in the background.**\n\nFurther, the copilot never directly replies to contacts; instead, replies are suggested and can be edited.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F99979b85638c80808db1a1d41ac3df549735b72f-1600x646.webp%3Ffit%3Dmax%26auto%3Dformat&w=1920&q=75)\n\nCounselor copilot takes into account contact context and chat history to provide real\\-time reply suggestions to the counselors\n\nSpecifically, the copilot automates counselor tasks that include but are not limited to:\n\n1. Retrieving and synthesizing contact data from complex PDFs in real\\-time. This also provides counselors context on their contacts when conversations are initiated.\n2. Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor.\n3. Using existing resources and guidelines from the organization to suggest appropriate replies.\n\n!",
        "703d5ca5-9ff7-4cd3-b9b7-c6041ebaeca5": "```\nfrom llama_index import Document\nfrom llama_index.embeddings import OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor\nfrom llama_index.ingestion import IngestionPipeline, IngestionCache\nfrom llama_index.ingestion.cache import RedisCache\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nimport qdrant_client\nclient = qdrant_client.QdrantClient(location=\":memory:\")\nvector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    cache=IngestionCache(cache=RedisCache(), collection=\"test_cache\"),\n    vector_store=vector_store,\n)\n# Ingest directly into a vector db\npipeline.run(documents=[Document.example()])\n# Create your index\nfrom llama_index import VectorStoreIndex\nindex = VectorStoreIndex.from_vector_store(vector_store)\n```\nCustom Transformations\n======================\n\nImplementing custom transformations is easy! Let\u2019s add a transform to remove special characters from the text before calling embeddings.\n\nThe only real requirement for transformations is that they must accept a list of nodes and return a list of nodes.\n\n\n```\nimport re\nfrom llama_index import Document\nfrom llama_index.embeddings import OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.ingestion import IngestionPipeline\nfrom llama_index.schema import TransformComponent\n\nclass TextCleaner(TransformComponent):\n  def __call__(self, nodes, **kwargs):\n    for node in nodes:\n      node.text = re.sub(r'[^0-9A-Za-z ]', \"\", node.text)\n    return nodes\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TextCleaner(),\n        OpenAIEmbedding(),\n    ],\n)\nnodes = pipeline.run(documents=[Document.example()])\n```\nNode Parsing/Text Splitting \u2014 Flattened and Simplified Interface\n================================================================\n\nWe\u2019ve made our interface for parsing and splitting text a lot cleaner.\n\nBefore:\n=======\n\n\n```\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.node_parser.extractors import (\n\tMetadataExtractor, TitleExtractor\n) \nfrom llama_index.text_splitter import SentenceSplitter\n\nnode_parser = SimpleNodeParser(\n  text_splitter=SentenceSplitter(chunk_size=512),\n  metadata_extractor=MetadataExtractor(\n  extractors=[TitleExtractor()]\n ),\n)\nnodes = node_parser.get_nodes_from_documents(documents)\n```\nAfter:\n======",
        "455d461f-1483-47fb-beb7-fd6dc71ee088": "```\nfrom llama_parse import LlamaParse\n\nparser = LlamaParse(\n    api_key=\"llx-...\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n    verbose=True\n)\n```\nFor unlimited commercial use of LlamaParse, [get in touch](https://llamaindex.ai/contact) with us.\n\n**Next Steps**\n\nOur early users have already given us important feedback on what they\u2019d like to see next. Currently we primarily support PDFs with tables, but we are also building out better support for figures, and and an expanded set of the most popular document types: .docx, .pptx, .html.\n\nManaged Ingestion and Retrieval\n===============================\n\nOur other main offering in LlamaCloud is a managed ingestion and retrieval API which allows you to easily declare performant data pipelines for any context\\-augmented LLM application.\n\nGet clean data for your LLM application, so you can spend less time wrangling data and more time writing core application logic. LlamaCloud empowers enterprise developers with the following benefits:\n\n* **Engineering Time Savings:** Instead of having to write custom connectors and parsing logic in Python, our APIs allow you to directly connect to different data sources.\n* **Performance:** we provide good out\\-of\\-the\\-box performance for different data types, while offering an intuitive path for experimentation, evaluation, and improvement.\n* **Ease Systems Complexity:** Handle a large number of data sources with incremental updates.\n\nLet\u2019s do a brief tour through the core components!\n\n1. **Ingestion:** Declare a managed pipeline to process and transform/chunk/embed data backed by our 150\\+ data sources in LlamaHub and our 40\\+ storage integrations as destinations. Automatically handle syncing and load balancing. Define through the UI or our open\\-source library.\n2. **Retrieval:** Get access to state\\-of\\-the\\-art, advanced retrieval backed by our open\\-source library and your data storage. Wrap it in an easy\\-to\\-use REST API that you can consume from any language.\n3. **Playground:** Interactive UI to test and refine your ingestion/retrieval strategies pre\\-deployment, with evaluations in the loop.\n\n\n\nLlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment.\n\n![](/blog/images/1*GyTHgPBTZwQW-k_gmKdxwA.png)\n\nLlamaCloud Retrieval: Access advanced retrieval over your storage system via an API.\n\n![](/blog/images/1*xMLXXokE_cJt0cmM9gDdpw.png)We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you\u2019re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come [talk to us](https://llamaindex.ai/contact).\n\nLaunch Partners and Collaborators\n=================================\n\nWe opened up access to LlamaParse at [our hackathon](https://rag-a-thon.devpost.com/) we co\\-hosted with [Futureproof Labs](https://www.futureproofsv.com/) and [Datastax](https://www.datastax.com/) at the beginning of February. We saw some incredible applications of LlamaParse in action, [including parsing building codes for Accessory Dwelling Unit (ADU) planning](/pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f), [parsing real\\-estate disclosures for home buying](https://devpost.com/software/home-ai), and dozens more.\n\nEric Ciarla, co\\-founder at Mendable AI, incorporated LlamaParse into Mendable\u2019s data stack: \u201cWe integrated LlamaParse into our [open source data connector repo](https://github.com/mendableai/data-connectors) which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.\u201d\n\nWe\u2019re also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem, from storage to compute.",
        "bf6a6a35-835e-490f-be3f-e1b0519a1134": "\ud83c\udfa5\u00a0**Webinars:**\n\n* [Register for a webinar](https://lu.ma/v1bdat63) with [Tianjun Zhang](https://www.linkedin.com/in/tianjun-zhang-333bb2126/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3B1dQCZFffT4aXk6ePSYdUYg%3D%3D)\u00a0and [Shishir Patil](https://www.linkedin.com/in/shishir-patil/overlay/about-this-profile/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BNG6wPCQHTaWKxcdltRvvjw%3D%3D)\u00a0on how to do retrieval\\-augmented fine\\-tuning (RAFT).\n* [Webinar](https://www.youtube.com/watch?v=TeEX7CoHT9k) with [Daniel](https://twitter.com/dani_avila7) on [CodeGPT](https://codegpt.co/) \\- a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components.\n* [Vectara\u2019s](https://twitter.com/vectara?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) [Panel Discussion](https://www.youtube.com/watch?v=R5pddHfUThQ&t=351s) on 'Why RAG will Never Die?\u2019.",
        "9a87b6e8-0e62-45de-9d33-f1c285736b37": "Yo, LlamaIndex Fans \ud83e\udd99,\n\nDive into a week brimming with thrilling developments at LlamaIndex! The dynamic input from our community and our rich selection of learning materials are all set to enhance your journey with LlamaIndex.\n\nLast week, the LlamaIndex ecosystem took a significant leap forward with the launch of LlamaCloud, a suite of advanced services designed for **production\\-level**\u00a0**context enhancement** in LLM and RAG applications:\n\n* **LlamaParse:** Offers sophisticated parsing for complex documents, making it possible to answer detailed queries.\n* **Managed Ingestion and Retrieval API:** Facilitates easier data management, connecting with over 150 sources and 40\\+ storage solutions.\n\nLlamaParse is now available for a public preview, primarily focusing on PDFs with a user cap, while the API is in a private preview for select enterprise partners. If you haven\u2019t explored these new features yet, we invite you to [check them out](https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b) for more details or to discuss commercial terms.\n\nYour innovation inspires us! We\u2019re eager to see the projects, articles, or videos that inspire you. Share your remarkable works with us at [news@llamaindex.ai](mailto:news@llamaindex.ai). And if you haven\u2019t already, subscribe to our newsletter on our website to receive the latest LlamaIndex updates straight to your inbox.\n\n\ud83e\udd29 **The highlights:**\n\n1. **Enhanced RAG Retrieval with Sub\\-Document Summaries:** Introducing a novel chunking method that improves RAG performance by incorporating hierarchical metadata into chunks, ensuring precise and context\\-aware information retrieval. [Notebook](https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb), [Tweet](https://x.com/llama_index/status/1761793821422264757?s=20).\n2. **MistralAI Cookbook:** A comprehensive guide to leveraging the Mistral\\-Large model from MistralAI, featuring near\\-GPT\\-4 reasoning, function calling, and JSON output for cutting\\-edge applications. [Docs](https://docs.llamaindex.ai/en/latest/cookbooks/mistralai.html), [Tweet](https://x.com/llama_index/status/1762231085243719748?s=20).\n3. **Gemma Cookbook:** A comprehensive guide to using Gemma, GoogleDeepMind\u2019s latest LLM offering, with options for 2B and 7B parameters, facilitating the development of local RAG systems on your laptop. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/ollama_gemma.ipynb), [Tweet](https://x.com/jerryjliu0/status/1760471196402069771?s=20).\n4. **ColBERT Integration:** Document reranking with ColBERT via LlamaIndex, delivering a solution that is about 100x faster than BERT\\-based models for more efficient data processing. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/node_postprocessor/ColbertRerank.ipynb), [Tweet](https://x.com/llama_index/status/1760830777179471933?s=20).\n5. **Counselor Copilot \u2014 Social Impact Through RAG:** Spotlight on Counselor Copilot, an innovative RAG project supporting the Trevor Project\u2019s crisis counselors, providing real\\-time assistance with context, suggestions, and actions to aid LGBTQ\\+ youth effectively. [BlogPost](https://blog.llamaindex.ai/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3), [Tweet](https://x.com/llama_index/status/1761433854458614075?s=20).",
        "3ab53472-53ad-4a2c-9975-25e84fa18232": "Introduction\n============\n\nGPT\\-4V has amazed us with its ability to analyze images and even generate website code from visuals.\n\nThis blog post investigates GPT\\-4V\u2019s proficiency in interpreting bar charts, scatter plots, and tables. We aim to assess whether specific questioning and chain of thought prompting can yield better responses compared to broader inquiries. Our demonstration seeks to determine if GPT\\-4V can exceed these known limitations with precise questioning and systematic reasoning techniques.\n\nWe observed in these experiments that asking specific questions, rather than general ones, yields better answers. Let\u2019s delve into these experiments.\n\nYou can also follow along with this blog post in our [Google Colab Notebook](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_experiments_cot.ipynb).\n\n\n> NOTE: This blog post aims to inform the community about GPT\\-4V\u2019s performance, though the results might not be universally applicable. We strongly advise conducting tests with similar questions on your own dataset before drawing conclusions.\n\nExperiment:\n===========\n\nWhat we\u2019ll do is take an image of each of a bar chart, scatter plot, and table and analyze them by asking three types of questions.\n\n1. General Question: Simply ask, \u201cAnalyse the image.\u201d\n2. Specific Question: Specific Question the performance of a certain category by providing more details.\n3. Chain of Thought Prompting: Use a step\\-by\\-step reasoning method to walk through the analysis.\n\nThese guidelines aim to test how different questioning techniques might improve the precision of the information we gather from the images.\n\nData for experiments:\n=====================\n\nTo test out we selected the following images from [Llama2](https://arxiv.org/pdf/2307.09288.pdf) and [MistralAI](https://arxiv.org/pdf/2310.06825.pdf) papers.\n\n1. Violation percentage of safety with different LLMs across categories. (Llama2 paper)\n2. Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper)\n3. Performances of different LLMs across various NLP tasks. (Llama2 paper)\n\nLet\u2019s go through each of these data points one by one.\n\nImage 1 \u2014 Violation percentage of safety with different LLMs across categories.\n===============================================================================\n\n![](/blog/images/1*fJ95qaVHW8UciP-49DRVYw.png)General Question:\n=================\n\n**Question:** Analyse the Image.\n\n**Answer:**",
        "6edacfe6-bfc0-4681-bd43-ccfc02fbf382": "**Hello LlamaIndex Enthusiasts!**\n\nWelcome to the fifth edition of our LlamaIndex Update series.\n\n**Most Important Takeaways:**\n-----------------------------\n\n1. We\u2019ve open\\-sourced [**SECInsights.ai**](http://secinsights.ai/) \u2014 your gateway to the production RAG framework.\n2. Replit templates \u2014 kickstart your projects with zero environment setup hassles.\n3. Build RAG from scratch and get hands\\-on with our processes.\n\nBut wait, there\u2019s more!\n\n* Feature Releases and Enhancements\n* Fine\\-Tuning Guides\n* Retrieval Tips for RAG\n* Building RAG from Scratch Guides\n* Tutorials\n* Integration with External Platforms\n* Events\n* Webinars\n\nSo, let\u2019s embark on this journey together. Dive in and explore the offerings of the fifth edition of the LlamaIndex Update series!\n\n**Feature Releases and Enhancements**\n=====================================\n\n1. **Open\\-Sourced RAG Platform**: LlamaIndex open\\-sourced [**http://secinsights.ai**](http://secinsights.ai), accelerating RAG app development with chat\\-based Q\\&A features. [Tweet](https://twitter.com/llama_index/status/1699116440056651976?s=20)\n2. **Linear Adapter Fine\\-Tuning**: LlamaIndex enables efficient fine\\-tuning of linear adapters on any embedding without re\\-embedding, enhancing retrieval/RAG across various models. [Tweet](https://twitter.com/llama_index/status/1699566421506671043?s=20), [Docs](https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.html), [BlogPost](https://medium.com/llamaindex-blog/fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383)\n3. **Hierarchical Agents**: By structuring LLM agents in a parent\\-child hierarchy, we enhance complex search and retrieval tasks across diverse data, offering more reliability than a standalone agent. [Tweet](https://twitter.com/llama_index/status/1699929022027718729?s=20)\n4. **SummaryIndex**: We\u2019ve renamed ListIndex to SummaryIndex to make it clearer what its main functionality is. Backward compatibility is maintained for existing code using ListIndex. [Tweet](https://twitter.com/llama_index/status/1698728395948081247?s=20)\n5. **Evaluation:** LlamaIndex\u2019s new RAG evaluation toolkit offers async capabilities, diverse assessment criteria, and a centralized BaseEvaluator for easier developer integrations. [Tweet](https://x.com/llama_index/status/1703074307763818775?s=20), [Docs](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/modules.html).\n6. **Hybrid Search for Postgres/pgvector**: LlamaIndex introduces a hybrid search for Postgres/pgvector. [Tweet](https://twitter.com/llama_index/status/1700892425592696915?s=20), [Docs](https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/postgres.html#hybrid-search).\n7. **Replit Templates:** LlamaIndex partners with Replit for easy LLM app templates, including ready\\-to\\-use Streamlit apps and full Typescript templates. [Tweet](https://x.com/llama_index/status/1702847763183235278?s=20), [Replit Templates](https://replit.com/@LlamaIndex).\n\n**LlamaIndex.TS:**\n------------------\n\n1. Launches with MongoDBReader and type\\-safe metadata. [Tweet](https://x.com/llama_index/status/1702382520631959721?s=20).\n2. Launches with chat history, enhanced keyword index, and Notion DB support. [Tweet](https://twitter.com/llama_index/status/1701292211764338898?s=20).\n\nFine\\-Tuning Guides:\n====================\n\n1.",
        "0dc46c37-b8b6-49b5-887a-50d691b65464": "Here\u2019s what that would look like:\n\n\n\n```\n  \n# example for multiple query engine tools  \nquery_engine_tools = [  \n    QueryEngineTool(  \n        query_engine=sept_engine,  \n        metadata=ToolMetadata(  \n            name=\"sept_22\",  \n            description=\"Provides information about Uber quarterly financials ending September 2022\",  \n        ),  \n    ),  \n    QueryEngineTool(  \n        query_engine=june_engine,  \n        metadata=ToolMetadata(  \n            name=\"june_22\",  \n            description=\"Provides information about Uber quarterly financials ending June 2022\",  \n        ),  \n    )  \n]\n```\n\n\n You can read more about the tools available in LlamaIndex\n [here](https://docs.llamaindex.ai/en/stable/core_modules/agent_modules/tools/root.html).\n\n\n\nHowever, we\u2019re currently sticking to just one QueryEnginerTool for now.\n\n\n\n The `SubQuestionQueryEngine` module breaks down a complex query\n into many sub\\-questions and their target query engine for execution. After\n executing all sub\\-questions, all responses are gathered and sent to a response\n synthesizer to produce the final response. Using this module is essential\n because generating insights from annual reports requires complex queries that\n need to retrieve information from multiple nodes within the vector DB.\n\n\n\n\n![](/blog/images/1*hZ_rhUF78sVQAZNHMxjy7w.png)\n\n\nSubQuestionQueryEngine at work\n\n\n\nPrompt Engineering\n==================\n\n\n\n Prompt engineering is essential to the entire process mainly for two reasons:\n\n\n\n1. To provide clarity to the agent as to what it needs to retrieve from the\n vector DB by writing precise and relevant queries\n2. And then control the quality of the output generated from the retrieved\n context by providing a structure and description for the output to be\n generated.\n\n\n\n Both these points are handled by using\n `PromptTemplate` and `PydanticOutputParser` module in\n `langchain` .\n\n\n\n\n Using the `PydanticOutputParser` we write the description for the\n different sections of the insights to be generated. After having a few\n conversations with finance experts, I concluded generating insights for these\n 4 sections: different sections: Fiscal Year Highlights, Strategic Outlook and\n Future Direction, Risk Management, Innovation and R\\&D. Now let\u2019s write the\n `pydantic` class for these sections:\n\n\n\n\n```\nfrom pydantic import BaseModel, Field  \n  \nclass FiscalYearHighlights(BaseModel):  \n    performance_highlights: str = Field(..., description=\"Key performance metrics and financial stats over the fiscal year.\")  \n    major_events: str = Field(..., description=\"Highlight of significant events, acquisitions, or strategic shifts that occurred during the year.\")  \n    challenges_encountered: str = Field(..., description=\"Challenges the company faced during the year and, if and how they managed or overcame them.\")  \n  \nclass StrategyOutlookFutureDirection(BaseModel):  \n    strategic_initiatives: str = Field(..., description=\"The company's primary objectives and growth strategies for the upcoming years.\")  \n    market_outlook: str = Field(..., description=\"Insights into the broader market, competitive landscape, and industry trends the company anticipates.\")  \n  \nclass RiskManagement(BaseModel):  \n    risk_factors: str = Field(..., description=\"Primary risks the company acknowledges.\")  \n    risk_mitigation: str = Field(..., description=\"Strategies for managing these risks.\")  \n  \nclass InnovationRnD(BaseModel):  \n    r_and_d_activities: str = Field(..., description=\"Overview of the company's focus on research and development, major achievements, or breakthroughs.\")  \n    innovation_focus: str = Field(..., description=\"Mention of new technologies, patents, or areas of research the company is diving into.\")\n```\n\n\n**Note: These sections and their description are for generic use cases. They\n can be changed to suit your particular needs.**\n\n\n\n\n These pydantic classes will provide the format and description for each\n section to the prompt. So let\u2019s write a function that allows us to plug in any\n pydantic class to a prompt:",
        "a75f7edb-b10b-46df-a0f3-35ae473794b7": "RAG is only as Good as your Data\n--------------------------------\n\nBuilding production\\-ready LLM applications is hard. We've been chatting with hundreds of users, ranging from Fortune 500 enterprises to pre\\-seed startups and here's what they tell us they struggle with:\n\n* **Data Quality Issues**: Most companies deal with large sets of complex, heterogeneous documents. Think PDFs with messy formatting, images, tables across multiple pages, different languages \\- the list goes on. Ensuring high\\-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications.\n* **Scalability Hurdles**: Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either.\n* **Accuracy Concerns**: Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs.\n* **Configuration Overload:** Fine\\-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task.\n\nAs developers shift from prototypes towards building production applications \\- complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data.\n\nTo address these difficulties, we soft\\-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production\\-grade context\\-augmentation to your LLM and RAG applications. LlamaParse can already support 50\\+ languages and 100\\+ document formats. The adoption has been incredible \\- we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here\u2019s an example from Dean Barr, Applied AI Lead at Carlyle:\n\n\n> As an AI Applied Data Scientist who was granted one of the first ML patents in the U.S., and who is building cutting\\-edge AI capabilities at one of the world's largest Private Equity Funds, I can confidently say that LlamaParse from LlamaIndex is currently the best technology I have seen for parsing complex document structures for Enterprise RAG pipelines. Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to maintaining data integrity in advanced RAG and agentic model building.\n\nThe Rise of Centralized Knowledge Management\n--------------------------------------------\n\nWe have designed LlamaCloud to cater to the need of\u00a0**production\\-grade**\u00a0**context\\-augmentation**\u00a0for your LLM and RAG applications. Let's take a tour of what LlamaCloud brings to the table:\n\n1. **LlamaParse**: Our state\\-of\\-the\\-art parser that turns complex documents with tables and charts into LLM\\-friendly formats. You can learn more about [LlamaParse here](https://docs.cloud.llamaindex.ai/llamaparse/getting_started).\n2. **Managed Ingestion**: Connect to enterprise data sources and your choice of data sinks with ease. We support [multiple data sources](https://docs.cloud.llamaindex.ai/llamacloud/data_sources) and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications.\n3. **Advanced Retrieval**: LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents.\n4. **LlamaCloud Playground**: An interactive UI to test and refine your ingestion and retrieval strategies before deployment.\n5. **Scalability and Security**: Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs.\n\nThis video gives a detailed walk through of LlamaCloud:\n\nPlayOur customers tell us that LlamaCloud enables developers to spend less time setting up and iterating on their data pipelines for LLM use cases, allowing them to iterate through the LLM application development lifecycle much more quickly. Here\u2019s what Teemu Lahdenpera, CTO at [Scaleport.ai](http://Scaleport.ai) had to say:\n\n\n> LlamaCloud has really sped up our development timelines. Getting to technical prototypes quickly allows us to show tangible value instantly, improving our sales outcomes. When needed, switching from the LlamaCloud UI to code has been really seamless. The configurable parsing and retrieval features have significantly improved our response accuracy.",
        "e9e7f1e0-1ed4-4e71-8826-b0e4965b456f": "*This is a guest post from our friends at Mozilla about [Llamafile](https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-democratizing-ai/)*\n\n[llamafile](https://github.com/Mozilla-Ocho/llamafile), an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from [HuggingFace](https://huggingface.co/models?sort=trending&search=llamafile) then run the file. That's it. **On most computers, you won't need to install anything.**\n\nThere are a few reasons why you might want to run an LLM on your laptop, including:\n\n1\\. Privacy: Running locally means you won't have to share your data with third parties.\n\n2\\. High availability: Run your LLM\\-based app without an internet connection.\n\n3\\. Bring your own model: You can easily test many different open\\-source LLMs (anything available on HuggingFace) and see which one works best for your task.\n\n4\\. Free debugging/testing: Local LLMs allow you to test many parts of an LLM\\-based system without paying for API calls.\n\nIn this blog post, we'll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we'll show how to use LlamaIndex with your llamafile as the LLM \\& embedding backend for a local RAG\\-based research assistant. You won't have to sign up for any cloud service or send your data to any third party\\-\\-everything will just run on your laptop.\n\nNote: You can also get all of the example code below as a Jupyter notebook from our [GitHub repo](https://github.com/Mozilla-Ocho/llamafile-llamaindex-examples).\n\nDownload and run a llamafile\n----------------------------\n\nFirst, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There's nothing to install or configure (with a few caveats, discussed [here](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gotchas)).\n\nEach llamafile bundles 1\\) model weights \\& metadata in gguf format \\+ 2\\) a copy of [\\`llama.cpp\\`](https://github.com/ggerganov/llama.cpp) specially compiled using \\[Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT\\-like browser interface, a CLI, and an OpenAI\\-compatible REST API for chat models.\n\nThere are only 2 steps to setting up a llamafile:\n\n1\\. Download a llamafile\n\n2\\. Make the llamafile executable\n\nWe'll go through each step in detail below.",
        "2fcdee48-fd5d-430b-901d-c14da93feefa": "Here\u2019s our weekly look at developments across the LLM space and RAG (Retrieval Augmented Generation) in particular, as well as the latest news and features from your favorite open source library. If you\u2019ve got a project (or a blog post, or a video) that you think people should hear about, we\u2019re happy to feature it in here! Drop us a line at [news@llamaindex.ai](mailto:news@llamaindex.ai).\n\nThis update is now available in handy email form! Just head to our [home page](https://llamaindex.ai) and enter your email to sign up.\n\n\ud83e\udd29 **First, the highlights:**\n\n1. **Full observability with Arize AI Phoenix**: we launched a one\\-code\\-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local. [Notebook](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb), [Tweet](https://twitter.com/llama_index/status/1708866998149816540?s=20).\n2. **RetrieverEvaluator**: new in the library, \u201cRetrieverEvaluator\u201d allows enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments. [Docs](https://gpt-index.readthedocs.io/en/latest/examples/evaluation/retrieval/retriever_eval.html), [Tweet](https://twitter.com/llama_index/status/1704884477552660612?s=20).\n3. **HuggingFace Embeddings**: we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format. [Docs](https://gpt-index.readthedocs.io/en/latest/examples/embeddings/huggingface.html), [Tweet](https://x.com/llama_index/status/1706096762933653962?s=20).\n4. **Multi\\-Document Agents**: we\u2019ve introduced v0 experimental support for multi\\-document agents for advanced QA, beyond typical top\\-k RAG. It supports diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency. [Docs](https://gpt-index.readthedocs.io/en/latest/examples/agent/multi_document_agents.html), [Tweet](https://twitter.com/jerryjliu0/status/1708523212366393403?s=20).\n\n\ud83c\udfc6 **Congratulations to our Streamlit Hackathon Winners!**\n\nWe love seeing people build amazing things with LlamaIndex!\n\n1. NewsGPT by Kang\\-Chi Ho: <https://buff.ly/46jkutx>\n2. FinSight by Vishwas Gowda: <https://buff.ly/3PzOnyC>\n\n\u2728 **Feature Releases** **and Enhancements:**\n\n1. **Multi\\-Document Agents**: we introduced multi\\-document agents (V0\\) for advanced QA, beyond typical top\\-k RAG. They support diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency. [Docs](https://gpt-index.readthedocs.io/en/latest/examples/agent/multi_document_agents.html), [Tweet](https://twitter.com/jerryjliu0/status/1708523212366393403?s=20).\n2. **Ensemble Retriever:** we\u2019re addressing the RAG challenge of determining chunk size by experimenting with diverse document chunking and ensembling for retrieval. [Docs](https://gpt-index.readthedocs.io/en/latest/examples/retrievers/ensemble_retrieval.html), [Tweet](https://twitter.com/jerryjliu0/status/1707541270934212737?s=20).\n3.",
        "f30e3c24-ced3-409e-a446-f3e71cec33a7": "(Authors: Andrei Fajardo and Jerry Liu @ LlamaIndex)\n\nToday we\u2019re excited to introduce **Llama Datasets** \ud83e\udd99 \ud83d\udcdd\u2014 a set of community\\-contributed datasets that allow users to easily benchmark their RAG pipelines for different use cases. A dataset consists of both question\\-answer pairs as well as source context. To use them, download them from LlamaHub; then evaluate your RAG pipeline using the dataset \\+ a set of evaluation metrics.\n\nWe\u2019re launching with an initial set of 10 evaluation datasets and we\u2019ll be adding more! We\u2019ve also made it super easy to contribute your own dataset \u2014 upload your source documents \\+ QA pairs (generated manually or synthetically).\n\nContext\n=======\n\nA big problem in building production RAG is evaluation. Unlike traditional software systems, LLM systems (and ML systems more generally) are stochastic black\\-boxes designed to model noisy real\\-world signals. This means that developers can\u2019t easily define unit tests that assert deterministic behavior \u2014 there may always be an input that causes an error. Because developers don\u2019t quite know what goes out given what goes in, they need to define an **evaluation dataset** that\u2019s reflective of their production use cases, and evaluate their system over this dataset using a set of **evaluation metrics**.\n\nWe\u2019ve presented [extensively on this topic](https://docs.google.com/presentation/d/1wtlEJC6SXLsoGkDdCkxC-Y_V8bvyYsi9Ozh59VMI22I/edit?usp=sharing) \u2014 every AI engineer should setup evaluation before trying to optimize their LLM or RAG application with advanced techniques.\n\nBut we\u2019ve increasingly found that defining the **right evaluation dataset is hard and use\\-case dependent**. Evaluating over academic benchmarks, like BEIR and HotpotQA oftentimes fail to generalize to specific use cases. Certain parameters that work well on certain data domains (e.g. SEC filings) may fail on others (e.g. research papers).\n\nThat\u2019s what inspired us to create Llama Datasets. Instead of being prescriptive on the data you must use, we\u2019ve decided to create a hub where you can easily pick and choose the right datasets for your use case!",
        "d25ab7ac-8d24-4f4f-b968-2caf75219d1a": "> Master LlamaIndex with our course developed in collaboration with Activeloop, TowardsAI, \\& Intel. Learn to apply advanced retrieval across industries in 40\\+ lessons. This is a guest post from Activeloop.\n\nLlamaIndex is proud to collaborate with Activeloop, Towards AI, and the Intel Disruptor Initiative to offer a free course on \u201c[Advanced Retrieval Augmented Generation for Production](https://learn.activeloop.ai/courses/rag),\u201d a part of the Gen AI 360: Foundational Model Certification series. This comprehensive course takes a hands\\-on approach to applying RAG techniques across various industries, including legal, biomedical, healthcare, e\\-commerce, and finance.\n\nThe free course is designed for practical learning and invites participants to tackle real business challenges, such as developing a multi\\-modal AI shopping assistant. The course has over 40 lessons, 7 interactive projects, and 2 hours of video content, including from LlamaIndex CEO Jerry Liu. In 20\\+ hours of learning, the curriculum is geared towards enabling GenAI tinkerers, professionals, and executives to apply LlamaIndex and Deep Lake, [Activeloop](http://activeloop.ai)\u2019s database for AI in production. Participants who complete the course will be awarded a certificate at no cost.\n\n\n> In the rapidly evolving business landscape, leveraging Retrieval Augmented Generation (RAG) tools like Llamalndex \\& Deep Lake by Activeloop is essential for enterprises seeking a competitive edge in GenAI. This course is tailored to quickly upskill your team in GenAI workflows, emphasizing the integration of Activeloop\u2019s advanced features like Deep Memory with Llamalndex for unmatched retrieval accuracy. It\u2019s a strategic investment to enhance your team\u2019s capabilities, ensuring your enterprise stays at the forefront of AI innovation\n> \n> \\- Jerry Liu, CEO \\& Co\\-Founder, LlamaIndex\n\nHighlighted projects across several industries:\n===============================================\n\n* **Healthcare:** Advanced RAG for Pill Searching. Combine cutting\\-edge NLP and computer vision techniques to build an AI app that recognizes pills from images, and lists their side effects and instructions to use. This project offers hands\\-on experience with the latest AI technologies like Segment Anything or GPT\\-4\\-vision.\n* **Legal:** Patent Generation and Search Engine. Gain practical knowledge in constructing a system like PatentPT, which incorporates a fine\\-tuned LLM to search or create patents. Learn how to build a meta\\-agent to smartly route user inquiries, ensuring a fluid chat experience with a database of 8 million USPTO patents for comprehensive retrieval and generation capabilities.\n* **E\\-commerce:** AI\\-Powered Shopping Assistant for Outfit Recommendations. Build a multi\\-modal AI assistant that curates outfit suggestions for any occasion, weather, or budget!\n\nWhat will I learn?\n==================\n\n* **Challenges with Naive RAG:** We\u2019ll address common issues such as low precision, recall, and suboptimal response generation. Strategies for refining data processing, enhancing embedding models, refining retrieval algorithms, and optimizing prompt usage will be explored to improve RAG system performance.\n* **Advanced RAG with LlamaIndex:** Delve into basic and advanced RAG methods using LlamaIndex. The course covers the essential aspects of LlamaIndex required for RAG application development, complemented by Activeloop\u2019s Deep Memory module, which natively integrates seamlessly with LlamaIndex to enhance retrieval accuracy by an average of 22%. Topics will range from small to large\\-scale retrieval, handling structured and unstructured data, and querying techniques.\n* **RAG Agents:** This module focuses on the creation and application of RAG agents with LlamaIndex, including advanced querying, summarizing databases, and designing AI assistants using various APIs.\n\nProduction\\-grade apps: Learn to fine\\-tune the LlamaIndex RAG pipeline for professional deployment, evaluate RAG systems crafted with LlamaIndex, and ensure your models\u2019 observability and effectiveness.",
        "31296fde-83de-4b0e-8847-a164414056ce": "* [Guide](https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_nim/) to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step\\-by\\-step guide teaches you how to run models, generate embeddings, and re\\-rank data for optimal results.\n* [Guide](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_advanced.ipynb) to Constructing Knowledge Graphs with LLMs\\*\\*,\\*\\* build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights.\n\n**\ud83d\udda5\ufe0f\u00a0Demos:**\n-------------\n\n* [Omakase RAG Orchestrator](https://github.com/ammirsm/llamaindex-omakase-rag), a project developed by [Amir Mehr](https://x.com/thatisamir), is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full\\-featured RAG API, data source management, user access control, and an admin panel.\n* [gmail\\-extractor](https://github.com/run-llama/gmail-extractor), a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision.\n\n**\u270d\ufe0f Tutorials:**\n-----------------\n\n* Sherlock Xu\u2019s [tutorial](https://www.bentoml.com/blog/serving-a-llamaindex-rag-app-as-rest-apis) from BentoML on Serving A LlamaIndex RAG App as REST APIs.\n\n**\ud83d\udcd1\u00a0Papers:**\n-------------\n\n* FinTextQA, a new benchmark dataset for long\\-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto\\-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question\\-answering system for financial text.\n\n**\ud83d\udcf9\u00a0Webinar:**\n--------------\n\n* [Webinar](https://www.youtube.com/watch?v=o0DPxvgML5c) with authors of memary \\- Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open\\-source reference implementation for long\\-term memory in autonomous agents\n\n\ud83d\udcc5\u00a0**Events:**\n-------------\n\n* [Join](https://www.meetup.com/nlp_london/events/301171675/) Pierre from LlamaIndex along with speakers from Weaviate, and Weights \\& Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.",
        "912633cb-47a0-4f0b-b22c-ed7481a2836c": "**\ud83d\udda5\ufe0f\u00a0Demos:**\n-------------\n\n* [Laurie Voss](https://x.com/seldo)\u2019s [LLM\\-powered file organizer project](https://github.com/run-llama/file-organizer) that categorizes files into folders based on LLM\\-decided categories without renaming them, ensuring important filenames remain intact. It organizes your files in multiple passes to balance folder sizes, resulting in descriptive yet practical folder names to help you find files easily.\n\n**\u270d\ufe0f Tutorials:**\n-----------------\n\n* [Laurie Voss](https://x.com/seldo)\u2019s [video tutorial](https://www.youtube.com/watch?v=MXPYbjjyHXc) on transitioning from basic RAG to fully agentic knowledge retrieval, featuring real\\-world code examples that cover routing, memory, planning, tool use, and advanced agentic reasoning methods like Chain of Thought and Tree of Thought, along with insights into observability, controllability, and customizability.\n* [Prince krampah](https://x.com/Prince_krampah)'s [tutorials](https://x.com/llama_index/status/1799463683179098203) on Agentic RAG Systems, offering comprehensive insights into advanced system building with detailed explanations on router query engines, function calling, and multi\\-step reasoning across complex documents.\n* [kingzzm\u2019s](https://x.com/kingzzm) [tutorial](https://generativeai.pub/advanced-rag-retrieval-strategy-query-rewriting-a1dd61815ff0) on Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub\\-question decomposition, HyDE for aligning questions with embedding semantics, and step\\-back prompting for tackling complex queries more effectively.\n* [Rajdeep Borgohain](https://x.com/rborgohain4)'s [tutorial](https://docs.inferless.com/cookbook/serverless-customer-service-bot) to build a customer\\-support voicebot with advanced features like speech\\-to\\-text and text\\-to\\-speech, integrated into a RAG pipeline for efficient handling of customer support exchanges using Inferless, LlamaIndex, faster\\-whisper, Piper, and Pinecone.\n* [Pavan Mantha](https://x.com/pavan_mantha1)'s [tutorial](https://towardsdev.com/production-ready-secure-and-powerful-ai-implementations-with-azure-services-671b68631212) on securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant.\n\n**\ud83d\udcf9\u00a0Webinar:**\n--------------\n\n* [Join us](https://lu.ma/kqxmbuou) for our webinar with [Tomaz Bratanic](https://x.com/tb_tomaz) from Neo4j on LlamaIndex property graph for insights into high\\-level and low\\-level graph construction, retrieval, and knowledge graph agents.",
        "2966c1bd-887b-4be8-9b81-3fd1a5717a70": "* We introduced `OpenAIAssistantAgent` abstractions for seamless connection of OpenAI Assistants API with your chosen vector database. [Docs](https://t.co/W78d2WCpnn), [Tweet](https://twitter.com/jerryjliu0/status/1722276583883657388?s=20).\n* We introduced a new agent leveraging OpenAI Assistants API with features like in\\-house code interpretation, file retrieval, and function calling for external tools integration. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/openai_assistant_agent.ipynb), [Tweet](https://x.com/llama_index/status/1721949693754917035?s=20).\n\n**\ud83c\udfa5** Demos:\n\n* MechGPT by Professor [**Markus J. Buehler**](https://twitter.com/ProfBuehlerMIT) showcases the integration of LLM fine\\-tuning and knowledge graph creation with LlamaIndex, leading to interesting insights in cross\\-disciplinary scientific research and hypothesis generation. [Tweet](https://x.com/llama_index/status/1723379654550245719?s=20), [Paper](https://t.co/l8J55BqUfn).\n\n**\ud83d\uddfa\ufe0f Guides:**\n\n* We released a concise [slide deck](https://docs.google.com/presentation/d/1i1bUDWXeCYPd6O8pio57ST6AQIuSTWXM3rvvkvrBpBM/edit#slide=id.p) that aggregates over 10\\+ newly shipped features, guides, and analyses, complete with links to accompanying notebooks for developer use based on OpenAI\u2019s recent updates.\n* We also released a full [cookbook](https://docs.llamaindex.ai/en/latest/examples/agent/openai_assistant_query_cookbook.html) showing how you can build advanced RAG with the Assistants API \u2014 beyond just using the in\\-house Retrieval tool.\n* We produced a [guide](https://docs.llamaindex.ai/en/latest/examples/agent/openai_retrieval_benchmark.html) on evaluating the OpenAI Assistant API vs RAG with LlamaIndex.\n* Here\u2019s a [guide](https://github.com/run-llama/llama_index/blob/main/docs/examples/response_synthesizers/long_context_test.ipynb) on evaluating How well long\\-context LLMs (gpt\\-4\\-turbo, claude\\-2\\) recall specifics in BIG documents? (\\>\\= 250k tokens).\n* Here\u2019s another [guide](https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/openai_json_vs_function_calling.ipynb) that highlights how function calling simplifies structured data extraction, while JSON mode ensures format correctness without schema enforcement.\n* Finally, we released a guide to craft a GPT Builder, enabling an agent to programmatically construct another task\\-specific agent. This builder streamlines the creation of systems for specific functions. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/agent/agent_builder.ipynb), [Tweet](https://x.com/jerryjliu0/status/1721639447207583882?s=20).\n\n**\u270d\ufe0f Tutorials:**\n\n* [**Bhavesh Bhat**](https://twitter.com/_bhaveshbhatt) gave us a [tutorial](https://twitter.com/_bhaveshbhatt/status/1721551513103839392) on How to Chat with YouTube Videos Using LlamaIndex.\n* [David Garnitz](https://twitter.com/DGarnitz)\u2019s tutorial blog explores the use of VectorFlow alongside ArizePhoenix, Weaviate, and LlamaIndex to manage large data sets.",
        "1d2322c5-ae04-4a48-bdf2-2344fc2108a2": "[Tweet](https://twitter.com/llama_index/status/1787296562504425903), [Blog post](https://medium.com/rahasak/optimizing-rag-supervised-embeddings-reranking-with-your-data-with-llamaindex-88344ff89da7)\n\n**\ud83d\uddfa\ufe0f Guides:**\n\n* Hanane Dupouy walks us through building an agent that can perform complex financial calculations. [Tweet](https://twitter.com/llama_index/status/1785325832317415641), [Slides](https://www.linkedin.com/posts/hanane-d-algo-trader_anthropic-agent-rag-with-complex-financial-activity-7190389324343881728-TOXX/?utm_source=share&utm_medium=member_desktop)\n* Plaban Nayak sets up a local, open\\-source RAG pipeline that uses Llama 3 and Qdrant to demonstrate how to improve the accuracy of your RAG with reranking. [Tweet](https://twitter.com/llama_index/status/1786093311658451337), [Blog post](https://nayakpplaban.medium.com/build-an-advanced-reranking-rag-system-using-llama-index-llama-3-and-qdrant-a8b8654174bc)\n* Jason Zhou talks about the components needed for agentic RAG. [Tweet](https://twitter.com/llama_index/status/1786830550441099524)\n* Divyanshu Dixit walks us through agents dedicated to workflow automation. [Tweet](https://twitter.com/divyanshu_van/status/1786786672648110415), [Blog post](https://div.beehiiv.com/p/need-talk-agents)\n\n**\u270d\ufe0f Tutorials:**\n\n* Tyler Hutcherson of Redis and our own Laurie Voss walk you through building agentic RAG with semantic caching and other production\\-ready techniques. [Video](https://www.youtube.com/watch?v=mTNiGfYfdWY&t=5s), [Notebook](https://github.com/redis-developer/agentic-rag/blob/main/Agentic_RAG_Redis_LlamaIndex.ipynb)\n* Cleanlab has a tutorial on getting trustworthiness scores from your RAG pipeline to allow you to avoid hallucinations and course\\-correct. [Tweet](https://twitter.com/llama_index/status/1786914595342589978), [Notebook](https://help.cleanlab.ai/tutorials/tlm_rag/)\n\n\ud83c\udfa5\u00a0**Webinars:**\n\n* On May 8 we\u2019ll be co\\-hosting a webinar with Pulumi on deploying AI applications to AWS. [Tweet](https://twitter.com/llama_index/status/1785456924852715907)\n* Our own Andrei and friends walk you from basic RAG through handling long\\-context RAG all the way to evaluating your RAG pipeline. [Tweet](https://twitter.com/llama_index/status/1785802821604127205), [Video](https://www.youtube.com/watch?v=bNqSRNMgwhQ), [Notebook](https://github.com/nerdai/talks/blob/main/2024/mlops/mlops-rag-bootcamp.ipynb)",
        "efb24751-06b2-4bf4-be0d-1adde9f1f1e4": "[Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/retrievers/recurisve_retriever_nodes_braintrust.ipynb), [Tweet](https://twitter.com/llama_index/status/1707485040018632776?s=20).\n5. **LocalAI**: We integrated LocalAI\\_API LLM support for on\\-prem runs or as an alternative to OpenAI LLM. [Tweet](https://x.com/llama_index/status/1708227476684734555?s=20).\n6. **HoneyHiveAI**: We integrated with HoneyHiveAI for enhanced multi\\-step RAG/agent pipeline monitoring. Log traces, gather user feedback, and utilize it for precise fine\\-tuning and evaluations. [Docs](https://docs.honeyhive.ai/quickstart/llamaindex), [Tweet](https://x.com/llama_index/status/1709311769947357425?s=20).\n7. **UnstructuredIO**: We integrated with UnstructuredIO to tackle the RAG challenge of querying embedded tables in 10\\-K filings. Now, seamlessly query any tabular data or text within a 10\\-K document. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/query_engine/sec_tables/tesla_10q_table.ipynb), [Tweet](https://twitter.com/jerryjliu0/status/1709352476456132702?s=20).\n8. **Clarifai**: We integrated with Clarifai, offering access to 40\\+ LLMs and various embedding models. [Tweet](https://twitter.com/llama_index/status/1710065802672476342?s=20).\n\n\ud83c\udfa5 **Webinars:**\n\n1. [Webinar](https://www.singlestore.com/resources/webinar-how-to-build-a-genai-app-with-llama-index/?utm_source=kunal-kushwaha&utm_medium=influencer&utm_campaign=How-to-Build-a-GenAI-App-with-LlamaIndex&campaignid=7014X0000029YtwQAE) by SingleStoreDB on How to Build a GenAI App with LlamaIndex.\n2. [Webinar](https://www.youtube.com/watch?v=C5NhoMBkaQU) on projects built during the SuperAGI Autonomous Agents Hackathon featuring evo.ninja, RicAI, Atlas and MunichAI.\n\n\ud83c\udf88 **Events:**\n\n1. Jerry Liu and Simon [conducted](https://github.com/anyscale/ray-summit-2023-training/tree/main/Ray-LlamaIndex/notebooks) a workshop on RAG \\+ Evaluation at RaySummit.\n2. [Yi Ding](https://twitter.com/yi_ding) spoke on \u2018LLM Quirks Mode\u2019 at MLOps community event.\n3. Jerry Liu spoke on Evals/ Benchmarking [and Advanced RAG techniques](https://docs.google.com/presentation/d/1v7T6ejrSo87ndGeGC7tt6zeq-cftu03WWw7WL8Jskug/edit?usp=sharing) at AIConf 2023\\.\n4. [Ravi Theja](https://twitter.com/ravithejads) conducted a workshop on Mastering RAG with LlamaIndex at PyCon India, 2023\\.\n5. [Ravi Theja](https://twitter.com/ravithejads) presented a [poster](https://x.com/ravithejads/status/1709431323989856348?s=20) on Automatic Knowledge Transfer(KT) Video generation on code bases using LlamaIndex at PyCon India, 2023\\.",
        "1c5261db-94c3-4f3c-82b6-b9044520eb61": "```\nSimpleCSVReader = download_loader(\"SimpleCSVReader\")\nloader = SimpleCSVReader(concat_rows=False)\ndocuments = loader.load_data(file=Path('./countries.csv'))\n\nprint(documents)\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n```\nDocuments from the CSV Loader\n\n\n```\n[Document(text='country, capital, type', doc_id='67c30c68-7d9f-4906-945b-9affc96f95d2', embedding=None, doc_hash='3a506ebea9c04655b51406d79fdf5e3a87c3d8ff5b5387aace3e5a79711a21b8', extra_info=None),\nDocument(text='Abkhazia, Sukhumi, countryCapital', doc_id='6e6be4b5-051f-48e0-8774-6d48e0444785', embedding=None, doc_hash='ea387d0eab94cc6c59f98c473ac1f0ee64093901673b43e1c0d163bbc203026e', extra_info=None),\n...]\n```\nThe CSV loader didn\u2019t create one Document per CSV row by default, but only one for the whole document, but you could configure it so that it turned the CSV into one document per row.\n\nLlamaIndex supports much more involved setups of different kinds of indexes, allows to chain them and even conditionally select one or the other. Here we just do the bare minimum to demonstrate our loaders.\n\nAfter setting up the indices with the appropriate loaders, and connected indexes, we now can use the index as an LLM query engine and execute our user query.\n\nTo demonstrate that the LLM still is able to use its world knowledge, we can ask in a mix of English (System), German (Question) and French (requested Answer).\n\n\n```\nqueryEngine = index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nProvide the answer in French.\nQuestion: Was ist die Hauptstadt von Albanien?\n\"\"\")\n```\nAs you can see in the response below it doesn\u2019t just answer our question correctly in French `La capitale de l\u2019Albanie est Tirana.`, but also provides which documents it used to generate the answer.",
        "2d95ba59-39f9-4d38-9ccd-876aa59e4a77": "```\nreranker_base = CohereRerank(top_n=5)\nreranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(\n    top_n=5\n)\nreranker_model_5_random = (\n    finetune_model_random_hard_negatives.get_finetuned_model(top_n=5)\n)\nreranker_model_5_cosine = (\n    finetune_model_cosine_hard_negatives.get_finetuned_model(top_n=5)\n)\n```\nTesting\n=======\n\nWe will conduct tests on the first 150 nodes from Uber using the following different rerankers.\n\n1. Without Reranker.\n2. Cohere Reranker.\n3. Fine\\-tuned reranker (Custom reranker) without hard negatives.\n4. Fine\\-tuned reranker (Custom reranker) with hard negatives selected at random.\n5. Fine\\-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity.\n\nLet\u2019s define the rerankers.\n\n\n```\nRERANKERS = {\n    \"WithoutReranker\": \"None\",\n    \"CohereRerank\": reranker_base,\n    \"CohereRerank_0\": reranker_model_0,\n    \"CohereRerank_5_random\": reranker_model_5_random,\n    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n}\n```\nCreate an Index and Retriever for evaluation purposes.\n\n\n```\n# Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\nindex_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)\n\nquery_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n)\n\nservice_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\nservice_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n\nvector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query)\n```\nDefine a function to display the results\n\n\n```\ndef display_results(embedding_name, reranker_name, eval_results):\n    \"\"\"Display results from evaluate.\"\"\"\n\n    metric_dicts = []\n    for eval_result in eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[\"hit_rate\"].mean()\n    mrr = full_df[\"mrr\"].mean()\n\n    metric_df = pd.DataFrame(\n        {\"Embedding\": [embedding_name], \"Reranker\": [reranker_name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n    )\n\n    return metric_df\n```\nLoop over different rerankers and evaluate retrieval performance using Custom Retriever.",
        "e49d29d6-c1df-4e86-b572-e45380ad35be": "Despite recent motivation to utilize NLP for wider range of real world\n applications, most NLP papers, tasks and pipelines assume raw, clean texts.\n However, many texts we encounter in the wild, including a vast majority of\n legal documents (e.g., contracts and legal codes), are not so clean, with many\n of them being visually structured documents (VSDs) such as PDFs. PDFs are\n versatile, preserving the visual integrity of documents, but they often pose a\n significant challenge when it comes to extracting and manipulating their\n contents.\n\n\n\n\n In this discussion, our focus will primarily be on text\\-only layered PDFs, a\n category often regarded by many as a resolved issue.\n\n\n\nComplexity of Parsing PDFs\n==========================\n\n\n1. **Layout Complexity**: PDFs can contain complex layouts, such\n as multi\\-column text, tables, images, and intricate formatting. This layout\n diversity complicates the extraction of structured data.\n2. **Font encoding issue**s: PDFs use a variety of font encoding\n systems, and some of these systems do not map directly to Unicode. This can\n make it difficult to extract the text accurately.\n3. **Non\\-linear text storage:** PDFs do not store text in the\n order it appears on the page. Instead, they store text in objects that can\n be placed anywhere on the page. This means that the order of the text in the\n underlying code may not match the order of the text as it appears visually.\n4. **Inconsistent use of spaces**: In some PDFs, spaces are not\n used consistently or are not used at all between words. This can make it\n difficult to even identify word boundaries.\n\n\nDo we need an efficient parser?\n===============================\n\n\n\n> In the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n>  Process Entire PDFs?\n\n\n\n This question gains relevance if the answer to this next question is \u201cYes\u201d.\n\n\n\n\n> Do we need Retrieval\\-Augmented Generation (RAG)?\n\n\n\n While LLMs are powerful, they have certain limitations in terms of the amount\n of text they can process at once and the scope of information they can\n reference. Further recent research have suggested LLM performance is often\n highest when relevant information occurs at the beginning or end of the input\n context, and significantly degrades when models must access relevant\n information in the middle of long contexts. Techniques like RAG help overcome\n these limitations, enabling more effective and efficient processing of large\n documents and broader information retrieval.\n\n\n\n\n> Still Skeptical? Let\u2019s ask an LLM for confirmation.\n\n\n\n![Chat GPT output mentioning the necessity of an efficient parser](/blog/images/1*3X_McbzikW3Uty6qaQKYgw.jpeg)\n\n\n Now that we\u2019ve established the importance of an efficient parser, it becomes\n instrumental in constructing an effective Retrieval\\-Augmented Generation (RAG)\n pipeline to address the limitations of an LLM. Let\u2019s explore how we are\n achieving this today. It\u2019s crucial to remember that the quality of the context\n fed to an LLM is the cornerstone of an effective RAG, as the saying goes,\n \u2018***Garbage In \u2014 Garbage Out***.\u2019\n\n\n\n\n In the context of building LLM\\-related applications,\n **chunking** is the process of breaking down large pieces of text\n into smaller segments. It\u2019s an essential technique that helps optimize the\n relevance of the content we get back from a database once we use the LLM to\n embed content. Some of the strategies involved are",
        "3893a73c-a22f-41b8-b9d2-8002eb3362b2": "Hello, Llama Lovers! \ud83e\udd99\n\nWelcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to share some exciting updates about `llama-agents`, along with demos, extensive guides, and in\\-depth tutorials to enhance your understanding of our tools.\n\nBefore we dive into our newsletter, we\u2019re excited to announce the return of Community Office Hours. If you have use\\-cases, in\\-depth questions, or feedback for the team at LlamaIndex, join us during our community office hours! We\u2019ll set up a 15\\-30 minute Zoom call to discuss it.\n\n[**Sign up here**](https://docs.google.com/forms/d/e/1FAIpQLSefrnmxQWD-1OhSP51kUKtdbw9EGDjrMLefkZFACKD19TKsuQ/viewform?usp=sf_link) to participate.\n\n\ud83e\udd29\u00a0**The highlights:**\n---------------------\n\n* **Multi\\-Agent Kubernetes Kit Launched:** Deploy multi\\-agent systems easily with our new Kubernetes Starter Kit featuring ready\\-to\\-use tools and configurations. [Notebook](https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes), [Tweet](https://x.com/llama_index/status/1807801281324765469).\n* **Enhanced Communication with RabbitMQ:** Boost multi\\-agent system reliability and scalability in production with our new RabbitMQ integration. [Notebook](https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq), [Tweet](https://x.com/llama_index/status/1810342085171855753).\n* **Reflection as a Service Guide:** Improve agent reliability with our guide on building Reflection as a Service, perfect for output validation and correction. [Notebook](https://github.com/run-llama/llama-agents/blob/main/examples/reflection/toxicity_reflection_service.ipynb), [Tweet](https://x.com/llama_index/status/1808898730638389262).\n* **Corrective RAG as a Service Guide:** Create a self\\-correcting RAG that ensures context relevance and integrates search fallbacks before generation. [Notebook](https://github.com/run-llama/llama-agents/blob/main/examples/corrective_rag.ipynb), [Tweet](https://x.com/llama_index/status/1809282069606068486).\n* **Tutorial series on Property Graphs:** 6\\-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j, and Ollama. [Videos](https://www.youtube.com/playlist?list=PLTZkGHtR085ZYstpcTFWqP27D-SPZe6EZ), [Tweet](https://x.com/llama_index/status/1810410943215710510).\n\n**\u2728 Feature Releases and Enhancements:**\n----------------------------------------\n\n1. We have launched a Multi\\-Agent on Kubernetes Starter Kit to build and deploy a multi\\-agent system using Docker Compose and Kubernetes using llama\\-agents. This kit includes prebuilt agent loops and tools, as well as Dockerfiles and Kubernetes manifests for easy production deployment. [Notebook](https://github.com/run-llama/llama-agents/tree/main/examples/docker-kubernetes), [Tweet](https://x.com/llama_index/status/1807801281324765469).\n2. We have integrated RabbitMQ with llama\\-agents to enhance multi\\-agent communication, offering scalability and reliability for handling large request volumes in production. [Notebook](https://github.com/run-llama/llama-agents/tree/main/examples/rabbitmq), [Tweet](https://x.com/llama_index/status/1810342085171855753).\n3. \\[[Yi\\-01\\.AI](http://Yi-01.AI)]([http://Yi\\-01\\.AIhttps](http://Yi-01.AIhttps)://x.com/01AI\\_Yi) is integrated with LlamaIndex for enhanced retrieval and indexing, streamlining the development of smarter, faster RAG applications. [Docs](https://docs.llamaindex.ai/en/latest/examples/llm/yi/).\n4.",
        "2c4b09b1-4fb6-48a8-9c4e-e00968b7cd55": "[Azure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167)",
        "15465679-068c-4e2b-82da-5d6e86ee7c24": "Hello, LlamaIndex Family! \ud83e\udd99\n\nWelcome to another thrilling weekly update from LlamaGalaxy! We're excited to bring you a variety of outstanding updates, including the Chain of Abstraction LlamaPack, create\\-tsi, demos, guides, tutorials, and much more.\n\nBefore we delve into these updates, we have an exciting tutorial series on Agents and Tools for you to check out. Perfect for beginners, this series covers everything from advanced QA/RAG implementations to step\\-wise execution. By the end, you\u2019ll have gained a deeper understanding of how to use agent reasoning with tool use to build simple applications. Check them out:\n\n1. [Overview](https://www.youtube.com/watch?v=-AuHlVMyEA0)\n2. [ReAct agents](https://youtu.be/pRUc6JPw6CY)\n3. [Function Calling agents](https://youtu.be/6INvyrC4WrA)\n4. [Retrieval\\-Augmented agent](https://youtu.be/K7h17Jjtbzg)\n5. [Controlling tool outputs](https://youtu.be/gFRbkRtLGZQ)\n6. [Agents with step\\-by\\-step execution](https://youtu.be/JGkSxdPFgyQ)\n\n\ud83e\udd29\u00a0**The highlights:**\n---------------------\n\n* **Chain of Abstraction LlamaPack:** Chain of Abstraction technique as llamapack a method enabling multi\\-step reasoning for enhanced tool use introduced by Silin Gao's team. [LlamaPack](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb), [Tweet](https://x.com/llama_index/status/1778845258119524640).\n* **Create\\-tsi Toolkit:** Launched a toolkit for building full\\-stack RAG applications with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers. [Code](https://github.com/telekom/create-tsi), [Tweet](https://x.com/llama_index/status/1778812761893650551).\n* **Improved Agent Control**: **`return_direct`** feature in tools allows direct output returns, reducing costs and enhancing response efficiency. [Docs](https://docs.llamaindex.ai/en/latest/examples/agent/return_direct_agent/), [Tweet](https://x.com/llama_index/status/1778072285003550932).\n\n**\u2728 Feature Releases and Enhancements:**\n----------------------------------------\n\n1. We have introduced the Chain of Abstraction Technique Developed by Silin Gao, and team as LlamaPack, this new method enables LLMs to generate multi\\-step reasoning chains for efficient sequence planning, enhancing tool use beyond single\\-shot functions. [LlamaPack](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/coa_agent.ipynb), [Tweet](https://x.com/llama_index/status/1778845258119524640).\n2. We have launched create\\-tsi: A toolkit in collaboration with T\\-Systems and Marcus Schiesser to generate GDPR\\-compliant, full\\-stack AI applications via a CLI interface. Build enterprise\\-grade RAG bots with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers. [Code](https://github.com/telekom/create-tsi), [Tweet](https://x.com/llama_index/status/1778812761893650551).\n3. We have introduced **`return_direct`** feature in tools that enhances agent controllability by allowing direct output returns as final responses. This optimizes for reduced latency and costs, and effectively halts the agent after crucial actions like booking confirmations or answering queries. [Docs](https://docs.llamaindex.ai/en/latest/examples/agent/return_direct_agent/), [Tweet](https://x.com/llama_index/status/1778072285003550932).\n\n**\ud83c\udfa5\u00a0Demos:**\n------------\n\n* [RAG\\-enhanced MetaGPT](https://x.com/llama_index/status/1777851305308102845): A robust multi\\-agent framework that features structured team dynamics for problem\\-solving, now supercharged with domain\\-specific knowledge from LlamaIndex modules. This framework supports diverse data inputs, sophisticated retrieval options, and efficient data management for enhanced agent performance.",
        "66605346-838e-4735-90d9-946a9e70a1f0": "```\nfrom transformers import load_tool\nprompt_assistant = load_tool(\n    \"llamaindex/text2image_prompt_assistant\",\n    openai_api_key=\"your_api_key\",\n    model_name='text-davinci-003',\n    temperature=0.3,  # increase or decrease this to control variation\n    verbose=True\n)\n\nfrom transformers import OpenAiAgent\nagent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"your_api_key\")\n\n# replace the existing tool\nagent.toolbox['image_generator'] = prompt_assistant\n\nagent.run(\"Draw me a picture a mountain.\")\n```\nUsing Our new LlamaIndex Prompt Assistant tool, we get a much more stylized result. In the terminal, we see the prompt was re\\-written as \u201ca majestic mountain peak, surrounded by lush greenery, with a stunning sunset in the background,\u201d which resulted in the following image:\n\n![](/blog/images/1*1YbUcsgYESVAccKpJzt2Wg.png)\n\nImage generated by our Text2Image Prompt Assistant tool.\n\nLooks great! With the temperature variable, we can control how varied the generated prompts become. With a temperature above zero, each prompt generated by LlamaIndex with the same agent prompt will be brand new!\n\nConclusion\n==========\n\nIn conclusion, we have demonstrated how LlamaIndex can be used to augment LLM agents, by implementing a Text2Image Prompt Assistant tool with a Transformers Agent. Using a vector database created from DiffusionDB, LlamaIndex can suggest better prompts when generating images.\n\nCustom tools in Transformers Agents are easily distributed and shared using Hugging Face Spaces, and we are excited to see what other people build and share!",
        "ccaa3fb4-59f5-4910-8fbf-a1a04d7a27e2": "Evaluation suite from \\`InformationRetrievalEvaluator\\`\n\n![](/blog/images/1*Eyl8G6n3yHfikacz_HVaIg.png)Conclusion\n==========\n\nWe successfully finetuned an embedding model over unlabeled, unstructured data to give better retrieval performance for downstream RAG systems. We show a 5\u201310% improvement across all metrics!\n\n**Resources**\n\n(copied from intro)\n\n* Repo: [https://github.com/run\\-llama/finetune\\-embedding](https://github.com/run-llama/finetune-embedding)\n* Notebooks: [Dataset Generation](https://github.com/run-llama/finetune-embedding/blob/main/generate_dataset.ipynb), [Finetuning](https://github.com/run-llama/finetune-embedding/blob/main/finetune.ipynb), [Evaluation](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)",
        "bf1e1fab-1673-444b-9809-38b09391ba66": "Hi there, LlamaIndex followers! \ud83e\udd99\n\nWelcome to another thrilling weekly update from the LlamaUniverse. We're excited to bring you a fantastic array of updates, including Privacy\\-Preserving In\\-Context Learning with LlamaPacks and RAG Networks. Dive into our guides on MistralAI, explore Gemma LLMs, and enjoy a plethora of engaging tutorials using LlamaIndex, alongside upcoming webinars and events.",
        "8587b85c-5b77-41ff-816e-0fc8ee93e3a3": "One of the many amazing feats that LLMs are capable of is generating executable code. This can be used to solve a variety of complex problems that require calculations and fixed logic that traditional computing excels at but LLMs can struggle to perform directly. When building agents to perform complex tasks, equipping your agent with code execution as an available tool can be a powerful strategy.\n\nHowever, this strategy comes with a major drawback: executable code can be flawed or even dangerous to execute, and detecting whether code will be problematic prior to executing it is arguably an expression of the [Halting Problem](https://en.wikipedia.org/wiki/Halting_problem), making it impossible to guarantee success at detection.\n\nThe solution is [sandboxing](https://en.wikipedia.org/wiki/Sandbox_(computer_security)), to isolate potentially problematic code from the host environment. Now, thanks to dynamic sessions in Azure Container Apps, the ability to execute sandboxed code generated by an LLM is simple directly from LlamaIndex. It\u2019s implemented as a tool that can be used by any LlamaIndex agent.\n\nIn this blog post we\u2019ll show you exactly how to use the new Azure Code Interpreter tool and walk you through a couple of examples of how to make the most of it. You can see the full code in [this notebook](https://docs.llamaindex.ai/en/latest/examples/tools/azure_code_interpreter/) and read more in the [tool documentation](https://llamahub.ai/l/tools/llama-index-tools-azure-code-interpreter) on LlamaHub and on [learn.microsoft.com](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter?tabs=azure-cli).\n\nSet up Azure Container Apps dynamic sessions\n--------------------------------------------\n\nFirst, install our python packages including the tool:\n\n\n```\npip install llama-index\npip install llama-index-llms-azure\npip install llama-index-tools-azure-code-interpreter\n```\nIn [the notebook](https://docs.llamaindex.ai/en/latest/examples/tools/azure_code_interpreter/) we\u2019re using GPT 3\\.5 Turbo hosted on Azure as the LLM, but you can use any LLM capable of tool use:\n\n\n```\nfrom llama_index.llms.azure_openai import AzureOpenAI\nllm = AzureOpenAI(\n    model=\"gpt-35-turbo\",\n    deployment_name=\"gpt-35-deploy\",\n    api_key=api_key,\n    azure_endpoint=azure_endpoint,\n    api_version=api_version,\n)\n```\nOnce you\u2019ve got your LLM set up, you\u2019ll need to [create a session pool](https://learn.microsoft.com/en-us/azure/container-apps/sessions-code-interpreter?tabs=azure-cli#code-interpreter-session-pool) to host your executions. Doing this will give you a pool management endpoint URL that you can provide to LlamaIndex like this:\n\n\n```\n# Import the AzureCodeInterpreterToolSpec from llama_index\nfrom llama_index.tools.azure_code_interpreter import (\n    AzureCodeInterpreterToolSpec,\n)\n\n# Create the AzureCodeInterpreterToolSpec with the pool_managment_endpoint set to your session management endpoint\n# It is optional to set the local_save_path, but it is recommended to set it to a path where the tool can automatically save any intermediate data generated from Python code's output.\nazure_code_interpreter_spec = AzureCodeInterpreterToolSpec(\n    pool_managment_endpoint=\"your-pool-management-endpoint\",\n    local_save_path=\"local-file-path-to-save-intermediate-data\",\n)\n```\nThis sets up a tool ready to be used with LlamaIndex. You\u2019re now ready to set up your agent:\n\n\n```\n# Import the ReActAgent\nfrom llama_index.core.agent import ReActAgent\n\n# Create the ReActAgent and inject the tools defined in the AzureDynamicSessionsToolSpec\nagent = ReActAgent.from_tools(\n    azure_code_interpreter_spec.to_tool_list(), llm=llm, verbose=True\n)\n```\nIn this example we\u2019re providing only a single tool, but you could provide any other tools you like to your ReAct agent. Now you\u2019ve got an agent, you\u2019re ready to ask it to perform tasks!\n\nDynamic sessions code interpreter in action\n-------------------------------------------\n\nIn our first example, we\u2019re going to ask the agent the time in Seattle. This is usually a tricky task for LLMs, which don\u2019t know what time it is anywhere!",
        "2b23a55a-7db2-43cd-860d-b6195d97350c": "\ud83c\udfa5\u00a0**Webinars:**\n\n[Register for a webinar](https://lu.ma/z2vhi06e) with [**Daniel Huynh**](https://twitter.com/dhuynh95) featuring LaVague, an agent that can navigate the web in your Jupyter/Colab notebook.",
        "a168398f-72c6-429c-b9b8-78731da2fbb9": "Salutations, LlamaIndex fans! \ud83e\udd99\n\nIt's been another thrilling week in LlamaLand! With the release of Anthropic\u2019s new models Claude\\-3 Opus, Sonnet, and Haiku, we have numerous tutorials, cookbooks, and updates to share with you.\n\nA quick reminder: we are running our [first ever user survey](https://www.surveymonkey.com/r/9W8VX2H). It takes only 3 minutes and it helps us out a lot! Now let\u2019s dive in.\n\n\ud83e\udd29\u00a0**The highlights:**\n\n1. **LlamaParse JSON Mode**: A new feature that transforms PDF content into structured data, simplifying RAG pipeline development for complex documents containing images, text and tables. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb), [Tweet](https://x.com/llama_index/status/1765439865351766135?s=20).\n2. **Hierarchical Code Splitting**: Enhance code understanding with a novel technique that organizes large codebases into a hierarchical structure for improved navigation and task\\-solving. [LlamaPack](https://llamahub.ai/l/llama-packs/llama-index-packs-code-hierarchy?from=llama-packs), [Tweet](https://x.com/llama_index/status/1766152269874266170?s=20).\n3. **Anthropic Cookbook Series**: Learn to build various LLM applications with Claude 3, ranging from simple to complex, through detailed guides and tutorials. [Cookbooks](https://github.com/anthropics/anthropic-cookbook/tree/main/third_party/LlamaIndex), [Tweet](https://x.com/llama_index/status/1767218890856358115?s=20).\n\n**\u2728 Feature Releases and Enhancements:**\n\n* We launched LlamaParse JSON Mode, a new feature that structures text and images from PDFs into a dict format. With the integration of multimodal models like claude\\-3 opus, it's now simpler to develop RAG pipelines for complex PDFs containing text, images, and tables. [Notebook](https://github.com/run-llama/llama_parse/blob/main/examples/demo_json.ipynb), [Tweet](https://x.com/llama_index/status/1765439865351766135?s=20).\n* We launched a novel hierarchical code splitting technique to enhance RAG/agents for code comprehension, featuring `CodeHierarchyNodeParser` by ryanpeach. This method breaks down large code files into a hierarchical structure, enabling a knowledge graph\\-like approach for efficient code navigation and task\\-solving. [LlamaPack](https://llamahub.ai/l/llama-packs/llama-index-packs-code-hierarchy?from=llama-packs), [Tweet](https://x.com/llama_index/status/1766152269874266170?s=20).\n* We integrated with Videodb to run RAG over video streams using LlamaIndex. This tool allows you to upload, search, and stream videos based on spoken words or visual scenes, now available as a built\\-in retriever in LlamaIndex. [Tweet](https://x.com/llama_index/status/1765481657765912599?s=20).\n\n**\ud83c\udfa5\u00a0Demos:**\n\n* [**Build an AI Browser Copilot**](https://github.com/mithril-security/LaVague?tab=readme-ov-file): a project by [**Daniel Huynh**](https://twitter.com/dhuynh95) that demonstrates how to create a browser agent using RAG, local embeddings, and Mixtral to execute browser tasks from a Colab notebook, showcased with a video on navigating HuggingFace datasets.\n* [RAG over your code](https://lightning.ai/lightning-ai/studios/chat-with-your-code-using-rag): a project by [**Akshay**](https://twitter.com/akshay_pachaar) on creating a local code assistant using LlamaIndex, MistralAI, and Streamlit to index and query GitHub repositories, offering a foundational guide for advanced code QA.",
        "50bcd1b7-947b-4498-9d0c-5f9bb48ea8aa": "We\u2019re happy to announce the recent integration of LlamaIndex with PostgresML \u2014 a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval\\-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier\\-to\\-manage RAG workflows.\n\n**The problem with typical RAG workflows**\n\nTypical Retrieval\\-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ffb15aa28a86540053e4ab8856dae711b355ceff6-3040x1408.png%3Ffit%3Dmax%26auto%3Dformat&w=3840&q=75)Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user\\-centric issues are compounded by other challenges:\n\n* Increased dev time to master new technologies\n* Complicated maintenance and scalability issues due to multiple points of failure\n* Costly vendors required for multiple services\n\nThe diagram above illustrates the complexity, showing how each component interacts across different services \u2014 exacerbating these problems.\n\n**Solution**\n\nThe PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows.\n\n![](/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fcca0a76c3254b6ba82bf10f40bd045f44c6e29e0-3520x1344.png%3Ffit%3Dmax%26auto%3Dformat&w=3840&q=75)By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call \u2014 resulting in improved performance and reduced latency. Additionally, the use of open\\-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow.\n\n**About PostgresML**\n\nPostgresML \\[[github](https://github.com/postgresml/postgresml) \\|\\| [website](https://postgresml.org/) \\|\\| [docs](https://postgresml.org/docs)] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in\\-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real\\-time outputs in one process, directly where your data resides.",
        "ca715b43-da3e-484d-b819-df0b16a8b440": "Summary\n=======\n\nIn this article, we compare how well LLM\\-powered agents with different degrees of complexity perform over practical data tasks (financial analysis). We compare the performance of agents with more *complex, unrestrained* interaction behavior (ReAct) with agents that contain *simpler, more constrained* interactions (routing). We specifically analyze how much complexity can be added to the agent layer vs. the tool layer.\n\nWe find that the choice of the language model matters a lot. ReAct agents that are powered by \u201cdumber\u201d models (in a tongue\\-in\\-cheek fashion we are referring to any non GPT\\-4 model as \u201cdumb\u201d) struggle to return relevant results over data. We find that constraining agent interaction behavior, and giving them access to more tools that can more explicitly perform complex actions, can help improve query performance over these less sophisticated LLMs. In contrast, more sophisticated models (GPT\\-4\\) can more reliably utilize the ReAct loop to execute a variety of complex data queries.\n\nThis blog post is quite detailed; we provide a *lot* of experiments and results below. Best of all, you can run this all yourself with our [example notebook](https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing)!\n\nOverview of Agents\n==================\n\nBuilding LLM\\-powered agents have gotten increasingly popular in the past few months. Frameworks like [LangChain](https://github.com/hwchase17/langchain) have made it much easier to create these agents according to a set of common abstractions.\n\nAt a high\\-level, an \u201cagent\u201d is essentially an automated decision engine, that can be used to interact with an external environment. The core agent loop looks something like the following:\n\n1. The agent has access to a set of \u201ctools\u201d, which are generic functions that it can perform. It has an awareness of each tool through some attached metadata, and it can call each tool (either as a function call or structured API).\n2. User feeds in a natural language input to the agent.\n3. Given the input, the agent **interacts with the set of tools** in some fashion and returns the response.\n\nThere\u2019s a variety of ways to perform **agent\\-tool interaction.**\n\n* The most popular is probably [ReAct](https://arxiv.org/abs/2210.03629): the agent reasons over the next action, constructs an action command, executes the action. It repeats these steps in an iterative loop until the task is complete.\n* There are other interaction modes too. Recently there was a paper on [Plan\\-and\\-solve Prompting](https://arxiv.org/pdf/2305.04091.pdf), which generates a plan beforehand (to decompose a complex task into simpler ones). Before ReAct there have also been related techniques on [Self\\-Ask](https://arxiv.org/abs/2210.03350) and [Chain of Thought Prompting](https://arxiv.org/abs/2201.11903).\n\n\u201cComplex\u201d vs. \u201cSimple\u201d Agent Interaction Techniques\n---------------------------------------------------\n\nWe classify techniques like ReAct are more *complex and unconstrained:* this is because they perform iterative reasoning and also break the input into smaller steps. Complicated agent interaction loops allow for more *freedom of behavior,* andcreate an increased burden on the LLM being used. The pro of complex interaction frameworks is that they can be more general and handle a broader class of queries over simple tools. The con is that if the LLM is not up to par, then these frameworks are prone to making mistakes; unconstrained behavior can lead to unexpected results.\n\nOn the other end of the spectrum, you can imagine a *simple and constrained* agent interaction mechanism, where the agent does one\\-step selection of the underlying tool to use, and returns the response from the tool. The agent essentially just acts as a router from the query to Tool. There are no steps to break down the question into smaller ones, and no iterative chain\\-of\\-thought loops.",
        "0ff2bf08-7cfa-49f2-afa2-039cc71d9431": "Strategic alliance and joint product promises to broaden the adoption of generative AI across industries\n\nArize AI, a pioneer and leader in AI observability and LLM evaluation, and LlamaIndex, a leading data framework for LLM applications, debuted a new joint offering today called LlamaTrace, a hosted version of Arize OSS Phoenix.\n\nAccording to a soon\\-to\\-release survey, 47\\.7% of AI engineers and developers building generative AI applications are leveraging retrieval today in their LLM Applications. By connecting data to generative AI, orchestration frameworks like LlamaIndex can be game\\-changers in accelerating generative AI development. However, for many teams and enterprises technical challenges remain in getting modern LLM systems \u2013 with layers of abstraction \u2013 ready for the real world.\n\nTo help, Arize and LlamaIndex are debuting an LLM tracing and observability platform that works natively with the LlamaIndex and Arize ecosystem. With a foundation based on [Arize Phoenix OSS](https://phoenix.arize.com/), the hosted version of Phoenix offers the ability to persist application telemetry data generated during AI development in order to better experiment, iterate, and collaborate in development or production.\n\nThe solution has a foundation in open source and features a fully hosted, online, persistent deployment option for teams that do not want to self host. AI engineers can instantly log traces, persist datasets, run experiments, run evaluations \u2013 and share those insights with colleagues.\n\nThe new offering is available today, and can be accessed through either a LlamaIndex or Arize account.\n\n\n> \u201cWe share a vision with LlamaIndex in enabling builders to reduce the time it takes to deploy generative AI into production but in a way that is super battle hardened for business\\-critical use cases,\u201d said Jason Lopatecki, CEO and Co\\-Founder of Arize. \u201cAs leaders in our respective spaces with a common philosophy in empowering AI engineers and developers, we\u2019re uniquely positioned here to do something that can move modern LLMOps forward and broaden adoption.\u201d\n\n\n> \u201cPrototyping a RAG pipeline or agent is easy, but every AI engineer needs the right data processing layer, orchestration framework, and experimentation/monitoring tool in order to take these applications to production. LlamaTrace by Arize offers the richest toolkit we\u2019ve seen in enabling developers to observe, debug, and evaluate every granular step of a very complex LLM workflow, and it nicely complements the production\\-ready data platform and orchestration framework that LlamaCloud and LlamaIndex offer.\u201d \\- Jerry Liu, CEO of LlamaIndex\n\n**About Arize AI**\n\nArize AI is an AI observability and LLM evaluation platform that helps teams deliver and maintain more successful AI in production. Arize\u2019s automated monitoring and observability platform allows teams to quickly detect issues when they emerge, troubleshoot why they happened, and improve overall performance across both traditional ML and generative use cases. Arize is headquartered in Berkeley, CA.",
        "2f647bb0-cb5a-47d5-a09e-55d9948b024c": "```\nfrom llama_index.llms import Gemini\n\n# completion\nresp = Gemini().complete(\"Write a poem about a magic backpack\")\n# chat\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = Gemini().chat(messages)\n# streaming (completion)\nllm = Gemini()\nresp = llm.stream_complete(\n    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n)\n# streaming (chat)\nllm = Gemini()\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = llm.stream_chat(messages)\n# async completion\nresp = await llm.acomplete(\"Llamas are famous for \")\nprint(resp)\n# async streaming (completion)\nresp = await llm.astream_complete(\"Llamas are famous for \")\nasync for chunk in resp:\n    print(chunk.text, end=\"\")\n```\nThe `Gemini` class of course has parameters that can be set. This includes `model_name`, `temperature`, `max_tokens`, and `generate_kwargs`.\n\nAs an example, you can do:\n\n\n```\nllm = Gemini(model=\"models/gemini-ultra\")\n```\nMulti\\-modal Model\n==================\n\n[Full Notebook Guide Here](https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/gemini.ipynb)\n\nIn this notebook, we test out the `gemini-pro-vision` variant that features **multi\\-modal inputs.** It contains the following features:\n\n* supports both `complete` and `chat` capabilities\n* supports streaming and async\n* Supports feeding in **multiple images** in addition to text in the completion endpoint\n* Future work: multi\\-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini\\-pro\\-vision.\n\nLet\u2019s walk through a concrete example. Let\u2019s say we are given a picture of the [following scene](https://storage.googleapis.com/generativeai-downloads/data/scene.jpg):\n\n\n\nScene from a street in New York City\n\n![](/blog/images/1*TLGf41-Mx262yqZfUdFpzg.jpeg)We can then initialize our Gemini Vision model, and ask it a question: \u201cIdentify the city where this photo was taken\u201d:\n\n\n```\nfrom llama_index.multi_modal_llms.gemini import GeminiMultiModal\nfrom llama_index.multi_modal_llms.generic_utils import (\n    load_image_urls,\n)\n\nimage_urls = [\n    \"&lt;https://storage.googleapis.com/generativeai-downloads/data/scene.jpg&gt;\",\n    # Add yours here!\n]\nimage_documents = load_image_urls(image_urls)\ngemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\ncomplete_response = gemini_pro.complete(\n    prompt=\"Identify the city where this photo was taken.\",\n    image_documents=image_documents,\n)\n```\nOur response is the following:\n\n\n```\nNew York City\n```\nWe can insert multiple images too. Here\u2019s an example with an image of Messi and the Colosseum.",
        "6629f11d-6e58-4e54-a8fb-1d2ffc0752c5": "feedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n\nl = TruLlama(app=query_engine, feedbacks=feedbacks)\n```\nExplore In Dashboard\n--------------------\n\nEvery query that is tracked can now be viewed in the TruLens dashboard. After running the feedback functions on a set of records (interactions), you can see the aggregate results of the evaluation on a leaderboard; then drill down into an app version and examine how it is performing on individual records. These steps can help you understand the quality of an app version and its failure modes.\n\n![](/blog/images/1*7xFzisSS6fBzPZZ97ER8Zw.png)In this example, the model is doing fairly well on the relevance and language match feedback evaluations, but seems to be doing poorly on qs\\_relevance. This can be an indicator that the retrieved chunks are often irrelevant. This can be a significant source of \u201challucinations\u201d in retrieval\\-augmented generative AI apps.\n\nWe can now drill down and identify specific instances where this may be an issue:\n\n![](/blog/images/1*msWv0wHaNddbSm7RzhuflA.png)Let\u2019s look at a good example first. \u201cWhat did the author do growing up?\u201d\n\n![](/blog/images/1*z39YNrreFi_8YCJ356BZ2g.png)In this example, we retrieved two chunks from the index both of which were fairly relevant to the the question and as a result the LLM summarizes it into a relevant and factually correct answer.\n\nOn the other hand, let\u2019s look at an example where this didn\u2019t go so well: \u201cWhere was the author born?\u201d. In this example, the app confidently provides an incorrect answer.\n\n![](/blog/images/1*78nAjBSUj22QmmdBZLu-Ig.png)In this example, the two pieces of context retrieved had moderate relevance to the question. Further, neither context contained the answer. Even though our relevance feedback function (which doesn\u2019t check for factual correctness) didn\u2019t detect an issue, because the underlying chunks were not very relevant, this was a strong indicator that something was off. Indeed, this is an example of the model hallucinating on a question that is fairly easy to fact check.\n\nIterate on your App\n-------------------\n\nOnce you find issues like this with your app, it can be helpful to iterate on your prompts, models and chunking approaches to optimize your app. As you do this, you can track the performance of each version of your model with TruLens. Here is an example of a dashboard with multiple iterations testing against each other.\n\n![](/blog/images/1*hMQURmCAGN9OXJnQN2Wa0Q.png)",
        "427e3b59-a42a-4b89-95dc-260e7abd7ec4": "What is Lyzr?\n-------------\n\n[Lyzr](https://www.lyzr.ai/) is a full\\-stack agent framework that specializes in building fully autonomous AI agents for enterprises. Their focus is on achieving Organizational General Intelligence (OGI) by harnessing agent data. Lyzr offers pre\\-built agents like [Jazon](https://www.lyzr.ai/jazon/), an AI sales development representative, and [Skott](https://www.lyzr.ai/skott/), an AI content marketing agent, as well as a no\\-code builder for custom agent creation. Their platform enables organizations to build, deploy, and manage AI agents that can handle complex tasks and workflows autonomously.\n\nHow does LlamaIndex help?\n-------------------------\n\nLlamaIndex plays a crucial role in Lyzr's technology stack:\n\n1. **Context augmentation**: LlamaIndex components supply essential context to Lyzr's agents, creating Retrieval\\-Augmented Generation (RAG) systems that enable them to perform focused and effective work.\n2. **Custom data access**: LlamaIndex\u2019s data connectors are the preferred way for Lyzr agents to access customer\\-specific information. LlamaIndex\u2019s huge library of connectors means they can connect no matter where the customer stores their data.\n3. **Flexible retrieval**: Lyzr uses LlamaIndex's customizable retrieval methods to optimize performance for different use cases. Lyzr\u2019s AutoRAG system determines the optimal retrieval model, chunk size, and other parameters based on input data and use case and passes those to LlamaIndex.\n\nWhat have the results been like?\n--------------------------------\n\nThe integration of LlamaIndex into Lyzr's framework has contributed to significant growth and improved performance:\n\n**1\\. Rapid revenue growth**: Lyzr's annual recurring revenue jumped from around $100,000 to about $1\\.5 million in less than 60 days.\n\n**2\\. Enhanced agent accuracy**: LlamaIndex's advanced capabilities and accurate RAG have led to highly accurate agents with very low error rates, providing a great alternative to OpenAI\u2019s Assistant API.\n\n**3\\. Scalability**: The flexibility provided by LlamaIndex has allowed Lyzr to sustain its growth and expand its agent offerings.\n\nWhat do customers think?\n------------------------\n\nCustomer reception of Lyzr's LlamaIndex\\-powered agents has been overwhelmingly positive:\n\n**1\\. High adoption rate**: 75% of Lyzr's customers use two or more AI agents including custom workflow agents, indicating strong adoption of Lyzr Agent Framework.\n\n**2\\. Customer Persona**: Lyzr\u2019s \u2018fully autonomous\u2019 AI agents seem to have captured customer\u2019s imagination with SaaS CTOs being the primary adopter of Lyzr AI Agents to automate their backend workflows in a more reliable, secure and predictable manner.\n\n**3\\. Positive testimonials**: Customers like SurePeople love Lyzr:\n\n\n> \"SurePeople is delighted to announce our partnership with Lyzr.ai, a key player in fortifying the scalability, security, and future\\-readiness of our AI infrastructure. Thanks to their versatile Agents, we're empowered to operate at the forefront of innovation, underpinned by a robust framework that bolsters our AI applications. In an ever\\-evolving landscape of artificial intelligence, Lyzr.ai's Agents ensure we remain at the cutting edge. Additionally, our collaboration has been enriched by their exceptionally skilled and cooperative team.\" \u2013 Niko Drakoulis, CEO of SurePeople\n\nWhat's next for Lyzr?\n---------------------\n\nLyzr has ambitious plans for the future, building on their success with LlamaIndex, including new agents such as [Kathy](https://www.lyzr.ai/kathy/), an AI competitor analyst, and Diane, an AI HR agent, with several others in the pipeline. They\u2019re also developing a framework called Lyzr AgentMesh to enable interaction between these different AI agents, creating a cohesive AI\\-driven workforce.",
        "cd112d30-48fe-4111-a5e8-f668d979b526": "```\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n\n# load documents\ndocuments = SimpleDirectoryReader('./data').load_data()\n\n# Create Index\nindex = GPTVectorStoreIndex.from_documents(documents)\n\n# save index to disk\nindex.set_index_id(\"vector_index\")\nindex.storage_context.persist('storage')\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir='storage')\n# load index\nindex = load_index_from_storage(storage_context, index_id=\"vector_index\")\n\n# Query the index\nquery_engine = index.as_query_engine(similarity_top_k=3)\nresponse = query_engine.query(&lt;Query&gt;)\n\n# Response object has both response and source nodes.\n```\n\n**3\\. Evaluation**\n==================\n\nThe evaluation module can be used to answer the following three questions:\n\n1. Are the response generated and source nodes (context) matching? \u2014 Response \\+ Source Nodes (Context)\n2. Are response generated, source nodes (context), and query matching? \u2014 Query \\+ Response \\+ Source Nodes (Context)\n3. Which source nodes of the retrieved source nodes are used to generate a response? \u2014 Query \\+ Response \\+ Individual Source Nodes (Context)\n\nEvaluation can be done with some combination of the query, context, and response, combining these with LLM calls.\n\n**Response \\+ Source Nodes (Context)**\n--------------------------------------\n\nThis function answers the question: Are the response generated and source nodes (context) matching?\n\nThe response object for a given query returns both the response and source nodes (context) with which it generated the response. We can now evaluate the response against the retrieved sources \u2014 without taking into account the query! This allows you to measure hallucination \u2014 if the response does not match the retrieved sources, this means that the model may be \u201challucinating\u201d an answer since it is not rooting the answer in the context provided to it in the prompt.\n\nThe result is a binary response \u2014 either \u201cYES/NO\u201d.\n\n* YES \u2014 Response and Source Nodes (Context) are matching.\n* NO \u2014 Response and Source Nodes (Context) are not matching.\n\n\n```\nfrom llama_index.evaluation import ResponseEvaluator\n\n# build service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n# Build index and get response object\n...\n\n# define evaluator\nevaluator = ResponseEvaluator(service_context=service_context)\n\n# evaluate using the response object\neval_result = evaluator.evaluate(response)\n```\n![](/blog/images/1*6H0F9yAXuIMxoIReDN9eKw.png)**Query \\+ Response \\+ Source Nodes (Context)**\n-----------------------------------------------\n\nThis function answers the question: Are response generated, source nodes (context), and query matching?\n\nOften with the \u201cResponse \\+ Source Nodes (Context)\u201d approach, the response generated is in line with the source nodes but may not be the answer to the query. Therefore, considering the query along with the response and source nodes is a good approach for a more accurate analysis.\n\nThe goal is to determine if the response \\+ source context answers the query. The result is a binary response \u2014 either \u201cYES/NO\u201d.\n\n* YES \u2014 Query, Response, and Source Nodes (Context) are matching.\n* NO \u2014 Query, Response, and Source Nodes (Context) are not matching.",
        "bfcffed5-da51-4e13-9c38-808d9a52ca60": "[Logan](https://twitter.com/LoganMarkewich)\u2019s [tutorial](https://www.youtube.com/watch?v=2c64G-iDJKQ) on a comprehensive understanding of embedding models, their benchmarking, and their implementation in LlamaIndex, with a focus on OpenAI and Instructor embeddings, enabling semantic search through numerical text representations.\n2. [Logan](https://twitter.com/LoganMarkewich)\u2019s [tutorial](https://www.youtube.com/watch?v=LQy8iHOJE2A) on the evaluation of query engines using LlamaIndex, learn to handle uncontrolled outputs and runtime costs while measuring performance with GPT\\-4\\.\n3. [Ravi Theja](https://twitter.com/ravithejads)\u2019s [tutorial](https://www.youtube.com/watch?v=A3iqOJHBQhM) on Key Components to build QA Systems.\n\nWebinars:\n=========\n\n1. [Webinar](https://www.youtube.com/watch?v=s8ZNLqi9hzc) with Didier Lopes, CEO/Co\\-Founder at OpenBB on LLMs for Investment Research.\n2. [Webinar](https://llamaindex-and.wandb.events/) on Building \\& Evaluating an Advanced Query Engine Over Your Data with Weights and Biases.\n3. [Webinar](https://www.youtube.com/watch?v=TdVbH7uJR_Y) with [Jason](https://twitter.com/jxnlco) Liu on From Prompt to Schema Engineering with Pydantic.\n\nEvents:\n=======\n\n1. LlamaIndex and Arize [workshop](https://arize.com/resource/llm-search-retrieval-systems-with-arize-and-llamaindex-powering-llms-on-your-proprietary-data/) on LLM Search \\& Retrieval Systems with Arize and LlamaIndex: Powering LLMs on Your Proprietary Data.\n2. LlamaIndex and TruLens [workshop](https://go.truera.com/event-llm-app-workshop-with-llamaindex-and-trulens?utm_campaign=event-2023-07-27-san-francisco&utm_source=twitter&utm_medium=social) on building an LLM App.\n3. [TPF](https://twitter.com/TheProductfolks) (The Product Folks) [workshop session](https://www.youtube.com/watch?v=2ul5XQXp-YI) on Building QA Systems With LlamaIndex by [Ravi Theja](https://twitter.com/ravithejads).\n4. [Ravi Theja](https://twitter.com/ravithejads) [talk](https://twitter.com/ravithejads/status/1684768609111801856?s=20) at the [Speciale VC](https://twitter.com/specialeinvest?lang=en) GenAI meetup in Chennai on Beyond the Basics: Leveraging LlamaIndex from Concept to Production.\n5. Data Agents session at TPF X Nexus VC [Buildathon](https://twitter.com/TheProductfolks/status/1685167361060737024?s=20) by [Ravi Theja](https://twitter.com/ravithejads).\n\nDemos:\n======\n\n1. [Tali.AI](https://twitter.com/TryTaliAI) at the Augment hackathon dove into the future of support roles by developing an Autonomous Support Bot using LlamaIndex. [Tweet](https://twitter.com/TryTaliAI/status/1683960220702371845?s=20)\n2. [SuperAGI](https://superagi.com/) integrated with LlamaIndex which enables AI agents to process a wide variety of data from both structured and unstructured sources including Docx, PDF, CSV files, videos, and images. [Tweet](https://twitter.com/_superAGI/status/1679058876023603201?s=20)",
        "45219ce9-a9b1-4e5e-b8ed-e518d2ba9db9": "Hello LlamaIndex Enthusiasts \ud83e\udd99,\n\nGet ready for an exciting week at LlamaIndex, teeming with dynamic community contributions and insightful learning resources. Dive into our range of new features, tutorials, guides, and events, all designed to enhance your LlamaIndex journey.\n\nWe\u2019re excited to announce our [very first in\\-person hackathon](https://rag-a-thon.devpost.com/), scheduled for February 2nd\\-4th. Join us to connect with fellow RAG enthusiasts and compete for prizes totaling over $4,000!\n\nIf you\u2019ve been working on a fascinating project, penned an insightful article, or produced an engaging video, we\u2019re eager to see it! Share your contributions with us at [news@llamaindex.ai](mailto:news@llamaindex.ai). Don\u2019t forget to subscribe to our newsletter on our [website](https://www.llamaindex.ai/) to receive all the latest updates directly in your inbox.\n\n\ud83e\udd29 **The highlights:**\n\n1. **Chain\\-of\\-Table:** Step\\-by\\-step table reasoning and operations for enhanced LLM tabular data understanding. [LlamaPack](https://llamahub.ai/l/llama_packs-tables-chain_of_table?from=llama_packs), [Tweet](https://x.com/llama_index/status/1746217167706894467?s=20).\n2. **LLM Self\\-Consistency:** Merges textual and symbolic reasoning with majority voting for precise answers. [LlamaPack](https://t.co/pGcRG4ieD4), [Tweet](https://twitter.com/llama_index/status/1746937012798800272?s=20).\n3. **Semantic Text Splitting in RAG:** Greg Kamradt\u2019s embedding similarity method for efficient document splitting. [LlamaPack](https://llamahub.ai/l/llama_packs-node_parser-semantic_chunking?from=all), [Tweet](https://x.com/llama_index/status/1745482959237615847?s=20).\n4. **Parallel RAG Ingestion:** Up to 15x faster document processing in LlamaIndex. [Notebook](https://github.com/run-llama/llama_index/blob/main/docs/examples/ingestion/parallel_execution_ingestion_pipeline.ipynb), [Tweet](https://x.com/llama_index/status/1745849571614539984?s=20).\n5. **TogetherAI\u2019s Embeddings Support:** Guide to build retrieval\\-augmented apps with MistralAI\u2019s 8x7b model and TogetherAI Embeddings. [Blogpost](https://www.together.ai/blog/rag-tutorial-llamaindex) , [Tweet](https://x.com/llama_index/status/1745551739368222815?s=20).\n\n**\u2728 Feature Releases and Enhancements:**\n\n* We launched Chain\\-of\\-Table Framework in LlamaPack for LLM Tabular Data Understanding. This approach enables step\\-by\\-step table reasoning and operations like adding columns, row selection, grouping, and sorting, mimicking a data scientist\u2019s method for concise data representation. [LlamaPack](https://llamahub.ai/l/llama_packs-tables-chain_of_table?from=llama_packs), [Tweet](https://x.com/llama_index/status/1746217167706894467?s=20).\n* We launched LLM Self\\-Consistency Mechanism for Tabular Data in LlamaPack. This method combines textual and symbolic reasoning, utilizing a novel mix self\\-consistency approach with majority voting to select the best answer. [LlamaPack](https://t.co/pGcRG4ieD4), [Tweet](https://twitter.com/llama_index/status/1746937012798800272?s=20).\n* We have Introduced Semantic Text Splitting in RAG with LlamaPack. Check Greg Kamradt\u2019s method of splitting documents based on embedding similarity between sentences. This auto\\-tuned threshold approach enhances RAG pipelines, soon to be available in LlamaPack using LlamaIndex abstractions.",
        "f961ebcd-475a-4749-8bdf-f4ad9d3b910d": "![](/blog/images/1*mu7MJbhPe18TmE1lzDHZQg.png)**Figure 2:** typical flow of document processing in LlamaIndex for a VectorStoreIndex\n\nWith Vectara (figure 3\\), this whole flow is replaced by a single \u201cindexing\u201d API call , and all this processing is instead performed in the backend by the Vectara platform.\n\n![](/blog/images/1*H7WWZV-L0csNVp2DgMjbSw.png)**Figure 3:** pre\\-processing with the VectaraIndex simplifies the complex ingest flow to a single step.\n\nHow does the VectaraIndex work?\n===============================\n\nLet\u2019s take a look at a simple question\\-answering example using VectaraIndex, in this case asking questions from one of Paul Graham\u2019s Essays.\n\n**Step 1: Setup your Vectara account and Index**\n\nTo get started, follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide: [signup](https://console.vectara.com/signup) for a free Vectara account, create a corpus (index), and generate your API key.\n\nThen setup your Vectara customer\\_id, corpus\\_id and api\\_key as environment variables, so that the VectaraIndex can access those easily, for example:\n\n\n```\nVECTARA_CUSTOMER_ID=<YOUR_CUSTOMER_ID>\nVECTARA_CORPUS_ID=<YOUR_CORPUS_ID>\nVECTARA_API_KEY=\"zwt_RbZfGT\u2026\"\n```\n**Step 2: Create a VectaraIndex instance with LlamaIndex**\n\nBuilding the Vectara Index is extremely simple:\n\n\n```\nfrom llama_index import SimpleDirectoryReader\nfrom llama_index.indices import VectaraIndex\nFrom pprint Import pprint\n\ndocuments = SimpleDirectoryReader(\"paul_graham\").load_data()\nindex = VectaraIndex.from_documents(documents)\n```\nHere we load Paul Graham\u2019s Essay using LlamaIndex\u2019s SimpleDirectoryReader into a single document. The from\\_documents() constructor is then used to generate the VectaraIndex instance.\n\nUnlike the common flow that uses LlamaIndex tools like data connectors, parsers and embedding models to process the input data, with VectaraIndex the documents are sent directly to Vectara via the [Indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing). Vectara\u2019s platform then processes, chunks, encodes and stores the text and embeddings into a Vectara corpus, making it available instantly for querying.\n\n**Step 3: Query**\n\nAfter the data is fully ingested, you can take advantage of the rich set of query constructs built into LlamaIndex. For example let\u2019s use the index to retrieve the top\\-k most relevant nodes:\n\n\n```\nretriever = index.as_retriever(similarity_top_k=7)\n# docs should contain the 7 most relevant documents for the query\ndocs = retriever.retrieve(\u201cWhat is the IBM 1401?\u201d)\npprint(docs[0].node.text)\n```\n\n> (\u2018My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \u201cdata processing.\u201d This was in 9th grade, so I was 13 or 14\\. The school district\u2019s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it.\u2019)\n\nHere we printed out the top matching Node given the query \u201cwhat is the IBM 1401?\u201d This in turn results in a call to Vectara\u2019s [Search API](https://docs.vectara.com/docs/api-reference/search-apis/search) that returns the top\\-k matching document segments.\n\nThose are transformed into NodeWithScore objects and thus can be used as usual with the rest of the LlamaIndex querying tools. For example we can use LlamaIndex\u2019s query\\_engine() to convert the retrieved matching document segments (nodes) into a comprehensive response to our question:",
        "90edfd44-f4b6-4311-aa57-b3c92962b798": "It\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe you\u2019ve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user\u2019s queries and data structures.\n\nIn either case, knowing where or how to begin may be a challenge in and of itself! If that\u2019s true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems.\n\nThe RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper ([\u201cRetrieval\\-Augmented Generation for Large Language Models: A Survey\u201d Gao, Yunfan, et al. 2023](https://arxiv.org/pdf/2312.10997.pdf)).\n\nBasic RAG\n=========\n\nMainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user\u2019s query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component.\n\n**LlamaIndex Basic RAG Recipe:**\n\n\n```\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex\n\n# load data\ndocuments = SimpleDirectoryReader(input_dir=\"...\").load_data()\n\n# build VectorStoreIndex that takes care of chunking documents\n# and encoding chunks to embeddings for future retrieval\nindex = VectorStoreIndex.from_documents(documents=documents)\n\n# The QueryEngine class is equipped with the generator\n# and facilitates the retrieval and generation steps\nquery_engine = index.as_query_engine()\n\n# Use your Default RAG\nresponse = query_engine.query(\"A user's query\")\n```\nSuccess Requirements for RAG\n============================\n\nIn order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements:\n\n1. Retrieval must be able to find the most relevant documents to a user query.\n2. Generation must be able to make good use of the retrieved documents to sufficiently answer the user query.\n\nAdvanced RAG\n============\n\nWith the success requirements defined, we can then say that building advanced RAG is really about the application of more sophisticated techniques and strategies (to the Retrieval or Generation components) to ensure that they are ultimately met. Furthermore, we can categorize a sophisticated technique as either one that addresses one of the two high\\-level success requirements independent (more or less) of the other, or one that addresses both of these requirements simultaneously.\n\nAdvanced techniques for Retrieval must be able to find the most relevant documents to a user query\n==================================================================================================\n\nBelow we briefly describe a couple of the more sophisticated techniques to help achieve the first success requirement.\n\n1. **Chunk\\-Size Optimization:** Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses.\n\n**LlamaIndex Chunk Size Optimization Recipe** ([notebook guide](https://github.com/run-llama/llama_index/blob/main/docs/examples/param_optimizer/param_optimizer.ipynb))**:**",
        "83a69a9d-ea17-4793-a84d-6d61e934a2b7": "***Action: tool\\_latest\\_index Action Input: {\u2018input\u2019: \u2018LlamaIndex\n connectors\u2019}***\n\n\n\nAdded user message to memory: LlamaIndex connectors\n\n\n\n \\=\\=\\= Calling Function \\=\\=\\=   \nCalling function: vector\\_tool\\_latest\\_index with\n args: {   \n\u201cinput\u201d: \u201cLlamaIndex connectors\u201d   \n}\n\n\n\n\n Got output: LlamaIndex provides data connectors that allow you to ingest your\n existing data from various sources and formats. These connectors can work with\n APIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\n bring your data into the LlamaIndex ecosystem and make it accessible for\n natural language access and retrieval. \\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\n\n\n\n\n***Observation: LlamaIndex offers data connectors that enable you to import\n your existing data from a variety of sources and formats. These connectors\n are compatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex\n connectors allows you to seamlessly integrate your data into the\n LlamaIndex ecosystem, making it readily available for natural language\n access and retrieval.***\n\n\n\n\n***Thought: I can answer without using any more tools.***\n\n\n\n\n***Answer: LlamaIndex connectors are used to import existing data from\n various sources and formats into the LlamaIndex ecosystem. These\n connectors are compatible with APIs, PDFs, SQL, and more, allowing\n seamless integration of data for natural language access and\n retrieval.***\n\n\n\nBelow is a snipped from the Colab notebook:\n\n\n\n![](/blog/images/1*JgnwszI2SejyB9CpQMI_iQ.png)\n\n\n\n[Source](https://github.com/cobusgreyling/LlamaIndex/blob/d8902482a247c76c7902ded143a875d5580f072a/Agentic_RAG_Multi_Document_Agents-v1.ipynb)\n\n\n\n\n\n This complex implementation from LlamaIndex is an example of multi\\-document\n agents which can:\n\n\n\n1. Select documents relevant to a user query\n2. Execute an agentic loop over the documents relevant to the query; including\n chain\\-of\\-thought, summarisation and reranking.\n\n\nIn Conclusion\n=============\n\n\nThis implementation by LlamaIndex illustrates a few key principles\u2026\n\n\n1. Agentic RAG, where an agent approach is followed for a RAG implementation\n adds resilience and intelligence to the RAG implementation.\n2. It is a good illustration of multi\\-agent orchestration.\n3. This architecture serves as a good reference framework of how scaling an\n agent can be optimised with a second tier of smaller worker\\-agents.\n4. Agentic RAG is an example of a controlled and well defined\n ***autonomous***\n***agent***\n implementation.\n5. One of the most sought\\-after enterprise LLM implementation types are RAG,\n Agentic RAG is a natural progression of this.\n6. It is easy to envisage how this architecture can grow and expand over an\n organisation with more sub bots being added.\n\n\n\n***\u2b50\ufe0f Follow me on*** [***LinkedIn***](https://www.linkedin.com/in/cobusgreyling/) ***for updates on Large Language Models \u2b50\ufe0f***",
        "fd489c96-b83d-4ecf-ba99-20cf17942f63": "\ud83c\udfa5 **Webinars:**\n\nWebinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open\\-source multi\\-modal models of LLaVa, which are competitive with GPT\\-4V, and a presentation on multi\\-modal use cases with LLaVa \\+ LlamaIndex by Haotian Zhang from the LlamaIndex team.\n\n**\ud83c\udfe2 Calling all enterprises:**\n\nAre you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise\\-ready and have sneak peeks at our upcoming products available for partners. Interested? [Get in touch.](https://docs.google.com/forms/d/e/1FAIpQLScBNdM2a_fn8UZOKmFQt6lBsrd1o6FflvsdPH-Pn3JkdlN_Rg/viewform)",
        "ab589ecc-b735-4d3d-82f4-439985165550": "1\\. Extraction\n\nImagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones.",
        "693561c5-264b-42fd-81bf-91fbcb456501": "Summary: Download and run a llamafile\n\n\n```\n# 1. Download the llamafile-ized model\nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n\n# 2. Make it executable (you only need to do this once)\nchmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n# 3. Run in server mode\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n```\nBuild a research assistant using LlamaIndex and llamafile\n---------------------------------------------------------\n\nNow, we'll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest\\-\\-for this post, we chose [homing pigeons](https://en.wikipedia.org/wiki/Homing_pigeon). We'll show how to prepare your data, index into a vector store, then query it.\n\nOne of the nice things about running an LLM locally is privacy. You can mix both \"public data\" like Wikipedia pages and \"private data\" without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won't have to worry about leaking data. Below, we'll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic.\n\nTo get started, download our example data:\n\n\n```\nmkdir data\n\n# Download 'The Homing Pigeon' manual from Project Gutenberg\nwget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n\n# Download some notes on homing pigeons\nwget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md\n```\nNext, we'll need to install LlamaIndex and a few of its integrations:\n\n\n```\n# Install llama-index\npip install llama-index-core\n# Install llamafile integrations and SimpleWebPageReader\npip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web\n```",
        "b414c7f5-88a6-4885-af67-057110d26969": "[Docs\\_AOI](https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/AzureOpenAI.html), [Notebook\\_personality](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/chat_engine/chat_engine_personality.ipynb), [Tweet](https://twitter.com/llama_index/status/1676981157513265153?s=20)\n14. LlamaIndex leverages LLM\u2019s to automatically extract metadata, significantly enhancing the relevance and precision of information retrieval. This is achieved through five key MetadataExtractor modules (SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, MetadataFeatureExtractor) that augment text with rich, context\\-specific details.\n[Docs](https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MetadataExtractionSEC.html), [Tweet](https://twitter.com/jerryjliu0/status/1677706208017518593?s=20)\n\n**Tutorials:**\n--------------\n\n1. [Anyscale tutorial](https://www.youtube.com/watch?v=Vd_8lS1iDBg) on \u201cHow to Build an LLM Query Engine in 10 Minutes using LlamaIndex.\u201d\n2. [Erika Cardenas tutorial](https://www.youtube.com/watch?v=Bu9skgCrJY8) on how to load data into Weaviate and how to connect LlamaIndex to a Weaviate instance using LlamaIndex.\n3. [Wenqi Glantz tutorial](https://betterprogramming.pub/refreshing-private-data-sources-with-llamaindex-document-management-1d1f1529f5eb) on Refreshing Private Data Sources with LlamaIndex Document Management.\n4. [Michael Hunger tutorial](https://medium.com/llamaindex-blog/enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7) onLoad in data from [neo4j](https://twitter.com/neo4j), [NebulaGraph](https://twitter.com/NebulaGraph), and index/query with LlamaIndex using GraphDB Cypher and GraphQL data loaders.\n5. [Pradip Nichite video tutorial](https://www.youtube.com/watch?v=XGBQ_f-Yy48) and [blogpost](https://blog.futuresmart.ai/mastering-llamaindex-create-save-load-indexes-customize-llms-prompts-embeddings) on Mastering LlamaIndex: Create, Save \\& Load Indexes, Customize LLMs, Prompts \\& Embeddings.\n\nWebinars And Podcasts:\n----------------------\n\n1. [Webinar](https://www.youtube.com/watch?v=bPoNCkjDmco) on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph).\n2. [Webinar](https://www.youtube.com/watch?v=gbyfXRxU0Gw) with Albus \u2014 a comprehensive Slackbot for enterprise search, [xpress.ai](http://Xpress.ai) \u2014 a low\\-code solution for building LLM workflows \\+ agents and [ImmigrantFirst.ai](https://t.co/QAJyGqZPcB) \u2014 assistant to help immigrants complete their EB\\-1A/O1 apps more efficiently.\n3. [Data Exchange Podcast](https://www.youtube.com/watch?v=NAoqOJrE8rQ&list=PLnTmH22EvTFTtWJRPTNzosDIDblnSg0PD&t=1s) with Ben Lorica on LlamaIndex\n\nEvents:\n-------\n\nRavi Theja gave talks on \u201cLlamaIndex: Basics To Production\u201d at Accel Partners and Together VC Fund in India.",
        "e12a2a4f-1e55-4d9f-803b-59cc36644628": "```\ndef generate_questions(user_query: str) -&gt; List[str]:\n  system_message = '''\n  You are given with Postgres table with the following columns.\n\n  city_name, population, country, reviews.\n\n  Your task is to decompose the given question into the following two questions.\n\n  1. Question in natural language that needs to be asked to retrieve results from the table.\n  2. Question that needs to be asked on the top of the result from the first question to provide the final answer.\n\n  Example:\n\n  Input:\n  How is the culture of countries whose population is more than 5000000\n\n  Output:\n  1. Get the reviews of countries whose population is more than 5000000\n  2. Provide the culture of countries\n  '''\n\n  messages = [\n      ChatMessage(role=\"system\", content=system_message),\n      ChatMessage(role=\"user\", content=user_query),\n  ]\n  generated_questions = llm.chat(messages).message.content.split('\\n')\n\n  return generated_questions\n\nuser_query = \"Get the summary of reviews of Iphone13\"\n\ntext_to_sql_query, rag_query = generate_questions(user_query)\n```\nData Retrieval \u2014 Executing the Primary Query\n============================================\n\nWhen we decompose a user\u2019s question into its constituent parts, the first step is to convert the \u201cDatabase Query in Natural Language\u201d into an actual SQL query that can be run against our database. In this section, we\u2019ll use the LlamaIndex\u2019s `NLSQLTableQueryEngine` to handle the conversion and execution of this SQL query.\n\n**Setting up the NLSQLTableQueryEngine:**\n\nThe `NLSQLTableQueryEngine` is a powerful tool that takes natural language queries and converts them into SQL queries. We initiate this by providing the necessary details:\n\n* `sql_database`: This represents our SQL database connection details.\n* `tables`: We specify which table(s) our query will be run against. In this scenario, we're targeting the `product_reviews` table.\n* `synthesize_response`: When set to `False`, this ensures we receive raw SQL responses without additional synthesis.\n* `service_context`: This is an optional parameter, which could be used to provide service\\-specific settings or plugins.\n\n\n```\nsql_query_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"product_reviews\"],\n    synthesize_response=False,\n    service_context=service_context\n)\n```\n**Executing the natural language Query:**\n\nAfter setting up the engine, the next step is executing our natural language query against it. The engine\u2019s `query()` method is used for this purpose.\n\n\n```\nsql_response = sql_query_engine.query(text_to_sql_query)\n```\n**Processing the SQL Response:**\n\nThe result of our SQL query is usually a list of rows (with each row represented as a list of reviews). To make it more readable and usable for the third step of processing summarizing reviews, we convert this result into a single string.\n\n\n```\nsql_response_list = ast.literal_eval(sql_response.response)\ntext = [' '.join(t) for t in sql_response_list]\ntext = ' '.join(text)\n```\nYou can check the generated SQL query in `sql_response.metadata[\"sql_query\"].`\n\nBy following this process, we\u2019re able to seamlessly integrate natural language processing with SQL query execution. Let\u2019s go with the last step in this process for getting a summary of the reviews.\n\nRefining and Interpreting the reviews with ListIndex:\n=====================================================\n\nAfter obtaining the primary set of results from the SQL query, there are often situations where further refinement or interpretation is required. This is where `ListIndex` from LlamaIndex plays a crucial role. It allows us to execute the secondary question on our obtained text data to get a refined answer.\n\n\n```\nlistindex = ListIndex([Document(text=text)])\nlist_query_engine = listindex.as_query_engine()\n\nresponse = list_query_engine.query(rag_query)\n\nprint(response.response)\n```\nNow let\u2019s wrap everything under a function and try out a few interesting examples:"
    },
    "relevant_docs": {
        "4bd4a58b-eff2-442d-b0b9-6b0c0b60bb9e": [
            "8e1c05ab-ea2c-44c7-b8eb-7e60c2fc73d1"
        ],
        "8b5ff9c1-fa4e-4bbf-aecd-c1a5804d597e": [
            "80efa253-8be1-409a-b576-a0ce6197e4eb"
        ],
        "e08dd455-4ab1-4d58-b365-0d7f9cd67c56": [
            "64e80c06-774a-4f9e-bc2b-792462024824"
        ],
        "94a717cd-877d-4abf-9180-b23b597b19d9": [
            "8267d662-6c10-4f11-a901-033a4cb96c7f"
        ],
        "57f14311-55e0-4367-aece-62a4a91d1627": [
            "da5f5482-180f-414c-9708-388eabbe48d4"
        ],
        "ac5194ba-01c9-4491-9dfb-e8b58967bbe6": [
            "8f1b4319-f725-4ee6-a043-4a2179cd6584"
        ],
        "9943db79-349f-475e-a594-18ff93483229": [
            "5369c749-7dbf-47a7-b77c-58b1525dfb12"
        ],
        "fa90f859-0fa4-4f76-8c0a-b8bd7bc2c3e6": [
            "12f81353-deb9-4d8f-8e57-a29e458bea6f"
        ],
        "c78fe20d-181a-4578-a698-df8ebb2f9eb9": [
            "1c634647-7d6f-4a45-8fd1-95a51f97ccd0"
        ],
        "17fcfb4e-365b-4704-8129-bee59a2fe65d": [
            "0e0c1592-0d7d-4cc9-ac0a-d83a0174294d"
        ],
        "777638bc-05fe-4a76-b4ad-378756934e0d": [
            "24c49aae-48ee-4fcb-a0fd-0286689628fe"
        ],
        "5d96263c-2a55-44f3-935c-3492bf5a8b9e": [
            "276e9a9d-50e9-47c5-8591-6f1ad72ba996"
        ],
        "9b327dab-7d37-47f2-bc02-c6cfa95b6127": [
            "0a5b1a68-40ac-4cb6-8437-d1921e92d4df"
        ],
        "5822b3b1-5ab3-4e84-b577-e27ca20a780e": [
            "7b98eb30-6bbc-4eae-9f64-90359534a687"
        ],
        "5c2c8e68-5e43-475e-bbcf-5dc834d18307": [
            "5c1a8087-7108-45d1-adb3-a23ec33cc5ef"
        ],
        "76eb6ff9-a484-4239-acac-f311403e5c4d": [
            "03335930-50a7-4008-975f-90cce453fc3b"
        ],
        "f7d320fa-ea1a-4164-a5ad-7224e783eb7a": [
            "81f909a2-0c7b-4f11-b4d3-88e292fb1466"
        ],
        "3d26082b-d82d-49ca-a71d-96fee2959a57": [
            "d2bb959e-2b94-4db4-ad82-324c9aa080f1"
        ],
        "91e3303f-9354-451f-bd98-70049fc2a158": [
            "ce0e4e82-5307-4f35-b146-7e78c1f84d6e"
        ],
        "d7999cb0-c5cf-4304-b443-80fdccae78f6": [
            "5524dc7d-00a0-4e5f-a1f4-abd1c25bd434"
        ],
        "f04c7b56-1771-4bb8-a623-0e9f7580e2b0": [
            "22256500-ee96-4691-8f9d-ffedf642af07"
        ],
        "86872f28-51c7-44f3-9a39-acaa36cf77fd": [
            "171b0fff-daa5-4a9f-9511-7e46f93b7d4d"
        ],
        "104ab255-146d-4b28-9d98-e57405458e55": [
            "e614427b-66a2-4cef-995e-ae8169335e0d"
        ],
        "60a77268-68ff-42d7-a8ed-f089e14e19ba": [
            "12987fcc-6e16-4347-87b0-a50a62bdf97b"
        ],
        "eff68700-74fa-435d-87bf-8ce234fecdb3": [
            "0a606338-63ce-4ecf-a5f0-4caf97f62f3d"
        ],
        "7cd9fa97-4cc0-4058-b0f6-e34714bf2f08": [
            "a14fe588-26e3-4ef0-b410-26861d7224df"
        ],
        "80e25c83-e23c-4ae5-95d4-d7fd553a8d8a": [
            "b1dd1519-3d94-429d-8787-56fbb921040c"
        ],
        "0fa38b4b-37b5-42cf-a67a-bbd44db8290e": [
            "27c77623-ea2b-4036-9969-bef7dea561c9"
        ],
        "6b302f47-e9a7-4710-b2c1-206210b3ed50": [
            "5b1de1a3-9756-4c7b-ad69-318918fa493a"
        ],
        "f0a5ed7b-c7f4-439a-b04f-ddcb2a3e50b2": [
            "2b7b210b-8006-4f4c-8cb3-184f45950295"
        ],
        "84c35a1b-980a-49a1-8cd7-76f0b80f904d": [
            "3bd04756-2ec7-4806-8cc1-2a4ff8c7af0e"
        ],
        "bcea88d5-7ac3-425a-821c-07aea857d09b": [
            "12f81353-deb9-4d8f-8e57-a29e458bea6f"
        ],
        "7ad50fec-3974-48cd-abf3-e5077d094957": [
            "1b24ad1e-a7af-4544-a6cb-4f4c57f48a30"
        ],
        "5de3ddff-0b14-4a7d-acb0-d7be738ca849": [
            "d8f36680-df79-4869-8376-7423f67bae3e"
        ],
        "d8ea6c4c-da1f-4cae-baa8-25c2604f755d": [
            "1fcfb0dd-6082-4eca-9d9b-a9ba79682c0c"
        ],
        "a2350d6a-a006-432d-b73e-542d2e7bb8fc": [
            "c0bc7f63-a9c2-417c-8398-372a0afedbc0"
        ],
        "c135f1d6-d497-4e47-9bf7-ce264b962c35": [
            "46d110ae-ccac-47ca-a90d-e250cb1fd1fd"
        ],
        "5a82424f-c02b-476a-ae19-ae6b706182df": [
            "9fc3c729-cef5-4be8-a63f-95034ca4ca9b"
        ],
        "59fb5241-f066-4119-917c-19b1cede18a0": [
            "983d9c71-0370-496f-8951-eabd7e3c7fd9"
        ],
        "2d3e469e-0d94-4494-aa74-43a9e705f64c": [
            "54ac02cd-8612-4ba2-8729-f26e285d8703"
        ],
        "862add0e-847c-4fb6-8dd7-b6e03af74326": [
            "10647862-eacd-4a78-a72f-a86a8007add5"
        ],
        "cc3e9d8d-f114-46ae-bc0c-848718151c7d": [
            "d507d681-f22e-4313-8fea-f1551a242792"
        ],
        "19b12786-b35b-4fd6-a64a-724ca4a32cc9": [
            "b86d28ee-9a93-472f-90fa-71f83b88cbcd"
        ],
        "d312110c-4c7a-4a54-9ba7-66383cc62806": [
            "1e30b299-06d9-4dc7-aa60-1b15a5be8e95"
        ],
        "a266e0e2-7e0a-4dbe-8fc0-decf23f29622": [
            "2b32c68c-54a2-45fc-9a45-a6e704831231"
        ],
        "81cc8880-4162-4751-985e-d4d6abb6a5ce": [
            "6974f17f-17ab-46a7-9adf-c98647097290"
        ],
        "eaa1d1a1-b09e-40a0-9113-1444f6d4801a": [
            "703d5ca5-9ff7-4cd3-b9b7-c6041ebaeca5"
        ],
        "62cc5888-d4d5-437c-a236-43c2b3be2700": [
            "455d461f-1483-47fb-beb7-fd6dc71ee088"
        ],
        "8e083de4-fb44-4aea-b3be-7e2af35682d5": [
            "da5f5482-180f-414c-9708-388eabbe48d4"
        ],
        "dd7d895f-737d-485f-8037-910ea121bab0": [
            "bf6a6a35-835e-490f-be3f-e1b0519a1134"
        ],
        "2171ec17-9dc0-41c0-b134-81ca4b2fd033": [
            "9a87b6e8-0e62-45de-9d33-f1c285736b37"
        ],
        "0e488651-a4f4-487a-a0dc-3a2bb7375782": [
            "3ab53472-53ad-4a2c-9975-25e84fa18232"
        ],
        "7e1cb00d-7fc5-4a40-bc39-94f83af28f64": [
            "6edacfe6-bfc0-4681-bd43-ccfc02fbf382"
        ],
        "96d6cda2-7d63-41f3-bf32-38f4dbdbbc7b": [
            "0dc46c37-b8b6-49b5-887a-50d691b65464"
        ],
        "d6b63a3d-a5fe-41ba-a737-4d56068645f0": [
            "a75f7edb-b10b-46df-a0f3-35ae473794b7"
        ],
        "ecbd28af-c60e-412f-acde-366d397eb240": [
            "e9e7f1e0-1ed4-4e71-8826-b0e4965b456f"
        ],
        "a9f94d73-a8ec-40a6-9722-fa26b02c2490": [
            "2fcdee48-fd5d-430b-901d-c14da93feefa"
        ],
        "eaee1bb6-ee82-48be-b06b-1a9641bb48d1": [
            "f30e3c24-ced3-409e-a446-f3e71cec33a7"
        ],
        "0dc35933-8b54-48d7-9d55-34794d0065b5": [
            "d2bb959e-2b94-4db4-ad82-324c9aa080f1"
        ],
        "23dfcc9c-72ac-4ddf-be47-920138e59a00": [
            "d25ab7ac-8d24-4f4f-b968-2caf75219d1a"
        ],
        "dacf0a21-f279-44bf-aa4a-a1bde8eaac5b": [
            "31296fde-83de-4b0e-8847-a164414056ce"
        ],
        "810cab1e-f08a-46a8-9a84-31eb77bc9be8": [
            "81f909a2-0c7b-4f11-b4d3-88e292fb1466"
        ],
        "5cfc28db-573b-4b94-bb0c-d468d1891533": [
            "912633cb-47a0-4f0b-b22c-ed7481a2836c"
        ],
        "c1023d7c-b258-43b1-850e-140016ee3d00": [
            "d25ab7ac-8d24-4f4f-b968-2caf75219d1a"
        ],
        "f7ce5589-eff1-480b-ae00-d93ba2256165": [
            "03335930-50a7-4008-975f-90cce453fc3b"
        ],
        "5f0a9f9d-7638-48be-989e-a797d95c8523": [
            "2966c1bd-887b-4be8-9b81-3fd1a5717a70"
        ],
        "e956ac15-af8e-4f2f-823b-4f4dcf17ddc4": [
            "1d2322c5-ae04-4a48-bdf2-2344fc2108a2"
        ],
        "8013b35f-6b87-456d-8e56-34e8e8f96b5c": [
            "efb24751-06b2-4bf4-be0d-1adde9f1f1e4"
        ],
        "6f719b48-9bb2-42b0-b3c0-2a38e6a89cd7": [
            "5b1de1a3-9756-4c7b-ad69-318918fa493a"
        ],
        "2db52693-bcbb-4889-8a41-31632cbefbd4": [
            "1c5261db-94c3-4f3c-82b6-b9044520eb61"
        ],
        "1366bae5-879d-4d8a-bee8-703269b838c2": [
            "2d95ba59-39f9-4d38-9ccd-876aa59e4a77"
        ],
        "220591f1-de35-40f1-96bf-f076bcb2ed8b": [
            "e49d29d6-c1df-4e86-b572-e45380ad35be"
        ],
        "98183bc4-6907-4fee-9885-487fb6b087b2": [
            "3893a73c-a22f-41b8-b9d2-8002eb3362b2"
        ],
        "62f286c1-07fc-4779-9e31-1c9740d6b76c": [
            "2c4b09b1-4fb6-48a8-9c4e-e00968b7cd55"
        ],
        "ea9a87ca-5107-4ca2-9307-e117dbabde72": [
            "15465679-068c-4e2b-82da-5d6e86ee7c24"
        ],
        "875e9ad8-f241-44f5-a7c8-87893012110a": [
            "66605346-838e-4735-90d9-946a9e70a1f0"
        ],
        "5f729726-df9c-4480-9d3e-a33887f0c5e0": [
            "ccaa3fb4-59f5-4910-8fbf-a1a04d7a27e2"
        ],
        "c04069b6-4464-42e2-8f63-d5d5c1f7ce7a": [
            "bf1e1fab-1673-444b-9809-38b09391ba66"
        ],
        "d9dca60e-15d2-4895-b397-80fff430962b": [
            "8587b85c-5b77-41ff-816e-0fc8ee93e3a3"
        ],
        "1ad1c46c-9e6a-4a91-8ff6-5c0ed56e2d90": [
            "2b23a55a-7db2-43cd-860d-b6195d97350c"
        ],
        "a1826592-a23d-43cc-8ae1-5cc70c9c27dc": [
            "a168398f-72c6-429c-b9b8-78731da2fbb9"
        ],
        "59b3ea63-64f0-46ef-8b04-f29474598015": [
            "50bcd1b7-947b-4498-9d0c-5f9bb48ea8aa"
        ],
        "3f989140-ed0a-497c-ad10-c96802cc1d5e": [
            "ca715b43-da3e-484d-b819-df0b16a8b440"
        ],
        "e4347893-5355-40c6-93d8-8316944dedc4": [
            "0ff2bf08-7cfa-49f2-afa2-039cc71d9431"
        ],
        "c1550823-1882-4490-89ed-7eb8170840c6": [
            "2f647bb0-cb5a-47d5-a09e-55d9948b024c"
        ],
        "e755e31a-08d9-44b5-9234-2bc852cfabbc": [
            "6629f11d-6e58-4e54-a8fb-1d2ffc0752c5"
        ],
        "7f28f416-d6f2-4e57-b49b-77d3ed0adf3d": [
            "427e3b59-a42a-4b89-95dc-260e7abd7ec4"
        ],
        "56d53e48-c23c-4e2e-afbf-ccd44e7950a4": [
            "cd112d30-48fe-4111-a5e8-f668d979b526"
        ],
        "4b987376-4e62-4841-925c-2313d653ef05": [
            "bfcffed5-da51-4e13-9c38-808d9a52ca60"
        ],
        "08e59e6d-e379-417c-9821-95627cbd55f6": [
            "45219ce9-a9b1-4e5e-b8ed-e518d2ba9db9"
        ],
        "f0a20c8d-69cd-463b-a674-fa14d7ffa0d1": [
            "f961ebcd-475a-4749-8bdf-f4ad9d3b910d"
        ],
        "e51d528f-189d-4c61-abfc-4b10252a7828": [
            "90edfd44-f4b6-4311-aa57-b3c92962b798"
        ],
        "6d25e3a7-f564-4fef-919a-d4106a329e91": [
            "9fc3c729-cef5-4be8-a63f-95034ca4ca9b"
        ],
        "f4bfab0c-18bc-47f1-b590-af02fed742fb": [
            "427e3b59-a42a-4b89-95dc-260e7abd7ec4"
        ],
        "82ffd23b-5e5f-4bbc-a36c-7c18bbd863ac": [
            "83a69a9d-ea17-4793-a84d-6d61e934a2b7"
        ],
        "03dad514-4d1a-43f3-8334-5b6eec978fbe": [
            "fd489c96-b83d-4ecf-ba99-20cf17942f63"
        ],
        "300eab46-6ba2-4f13-a7ef-650e2739f8da": [
            "ab589ecc-b735-4d3d-82f4-439985165550"
        ],
        "31e15687-6890-43e2-86ff-92168f7995c7": [
            "693561c5-264b-42fd-81bf-91fbcb456501"
        ],
        "4688c652-f222-47c4-ba47-1a87e0ce000e": [
            "b414c7f5-88a6-4885-af67-057110d26969"
        ],
        "ebf4e3a2-7b8b-4489-a2b7-fa7326a351c7": [
            "e12a2a4f-1e55-4d9f-803b-59cc36644628"
        ]
    },
    "mode": "text"
}