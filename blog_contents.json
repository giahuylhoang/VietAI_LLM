{
    "blog_posts": [
        {
            "date": "Dec 20, 2023",
            "author": "LlamaIndex",
            "title": "Two new llama-datasets and a Gemini vs. GPT showdown",
            "tags": [
                "benchmarking",
                "gpt-4",
                "gemini",
                "llamaindex",
                "evaluation"
            ],
            "content": "(Authored by Andrei Fajardo at LlamaIndex)The llama-dataset collection. Each labelled llama-dataset is comprised of its associated labelled examples. With examples, we make predictions with the appropriate object depending on the task. After making predictions, we can evaluate the performance of the object by measuring some distance between predictions and the corresponding references.IntroA few weeks back, we launched our very first set of llama-datasets, namely the LabelledRagDataset. The main purpose of these llama-datasets is to provide builders with the means to benchmark their LLM systems in an effective and efficient manner. In the couple of weeks since that launch date, we\u2019ve amassed over a dozen LabelledRagDatasets via both staff and community contributions (all of which are available for download through LlamaHub)!The fun doesn\u2019t stop there though: today we\u2019re introducing two new llama-dataset types: LabelledEvaluatorDataset and the LabelledPairwiseEvaluatorDataset. These new llama-dataset types are meant for evaluating or benchmarking an LLM evaluator. Indeed, the adopted standard for evaluating LLM responses is to use a strong LLM as an evaluator. This approach is certainly more scalable, faster, and cheaper than using human evaluators via crowdsourcing. However, these LLM evaluators themselves must also be continuously evaluated rather than blindly trusted.In this post, we provide a brief overview of the new llama-datasets as well as provide some very interesting results from benchmarking Google\u2019s Gemini and OpenAI\u2019s GPT models as LLM evaluators on the MT-Bench datasets which we\u2019ve converted into the new llama-dataset types.A primer on the new llama-datasetsBefore getting into the new llama-datasets, recall that with LabelledRagDataset our end goal was to use it to evaluate or benchmark a Retrieval-Augmented Generation (RAG) system. The way to do that with our llama-dataset abstractions is to build a QueryEngine (i.e., a RAG system) and then use it to make \u201cpredictions\u201d over the LabelledRagDataset. With the predictions in hand, we can evaluate the quality of these predictions by comparing it to the corresponding reference attributes of the LabelledRagDataset.Benchmarking flow with LabelledRagDataset. With a query engine, predictions are made over every labelled example. We can then compare predicted responses and contexts with the reference versions (i.e., labels). This flow is conveniently handled via the RagEvaluatorPack.In a similar vein, the new llama-datasets are meant to benchmark an LLM evaluator. Let\u2019s go through the first kind, the LabelledEvaluatorDataset. Here, instead of the RAG system making predictions on a LabelledRagDataset we have an LLM evaluator making \u201cpredictions\u201d over a LabelledEvaluatorDataset \u2014 predictions in this context means that the LLM evaluator is evaluating the response produced by another LLM model to a given query. As before, with the predictions in hand, we can measure the goodness of the LLM evaluator\u2019s evaluations by comparing it to the corresponding reference attributes of the LabelledEvaluatorDataset.Benchmarking flow with LabelledEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of the answer to the query and optional contexts and ground truth answer. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.The second llama-dataset we\u2019re introducing today can be considered an extension of the first one. The LabelledPairwiseEvaluatorDataset is similarly used for benchmarking an LLM evaluator. However, there is a subtle difference in the evaluation task as here the LLM evaluator compares two generated answers from two separate LLMs. Outside of this difference, the flow for using this llama-dataset to benchmark an evaluator remains the same.Benchmarking flow with LabelledPairwiseEvaluatorDataset. With a supplied evaluator, predictions are made over every example. In this context, a prediction is an evaluation of two answers to the query and optional contexts and ground truth answer. That is, the LLM evaluator ranks the two answers to determine the superior one. With these predictions in hand, we can evaluate how good the evaluations are by comparing them to the reference feedbacks and scores. A llama-pack called EvaluatorBenchmarkerPack makes benchmarking a one-liner.Benchmarking Gemini and GPT models as LLM evaluators: Gemini achieves GPT-3.5 performance!In this section, we will put our new llama-dataset types to use in order to pit Gemini Pro against GPT models. For this, we\u2019re going to use slightly adapted versions of the MT-Bench dataset. These adapted versions have been made available for download and use through LlamaHub along with today\u2019s release!Mini MT-Bench Single Grading DatasetThis llama-dataset is a LabelledEvaluatorDataset and is a miniature version of the MT-Bench single-grading dataset. In particular, we consider all of the 160 original questions (i.e., 80 x 2, since MT Bench is a two-turn question dataset), but only the responses produced by Llama2-70b. For the reference evaluations, we use GPT-4. As with the original LabelledRagDataset, we\u2019ve produced a new llama-pack EvaluatorBenchmarkerPack (of course, also made available in today\u2019s release!) to make benchmarking an LLM evaluator on the new llama-datasets relatively effortless. The below snippet of code is how you can replicate the results of this benchmarkfrom llama_index.llama_dataset import download_llama_dataset\nfrom llama_index.llama_pack import download_llama_pack\nfrom llama_index.evaluation import CorrectnessEvaluator\nfrom llama_index.llms import Gemini\nfrom llama_index import ServiceContext# download dataset\nevaluator_dataset, _ = download_llama_dataset(\n    \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"\n)# define evaluator\ngemini_pro_context = ServiceContext.from_defaults(\n    llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n)\nevaluator = CorrectnessEvaluator(service_context=gemini_pro_context)# download EvaluatorBenchmarkerPack and define the benchmarker\nEvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\nevaluator_benchmarker = EvaluatorBenchmarkerPack(\n    evaluator=evaluators[\"gpt-3.5\"],\n    eval_dataset=evaluator_dataset,\n    show_progress=True,\n)# produce the benchmark result\nbenchmark_df = await evaluator_benchmarker.arun(\n\t\tbatch_size=5,\n\t\tsleep_time_in_seconds=0.5\n)Benchmark ResultsInvalid_predictions occurs whenever the LLM evaluator fails to produce the desired output structure and as well as other exceptions. Correlations represent the correlations with the scores produced by each of the evaluators with the reference scores produced by the reference evaluator GPT-4. Similarly, the remaining two metrics, MAE (i.e., mean absolute error, which is a sum of the absolute differences between each pair of evaluator and reference scores) and Hamming (i.e., which counts how many times evaluator and reference scores are equivalent), are computed with the scores produced by the evaluators and those from the reference evaluator.ObservationsIt seems that Gemini-Pro and GPT-3.5 are quite close in terms of their closeness to the reference evaluator GPT-4!As for GPT-4 versus the reference GPT-4, this is mostly used for assessing self-consistency of the LLM, for which we see it does a fairly good job at that.MT-Bench Human Judgement DatasetFor this benchmark, we\u2019ll evaluate the LLM evaluators on the task of ranking two LLM responses, to determine which of the two is the better one. And it is for this such task that LabelledPairwiseEvaluatorDataset exists. The llama-dataset that we\u2019ve curated here is a slightly adapted version of the original MT-Bench Human Judgement dataset. Specifically, in the original dataset, there are some replications with respect to the triple (query, model A, model B) examples since for some of these more than one human evaluation was provided. Since our prompt allows the LLM evaluator to deem a tie, and to our knowledge, this wasn\u2019t made an option for the human evaluators, we have aggregated the results across the different human evaluations to get the proportion of times model A wins versus model B for each triple (query, model A, model B). We then say that human evaluators deem a tie if the proportion lies between 0.4 and 0.6. It should be emphasized here that the reference evaluations are provided by humans, and so the benchmark metrics that we produce and share here represent the LLM agreement with humans.(We skip showing the code snipped to produce the results here, because they\u2019re essentially the same as the previously shared code snipper with the exception of requiring a PairwiseComparisonEvaluator instead of a CorrectnessEvaluator.)Benchmark ResultsInvalid_predictions are as defined in the previous benchmark. Inconclusive\u2019s represent the case when an LLM evaluator flips its winner after prompting it with the same evaluation task but instead flipping the order of presentation of the two answers (i.e. to mitigate against position bias). Two agreement rates, with the inclusion and exclusion of ties, are also produced \u2014 note that these are both conditional in the event that the prediction (or evaluation) is valid.ObservationsIn terms of agreement rates, all three models seem quite close. Note again that these are conditional on the prediction/evaluation being valid. And so, one should \u201cdiscount\u201d these with the invalid and inconclusive counts.Gemini Pro and GPT-3.5 seem to be a bit more assertive than GPT-4 resulting in only 50\u201360 ties to GPT-4\u2019s 100 ties.Overall, it seems that Gemini Pro is up to snuff with GPT models, and would say that it outperforms GPT-3.5!Go now and evaluate your evaluators (and eat your veggies)!It is, for obvious reasons, important to evaluate your LLM evaluators, as these are now being relied upon to evaluate the performance of our LLM systems \u2014 a broken compass is not really helpful! With these newly introduced llama-datasets, we hope that it is easy for you to compile your own benchmark datasets on your own data, and then even easier to produce your benchmark metrics. As mentioned before, the two llama-datasets discussed in this blog are available for download and use through LlamaHub. Be sure to visit and make use of the datasets there to build an exhaustive benchmark suite! (We welcome contributed llama-datasets as well!)Related LinksMT-Bench Human Judgement Benchmarking NotebookMT-Bench Single Grading Benchmarking Notebook",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot"
            ],
            "link": "https://www.llamaindex.ai/blog//two-new-llama-datasets-and-a-gemini-vs-gpt-showdown-9770302c91a5"
        },
        {
            "date": "Dec 15, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model",
            "tags": [
                "open-source",
                "llamaindex",
                "gpt-4",
                "llm",
                "openai"
            ],
            "content": "IntroductionEvaluation is a critical component in enhancing your Retrieval-Augmented Generation (RAG) pipeline, traditionally reliant on GPT-4. However, the open-source Prometheus model has recently emerged as a notable alternative for such evaluation tasks.In this blog post, we will demonstrate how to effectively use the Prometheus model for evaluation purposes, integrating it smoothly with the LlamaIndex framework by comparing it with GPT-4 evaluation. Our primary focus will be on assessing RAG using our standard metrics: Correctness, Faithfulness, and Context Relevancy. To provide a clearer understanding, here\u2019s what each metric entails:Correctness: Assesses whether the generated answer aligns with the reference answer, given the query (this necessitates labeled data).Faithfulness: Measures if the answer remains true to the retrieved contexts, essentially checking for the absence of hallucinations.Context Relevancy: Evaluate the relevance of both the retrieved context and the answer to the query.For an in-depth exploration, our documentation is available here.For those who are exploring the Prometheus model for the first time, the paper summary by Andrei is an excellent resource to gain a better understanding.A crucial aspect to remember when using the Prometheus model is its dependence on rubric scores within the prompt for effective evaluation. An example of such Rubric scores in the context of Correctness Evaluationis as follows:###Score Rubrics: \nScore 1: If the generated answer is not relevant to the user query and reference answer. \nScore 2: If the generated answer is according to reference answer but not relevant to user query. \nScore 3: If the generated answer is relevant to the user query and reference answer but contains mistakes. \nScore 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise. \nScore 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.You\u2019ll find comprehensive details on this in the prompts section of this tutorial.For a detailed walkthrough of the code, feel free to follow along with our Google Colab Notebook accompanying this blog post. In the notebook, we conducted evaluations on both the Paul Graham Essay Textand the Llama2 Paper. However, for this blog post, we\u2019ll focus exclusively on the Llama2 Paper, as it revealed some particularly interesting insights.Outline:Setup Evaluation Pipeline.Download Dataset.Define LLMs (Prometheus, GPT-4) needed for evaluation.Define Correctness, Faithfulness, and Relevancy prompt templates.Define Prometheus, GPT-4 Evaluators, and Batch Eval Runner.Run the Correctness, Faithfulness, and Relevancy Evaluation over the Llama2 dataset.2. ResultsCorrectness Evaluation score distribution between Prometheus and GPT-4.Feedback comparison between Prometheus and GPT-4 for correctness evaluation.Faithfulness and Relevancy Evaluation scores with Prometheus and GPT-4.Hamming Distance comparison between Prometheus and GPT-4.Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy3. Summary with Cost Analysis.Setup Evaluation PipelinePlease be aware that certain functions mentioned here are not defined in detail within the blog post. We have showcased only the essential parts of the pipeline to provide an overview of its setup. For a comprehensive code walkthrough, we recommend visiting our Google Colab Notebook.Download DatasetWe will use the Llama2 paper dataset from Llama Datasets which contains 100 questions and their reference answers.from llama_index.llama_dataset import download_llama_dataset\n\nllama2_rag_dataset, llama2_documents = download_llama_dataset(\n    \"Llama2PaperDataset\", \"./data/llama2\"\n)Define Prometheus LLM hosted on HuggingFace And OpenAI for creating an Index (RAG) pipelineWe need to host the model on HF Inference endpoint using Nvidia A100 GPU, 80 GB RAM.from llama_index.llms import HuggingFaceInferenceAPI\nimport os\n\nHF_TOKEN = \"YOUR HF TOKEN\"\nHF_ENDPOINT_URL = \"HF END POINT URL\"\n\nprometheus_llm = HuggingFaceInferenceAPI(\n    model_name=HF_ENDPOINT_URL,\n    token=HF_TOKEN,\n    temperature=0.1,\n    do_sample=True,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1,\n)\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n\nfrom llama_index.llms import OpenAI\n\ngpt4_llm = OpenAI(\"gpt-4\")Prompt templates.We will use the same prompts for the Prometheus model and GPT-4 to make consistent performance comparisons.Correctness Evaluation Prompt:prometheus_correctness_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n   1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n   2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric. \n   3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'\n   4. Please do not generate any other opening, closing, and explanations. \n   5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n\n   ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n   \n   ###Generate answer to evaluate: {generated_answer} \n\n   ###Reference Answer (Score 5): {reference_answer}\n            \n   ###Score Rubrics: \n   Score 1: If the generated answer is not relevant to the user query and reference answer.\n   Score 2: If the generated answer is according to reference answer but not relevant to user query.\n   Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n   Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n   Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n    \n   ###Feedback:\"\"\"\nFaithfulness Evaluation Prompt:prometheus_faithfulness_eval_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\u201d\n5. Please do not generate any other opening, closing, and explanations.\n\n###The instruction to evaluate: Your task is to evaluate if the given piece of information is supported by context.\n\n###Information: {query_str}\n\n###Context: {context_str}\n\n###Score Rubrics:\nScore YES: If the given piece of information is supported by context.\nScore NO: If the given piece of information is not supported by context\n\n###Feedback: \"\"\"\n\nprometheus_faithfulness_refine_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n1. You are provided with evaluation task with the help of information, context information and an existing answer.\n2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n5. Please do not generate any other opening, closing, and explanations.\n\n###The instruction to evaluate: If the information is present in the context and also provided with an existing answer.\n\n###Existing answer: {existing_answer}\n\n###Information: {query_str}\n\n###Context: {context_msg}\n\n###Score Rubrics:\nScore YES: If the existing answer is already YES or If the Information is present in the context.\nScore NO: If the existing answer is NO and If the Information is not present in the context.\n\n###Feedback: \"\"\"Relevancy Evaluation Prompt:prometheus_relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n       1. You are provided with evaluation task with the help of a query with response and context.\n       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n       4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\u201d \n       5. Please do not generate any other opening, closing, and explanations. \n\n        ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n\n        ###Query and Response: {query_str} \n\n        ###Context: {context_str}\n            \n        ###Score Rubrics: \n        Score YES: If the response for the query is in line with the context information provided.\n        Score NO: If the response for the query is not in line with the context information provided.\n    \n        ###Feedback: \"\"\"\n\nprometheus_relevancy_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n   1. You are provided with evaluation task with the help of a query with response and context and an existing answer.\n   2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n   3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n   4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n   5. Please do not generate any other opening, closing, and explanations. \n\n   ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n\n   ###Query and Response: {query_str} \n\n   ###Context: {context_str}\n            \n   ###Score Rubrics: \n   Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n   Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n    \n   ###Feedback: \"\"\"Define Correctness, FaithFulness, Relevancy Evaluatorsfrom llama_index import ServiceContext\nfrom llama_index.evaluation import (\n    CorrectnessEvaluator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator,\n)\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\nimport tiktoken\n\n# Provide Prometheus model in service_context\nprometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n\n# CorrectnessEvaluator with Prometheus model\nprometheus_correctness_evaluator = CorrectnessEvaluator(\n    service_context=prometheus_service_context,\n    parser_function=parser_function,\n    eval_template=prometheus_correctness_eval_prompt_template,\n)\n\n# FaithfulnessEvaluator with Prometheus model\nprometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n# RelevancyEvaluator with Prometheus model\nprometheus_relevancy_evaluator = RelevancyEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n# Set the encoding model to `gpt-4` for token counting.\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n\n# Provide GPT-4 model in service_context\ngpt4_service_context = ServiceContext.from_defaults(\n    llm=gpt4_llm, callback_manager=callback_manager\n)\n\n# CorrectnessEvaluator with GPT-4 model\ngpt4_correctness_evaluator = CorrectnessEvaluator(\n    service_context=gpt4_service_context,\n    # parser_function=parser_function,\n)\n\n# FaithfulnessEvaluator with GPT-4 model\ngpt4_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n# RelevancyEvaluator with GPT-4 model\ngpt4_relevancy_evaluator = RelevancyEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n# create a dictionary of evaluators\nprometheus_evaluators = {\n    \"correctness\": prometheus_correctness_evaluator,\n    \"faithfulness\": prometheus_faithfulness_evaluator,\n    \"relevancy\": prometheus_relevancy_evaluator,\n}\n\ngpt4_evaluators = {\n    \"correctness\": gpt4_correctness_evaluator,\n    \"faithfulness\": gpt4_faithfulness_evaluator,\n    \"relevancy\": gpt4_relevancy_evaluator,\n}Function to run batch evaluations on defined evaluatorsfrom llama_index.evaluation import BatchEvalRunner\n\n\nasync def batch_eval_runner(\n    evaluators, query_engine, questions, reference=None, num_workers=8\n):\n    batch_runner = BatchEvalRunner(\n        evaluators, workers=num_workers, show_progress=True\n    )\n\n    eval_results = await batch_runner.aevaluate_queries(\n        query_engine, queries=questions, reference=reference\n    )\n\n    return eval_resultsGet Query Engine, Questions, and References.query_engine, rag_dataset = create_query_engine_rag_dataset(\"./data/llama2\")\n\nquestions = [example.query for example in rag_dataset.examples]\n\nreference = [[example.reference_answer] for example in rag_dataset.examples]Compute Correctness, Faithfulness, and Relevancy Evaluation.prometheus_eval_results = await batch_eval_runner(\n    prometheus_evaluators, query_engine, questions, reference\n)\n\ngpt4_eval_results = await batch_eval_runner(\n    gpt4_evaluators, query_engine, questions, reference\n)ResultsCorrectness Evaluation score distribution.With the Prometheus Model:3.0: 56.0, \n1.0: 26.0, \n5.0: 9.0, \n4.0: 8.0, \n2.0: 1.0With GPT-4 Model:4.5: 57.99, \n1.0: 6.0, \n4.0: 12.0, \n5.0: 10.0, \n2.0: 5.0, \n3.5: 5.0, \n2.5: 3.0, \n3.0: 1.0Observation:Prometheus Evaluator gives more 3.0 scores compared to GPT-4 which gives 4.5. Let\u2019s investigate it with the help of feedback.Feedback comparison between Prometheus and GPT-4 for correctness.Query: Based on the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models,\u201d what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed?Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.Reference Answer: The two primary objectives achieved in the work described in the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models\u201d are: 1. The development and release of a collection of pretrained and fine-tuned large language models (LLMs) specifically optimized for dialogue use cases. 2. The demonstration that these fine-tuned LLMs, referred to as Llama 2-Chat, outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models, particularly in terms of helpfulness and safety based on human evaluations. The range of parameters for the large language models developed in this work is from 7 billion to 70 billion parameters.Prometheus Feedback: The generated response is relevant to the user query and correctly identifies the two primary objectives of the work described in the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models.\u201d However, it does not mention the demonstration of the fine-tuned LLMs outperforming open-source chat models on most benchmarks tested, which is a key point in the reference response. The range of parameters for the large language models developed is correctly identified, but the response does not mention the specific models referred to as Llama 2-Chat. So the overall score is 3.Prometheus Score: 3.0GPT-4 Feedback: The generated answer is relevant and almost fully correct. It correctly identifies the two primary objectives and the range of parameters for the large language models. However, it misses the detail about Llama 2-Chat outperforming other models on most benchmarks and potentially being a suitable substitute for closed-source models.GPT-4 Score: 4.5Observation:The feedback from Prometheus is a little more precise compared to GPT-4 and it penalizes and gives a score of 3.0but GPT-4 gives a score of 4.5. Prometheus penalizes more even if some of the text in the reference answer is missed in the generated answer.Faithfulness and Relevancy Evaluation scores.With the Prometheus Model:faithfulness Score: 0.39 \nrelevancy Score: 0.57With GPT-4 Model:faithfulness Score: 0.93\nrelevancy Score: 0.98Observation:We see a higher faithfulness and relevancy score with GPT-4 compared to the Prometheus model. Let\u2019s investigate it through feedback.Hamming Distance comparison between Prometheus and GPT-4:Faithfulness Hamming Distance: 58 \nRelevancy Hamming Distance: 41Observation:The comparison reveals that approximately 42% of the scores in case of Faithfulness and 59% in case of Relevancy are common between Prometheus and GPT-4 evaluations. This indicates a decent amount of correlation in terms of faithfulness and relevance scoring between the Prometheus and GPT-4 models.Feedback comparison between Prometheus and GPT-4 for Faithfulness and RelevancyQuery: Based on the abstract of \u201cLlama 2: Open Foundation and Fine-Tuned Chat Models,\u201d what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.Context-1: Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron\u2217Louis Martin\u2020Kevin Stone\u2020 Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom\u2217 GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. \u2217Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com \u2020Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023Context-2: (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto transparency and safety, we plan to make further improvements to Llama 2-Chat in future work. 36Prometheus Faithfulness Feedback: The information provided in the context is not supported by the given information. The context is about the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. However, the information provided in the context does not align with the given information. The context does not mention the range of parameters for the large language models developed, which is the primary objective mentioned in the information. The context only talks about the development and release of Llama 2 and its optimization for dialogue use cases, but it does not provide any information about the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NOPrometheus Faithfulness Score: 0.0Prometheus Relevancy Feedback: The response is not in line with the context information provided. The query asked for the two primary objectives achieved in the work and the range of parameters for the large language models developed. However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NOPrometheus Relevancy Score: 0.0GPT-4 Faithfulness Feedback: The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YESGPT-4 Faithfulness Score: 1.0GPT-4 Relevancy Feedback: The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YESGPT-4 Relevancy Score: 1.0Observation:Prometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information.GPT-4: Evaluates it correctly, unlike the Prometheus model.SummaryThe cost for evaluation (approx.): $1.5 for Prometheus Model and $15 for GPT4.The Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application.If a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4.The faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4.Note:You can check detailed analysis with code on Google Colab Notebook.The endpoint on HF is served on AWS Nvidia A100G \u00b7 1x GPU \u00b7 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.)We used the Prometheus model for the analysis here. We also made a similar analysis with the GPTQ Quantized version of the Prometheus model and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open-sourcing the model and Tom Jobbins for the quantized version of the model.References:Prometheus paper.Prometheus model on HuggingFace.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"
        },
        {
            "date": "May 28, 2024",
            "author": "PostgresML",
            "title": "Simplify your RAG application architecture with LlamaIndex + PostgresML",
            "tags": [
                "managed-indexes"
            ],
            "content": "We\u2019re happy to announce the recent integration of LlamaIndex with PostgresML \u2014 a comprehensive machine learning platform built on PostgreSQL. The PostgresML Managed Index allows LlamaIndex users to seamlessly manage document storage, splitting, embedding, and retrieval. By using PostgresML as the backend, users benefit from a streamlined and optimized process for Retrieval-Augmented Generation (RAG). This integration unifies embedding, vector search, and text generation into a single network call, resulting in faster, more reliable, and easier-to-manage RAG workflows.The problem with typical RAG workflowsTypical Retrieval-Augmented Generation (RAG) workflows come with significant drawbacks, particularly for users.Poor performance is a major issue, as these workflows involve multiple network calls to different services for embedding, vector storage, and text generation, leading to increased latency. Additionally, there are privacy concerns when sensitive data is sent to various LLM providers. These user-centric issues are compounded by other challenges:Increased dev time to master new technologiesComplicated maintenance and scalability issues due to multiple points of failureCostly vendors required for multiple servicesThe diagram above illustrates the complexity, showing how each component interacts across different services \u2014 exacerbating these problems.SolutionThe PostgresML Managed Index offers a comprehensive solution to the challenges of typical RAG workflows.By managing document storage, splitting, embedding generation, and retrieval all within a single system, PostgresML significantly reduces dev time, scaling costs, and overall spend when you eliminate the need for multiple point solutions. Most importantly, it enhances the user experience by consolidating embedding, vector search, and text generation into a single network call \u2014 resulting in improved performance and reduced latency. Additionally, the use of open-source models ensures transparency and flexibility, while operating within the database addresses privacy concerns and provides users with a secure and efficient RAG workflow.About PostgresMLPostgresML [github || website || docs] allows users to take advantage of the fundamental relationship between data and models, by moving the models to your database rather than constantly moving data to the models. This in-database approach to AI architecture results in more scalable, reliable and efficient applications. On the PostgresML cloud, you can perform vector operations, create embeddings, and generate real-time outputs in one process, directly where your data resides.Key highlights:Model Serving - GPU accelerated inference engine for interactive applications, with no additional networking latency or reliability costsModel Store - Access to open-source models including state of the art LLMs from Hugging Face, and track changes in performance between versionsModel Training - Train models with your application data using more than 50 algorithms for regression, classification or clustering tasks; fine tune pre-trained models like Llama and BERT to improve performanceFeature Store - Scalable access to model inputs, including vector, text, categorical, and numeric data: vector database, text search, knowledge graph and application data all in one low-latency systemPython and JavaScript SDKs - SDK clients can perform advanced ML/AI tasks in a single SQL request without having to transfer additional data, models, hardware or dependencies to your applicationServerless deployments - Enjoy instant autoscaling, so your applications can handle peak loads without overprovisioningPostgresML has a range of capabilities. In the following sections, we\u2019ll guide you through just one use case \u2013 RAG \u2013 and how to use the PostgresML Managed Index on LlamaIndex to build a better RAG app.How it works in LlamaIndexLet\u2019s look at a simple question-answering example using the PostgresML Managed Index. For this example, we will be using Paul Graham\u2019s essays.Step 1: Get Your Database Connection StringIf you haven\u2019t already, create your PostgresML account. You\u2019ll get $100 in free credits when you complete your profile.Set the PGML_DATABASE_URL environment variable:export PGML_DATABASE_URL=\"{YOUR_CONNCECTION_STRING}\"Alternatively, you can pass the pgml_database_url argument when creating the index.Step 2: Create the PostgresML Managed IndexFirst install Llama_index and the PostgresML Managed Index component:pip install llama_index llama-index-indices-managed-postgresmlThen load in the data:mkdir data\ncurl -o data/paul_graham_essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txtFinally create the PostgresML Managed Index:from llama_index.core.readers import SimpleDirectoryReader\nfrom llama_index.indices.managed.postgresml import PostgresMLIndex\n\n\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = PostgresMLIndex.from_documents(\n    documents, collection_name=\"llama-index-example\"\n)Note the collection_name is used to uniquely identify the index you are working with.Here we are using the SimpleDirectoryReader to load in the documents and then we construct the PostgresMLIndex from those documents.This workflow does not require document preprocessing. Instead, the documents are sent directly to PostgresML where they are stored, split, and embedded per the pipeline specification. For more information on pipelines see: https://postgresml.org/docs/api/client-sdk/pipelines Custom Pipelines can be passed into the PostgresML index at creation, but by default documents are split using the recursive_character splitter and embedded with intfloat/e5-small-v2 .Step 3: QueryingNow that we have created our index we can use it for retrieval and querying:retriever = index.as_retriever()\ndocs = retriever.retrieve(\"Was the author puzzled by the IBM 1401?\")\nfor doc in docs:\n    print(doc)PostgreML does embedding and retrieval in a single network call. Compare this query against other common LlamaIndex embedding and vector storage configurations and you will notice a significant speed up.Using the PostgresML Index as a query_engine is just as easy:response = index.as_query_engine().query(\"Was the author puzzled by the IBM 1401?\")\nprint(response)Once again, notice how fast the response was! The PostgresML Managed Index is doing embedding, retrieval, and augmented generation in one network call. The speed up becomes even more apparent when streaming:query_engine = index.as_query_engine(streaming=True)\nresults = query_engine.query(\"Was the author puzzled by the IBM 1401?\")\nfor text in results.response_gen:\n    print(text, end=\"\", flush=True)Note that by default the query_engine uses meta-llama/Meta-Llama-3-8B-Instruct but this is completely configurable.Key takeawaysThe PostgresML Managed Index uniquely unifies embedding, vector search, and text generation into a single network call. LlamaIndex users can expect faster, more reliable, and easier-to-manage RAG workflows by using PostgresML as the backend.To get started with PostgresML and LlamaIndex, you can follow the PostgresML intro guide to setup your account, and the examples above with your own data.",
            "related_posts": [
                "Simplify your RAG application architecture with LlamaIndex + PostgresML"
            ],
            "link": "https://www.llamaindex.ai/blog//simplify-your-rag-application-architecture-with-llamaindex-postgresml"
        },
        {
            "date": "Nov 10, 2023",
            "author": "Jerry Liu",
            "title": "Multi-Modal RAG",
            "tags": [
                "gpt-4v",
                "llamaindex",
                "llm",
                "vision",
                "rag"
            ],
            "content": "(co-authored by Haotian Zhang, Laurie Voss, and Jerry Liu @ LlamaIndex)OverviewIn this blog we\u2019re excited to present a fundamentally new paradigm: multi-modal Retrieval-Augmented Generation (RAG). We present new abstractions in LlamaIndex that now enable the following:Multi-modal LLMs and EmbeddingsMulti-modal Indexing and Retrieval (integrates with vector dbs)Multi-Modal RAGOne of the most exciting announcements at OpenAI Dev Day was the release of the GPT-4V API. GPT-4V is a multi-modal model that takes in both text/images, and can output text responses. It\u2019s the latest model in a recent series of advances around multi-modal models: LLaVa, and Fuyu-8B.This extends the capabilities of LLMs in exciting new directions. In the past year, entire application stacks have emerged around the text-in/text-out paradigm. One of the most notable examples is Retrieval Augmented Generation (RAG) \u2014 combine an LLM with an external text corpus to reason over data that the model isn\u2019t trained on. One of the most significant impacts of RAG for end-users was how much it accelerated time-to-insight on unstructured text data. By processing an arbitrary document (PDF, web page), loading it into storage, and feeding it into the context window of an LLM, you could extract out any insights you wanted from it.The introduction of GPT-4V API allows us to extend RAG concepts into the hybrid image/text domain, and unlock value from an even greater corpus of data (including images).Think about all the steps in a standard RAG pipeline and how it can be extended to a multi-modal setting.Input: The input can be text or images.Retrieval: The retrieved context can be text or images.Synthesis: The answer can be synthesized over both text and images.Response: The returned result can be text and/or images.This is just a small part of the overall space too. You can have chained/sequential calls that interleave between image and text reasoning, such as Retrieval Augmented Image Captioning or multi-modal agent loops.Abstractions in LlamaIndexWe\u2019re excited to present new abstractions in LlamaIndex that help make multi-modal RAG possible. For each abstraction, we explicitly note what we\u2019ve done so far and what\u2019s still to come.Multi-modal LLMWe have direct support for GPT-4V via our OpenAIMultiModal class and support for open-source multi-modal models via our ReplicateMultiModal class (currently in beta, so that name might change). Our SimpleDirectoryReader has long been able to ingest audio, images and video, but now you can pass them directly to GPT-4V and ask questions about them, like this:from llama_index.multi_modal_llms import OpenAIMultiModal\nfrom llama_index import SimpleDirectoryReader\n\nimage_documents = SimpleDirectoryReader(local_directory).load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=300\n)\nresponse = openai_mm_llm.complete(\n    prompt=\"what is in the image?\", image_documents=image_documents\n) This is a new base model abstraction. Unlike our default LLM class, which has standard completion/chat endpoints, the multi-modal model (MultiModalLLM) can take in both image and text as input.This also unifies the interface between both GPT-4V and open-source models.ResourcesWe have initial implementations for both GPT-4V and vision models hosted on Replicate. We also have a docs page for multi-modal models:Multi-modal docs pageGPT-4VReplicateDisplayed image and example output from GPT-4V given text query \u201cDescribe image as alternative text\u201dWhat\u2019s still to come:More multi-modal LLM integrationsChat endpointsStreamingMulti-Modal EmbeddingsWe introduce a new MultiModalEmbedding base class that can embed both text and images. It contains all the methods as our existing embedding models (subclasses BaseEmbedding ) but also exposes get_image_embedding.Our primary implementation here is ClipEmbedding with the CLIP model. See below for a guide on using this in action.What\u2019s still to comeMore multi-modal embedding integrationsMulti-Modal Indexing and RetrievalWe create a new index, a MultiModalVectorIndex that can index both text and images into underlying storage systems \u2014 specifically a vector database and docstore.Unlike our existing (most popular) index, the VectorStoreIndex , this new index can store both text and image documents. Indexing text is unchanged \u2014 it is embedded using a text embedding model and stored in a vector database. Indexing images involves a separate process:Embed the image using CLIPRepresent the image node as a base64 encoding or path, and store it along with its embedding in a vector db (separate collection from text).We store images and text separately since we may want to use a text-only embedding model for text as opposed to CLIP embeddings (e.g. ada or sbert).During retrieval-time, we do the following:Retrieve text via vector search on the text embeddingsRetrieve images via vector search on the image embeddingsBoth text and images are returned as Nodes in the result list. We can then synthesize over these results.What\u2019s still to ComeMore native ways to store images in a vector store (beyond base64 encoding)More flexible multi-modal retrieval abstractions (e.g. combining image retrieval with any text retrieval method)Multi-modal response synthesis abstractions. Currently the way to deal with long text context is to do \u201ccreate-and-refine\u201d or \u201ctree-summarize\u201d over it. It\u2019s unclear what generic response synthesis over multiple images and text looks like.Notebook WalkthroughLet\u2019s walk through a notebook example. Here we go over a use case of querying Tesla given screenshots of its website/vehicles, SEC fillings, and Wikipedia pages.We load the documents as a mix of text docs and images:documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()We then define two separate vector database collections in Qdrant: a collection for text docs, and a collection for images. We then define a MultiModalVectorStoreIndex .# Create a local Qdrant vector store\nclient = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n\ntext_store = QdrantVectorStore(\n    client=client, collection_name=\"text_collection\"\n)\nimage_store = QdrantVectorStore(\n    client=client, collection_name=\"image_collection\"\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\n\n# Create the MultiModal index\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents, storage_context=storage_context, image_vector_store=image_store\n)We can then ask questions over our multi-modal corpus.Example 1: Retrieval Augmented CaptioningHere we copy/paste an initial image caption as the input to get a retrieval-augmented output:retriever_engine = index.as_retriever(\n    similarity_top_k=3, image_similarity_top_k=3\n)\n# retrieve more information from the GPT4V response\nretrieval_results = retriever_engine.retrieve(query_str)The retrieved results contain both images and text:Retrieved Text/Image ResultsWe can feed this to GPT-4V to ask a followup question or synthesize a coherent response:Synthesized ResultExample 2: Multi-Modal RAG QueryingHere we ask a question and get a response from the entire multi-modal RAG pipeline. The SimpleMultiModalQueryEngine first retrieves the set of relevant images/text, and feeds the input to a vision model in order to synthesize a response.from llama_index.query_engine import SimpleMultiModalQueryEngine\n\nquery_engine = index.as_query_engine(\n    multi_modal_llm=openai_mm_llm,\n    text_qa_template=qa_tmpl\n)\n\nquery_str = \"Tell me more about the Porsche\"\nresponse = query_engine.query(query_str)The generated result + sources are shown below:",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//multi-modal-rag-621de7525fea"
        },
        {
            "date": "Mar 13, 2024",
            "author": "LlamaIndex",
            "title": "Launching the first GenAI-native document parsing platform",
            "tags": [
                "llm",
                "parsing",
                "llamaparse"
            ],
            "content": "Our mission at LlamaIndex is to connect the world\u2019s data to the power of LLMs, and today we\u2019re pleased to announce our latest big step towards that goal with the world\u2019s first GenAI-native document parsing platform, LlamaParse.We launched the first public version of LlamaParse 3 weeks ago and the response has been huge with well over 2,000 users parsing over 1 million pages! We\u2019ve been hard at work releasing hundreds of bug fixes and new features since then, and today we\u2019re releasing a game-changing new feature, GenAI-powered parsing instructions.Using LLMs for world-class parsingThe key insight behind parsing instructions is that you know what kind of documents you have, so you already know what kind of output you want. Why make the parser guess when an LLM-enabled parser can take simple, natural-language instructions from you and provide radically better parsing results?Example 1: rich table supportSince we first released LlamaParse it has featured industry-leading table extraction capabilities. Under the hood, this has been using LLM intelligence since the start. It seamlessly integrates with the advanced indexing/retrieval capabilities that the open-source framework offers, enabling users to build state-of-the-art document RAG. Now with JSON mode (see below) and parsing instructions, you can take this even further.Example 2: parsing comic booksParsing translated manga presents a particular challenge for a parser since a regular parser interprets the panels as cells in a table, and the reading order is right-to-left even though the book is in English, as shown in this extract from \"The manga guide to calculus\", by Hiroyuki Kojima:Using LlamaParse, you can give the parser plain, English-language instructions on what to do:The provided document is a manga comic book. \nMost pages do NOT have title. It does not contain tables. \nTry to reconstruct the dialogue happening in a cohesive way.(You can see the full code in our demonstration notebook, including what it looks like to parse this without the instructions)The result is a perfect parse!# The Asagake Times\n\nSanda-Cho Distributor\n\nA newspaper distributor?\n\nDo I have the wrong map?Example 3: mathematical equationsAnother challenging format for parsing is complex mathematical equations (by coincidence, the manga we picked as an example is all about how to do mathematics):To parse this, we take the same instructions as before and add one sentence: Output any math equation in LATEX markdown (between $$) . The result of parsing is clear LaTeX instructions, which render the equations perfectly:Anything an LLM can do, our parser can doYou can use this kind of natural-language instruction to do all sorts of advanced pre-processing on your documents \u2014 simplify language, include sentiment analysis, translate them to another language! We can\u2019t wait to see what you do with the power of LlamaParse.JSON modeParsing instructions are definitely the headline feature, but we have dozens of other features new to LlamaParse since launch. A standout is JSON mode, a rich programmatic format perfect for when you want more precision about exactly what you want to parse out. JSON mode\u2019s output includesthe full structure of the document that was parsedtables, text and headings markedtables are available as CSV and JSONimages are marked and available for extraction (see below)a wealth of metadata about each nodeIf you are building a custom RAG strategy JSON mode gives you everything you need to build it. Check out our JSON mode examples!Image extractionOne of the best features of JSON mode is image extraction: every page that contains images comes with a list of images, marked up with metadata including their size and position on the page, and you can retrieve these images directly and include them in your indexing to extract even more information from your complex, image-heavy documents.Expanded document typesWe launched LlamaParse with exceptional support for PDFs, and we have continued to expand its capability every day. We\u2019ve also added support for a large array of document types:Microsoft Word (.doc, .docx)Microsoft PowerPoint (.pptx)Rich Text Format (.rtf)Apple Pages (.pages)Apple Keynote (.key)ePub books (.epub)And dozens more!All of these document types \u201cjust work\u201d without any additional work on your part, and we are constantly expanding the list of supported file types. Check out this demo notebook where we demonstrate parsing a PowerPoint file.And one more thing\u2026 unlimited parsing!The huge demand for LlamaParse has included many people asking to go beyond our free daily limits via paid plans, and we\u2019re happy to answer those requests. Our pricing is simple:7000 pages/week are freeAdditional pages are $0.003/page, or $3 per 1000 pagesMaximum size for one document is 750 pagesAnd of course we retain our generous free tier of 1000 pages/day.The public version of LlamaParse is a hosted service. If you want to extend LlamaParse capabilities to build advanced document RAG, or wish to deploy LlamaParse in a private cloud, get in touch.",
            "related_posts": [
                "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//launching-the-first-genai-native-document-parsing-platform"
        },
        {
            "date": "Feb 8, 2024",
            "author": "Emanuel Ferreira",
            "title": "How to build LLM Agents in TypeScript with LlamaIndex.TS",
            "tags": [
                "ai",
                "retrieval-augmented",
                "agents",
                "machine-learning"
            ],
            "content": "Agents are autonomous systems that can execute end-to-end tasks without much or fewer instructions. These agents are capable of solving tasks related to questions and answering, using tools to achieve a desired behavior, or even planning tasks.In this article, we will explore a few capabilities of LlamaIndex.TS\u2019s built-in agents to achieve a set of goals. We have two parts:Building a simple agent for calculation.Using agents with your personal/private data to answer questions.All the full code examples will be at the end of the post.SetupTo start you need to have llamaindex package installed on your node environment setup and an OpenAI key.To install the package:npm install llamaindex To set up the OpenAI key you can set your environment variable:export OPENAI_API_KEY=sk-***************Agent for calculationsThe first agent will be responsible for getting a user input of numbers and based on this selecting the right tools to achieve the taskImport the classes:import { FunctionTool, OpenAIAgent } from \"llamaindex\";Function ToolThe first step will be creating tools that the agent will have access to, we will be creating an add function and a multiply function.OpenAI provides us a function calling API which we can send our function arguments and get back a responseWe start by creating two functions:// Define a function to sum two numbers\nfunction sum({ a, b }: { a: number; b: number }): number {\n  return a + b;\n}\n\n// Define a function to multiply two numbers\nfunction multiply({ a, b }: { a: number; b: number }): number {\n  return a * b;\n}Now we can set up the FunctionTool class which will be given to the agent, this class requires the properties of the function and metadata for the tool, helping the Large Language Models (LLMs) to identify which tool the LLM should use and the parameters.// Sum properties to give to the LLM\nconst sumJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The first number\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The second number\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\n// Multiply properties to give to the LLM\nconst multiplyJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The number to multiply\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The multiplier\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\n// Create sum function tool\nconst sumFunctionTool = new FunctionTool(sum, {\n  name: \"sum\",\n  description: \"Use this function to sum two numbers\",\n  parameters: sumJSON,\n});\n\n// Creat multiply function tool\nconst multiplyFunctionTool = new FunctionTool(multiply, {\n  name: \"multiply\",\n  description: \"Use this function to multiply two numbers\",\n  parameters: multiplyJSON,\n});Chat with AgentNow we have the tools to give to the agent, we can set up the agent:// Setup the agent with the respective tools\nconst agent = new OpenAIAgent({\n  tools: [sumFunctionTool, multiplyFunctionTool],\n  verbose: true,\n});And then ask a question:// Chat with LLM\nconst response = await agent.chat({\n  message: \"How much is 5 + 5? then multiply by 2\",\n});\n\n// Agent output\nconsole.log(String(response));Now the agent will choose the right tools to achieve the desired task using the functions provided by you. Then you should see an output as:=== Calling Function ===\nCalling function: sum with args: {\n  \"a\": 5,\n  \"b\": 5\n}\nGot output 10\n==========================\n=== Calling Function ===\nCalling function: multiply with args: {\n  \"a\": 10,\n  \"b\": 2\n}\nGot output 20\n==========================\nThe result of adding 5 and 5 is 10. When you multiply 10 by 2, the result is 20.Using Agents with your documentsThe second agent will be responsible for going through a set of Dan Abramov essays and answering questions based on the available data.Firstly, we will import the necessary classes and functionsImport the classes and functionsimport {\n  OpenAIAgent,\n  SimpleDirectoryReader,\n  VectorStoreIndex,\n  SummaryIndex,\n  QueryEngineTool,\n} from \"llamaindex\";Loading DocumentsNow we will load the documents and insert them into a local vector store index which will be responsible for storing the documents and allowing the agent to query the most relevant data for the task and a summarize vector index which can better help on tasks summary related.// Load the documents\nconst documents = await new SimpleDirectoryReader().loadData({\n  directoryPath: \"node_modules/llamaindex/examples\",\n});\n\n// Create a vector index from the documents\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\nconst summaryIndex = await SummaryIndex.fromDocuments(documents)Creating the Query Engine ToolNow we will create the tooling that allows the Agents to access the Vector Index:// Create a query engine from the vector index\nconst abramovQueryEngine = vectorIndex.asQueryEngine();\nconst abramovSummaryEngine = summaryIndex.asQueryEngine();\n\n// Create a QueryEngineTool with the vector engine\nconst vectorEngineTool = new QueryEngineTool({\n  queryEngine: abramovQueryEngine,\n  metadata: {\n    name: \"abramov_query_engine\",\n    description: \"Use this engine to answer specific questions about Abramov\",\n  },\n});\n\n// Create a QueryEngineTool with the summary engine\nconst summaryEngineTool = new QueryEngineTool({\n  queryEngine: abramovSummaryEngine,\n  metadata: {\n    name: \"abramov_summary_engine\",\n    description: \"Use this engine to generate summaries about Abramov\",\n  },\n});Creating the OpenAI AgentNow we can create and provide the necessary tools for the agent// Setup the agent \nconst agent = new OpenAIAgent({\n  tools: [vectorEngineTool, summaryEngineTool],\n  verbose: true,\n});Chat with AgentNow you can chat with the agent about Dan Abramov and it will select the right tools to achieve the goal.\n// Chat with Agent\nconst response = await agent.chat({\n  message: \"Where he worked in his 20s?\",\n});\n\n// Log output\nconsole.log(String(response));The tool output=== Calling Function ===\nCalling function: abramov_query_engine with args: {\n  \"query\": \"work experience in 20s\"\n}\nGot output The individual had their first job as a software developer in their 20s. They worked for a Russian-American outsourcing company and their salary was $18k/year.\n==========================\nIn his 20s, Abramov worked as a software developer for a Russian-American outsourcing company. His salary during that time was $18,000 per year.ConclusionAutonomous agents are powerful when talking about creating workflows and automation that can take your business to the next level or make your life easier.There is still a long way to go before they are fully autonomous but the arrival of LLMs is allowing the first steps of these reasoning and decision-making enginesDo you already use agents in your day-to-day? and wanna discuss agents or help with agents? Reach me on TwitterReferencesCode example: https://github.com/EmanuelCampos/agents-typescript-exampleLLamaIndexTS Documentation: https://ts.llamaindex.ai/modules/agent/openaihttps://www.llamaindex.ai/",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"
            ],
            "link": "https://www.llamaindex.ai/blog//how-to-build-llm-agents-in-typescript-with-llamaindex-ts-a88ed364a7aa"
        },
        {
            "date": "Jul 11, 2024",
            "author": "LlamaIndex",
            "title": "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
            "tags": [
                "evaluation",
                "observability"
            ],
            "content": "Strategic alliance and joint product promises to broaden the adoption of generative AI across industriesArize AI, a pioneer and leader in AI observability and LLM evaluation, and LlamaIndex, a leading data framework for LLM applications, debuted a new joint offering today called LlamaTrace, a hosted version of Arize OSS Phoenix.According to a soon-to-release survey, 47.7% of AI engineers and developers building generative AI applications are leveraging retrieval today in their LLM Applications. By connecting data to generative AI, orchestration frameworks like LlamaIndex can be game-changers in accelerating generative AI development. However, for many teams and enterprises technical challenges remain in getting modern LLM systems \u2013 with layers of abstraction \u2013 ready for the real world.To help, Arize and LlamaIndex are debuting an LLM tracing and observability platform that works natively with the LlamaIndex and Arize ecosystem. With a foundation based on Arize Phoenix OSS, the hosted version of Phoenix offers the ability to persist application telemetry data generated during AI development in order to better experiment, iterate, and collaborate in development or production.The solution has a foundation in open source and features a fully hosted, online, persistent deployment option for teams that do not want to self host. AI engineers can instantly log traces, persist datasets, run experiments, run evaluations \u2013 and share those insights with colleagues.The new offering is available today, and can be accessed through either a LlamaIndex or Arize account.\u201cWe share a vision with LlamaIndex in enabling builders to reduce the time it takes to deploy generative AI into production but in a way that is super battle hardened for business-critical use cases,\u201d said Jason Lopatecki, CEO and Co-Founder of Arize. \u201cAs leaders in our respective spaces with a common philosophy in empowering AI engineers and developers, we\u2019re uniquely positioned here to do something that can move modern LLMOps forward and broaden adoption.\u201d\u201cPrototyping a RAG pipeline or agent is easy, but every AI engineer needs the right data processing layer, orchestration framework, and experimentation/monitoring tool in order to take these applications to production. LlamaTrace by Arize offers the richest toolkit we\u2019ve seen in enabling developers to observe, debug, and evaluate every granular step of a very complex LLM workflow, and it nicely complements the production-ready data platform and orchestration framework that LlamaCloud and LlamaIndex offer.\u201d - Jerry Liu, CEO of LlamaIndexAbout Arize AIArize AI is an AI observability and LLM evaluation platform that helps teams deliver and maintain more successful AI in production. Arize\u2019s automated monitoring and observability platform allows teams to quickly detect issues when they emerge, troubleshoot why they happened, and improve overall performance across both traditional ML and generative use cases. Arize is headquartered in Berkeley, CA.",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "One-click Open Source RAG Observability with Langfuse",
                "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications"
        },
        {
            "date": "Dec 19, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201312\u201319",
            "tags": [
                "llamaindex",
                "rag",
                "newsletter",
                "ai",
                "llm"
            ],
            "content": "What\u2019s up, Llama Followers \ud83e\udd99,We\u2019re excited to bring you another week packed with the latest updates, features, exciting community demos, insightful tutorials, guides, and webinars. This week, don\u2019t miss our special holiday workshop on 12/21, where we\u2019ll dive into innovative LLM + RAG use cases with Google Gemini team.Got a groundbreaking project, compelling article, or captivating video? We\u2019re all ears! Reach out to us at news@llamaindex.ai. Remember to subscribe to our newsletter via our website to get all these exciting developments straight to your inbox.\ud83e\udd29 First, the highlights:Google Gemini Partnership: Now offering day 1 support for Gemini API on LlamaIndex, complete with comprehensive cookbooks for advanced RAG capabilities. Tweet.MistralAI Integrations: Introduced day-0 integrations with MistralAI LLMs and Embedding model for building RAG solutions on LlamaIndex. Notebook, Tweet.Docugami Multi-Doc Llama Dataset: Launched the Multi-Doc SEC 10Q Dataset by Taqi Jaffri, offering a range of question complexities for advanced RAG research. Docs, Tweet.Proposition-Based Retrieval: Implemented a new retrieval unit based on propositions, enhancing QA performance with LLMs. Docs, Tweet.RAG Pipeline Enhancement Guide: Introduced a guide featuring modules like Routing, Query-Rewriting, and Agent Reasoning for more complex QA over documents. Docs.\u2728 Feature Releases and Enhancements:We launched a partnership with Google Gemini, offering day 1 support for the Gemini API on LlamaIndex, including full-feature support for Gemini (text and multi-modal) and Semantic Retriever API, complemented by three comprehensive cookbooks: Gemini LLM, Gemini Multi-modal, and Semantic Retriever API, promising advanced RAG capabilities and multi-modal integrations. Tweet.We introduced day-0 integrations with the MistralAI LLMs (mistral-tiny, mistral-small, mistral-medium) and the MistralAI Embedding model for building RAG solutions with LlamaIndex both on Python and Typescript versions. Notebook, Tweet.We launched the COVID-QA dataset on LlamaHub, a human-annotated, substantial set of 300+ QA pairs about COVID from various web articles, complete with source URLs for easy integration into RAG pipelines, offering ample scope for improvement. Docs, Tweet.We launched a new multi-modal template in Create-llama, enabling image input and output generation using the latest GPT-4-vision model from OpenAI, expanding possibilities for diverse use cases. Docs, Tweet.We have introduced Proposition-Based Retrieval in LlamaIndex: Implementing a new retrieval unit based on propositions, as introduced in the \u2018Dense X Retrieval\u2019 paper, enhancing QA performance with LLMs by indexing propositions and linking to the underlying text. Docs, Tweet.We partnered with Docugami to launch a new Multi-Doc SEC 10Q Dataset by Taqi Jaffri, aimed at advancing QA datasets for RAG evaluation. This dataset offers a range of question complexities: Single-Doc, Single-Chunk RAG; Single-Doc, Multi-Chunk RAG; and Multi-Doc RAG, addressing the need for more intricate datasets in RAG research. Docs, Tweet.We launched a SharePoint data loader, enabling direct integration of SharePoint files into LLM/RAG pipelines. Docs, Tweet.\ud83d\udc40 Community Demos:MemoryCache: Mozilla\u2019s new experimental project that curates your online experience into a private, on-device RAG application using PrivateGPT_AI and LlamaIndex, enhancing personal knowledge management while maintaining privacy. Website, Repo.OpenBB Finance showcases its enhanced chat widget feature in Terminal Pro, utilizing LlamaIndex\u2019s data chunking combined with Cursor AI for improved large context management and accuracy. TweetAI Chatbot Starter (from the DataStax team), a web server powered by AstraDB and LlamaIndex, allows easy setup for chatting over web documentation. It can be used as a standalone service or integrated into full-stack applications, with simple credential setup and document ingestion. Repo, Tweet.Na2SQL (by Harshad) to ****Build an End-to-End SQL Analyst App on Streamlit featuring interactive database viewing, SQL query displays, and integration with Llama Index. Blog, Repo.LionAGI (by Ocean Li) is an agent framework for efficient data operations and support for concurrent calls and JSON mode with OpenAI. Check it to integrate it with a Llama Index RAG pipeline for automated AI assistants like an ArXiv research assistant. Docs, Repo.Local RAG for Windows (from Marklysze): A comprehensive resource for integrating advanced LLMs into RAG workflows using Windows Subsystem for Linux, featuring five detailed cookbooks.\ud83d\uddfa\ufe0f Guides:Guide for enhancing RAG pipelines with a Query Understanding Layer, featuring modules like Routing, Query-Rewriting, Sub-Question creation, and Agent Reasoning, all designed to enable more complex and \u2018agentic\u2019 QA over documents.Guide to Building a Restaurant Recommendation QA System with Gemini to extract structured image data and utilize multi-modal Retrieval-Augmented Generation for enhanced query responses.Guide to building Advanced RAG with Safety Guardrails to create constrained RAG systems with Gemini API\u2019s semantic search, safety features, and Google Semantic Retriever integrations.Guide on Qdrant\u2019s Multitenancy with LlamaIndex on setting up payload-based partitioning for user data isolation in vector services.Guide on using Prometheus \u2014 an open-source 13B LLM for RAG Evaluations, comparing it with GPT-4 evaluation with insights on its performance in terms of cost-effectiveness, accuracy, and scoring biases.\u270d\ufe0f Tutorials:Laurie\u2019s Advanced Querying & Retrieval Techniques comprehensive code-level tutorial on 7 advanced querying and retrieval techniques including SubQuestionQuery Engine, Small-to-big retrieval, Metadata filtering, Hybrid search, Recursive Retrieval, Text to SQL, and Multi-document agents.Hubel Labs\u2019 Advanced RAG video tutorial with Llamaindex & OpenAI GPT: Sentence Window Retrieval vs Basic ChunkingDevelopers Digest video tutorial on getting started with llamaindex.ts .Anil\u2019s tutorial on How to train a custom GPT on your data with EmbedAI + LlamaIndex.\ud83c\udfa5 Webinars:Tony Kipkemboi (Streamlit) and Yi Ding (LlamaIndex) webinar on Demystifying RAG apps with LlamaIndex!\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-12-19-2965a2d03726"
        },
        {
            "date": "May 29, 2024",
            "author": "LlamaIndex",
            "title": "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs",
            "tags": [
                "knowledge-graphs"
            ],
            "content": "We're thrilled to announce a new feature in LlamaIndex that expands our knowledge graph capabilities to be more flexible, extendible, and robust. Introducing the Property Graph Index!Why Property Graphs?Traditional knowledge graph representations like knowledge triples (subject, predicate, object) are limited in expressiveness. They lack the ability to:Assign labels and properties to nodes and relationshipsRepresent text nodes as vector embeddingsPerform both vector and symbolic retrievalOur existing KnowledgeGraphIndex was burdened with these limitations, as well as general limitations on the architecture of the index itself.The Property Graph Index solves these issues. By using a labeled property graph representation, it enables far richer modeling, storage and querying of your knowledge graph.With Property Graphs, you can:Categorize nodes and relationships into types with associated metadataTreat your graph as a superset of a vector database for hybrid searchExpress complex queries using the Cypher graph query languageThis makes Property Graphs a powerful and flexible choice for building knowledge graphs with LLMs.Constructing Your GraphThe Property Graph Index offers several ways to extract a knowledge graph from your data, and you can combine as many as you want:1. Schema-Guided Extraction: Define allowed entity types, relationship types, and their connections in a schema. The LLM will only extract graph data that conforms to this schema.from llama_index.indices.property_graph import SchemaLLMPathExtractor\n\nentities = Literal[\"PERSON\", \"PLACE\", \"THING\"]\nrelations = Literal[\"PART_OF\", \"HAS\", \"IS_A\"]\nschema = {\n    \"PERSON\": [\"PART_OF\", \"HAS\", \"IS_A\"],\n    \"PLACE\": [\"PART_OF\", \"HAS\"], \n    \"THING\": [\"IS_A\"],\n}\n\nkg_extractor = SchemaLLMPathExtractor(\n  llm=llm, \n  possible_entities=entities, \n  possible_relations=relations, \n  kg_validation_schema=schema,\n  strict=True,  # if false, allows values outside of spec\n)2. Implicit Extraction: Use LlamaIndex constructs to specify relationships between nodes in your data. The graph will be built based on the node.relationships attribute. For example, when running a document through a node parser, the PREVIOUS, NEXT and SOURCE relationships will be captured.from llama_index.core.indices.property_graph import ImplicitPathExtractor\n\nkg_extractor = ImplicitPathExtractor()3. Free-Form Extraction: Let the LLM infer the entities, relationship types and schema directly from your data in a free-form manner. (This is similar to how the KnowledgeGraphIndex works today.)from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n\nkg_extractor = SimpleLLMPathExtractor(llm=llm)Mix and match these extraction approaches for fine-grained control over your graph structure.from llama_index.core import PropertyGraphIndex\n\nindex = PropertyGraphIndex.from_documents(docs, kg_extractors=[...])EmbeddingsBy default, all graph nodes are embedded. While some graph databases support embeddings natively, you can also specify and use any vector store from LlamaIndex on top of your graph database.index = PropertyGraphIndex(..., vector_store=vector_store)Querying Your GraphThe Property Graph Index supports a wide variety of querying techniques that can be combined and run concurrently.1. Keyword/Synonym-Based Retrieval: Expand your query into relevant keywords and synonyms and find matching nodes.from llama_index.core.indices.property_graph import LLMSynonymRetriever\n\nsub_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)2. Vector Similarity: Retrieve nodes based on the similarity of their vector representations to your query.from llama_index.core.indices.property_graph import VectorContextRetriever\n\nsub_retriever = VectorContextRetriever(\n  index.property_graph_store, \n  vector_store=index.vector_store,\n  embed_model=embed_model,\n)3. Cypher Queries: Use the expressive Cypher graph query language to specify complex graph patterns and traverse multiple relationships.from llama_index.core.indices.property_graph import CypherTemplateRetriever\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\n\nclass Params(BaseModel):\n \u201c\u201d\u201dParameters for a cypher query.\u201d\u201d\u201d\n names: list[str] = Field(description=\u201dA list of possible entity names or keywords related to the query.\u201d)\n \ncypher_query = \"\"\"\n   MATCH (c:Chunk)-[:MENTIONS]->(o) \n   WHERE o.name IN $names\n   RETURN c.text, o.name, o.label;\n\"\"\"\n   \nsub_retriever = CypherTemplateRetriever(\n index.property_graph_store, \n Params, \n cypher_query,\n llm=llm,\n)Instead of providing a template, you can also let the LLM write the entire cypher query based on context from the query and database:from llama_index.core.indices.property_graph import TextToCypherRetriever\n\nsub_retriever = TextToCypherRetriever(index.property_graph_store, llm=llm)4. Custom Graph Traversal: Define your own graph traversal logic by subclassing key retriever components.These retrievers can be combined and composed for hybrid search that leverages both the graph structure and vector representations of nodes.from llama_index.indices.property_graph import VectorContextRetriever, LLMSynonymRetriever\n\nvector_retriever = VectorContextRetriever(index.property_graph_store, embed_model=embed_model)  \nsynonym_retriever = LLMSynonymRetriever(index.property_graph_store, llm=llm)\n\nretriever = index.as_retriever(sub_retrievers=[vector_retriever, synonym_retriever])Using the Property Graph StoreUnder the hood, the Property Graph Index uses a PropertyGraphStore abstraction to store and retrieve graph data. You can also use this store directly for lower-level control.The store supports:Inserting and updating nodes, relationships and propertiesQuerying nodes by ID or propertiesRetrieving relationship paths from a starting nodeExecuting Cypher queries (if the backing store supports it)from llama_index.graph_stores.neo4j import Neo4jPGStore\n\ngraph_store = Neo4jPGStore(\n    username=\"neo4j\",\n    password=\"password\",\n    url=\"bolt://localhost:7687\",\n)\n\n# insert nodes\nnodes = [\n    EntityNode(name=\"llama\", label=\"ANIMAL\", properties={\"key\": \"value\"}),\n    EntityNode(name=\"index\", label=\"THING\", properties={\"key\": \"value\"}), \n]\ngraph_store.upsert_nodes(nodes)\n\n# insert relationships  \nrelations = [\n    Relation(\n        label=\"HAS\",\n        source_id=nodes[0].id, \n        target_id=nodes[1].id,\n    )\n]\ngraph_store.upsert_relations(relations)\n\n# query nodes\nllama_node = graph_store.get(properties={\"name\": \"llama\"})[0]\n\n# get relationship paths  \npaths = graph_store.get_rel_map([llama_node], depth=1)\n\n# run Cypher query\nresults = graph_store.structured_query(\"MATCH (n) RETURN n LIMIT 10\")  Several backing stores are supported, including in-memory, disk-based, and Neo4j.Learn MoreProperty Graph Index OverviewBasic Usage NotebookAdvanced Usage with Neo4jUsing the Property Graph Store DirectlyA huge thanks to our partners at Neo4j for their collaboration on this launch, especially Tomaz Bratanic for the detailed integration guide and design guidance.We can't wait to see what you build with the new Property Graph Index! As always, feel free to join our Discord to share your projects, ask questions, and get support from the community.Happy building!The LlamaIndex Team",
            "related_posts": [
                "Customizing property graph index in LlamaIndex",
                "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms"
        },
        {
            "date": "Jan 2, 2024",
            "author": "Logan Markewich",
            "title": "Scaling LlamaIndex with AWS and Hugging Face",
            "tags": [
                "aws",
                "llamaindex",
                "etl",
                "data-processing",
                "hugging-face"
            ],
            "content": "Over the holidays, I was running some retrieval benchmarks with LlamaIndex. I found myself rebuilding an index repeatedly with 30K documents, and finding waiting 10\u201320 minutes each time was too grating.So to solve this, issue, I decided to bite the bullet and figure out how to deploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my data. This brought the processing time down to around 5 minutes!Proposed system architectureIf you want to skip the detailed steps, you can jump to the code at the following repository:https://github.com/run-llama/llamaindex_aws_ingestionNOTE: I am not an AWS expert, and had zero experience with it before this project. There are likely ways to improve upon the design I came up with. This blog merely documents my first foray into getting a system working on AWS. My hope is that this helps other people get started, and opens the door for other engineers deploying more scale-able systems.Step 1: Figuring out how AWS worksTo use AWS effectively, there are several packages and tools that you will need:AWS account signupInstall AWS CLIUsed to authenticate your AWS account for CLI toolsInstall eksctlUsed to create EKS clusters easilyInstall kubectlUsed to configure and debug deployments, pods, services, etc.Install DockerAs you will see, nearly all AWS deployments revolve around yaml files that describe what you are deploying and how they connect together, as well as some CLI commands to actually run the deployment.If at any time you aren\u2019t sure what\u2019s going on, I found it helpful to visit the AWS dashboard and explore the resources I had actually deployed. Usually, you will want to visit. I had the pages below favourited in AWS. Also, remember to set your region properly in the top right!My AWS console favouritesNote on how deployments workFor a majority of deployments, you will typically haveThe clusterThe deployed app, scaled to X replicasA load balancer, to balance the incoming requests between X replicasIn the examples below, most will have a yaml for the deployed app, a yaml for the load balancer, and a command to create the cluster you want to run on.Helpful CLI CommandsA few CLI commands proved to be extremely helpful for debugging and monitoring deployments.# get the state of pods/deployments\nkubectl get pods\nkubectl get deployments\n\n# useful for seeing logs/events of pods + full yaml config\nkubectl describe pod <pod name>\nkubectl logs <pod name>\n\n# list clusters kubectl knows about\nkubectl config get-contexts\n\n# switch kubectl to another cluster\nkubectl config use-context <context name>\n\n# delete things\nkubectl delete <pod/deployment/service> <name>Step 2: Deploying Text Embeddings InterfaceIn order to run embeddings fast, we will deploy an embeddings server using HuggingFace\u2019s Text Embedding Interface (TEI). This server has production-level features and optimizations out-of-the-box, including continuous batching, flash-attention, rust implementation, and more. HuggingFace provides prebuilt docker images to simplify deployment.However, the first step to running embeddings fast is to have a GPU. If you just signed up for AWS, you will have to request a quota increase. For me, I requested a few times for G5 instances (which run an Nvidia A10G GPU), and after a few days of testing on CPU, AWS gave me access to use up to 4 G5 instances.Once you have a quota for GPU instances (like G5 nodes), you can create your cluster and deployeksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\nsleep 5\nkubectl create -f ./tei-deployment.yaml\nsleep 5\nkubectl create -f ./tei-service.yaml\nsleep 5\necho \"Embeddings URL is: &lt;http://$&gt;(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"The code above will create a cluster, a deployment (i.e. our TEI server) and a load balancer server.You can see the yaml configs in the repo, and you can edit them as needed.NOTE: Make sure to write down the URL printed at the end! If you forget, you can get the URL in the EKS page on AWS. You\u2019ll want the external IP for the load balancer.Step 3: Deploying RabbitMQRabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each Document object is processed independently by a consumer.Deploying RabbitMQ on EKS was a little tricky, but using the RabbitMQ operator installed with krew, many things are abstracted away.First, you need to create your cluster. For whatever reason, this didn\u2019t work unless I also specified the zoneseksctl create cluster \\\n  --name mqCluster \\\n  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1fSince RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use EBS for storage. This was a frustrating step to figure out since most existing guides skip this detail!eksctl utils associate-iam-oidc-provider \\\n  --cluster=mqCluster \\\n  --region us-east-1 \\\n  --approve\nsleep 5\neksctl create iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    --namespace kube-system \\\n    --cluster mqCluster \\\n    --role-name AmazonEKS_EBS_CSI_DriverRole \\\n    --role-only \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve\nsleep 5\neksctl create addon \\\n  --name aws-ebs-csi-driver \\\n  --cluster mqCluster \\\n  --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n  --forceFrom there, we can install the RabbitMQ operator and create our deploymentkubectl apply -f &lt;https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&gt;\nsleep 5\nkubectl apply -f rabbitmqcluster.yaml\nsleep 5\necho \"RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"As usual, the code for all this can be found in the git repo.NOTE: Make sure to write down the URL printed at the end! If you forget, you can get the URL in the EKS page on AWS. You\u2019ll want the external IP for the load balancer.You can monitor your RabbitMQ queues by visiting \u201c<rabbitmq_url>:15672\u201d and signing in with \u201cguest\u201d/\u201dguest\u201d.Step 4: Deploying IngestionPipeline WorkersThis is where the real meat of work comes in. We need to create a consumer that will endlessly pull from our RabbitMQ queue, ingest data with the help of TEI, and then put that data into our vector db.To do this, we can make a FastAPI server that does two thingsStarts a thread to consume from our queueStarts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.)First, we write our code, as you can see in worker.pyThen, we dockerize our app by creating a simple Dockerfile and running:docker build -t <image_name> .\ndocker tag <image_name>:latest <image_name>:<version>\ndocker push <image_name>:<version>With our app dockerized, we can complete the worker-deployment.yaml file by filling inOur embeddings URL under TEI_URLOur rabbit-mq URL under RABBITMQ_URLOur image name under container imageOur cluster details (in this case, a weaviate URL and API key)With the yaml file complete, now we can properly deploy the workereksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\nsleep 5\nkubectl create -f ./worker-deployment.yaml\nsleep 5\nkubectl create -f ./worker-service.yamlStep 5: Making a User-Facing Lambda FunctionOur lambda function will rely on a single external dependency \u2014 pika \u2014 which is used to communicate with RabbitMQ.Create a python file called lambda_function.py with the following code:import pika\nimport json\n\ndef lambda_handler(event, context):\n    try:\n        body = json.loads(event.get('body', '{}'))\n    except:\n        body = event.get('body', {})\n        \n    user = body.get('user', '')\n    documents = body.get('documents', [])\n    if not user or not documents:\n        return {\n            'statusCode': 400,\n            'body': json.dumps('Missing user or documents')\n        }\n    \n    credentials = pika.PlainCredentials(\"guest\", \"guest\")\n    parameters = pika.ConnectionParameters(\n        host=\"hostname.amazonaws.com\", \n        port=5672, \n        credentials=credentials\n    )\n    \n    connection = pika.BlockingConnection(parameters=parameters)\n    channel = connection.channel()\n    channel.queue_declare(queue='etl')\n\n    for document in documents:\n        data = {\n            'user': user,\n            'documents': [document]\n        }\n        channel.basic_publish(\n            exchange=\"\", \n            routing_key='etl', \n            body=json.dumps(data)\n        )\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Documents queued for ingestion')\n    }The function above processes incoming requests, and publishes each document as a single message in our rabbitmq cluster.To deploy a lambda file with dependencies, we need to create a zip of our lambda function + all dependencies. To do this, we can create a requirements.txt file with our dependencies and run:pip install -r requirements.txt -t .\nzip -r9 ../ingestion_lambda.zip . -x \"*.git*\" \"*setup.sh*\" \"*requirements.txt*\" \"*.zip*\"With our code and zip file in hand, head over to the Lambda AWS page in your browser.Select Create functionGive it a name, select a python runtime (I used Python 3.11)Click Create function at the bottomIn the code editor, you\u2019ll see an Upload from button \u2014 click that, and upload your zip fileClick test, give the test a name, and paste the following JSON{\n    \"body\": {\"user\": \"Test\", \"documents\": [{\"text\": \"test\"}]}\n}Once the test works, the Deploy button will not be grayed out, and you can click it.Your public URL will be listed in the upper right pane under Function URL \u2014 this is the URL you can use to call your lambda function from anywhere!Step 6: Reap the Scaling BenefitsNow, we can run our system end-to-end!To ingest data, you can run:import requests\nfrom llama_index import Document, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# this will also be the namespace for the vector store \n# -- for weaviate, it needs to start with a captial and only alpha-numeric\nuser = \"Loganm\" \n\n# upload in batches\nfor batch_idx in range(0, len(documents), 30):\n  documents_batch = documents[batch_idx:batch_idx+30]\n  body = {\n    'user': user,\n    'documents': [doc.json() for doc in documents_batch]\n  }\n\n # use the URL of our lambda function here\n response = requests.post(\"&lt;lambda_url&gt;\", json=body)\n print(response.text)Then, to use our data:from llama_index import VectorStoreIndex\nfrom llama_index.vector_stores import WeaviateVectorStore\nimport weaviate\n\nauth_config = weaviate.AuthApiKey(api_key=\"...\")\nclient = weaviate.Client(url=\"...\", auth_client_secret=auth_config)\nvector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=\"&lt;user&gt;\")\nindex = VectorStoreIndex.from_vector_store(vector_store)Step 7: Clean-upAWS doesn\u2019t make it easy to estimate costs of all this. But after running and testing things for a few days, I had only spent ~$40CAD. This included leaving some services running overnight (whoops!).When you are done with your deployment, you\u2019ll want to delete the resources so that you aren\u2019t charged for things you aren\u2019t using. To delete my clusters, I ran the following:eksctl delete cluster embeddings\neksctl delete cluster mq-worker\nkubectl rabbitmq delete production-rabbitmqclusterThen, in the AWS UI console, I deleted any remaining resources on the EC2 and CloudFormation pages, as well as double-checking that everything was deleted on the EKS page.ConclusionUsing this setup, I was able to reduce index-construction times for creating large indexes dramatically. Before, it would take about 10\u201320 minutes to create the index for 25K documents, and with this setup (2 rabbitmq nodes, 2 workers, 2 embeddings), it was down to 5 minutes! And with more scaling, it could be even faster.Next StepsFrom here, there are several improvements that I can think ofbetter secrets managementadding auto-scalingadding a retrieval lambda function (would require making a docker image for lambda + llama-index)adding queue stats to the fastapi serverdeploying redis for document management on the IngestionPipelineI encourage anyone to take this work and build off it. Be sure share any improvement on the github repository as well!",
            "related_posts": [
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse",
                "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud"
            ],
            "link": "https://www.llamaindex.ai/blog//scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"
        },
        {
            "date": "Oct 18, 2023",
            "author": "Emanuel Ferreira",
            "title": "Improving RAG effectiveness with Retrieval-Augmented Dual Instruction Tuning (RA-DIT)",
            "tags": [
                "ai",
                "large-language-models",
                "machine-learning",
                "fine-tuning",
                "llamaindex"
            ],
            "content": "IntroductionLarge Language Models (LLMs) improve performance by accessing external data for background knowledge tasks related. However, existing approaches require costly modifications during LM\u2019s pre-training or integrating the data store after the model has been trained. On the downside, both strategies lead to suboptimal performance.To address this problem an AI Research team at Meta has proposed a method called RA-DIT: RETRIEVAL-AUGMENTED DUAL INSTRUCTION TUNING that allows any LLM to be upgraded to include retrieval features.In this blog post, we will explore RA-DIT capabilities to have better performance on Retrieval Augmentation Generation (RAG) through building the dataset and fine-tuning the models.The RA-DIT approach involves two distinct fine-tuning steps:Update a pre-trained LM to better use retrieved information.Update the retriever to return more relevant resultsHow it worksThe RA-DIT approach separately fine-tunes the LLM and the retriever. The LLM is updated to maximize the probability of the correct answer given the retrieval-augmented instructions, while the retriever is updated to minimize how much the document is semantically similar (relevant) to the query.Below we are going through each step from generating the fine-tuning dataset, fine-tuning the language model for better predictions, and refining the retrieval search process.Fine-tuning DatasetThe fine-tuning dataset is tailored to enhance the language model\u2019s ability to leverage knowledge and boost its contextual awareness during prediction generation. Generating Q/A pairs, summarizing data, and incorporating chain-of-thought reasoning can lead to improved results when integrated with the models.Following our LamaIndex implementation, we retrieve the top_k nodes, generate Q/A pairs from the documents, and then augment the data. We use the Q/A pairs through the QueryResponseDataset module, which returns a (query, response) pair for the fine-tuning dataset. While the retrieval fine-tuning data set is created on Q/A pairs data.Language Model Fine-tuningWith our fine-tuning dataset in hand, we can refine our LLM to achieve two main benefits: Adapt the LLM to better utilization of relevant background knowledge and train the LLM to produce accurate predictions even with incorrectly retrieved chunks, empowering the model to rely on its own knowledge.Retriever Fine-tuningThe retriever is fine-tuned using the LM-Supervised Retrieval (LSR) method. In this approach, the LLM assesses the information fetched by the retriever. If the LLM finds the information misaligned with the given query, it sends feedback to the retriever. Using this feedback, the retriever refines its search process, ensuring it fetches data that the LLM can effectively use. This collaboration enhances the overall quality of the answers provided.EvaluationTo assess the suggested method, the authors employed specific datasets and metrics. Let\u2019s delve into each of these to grasp the experimental results better.MetricsAn \u201cexact match\u201d (EM) metric was used to measure how closely the model\u2019s prediction matches the ground truth answer.DatasetThe methodology was tested on two distinct tasks:Knowledge-intensive tasks.Commonsense reasoning.Let\u2019s explore the datasets utilized for both of these tasks.Knowledge-intensive datasetFor knowledge-intensive tasks the selected datasets predominantly focus on the model\u2019s capacity to access, understand, and relay deep and specific knowledge. They encompass questions rooted in facts, general trivia, and complex domain-specific queries;The datasets used are MMLU, Natural Questions (NQ), TriviaQA, and a subset of tasks from the KILT benchmark.Commonsense reasoning datasetCommonsense reasoning datasets challenge the model\u2019s ability to reason and infer based on general knowledge and everyday scenarios. They contain questions and scenarios that typically don\u2019t rely on deep domain knowledge but rather on intuitive and general world understanding.The datasets used are BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-E, ARC-C, OBQA.For a better understanding of how these datasets were utilized you can check the paper for better understanding.ResultsIn a comparative analysis of model performance on knowledge-intensive (Table 2 below) and commonsense reasoning tasks (Table 3 below), three models were considered:LLAMA 65BLLAMA 65B REPLUG (only retrieval augmentation)RA-DIT 65BKnowledge Intensive TasksEvaluations are conducted in 0-shot, 5-shot, and 64-shot fine-tuning settings.0-shot Analysis:RA-DIT 65B demonstrated superior performance with an average EM score of 50.5 across all tasks.It outperformed LLAMA 65B REPlug (43.1 average) and significantly surpassed LLAMA 65B (32.9 average).5-shot Analysis:RA-DIT 65B maintained its lead with an average EM score of 55.2.LLAMA 65B REPlug followed closely with 52.7, while LLAMA 65B achieved an average of 45.0.In a separate evaluation for 64-shot fine-tuning, two models were analyzed: ATLAS and RA-DIT 65B.64-shot Fine-tuning:RA-DIT 65B achieved an average performance of 60.9 across all tasks, slightly surpassing ATLAS, which obtained an average score of 56.8Commonsense reasoningRA-DIT 65B was benchmarked in order to evaluate the impact of retrieval-augmented instruction tuning on the LLMs parametric knowledge and reasoning capabilities.In this experiment without retrieval augmentation, RA-DIT showed improvements over base LLAMA 65B models on 7 of 8 evaluation datasets, indicating that the parametric knowledge and reasoning capabilities of the LLM component are in general preserved.In summary, RA-DIT 65B consistently delivered great results, surpassing its competitors in multiple scenarios, underscoring its proficiency and aptitude in knowledge-intensive tasks while showing that the parametric knowledge and reasoning capabilities of the LLM are still preserved.ConclusionThe RA-DIT approach provides a structured method to enhance how Large Language Models utilize external data. Through the dual fine-tuning of both the model and the retriever, we target better accuracy and context-awareness in responses.The incorporation of the LSR technique fosters a more efficient data retrieval process, ensuring that the generated answers are both relevant and informed, the final results show that RA-DIT surpasses un-tuned RALM approaches like REPLUG showing competitive results.You can explore more about the LLamaIndex implementation at: https://docs.llamaindex.ai/en/stable/examples/finetuning/knowledge/finetune_retrieval_aug.html#fine-tuning-with-retrieval-augmentationReferencesRA-DIT paper: https://arxiv.org/abs/2310.01352Connect with me on Twitter and Linkedin",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//improving-rag-effectiveness-with-retrieval-augmented-dual-instruction-tuning-ra-dit-01e73116655d"
        },
        {
            "date": "Jun 11, 2024",
            "author": "Tomaz Bratanic",
            "title": "Customizing property graph index in LlamaIndex",
            "tags": [
                "neo4j",
                "knowledge-graphs"
            ],
            "content": "Learn how to implement entity deduplication and custom retrieval methods to increase GraphRAG accuracyThis is a guest post by Neo4JThe property graph index is an excellent addition to LlamaIndex and an upgrade from the previous knowledge graph integration. First, the data representation is slightly different. In the previous integration, the graph was represented with triples, but now we have a proper property graph integration where nodes have labels and optionally node properties.Example of a property graph model.Each node is assigned a label indicating its type, such as Person, Organization, Project, or Department. Nodes and relationships may also store node properties for other relevant details, such as the date of birth or project start and end date, as shown in this example.Second, the property graph index is designed to be modular, so you can use one or multiple (custom) knowledge graph constructors as well as retrievers, making it an incredible tool to build your first knowledge graph or customize the implementation for your specific needs.Property graph workflowThe image illustrates the property graph integration within the LlamaIndex , beginning with documents being passed to graph constructors. These constructors are modular components responsible for extracting structured information, which is then stored in a knowledge graph. The graph can be built using various or custom modules, highlighting the system\u2019s flexibility to adapt to different data sources or extraction needs.Graph retrievers then access the knowledge graph to retrieve data. This stage is also modular, allowing for the use of multiple retrievers or custom solutions designed to query specific types of data or relationships within the graph. Finally, the retrieved data is used by a LLM to generate an answer, representing the output or the insight derived from the process. This flow emphasizes a highly adaptable and scalable system where each component can be independently modified or replaced to enhance the overall functionality or to tailor it to specific requirements.In this blog post you will learn how to:Construct a knowledge graph using a schema-guided extractionPerform entity deduplication using a combination of text embedding and word similarity techniquesDesign a custom graph retrieverFinally, you will implement a question answering flow using the custom retrieverThe code is available on GitHub.Environment setupIn this blog post, we will use Neo4j as the underlying graph store. The easiest way is to get started is to a free instance on Neo4j Aura, which offers cloud instances of the Neo4j database. Alternatively, you can also set up a local instance of the Neo4j database by downloading the Neo4j Desktop application and creating a local database instance.from llama_index.graph_stores.neo4j import Neo4jPGStore\n\nusername=\"neo4j\"\npassword=\"stump-inlet-student\"\nurl=\"bolt://52.201.215.224:7687\"\n\ngraph_store = Neo4jPGStore(\n    username=username,\n    password=password,\n    url=url,\n)Additionally, you will require a working OpenAI API key.import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-\"DatasetIn this blog post, we will use a sample news article dataset fetched from Diffbot, which I\u2019ve made available on GitHub for easier access.Sample records from the dataset.Since the property graph index operates with documents, we have to wrap the text from the news as LlamaIndex documents.import pandas as pd\nfrom llama_index.core import Document\n\nnews = pd.read_csv(\n  \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\")\ndocuments = [Document(text=f\"{row['title']}: {row['text']}\") for i, row in news.iterrows()]Graph constructionAs mentioned, LlamaIndex provides multiple out-of-the-box graph constructors. In this example, we will use the SchemaLLMPathExtractor, which allows us to define the schema of the graph structure we want to extract from documents.Schema-guided graph structure extraction.We begin by defining the types of nodes and relationships we want the LLM to extract.entities = Literal[\"PERSON\", \"LOCATION\", \"ORGANIZATION\", \"PRODUCT\", \"EVENT\"]\nrelations = Literal[\n    \"SUPPLIER_OF\",\n    \"COMPETITOR\",\n    \"PARTNERSHIP\",\n    \"ACQUISITION\",\n    \"WORKS_AT\",\n    \"SUBSIDIARY\",\n    \"BOARD_MEMBER\",\n    \"CEO\",\n    \"PROVIDES\",\n    \"HAS_EVENT\",\n    \"IN_LOCATION\",\n]As you can see, we are focusing our graph extraction around people and organizations. Next, we will specify the relationships associated with each node label.# define which entities can have which relations\nvalidation_schema = {\n    \"Person\": [\"WORKS_AT\", \"BOARD_MEMBER\", \"CEO\", \"HAS_EVENT\"],\n    \"Organization\": [\n        \"SUPPLIER_OF\",\n        \"COMPETITOR\",\n        \"PARTNERSHIP\",\n        \"ACQUISITION\",\n        \"WORKS_AT\",\n        \"SUBSIDIARY\",\n        \"BOARD_MEMBER\",\n        \"CEO\",\n        \"PROVIDES\",\n        \"HAS_EVENT\",\n        \"IN_LOCATION\",\n    ],\n    \"Product\": [\"PROVIDES\"],\n    \"Event\": [\"HAS_EVENT\", \"IN_LOCATION\"],\n    \"Location\": [\"HAPPENED_AT\", \"IN_LOCATION\"],\n}For example, a person can have the following relationships:WORKS_ATBOARD_MEMBERCEOHAS_EVENTThe schema is quite specific except for the EVENT node label, which is slightly more ambiguous and allows the LLM to capture various types of information.Now that we have defined the graph schema, we can input it into the SchemaLLMPathExtractor and use it to construct a graph.from llama_index.core import PropertyGraphIndex\n\nkg_extractor = SchemaLLMPathExtractor(\n    llm=llm,\n    possible_entities=entities,\n    possible_relations=relations,\n    kg_validation_schema=validation_schema,\n    # if false, allows for values outside of the schema\n    # useful for using the schema as a suggestion\n    strict=True,\n)\n\nNUMBER_OF_ARTICLES = 250\n\nindex = PropertyGraphIndex.from_documents(\n    documents[:NUMBER_OF_ARTICLES],\n    kg_extractors=[kg_extractor],\n    llm=llm,\n    embed_model=embed_model,\n    property_graph_store=graph_store,\n    show_progress=True,\n)This code extracts graph information from 250 news articles, but you can adjust the number how you see fit. There are 2500 articles in total.Note that extracting 250 articles takes about 7 minutes with GPT-4o. However, you can accelerate the process by employing parallelization through the\u00a0num_workers\u00a0parameter.We can visualize a small subgraph to inspect what was stored.Text chunks are blue, while entity nodes are all the rest.The constructed graph contains both text chunks (blue), which contain text and embeddings. If an entity was mentioned in the text chunk, there is a MENTIONS relationships between the text chunk and entity. Additionally, entities can have relationships to other entities.Entity deduplicationEntity deduplication or disambiguation is an important but often overlooked step in graph construction. Essentially, it is a cleaning step where you try to match multiple nodes that represent a single entity and merge them together into a single node for better graph structural integrity.For example, in our constructed graph I could find some examples that could be merged.Potential entity duplicates.We will use a combination of text embedding similarity and word distance to find potential duplicates. We start by defining the vector index on our entities in the graph.graph_store.structured_query(\"\"\"\nCREATE VECTOR INDEX entity IF NOT EXISTS\nFOR (m:`__Entity__`)\nON m.embedding\nOPTIONS {indexConfig: {\n `vector.dimensions`: 1536,\n `vector.similarity_function`: 'cosine'\n}}\n\"\"\")The next Cypher query finds duplicates and is quite involved and I took me, Michael Hunger, and Eric Monk a couple of hours to perfect it.similarity_threshold = 0.9\nword_edit_distance = 5\ndata = graph_store.structured_query(\"\"\"\nMATCH (e:__Entity__)\nCALL {\n  WITH e\n  CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n  YIELD node, score\n  WITH node, score\n  WHERE score > toFLoat($cutoff)\n      AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n           OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n      AND labels(e) = labels(node)\n  WITH node, score\n  ORDER BY node.name\n  RETURN collect(node) AS nodes\n}\nWITH distinct nodes\nWHERE size(nodes) > 1\nWITH collect([n in nodes | n.name]) AS results\nUNWIND range(0, size(results)-1, 1) as index\nWITH results, index, results[index] as result\nWITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n        CASE WHEN index <> index2 AND\n            size(apoc.coll.intersection(acc, results[index2])) > 0\n            THEN apoc.coll.union(acc, results[index2])\n            ELSE acc\n        END\n)) as combinedResult\nWITH distinct(combinedResult) as combinedResult\n// extra filtering\nWITH collect(combinedResult) as allCombinedResults\nUNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\nWITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\nWHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n    WHERE x <> combinedResultIndex\n    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n)\nRETURN combinedResult  \n\"\"\", param_map={'cutoff': similarity_threshold, 'distance': word_edit_distance})\nfor row in data:\n    print(row)Without getting into too many details, we use a combination of text embeddings and word distance to find potential duplicates in our graph. You can tune similarity_threshold and word_distance to find the best combination that detects as many duplicates without too much false positives. Unfortunately, entity disambiguation is a hard problem and there are no perfect solutions. With this approach, we get quite good results, but there are some false positives in there as well:['1963 AFL Draft', '1963 NFL Draft']\n['June 14, 2023', 'June 15 2023']\n['BTC Halving', 'BTC Halving 2016', 'BTC Halving 2020', 'BTC Halving 2024', 'Bitcoin Halving', 'Bitcoin Halving 2024']It is up to you to tweak the dials, and maybe add some manual exceptions before merging duplicate nodes.Implementing a custom retrieverGreat, we have constructed a knowledge graph based on the news dataset. Now, let\u2019s examine our retriever options. At the moment, there are four existing retrievers available:LLMSynonymRetriever: takes the query, and tries to generate keywords and synonyms to retrieve nodes (and therefore the paths connected to those nodes).VectorContextRetriever: retrieves nodes based on their vector similarity, and then fetches the paths connected to those nodesTextToCypherRetriever: uses a graph store schema, your query, and a prompt template in order to generate and execute a cypher queryCypherTemplateRetriever: Rather than letting the LLM have free-range of generating any cypher statement, we can instead provide a cypher template and have the LLM fill in the parameters.Additionally, implementing a custom retriever is straightforward, so that is exactly what we will do here. Our custom retriever will first identify entities in the input query and then execute the VectorContextRetriever for each identified entity separately.First, we will define the entity extraction model and prompt.from pydantic import BaseModel\nfrom typing import Optional, List\n\n\nclass Entities(BaseModel):\n    \"\"\"List of named entities in the text such as names of people, organizations, concepts, and locations\"\"\"\n    names: Optional[List[str]]\n\n\nprompt_template_entities = \"\"\"\nExtract all named entities such as names of people, organizations, concepts, and locations\nfrom the following text:\n{text}\n\"\"\"Now we can progress to the custom retriever implementation.from typing import Any, Optional\n\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.core.retrievers import CustomPGRetriever, VectorContextRetriever\nfrom llama_index.core.vector_stores.types import VectorStore\nfrom llama_index.program.openai import OpenAIPydanticProgram\n\n\nclass MyCustomRetriever(CustomPGRetriever):\n    \"\"\"Custom retriever with entity detection.\"\"\"\n    def init(\n        self,\n        ## vector context retriever params\n        embed_model: Optional[BaseEmbedding] = None,\n        vector_store: Optional[VectorStore] = None,\n        similarity_top_k: int = 4,\n        path_depth: int = 1,\n        include_text: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Uses any kwargs passed in from class constructor.\"\"\"\n        self.entity_extraction = OpenAIPydanticProgram.from_defaults(\n            output_cls=Entities, prompt_template_str=prompt_template_entities\n        )\n        self.vector_retriever = VectorContextRetriever(\n            self.graph_store,\n            include_text=self.include_text,\n            embed_model=embed_model,\n            similarity_top_k=similarity_top_k,\n            path_depth=path_depth,\n        )\n\n    def custom_retrieve(self, query_str: str) -> str:\n        \"\"\"Define custom retriever with entity detection.\n\n        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n        \"\"\"\n        entities = self.entity_extraction(text=query_str).names\n        result_nodes = []\n        if entities:\n            print(f\"Detected entities: {entities}\")\n            for entity in entities:\n                result_nodes.extend(self.vector_retriever.retrieve(entity))\n        else:\n            result_nodes.extend(self.vector_retriever.retrieve(query_str))\n        final_text = \"\\n\\n\".join(\n            [n.get_content(metadata_mode=\"llm\") for n in result_nodes]\n        )\n        return final_textThe MyCustomRetriever class has only two methods. You can use the init method to instantiate any functions or classes you will be using in the retriever. In this example, we instantiate the entity detection OpenAI program along with the vector context retriever.The custom_retrieve method is called during retrieval. In our custom retriever implementation, we first identify any relevant entities in the text. If any entities are found, we iterate and execute the vector context retriever for each entity. On the other hand, if no entities are identified we pass the entire input to the vector context retriever.As you can observe, you can easily customize the retriever for your use-case by incorporating existing retrievers or starting from scratch as you can easily execute Cypher statements by using the structured_query method of the graph store.Question-answering flowLet\u2019s wrap it up by using the custom retriever to answer an example question. We need to pass the retriever to the RetrieverQueryEngine .from llama_index.core.query_engine import RetrieverQueryEngine\n\ncustom_sub_retriever = MyCustomRetriever(\n    index.property_graph_store,\n    include_text=True,\n    vector_store=index.vector_store,\n    embed_model=embed_model\n)\n\nquery_engine = RetrieverQueryEngine.from_args(\n    index.as_retriever(sub_retrievers=[custom_sub_retriever]), llm=llm\n)Let\u2019s test it out!response = query_engine.query(\n    \"What do you know about Maliek Collins or Darragh O\u2019Brien?\"\n)\nprint(str(response))\n# Detected entities: ['Maliek Collins', \"Darragh O'Brien\"]\n# Maliek Collins is a defensive tackle who has played for the Dallas Cowboys, Las Vegas Raiders, and Houston Texans. Recently, he signed a two-year contract extension with the Houston Texans worth $23 million, including a $20 million guarantee. This new deal represents a raise from his previous contract, where he earned $17 million with $8.5 million guaranteed. Collins is expected to be a key piece in the Texans' defensive line and fit well into their 4-3 alignment.\n# Darragh O\u2019Brien is the Minister for Housing and has been involved in the State\u2019s industrial relations process and the Government. He was recently involved in a debate in the D\u00e1il regarding the pay and working conditions of retained firefighters, which led to a heated exchange and almost resulted in the suspension of the session. O\u2019Brien expressed confidence that the dispute could be resolved and encouraged unions to re-engage with the industrial relations process.SummaryIn this blog post, we\u2019ve explored the intricacies of customizing the property graph index within LlamaIndex, focusing on implementing entity deduplication and designing custom retrieval methods to enhance GraphRAG accuracy. The property graph index allows for a modular and flexible approach, utilizing various graph constructors and retrievers to tailor the implementation to your specific needs. Whether you\u2019re building your first knowledge graph or optimizing for a unique dataset, these customizable components offer a powerful toolkit. We invite you to test out the property graph index integration to see how they can elevate your knowledge graph projects.As always, the code is available on GitHub.",
            "related_posts": [
                "Customizing property graph index in LlamaIndex",
                "Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs",
                "Multimodal RAG pipeline with LlamaIndex and Neo4j"
            ],
            "link": "https://www.llamaindex.ai/blog//customizing-property-graph-index-in-llamaindex"
        },
        {
            "date": "Feb 12, 2024",
            "author": "Rujun Gao",
            "title": "Pioneering the Future of Housing: Introducing GenAI-Driven ADU Planning",
            "tags": [],
            "content": "ADU Planner in actionIn the midst of a pressing housing shortage, the American dream of homeownership is being reimagined through the innovative concept of Accessory Dwelling Units (ADUs). These compact, efficient homes, nestled in the backyards of existing properties, are more than just a trend \u2014 they\u2019re a revolution in modular and prefabricated living solutions. In 2022, the ADU market, combined with modular and prefabricated houses, soared to an impressive $150 billion globally, with projections indicating a leap to $300 billion by 2032 (link).Yet, the journey to erecting an ADU has been far from simple. Traditionally, it involves extensive land surveys and numerous consultations with vendors to sift through an array of floor plans. Recognizing the need for a more efficient pathway, our team has crafted an innovative solution: a GenAI-powered ADU planning system. This cutting-edge tool deftly navigates through the maze of city building codes, effortlessly connecting users with local vendors and presenting viable floor plan options \u2014 all within our user-friendly app.Our mission is to streamline your ADU creation process, making it less daunting and more accessible. Imagine the potential when the complexities of planning are reduced to a few clicks \u2014 this is the power of GenAI at your fingertips.Award-Winning Innovation: ADU Planner Takes the LeadWe\u2019re thrilled to announce a milestone achievement: Our \u201cADU Planner\u201d project clinched the top honor at the LlamaIndex RAG Hackathon, held in the first weekend of February 2024. This victory underscores our commitment to transforming the ADU space with our pioneering technology.Discover the features that set our project apart and explore the full scope of our innovative planner through the following resources:\u00b7 Dive into the Details: Visit our Devpost project page for an in-depth look.\u00b7 Explore Our Code: For the tech enthusiasts, our codebase is available on GitHub.We\u2019re just getting started, and this recognition only fuels our drive to innovate and deliver solutions that matter.Watch Our Demo\n\nTechnologiesAt the heart of our \u201cADU Planner\u201d lies a synergy of cutting-edge technology and user-friendly design. The seamless interface is crafted with React, providing an intuitive frontend experience, while our robust Flask backend is the powerhouse of functionality, energized by the latest AI from GPT-3.5/4V and augmented by the precision of the LlamaIndex PDF parser and the versatility of Google Maps.Here\u2019s how our workflow unfolds:1. Start with Simplicity: Users begin by entering their address into our frontend. This action triggers our Google geocoding-powered backend to spring into action, capturing a detailed satellite image of the property in question.2. Deciphering the Details: Next, the journey bifurcates into a dual analysis mode. One path involves parsing through local building codes, a task adeptly handled by GPT-3.5, to unearth specific ADU regulations such as minimum and maximum floor area constraints. Concurrently, GPT-4V meticulously scans the satellite imagery to pinpoint any potential obstructions and delineate the areas ripe for ADU development.3. Bringing Plans to Life: With the viable regions marked, our system embarks on a virtual quest, scouring local builders\u2019 websites for ADU floor plans that not only fit the legal criteria but also the physical realities of your property. These plans are then vividly rendered within the identified buildable zones.4. A Click to the Future: Engaging with a chosen floor plan is as simple as a click, transporting users to the builder\u2019s domain where immersive 3D renderings, pricing details, and more await. To cap it off, our tool thoughtfully prepares data exports to facilitate those all-important initial discussions with ADU builders.This is more than a tool \u2014 it\u2019s a gateway to turning your backyard into a space of possibility.FlowchartFlowchartCode SamplesQuerying PDFsIn the following example, we\u2019ll query local ADU building codes for Saratoga, CA. Once you\u2019ve setup your LlamaIndex and OpenAI API keys, you\u2019ll be able to parse the PDFs as follows:from llama_parse import LlamaParse\nparser = LlamaParse(result_type='markdown')\ndocs = sum([\n  parser.load_data(file) for file in [\n    'Documents/ADU_FAQ.pdf',\n    'Documents/ADU_Handbook.pdf',\n    'Documents/SCC-ADU-Guidebook-FINAL-9.8.23.pdf']], [])Then, we can index the documents into an ephemeral, or in-memory, Chroma DB embeddings store.chroma_client = chromadb.EphemeralClient()\nchroma_collection = chroma_client.get_or_create_collection('embeddings')\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(docs, storage_context=storage_context)Next, we can integrate the documents into our agentic flow by creating a tool for our agent to query them.tools = [\n  QueryEngineTool(\n       query_engine=index.as_query_engine(),\n       metadata=ToolMetadata(\n           name='saratoga_adu_codes',\n           description=('Provides information about ADU building codes for the city of Saratoga, CA.'),\n       ),\n   )\n]All together, our agent will be able to answer questions about the documents as follows:from llama_index.agent import OpenAIAgent\n\nagent = OpenAIAgent.from_tools(tools, verbose=True)\nagent.chat(\"\"\"\\\nAnswer the following questions regarding Accessory Dwelling Unit (ADU) construction planning in Saratoga, California (CA):\n- What are the typical side and rear setbacks for a detached ADU?\n\"\"\")\n\n\n&gt;&gt;&gt; 'For detached ADUs in Saratoga, California, the typical side and rear setbacks are a minimum of four feet from the lot lines.'Image analysisFirstly, we\u2019ll prepare an image and plot it for visual recognization.import os\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ninput_image_path = Path(\"input_images\")\nif not input_image_path.exists():\n    Path.mkdir(input_image_path)\n\nimage_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n\n# Filter out non-image files\nimage_paths = [str(input_image_path / img_path) for img_path in os.listdir(input_image_path) \n               if any(img_path.lower().endswith(ext) for ext in image_extensions)]\n\ndef plot_images(image_paths):\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths:\n        if os.path.isfile(img_path):\n            image = Image.open(img_path)\n\n            plt.subplot(2, 3, images_shown + 1)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown += 1\n            if images_shown &gt;= 6:  # Adjusted to match the subplot dimensions (2,3)\n                break\n\nplot_images(image_paths)Secondly, we can do a image analysis and recognize the property layout based on GPT-4V, using the following prompt sample as an example.instruction = '''\nYou are an intelligent Accessory Dwelling Units (ADU) architect designer and contractor.\nPlease analyze the image, describle the layout, and then describe the pool, property, and driveways in the format of:\n1. property: xxx\n2. pool: xxx\n...\n'''from llama_index.multi_modal_llms.openai import OpenAIMultiModal\nfrom llama_index import SimpleDirectoryReader\n\n# put your local directore here\nimage_documents = SimpleDirectoryReader(\"./input_images\").load_data()\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500, temperature = 0.0\n)\n\nresponse = openai_mm_llm.complete(\n    prompt = instruction,\n    image_documents=image_documents,\n)\n\nprint(response)Based on the aerial image provided, here is an analysis of the layout:\n\n1. Property: The property appears to be a residential lot with a single-family home. The house has a hipped roof with multiple sections, indicating a complex floor plan with possibly several rooms or wings. There is a landscaped area surrounding the house with various trees and shrubs, and the terrain seems to be sloped, as indicated by the terracing on the land.\n\n2. Pool: There is a kidney-shaped pool located to the northwest of the main house. It is surrounded by a paved area, likely for lounging and poolside activities, and is accessible via a curved pathway that leads from the house to the pool area.\n\n3. Driveways: It is not entirely clear from the image where the driveway is located, as the specific access point to the property is not visible. However, there seems to be a paved path leading from the bottom right of the image towards the house, which could be the driveway or a walkway. If it is the driveway, it would be located on the southeast side of the property, leading up to the house.\n\nPlease note that without a broader view or additional context, some details about the property, such as the exact location of the driveway or additional structures, may not be accurately determined.ConclusionsOur application markedly simplifies the ADU construction process for homeowners. It adeptly navigates complex tasks such as land surveying and identifying viable floor plans. While ADU vendors need to validate the chosen floor plan and cities must grant construction approval, our tool propels this progression by facilitating comprehensive at-home analysis. Looking ahead, we are poised to officially launch this application and forge partnerships with reputable ADU vendors, enhancing oversight in modular and prefabricated home construction. We are committed to personalizing our service further by adapting our recommendations to align with individual budget constraints and specific layout preferences, ensuring that our users\u2019 visions for their homes are realized with precision and care.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//pioneering-the-future-of-housing-introducing-genai-driven-adu-planning-ea950be71e2f"
        },
        {
            "date": "Jun 4, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-06-04",
            "tags": [],
            "content": "Hello, LlamaIndex Family! \ud83e\udd99We're thrilled to connect with you again and bring you the latest and greatest from the world of LlamaIndex. This week, we're excited to present an array of updates and a diverse lineup of content designed to enhance your LlamaIndex experience, particularly when working with Knowledge Graphs. From integrations and guides to demos and tutorials, we've got you covered with all the tools and insights you need.\ud83e\udd29\u00a0The highlights:Elevating Knowledge Graphs: The Property Graph Index, introduced in LlamaIndex, transforms how knowledge graphs (KGs) are built and queried. This powerful toolkit enhances graph searches with vector capabilities. Docs, Tweet.Spreadsheet Insights with LlamaParse: LlamaParse now supports spreadsheet parsing, turning complex Excel files into LLM-friendly tables for improved performance and data handling. Notebook, Tweet.Code Generation with Codestral: Codestral, a cutting-edge model from MistralAI, is now integrated into LlamaIndex. This code-generating tool supports over 80 programming languages. Docs, Tweet.\u2728 Feature Releases and Enhancements:We have introduced the Property Graph Index, a major feature that establishes LlamaIndex as the premier framework for building knowledge graphs (KGs) with LLMs. This sophisticated toolkit enables the construction and querying of KGs, allowing for joint vector and graph searches even in graph stores that lack native vector support. Docs, Tweet.We have launched support for parsing spreadsheets in LlamaParse, allowing you to convert complex Excel files and other spreadsheet formats into clean, LLM-friendly tables for improved RAG pipeline performance. Notebook, Tweet.We have integrated Codestral from MistralAI into LlamaIndex, providing day 0 support for this cutting-edge code-generating model trained on over 80 programming languages. Docs, Tweet.We have integrated PostgresML into LlamaIndex, perfect for those who love Postgres and want to build AI applications. It serves open-source models locally, handles embeddings, and allows you to train or fine-tune models directly in Python and JavaScript. Blogpost, Tweet.We have integrated with Milvus Lite to provide an easy start to vector search, offering day-1 support with LlamaIndex. Docs, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to Building a Custom Graph Retriever to create a custom graph retriever for your specific needs by combining vector search and graph search with reranking for improved results.Guide to Building GenAI Applications in minutes with NVIDIA's NIM inference microservices, offering an easy and fast way to deploy GenAI applications. This step-by-step guide teaches you how to run models, generate embeddings, and re-rank data for optimal results.Guide to Constructing Knowledge Graphs with LLMs**,** build knowledge graphs using local models and Neo4j, starting with defining entities and relationships, using SchemaLLMPathExtractor to create structured graphs, and querying to uncover insights.\ud83d\udda5\ufe0f\u00a0Demos:Omakase RAG Orchestrator, a project developed by Amir Mehr, is a web app template designed to help you build scalable RAG applications using Django, LlamaIndex, and Google Drive. It features a full-featured RAG API, data source management, user access control, and an admin panel.gmail-extractor, a project by Laurie project that trains a Python script with an LLM to extract structured data from Gmail. By iteratively improving the script based on email data, the LLM can effectively modify and enhance it to extract information with precision.\u270d\ufe0f Tutorials:Sherlock Xu\u2019s tutorial from BentoML on Serving A LlamaIndex RAG App as REST APIs.\ud83d\udcd1\u00a0Papers:FinTextQA, a new benchmark dataset for long-form financial question answering, has been introduced by Jian Chen and their team. This benchmark was evaluated using LlamaIndex's Auto-Merging and Sentence Window Retrievers, along with various embeddings, rerankers, and LLMs, offering a comprehensive question-answering system for financial text.\ud83d\udcf9\u00a0Webinar:Webinar with authors of memary - Julian Saks, Kevin Li, Seyeong Han. Memary is a fully open-source reference implementation for long-term memory in autonomous agents\ud83d\udcc5\u00a0Events:Join Pierre from LlamaIndex along with speakers from Weaviate, and Weights & Biases on June 12th at the London NLP meetup, focusing on the challenges and solutions for using LLMs with financial services data in production settings.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-06-04"
        },
        {
            "date": "Feb 2, 2024",
            "author": "Harshad Suryawanshi",
            "title": "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex",
            "tags": [
                "rag",
                "no-code",
                "llamaindex",
                "openai",
                "code-generation"
            ],
            "content": "Unlocking the power of AI should be as intuitive as using your favorite apps. That\u2019s the philosophy behind RAGArch, my latest creation designed to demystify and streamline the process of setting up Retrieval-Augmented Generation (RAG) pipelines. This tool is born from a simple vision: to provide a straightforward, no-code platform that empowers both seasoned developers and curious explorers in the world of AI to craft, test, and implement RAG pipelines with confidence and ease.FeaturesRAGArch leverages LlamaIndex\u2019s powerful LLM orchestration capabilities, to provide a seamless experience and granular control over your RAG pipeline.Intuitive Interface: RAGArch\u2019s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively.Custom Configuration: The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project\u2019s needs.Live Testing: Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome.One-Click Code Generation: Once you\u2019re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application.Tools and TechnologiesThe creation of RAGArch was made possible by integrating a variety of powerful tools and technologies:UI: StreamlitHosting: Hugging Face SpacesLLMs: OpenAI GPT 3.5 and 4, Cohere API, Gemini ProLLM Orchestration: LlamaindexEmbedding Models: \u201cBAAI/bge-small-en-v1.5\u201d, \u201cWhereIsAI/UAE-Large-V1\u201d, \u201cBAAI/bge-large-en-v1.5\u201d, \u201ckhoa-klaytn/bge-small-en-v1.5-angle\u201d, \u201cBAAI/bge-base-en-v1.5\u201d, \u201cllmrails/ember-v1\u201d, \u201cjamesgpt1/sf_model_e5\u201d, \u201cthenlper/gte-large\u201d, \u201cinfgrad/stella-base-en-v2\u201d and \u201cthenlper/gte-base\u201dVector Stores: Simple (Llamaindex default), Pinecone and QdrantDeep Dive into the CodeThe app.py script is the backbone of RAGArch, integrating various components to provide a cohesive experience. The following are the key functions of app.pyupload_fileThis function manages file uploads and uses Llamaindex's SimpleDirectoryReader to load documents into the system. It supports a wide array of document types, including PDFs, text files, HTML, JSON files, and more, making it versatile for processing diverse data sources.def upload_file():\n    file = st.file_uploader(\"Upload a file\", on_change=reset_pipeline_generated)\n    if file is not None:\n        file_path = save_uploaded_file(file)\n        \n        if file_path:\n            loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n            print(f\"Total documents: {len(loaded_file)}\")\n\n            st.success(f\"File uploaded successfully. Total documents loaded: {len(loaded_file)}\")\n        return loaded_file\n    return Nonesave_uploaded_fileThis utility function saves the uploaded file to a temporary location on the server, making it accessible for further processing. It\u2019s a crucial part of the file handling process, ensuring data integrity and availability.def save_uploaded_file(uploaded_file):\n    try:\n        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n            tmp_file.write(uploaded_file.getvalue())\n            return tmp_file.name\n    except Exception as e:\n        st.error(f\"Error saving file: {e}\")\n        return Noneselect_llmAllows users to select a Large Language Model and initializes it for use. You can choose from Google\u2019s Gemini Pro, Cohere, OpenAI\u2019s GPT 3.5 and GPT 4.def select_llm():\n    st.header(\"Choose LLM\")\n    llm_choice = st.selectbox(\"Select LLM\", [\"Gemini\", \"Cohere\", \"GPT-3.5\", \"GPT-4\"], on_change=reset_pipeline_generated)\n    \n    if llm_choice == \"GPT-3.5\":\n        llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-1106\")\n        st.write(f\"{llm_choice} selected\")\n    elif llm_choice == \"GPT-4\":\n        llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n        st.write(f\"{llm_choice} selected\")\n    elif llm_choice == \"Gemini\":\n        llm = Gemini(model=\"models/gemini-pro\")\n        st.write(f\"{llm_choice} selected\")\n    elif llm_choice == \"Cohere\":\n        llm = Cohere(model=\"command\", api_key=os.environ['COHERE_API_TOKEN'])\n        st.write(f\"{llm_choice} selected\")\n    return llm, llm_choiceselect_embedding_modelOffers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face\u2019s MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models.def select_embedding_model():\n    st.header(\"Choose Embedding Model\")\n    col1, col2 = st.columns([2,1])\n    with col2:\n        st.markdown(\"\"\"\n                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                    \"\"\")\n    model_names = [\n        \"BAAI/bge-small-en-v1.5\",\n        \"WhereIsAI/UAE-Large-V1\",\n        \"BAAI/bge-large-en-v1.5\",\n        \"khoa-klaytn/bge-small-en-v1.5-angle\",\n        \"BAAI/bge-base-en-v1.5\",\n        \"llmrails/ember-v1\",\n        \"jamesgpt1/sf_model_e5\",\n        \"thenlper/gte-large\",\n        \"infgrad/stella-base-en-v2\",\n        \"thenlper/gte-base\"\n    ]\n    selected_model = st.selectbox(\"Select Embedding Model\", model_names,  on_change=reset_pipeline_generated)\n    with st.spinner(\"Please wait\") as status:\n        embed_model = HuggingFaceEmbedding(model_name=selected_model)\n        st.session_state['embed_model'] = embed_model\n        st.markdown(F\"Embedding Model: {embed_model.model_name}\")\n        st.markdown(F\"Embed Batch Size: {embed_model.embed_batch_size}\")\n        st.markdown(F\"Embed Batch Size: {embed_model.max_length}\")\n\n\n    return embed_model, selected_modelselect_node_parser FunctionThis function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. I have included some of the most commonly used node parsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser and MarkdownNodeParser.def select_node_parser():\n    st.header(\"Choose Node Parser\")\n    col1, col2 = st.columns([4,1])\n    with col2:\n        st.markdown(\"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                    \"\"\")\n    parser_types = [\"SentenceSplitter\", \"CodeSplitter\", \"SemanticSplitterNodeParser\",\n                    \"TokenTextSplitter\", \"HTMLNodeParser\", \"JSONNodeParser\", \"MarkdownNodeParser\"]\n    parser_type = st.selectbox(\"Select Node Parser\", parser_types, on_change=reset_pipeline_generated)\n    \n    parser_params = {}\n    if parser_type == \"HTMLNodeParser\":\n        tags = st.text_input(\"Enter tags separated by commas\", \"p, h1\")\n        tag_list = tags.split(',')\n        parser = HTMLNodeParser(tags=tag_list)\n        parser_params = {'tags': tag_list}\n        \n    elif parser_type == \"JSONNodeParser\":\n        parser = JSONNodeParser()\n        \n    elif parser_type == \"MarkdownNodeParser\":\n        parser = MarkdownNodeParser()\n        \n    elif parser_type == \"CodeSplitter\":\n        language = st.text_input(\"Language\", \"python\")\n        chunk_lines = st.number_input(\"Chunk Lines\", min_value=1, value=40)\n        chunk_lines_overlap = st.number_input(\"Chunk Lines Overlap\", min_value=0, value=15)\n        max_chars = st.number_input(\"Max Chars\", min_value=1, value=1500)\n        parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n        parser_params = {'language': language, 'chunk_lines': chunk_lines, 'chunk_lines_overlap': chunk_lines_overlap, 'max_chars': max_chars}\n        \n    elif parser_type == \"SentenceSplitter\":\n        chunk_size = st.number_input(\"Chunk Size\", min_value=1, value=1024)\n        chunk_overlap = st.number_input(\"Chunk Overlap\", min_value=0, value=20)\n        parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n        \n    elif parser_type == \"SemanticSplitterNodeParser\":\n        if 'embed_model' not in st.session_state:\n            st.warning(\"Please select an embedding model first.\")\n            return None, None\n        \n        embed_model = st.session_state['embed_model']\n        buffer_size = st.number_input(\"Buffer Size\", min_value=1, value=1)\n        breakpoint_percentile_threshold = st.number_input(\"Breakpoint Percentile Threshold\", min_value=0, max_value=100, value=95)\n        parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n        parser_params = {'buffer_size': buffer_size, 'breakpoint_percentile_threshold': breakpoint_percentile_threshold}\n        \n    elif parser_type == \"TokenTextSplitter\":\n        chunk_size = st.number_input(\"Chunk Size\", min_value=1, value=1024)\n        chunk_overlap = st.number_input(\"Chunk Overlap\", min_value=0, value=20)\n        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {'chunk_size': chunk_size, 'chunk_overlap': chunk_overlap}\n\n    # Save the parser type and parameters to the session state\n    st.session_state['node_parser_type'] = parser_type\n    st.session_state['node_parser_params'] = parser_params\n    \n    return parser, parser_typeBelow the node parser selection, I have also included a preview of the first node of the text after splitting/parsing, just to give the users an idea of how the chunking is actually happening based the selected node parser and the relevant parameters.select_response_synthesis_methodThis function allows users to choose how the RAG pipeline synthesizes responses. I have included varioud response synthesis methods supported by Llamaindex including refine, tree_summarize, compact, simple_summarize, accumulate and compact_accumulate.Users can click on the more information link to get more details about response synthesis and the different types.def select_response_synthesis_method():\n    st.header(\"Choose Response Synthesis Method\")\n    col1, col2 = st.columns([4,1])\n    with col2:\n        st.markdown(\"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                    \"\"\")\n    response_modes = [\n        \"refine\",\n        \"tree_summarize\",  \n        \"compact\", \n        \"simple_summarize\", \n        \"accumulate\", \n        \"compact_accumulate\"\n    ]\n    selected_mode = st.selectbox(\"Select Response Mode\", response_modes, on_change=reset_pipeline_generated)\n    response_mode = selected_mode\n    return response_mode, selected_modeselect_vector_storeEnables users to choose a vector store, which is a critical component for storing and retrieving embeddings in the RAG pipeline. This function supports the selection from multiple vector store options including Simple (Llamaindex default), Pinecone and Qdrant.def select_vector_store():\n    st.header(\"Choose Vector Store\")\n    vector_stores = [\"Simple\", \"Pinecone\", \"Qdrant\"]\n    selected_store = st.selectbox(\"Select Vector Store\", vector_stores, on_change=reset_pipeline_generated)\n\n    vector_store = None\n    if selected_store == \"Pinecone\":\n        pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n        index = pc.Index(\"test\")\n        vector_store = PineconeVectorStore(pinecone_index=index)\n    elif selected_store == \"Qdrant\":\n        client = qdrant_client.QdrantClient(location=\":memory:\")\n        vector_store = QdrantVectorStore(client=client, collection_name=\"sampledata\")\n    st.write(selected_store)\n    return vector_store, selected_storegenerate_rag_pipeline FunctionThis core function ties together the selected components to generate a RAG pipeline. It initializes the pipeline with the chosen LLM, embedding model, node parser, response synthesis method, and vector store. It is triggered by pressing the \u2018Generate RAG Pipeline\u2019 button.def generate_rag_pipeline(file, llm, embed_model, node_parser, response_mode, vector_store):\n    if vector_store is not None:\n        # Set storage context if vector_store is not None\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    else:\n        storage_context = None\n\n    # Create the service context\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n\n    # Create the vector index\n    vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=True)\n    if storage_context:\n        vector_index.storage_context.persist(persist_dir=\"persist_dir\")\n\n    # Create the query engine\n    query_engine = vector_index.as_query_engine(\n        response_mode=response_mode,\n        verbose=True,\n    )\n\n    return query_enginegenerate_code_snippet FunctionThis function is the culmination of the user\u2019s selections, generating the Python code necessary to implement the configured RAG pipeline. It dynamically constructs the code snippet based on the chosen LLM, embedding model, node parser, response synthesis method, and vector store, including the parameters set for the node parser.def generate_code_snippet(llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice):\n    node_parser_params = st.session_state.get('node_parser_params', {})\n    print(node_parser_params)\n    code_snippet = \"from llama_index.llms import OpenAI, Gemini, Cohere\\n\"\n    code_snippet += \"from llama_index.embeddings import HuggingFaceEmbedding\\n\"\n    code_snippet += \"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n\"\n    code_snippet += \"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n\"\n    code_snippet += \"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n\"\n    code_snippet += \"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\\n\"\n    code_snippet += \"import qdrant_client\\n\\n\"\n\n    # LLM initialization\n    if llm_choice == \"GPT-3.5\":\n        code_snippet += \"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\\n\"\n    elif llm_choice == \"GPT-4\":\n        code_snippet += \"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\\n\"\n    elif llm_choice == \"Gemini\":\n        code_snippet += \"llm = Gemini(model='models/gemini-pro')\\n\"\n    elif llm_choice == \"Cohere\":\n        code_snippet += \"llm = Cohere(model='command', api_key='&lt;YOUR_API_KEY&gt;')  # Replace &lt;YOUR_API_KEY&gt; with your actual API key\\n\"\n\n    # Embedding model initialization\n    code_snippet += f\"embed_model = HuggingFaceEmbedding(model_name='{embed_model_choice}')\\n\\n\"\n\n    # Node parser initialization\n    node_parsers = {\n        \"SentenceSplitter\": f\"SentenceSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n        \"CodeSplitter\": f\"CodeSplitter(language={node_parser_params.get('language', 'python')}, chunk_lines={node_parser_params.get('chunk_lines', 40)}, chunk_lines_overlap={node_parser_params.get('chunk_lines_overlap', 15)}, max_chars={node_parser_params.get('max_chars', 1500)})\",\n        \"SemanticSplitterNodeParser\": f\"SemanticSplitterNodeParser(buffer_size={node_parser_params.get('buffer_size', 1)}, breakpoint_percentile_threshold={node_parser_params.get('breakpoint_percentile_threshold', 95)}, embed_model=embed_model)\",\n        \"TokenTextSplitter\": f\"TokenTextSplitter(chunk_size={node_parser_params.get('chunk_size', 1024)}, chunk_overlap={node_parser_params.get('chunk_overlap', 20)})\",\n        \"HTMLNodeParser\": f\"HTMLNodeParser(tags={node_parser_params.get('tags', ['p', 'h1'])})\",  \n        \"JSONNodeParser\": \"JSONNodeParser()\",\n        \"MarkdownNodeParser\": \"MarkdownNodeParser()\"\n    }\n    code_snippet += f\"node_parser = {node_parsers[node_parser_choice]}\\n\\n\"\n\n    # Response mode\n    code_snippet += f\"response_mode = '{response_mode}'\\n\\n\"\n\n    # Vector store initialization\n    if vector_store_choice == \"Pinecone\":\n        code_snippet += \"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\"\n        code_snippet += \"index = pc.Index('test')\\n\"\n        code_snippet += \"vector_store = PineconeVectorStore(pinecone_index=index)\\n\"\n    elif vector_store_choice == \"Qdrant\":\n        code_snippet += \"client = qdrant_client.QdrantClient(location=':memory:')\\n\"\n        code_snippet += \"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\\n\"\n    elif vector_store_choice == \"Simple\":\n        code_snippet += \"vector_store = None  # Simple in-memory vector store selected\\n\"\n\n    code_snippet += \"\\n# Finalizing the RAG pipeline setup\\n\"\n    code_snippet += \"if vector_store is not None:\\n\"\n    code_snippet += \"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\"\n    code_snippet += \"else:\\n\"\n    code_snippet += \"    storage_context = None\\n\\n\"\n\n    code_snippet += \"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n\"\n\n    code_snippet += \"_file = 'path_to_your_file'  # Replace with the path to your file\\n\"\n    code_snippet += \"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\"\n    code_snippet += \"if storage_context:\\n\"\n    code_snippet += \"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\"\n\n    code_snippet += \"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\"\n\n    return code_snippetConclusionRAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It\u2019s designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation.Your insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let\u2019s start a conversation on Linkedin. I\u2019m always eager to collaborate and share knowledge with fellow tech adventurers.GitHub RepoConnect with Me on LinkedInLive Demo",
            "related_posts": [
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-12",
                "LlamaIndex Newsletter 2024\u201302\u201327"
            ],
            "link": "https://www.llamaindex.ai/blog//ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"
        },
        {
            "date": "May 21, 2024",
            "author": "LlamaIndex",
            "title": "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions",
            "tags": [
                "microsoft",
                "azure"
            ],
            "content": "One of the many amazing feats that LLMs are capable of is generating executable code. This can be used to solve a variety of complex problems that require calculations and fixed logic that traditional computing excels at but LLMs can struggle to perform directly. When building agents to perform complex tasks, equipping your agent with code execution as an available tool can be a powerful strategy.However, this strategy comes with a major drawback: executable code can be flawed or even dangerous to execute, and detecting whether code will be problematic prior to executing it is arguably an expression of the Halting Problem, making it impossible to guarantee success at detection.The solution is sandboxing, to isolate potentially problematic code from the host environment. Now, thanks to dynamic sessions in Azure Container Apps, the ability to execute sandboxed code generated by an LLM is simple directly from LlamaIndex. It\u2019s implemented as a tool that can be used by any LlamaIndex agent.In this blog post we\u2019ll show you exactly how to use the new Azure Code Interpreter tool and walk you through a couple of examples of how to make the most of it. You can see the full code in this notebook and read more in the tool documentation on LlamaHub and on learn.microsoft.com.Set up Azure Container Apps dynamic sessionsFirst, install our python packages including the tool:pip install llama-index\npip install llama-index-llms-azure\npip install llama-index-tools-azure-code-interpreterIn the notebook we\u2019re using GPT 3.5 Turbo hosted on Azure as the LLM, but you can use any LLM capable of tool use:from llama_index.llms.azure_openai import AzureOpenAI\nllm = AzureOpenAI(\n    model=\"gpt-35-turbo\",\n    deployment_name=\"gpt-35-deploy\",\n    api_key=api_key,\n    azure_endpoint=azure_endpoint,\n    api_version=api_version,\n)Once you\u2019ve got your LLM set up, you\u2019ll need to create a session pool to host your executions. Doing this will give you a pool management endpoint URL that you can provide to LlamaIndex like this:# Import the AzureCodeInterpreterToolSpec from llama_index\nfrom llama_index.tools.azure_code_interpreter import (\n    AzureCodeInterpreterToolSpec,\n)\n\n# Create the AzureCodeInterpreterToolSpec with the pool_managment_endpoint set to your session management endpoint\n# It is optional to set the local_save_path, but it is recommended to set it to a path where the tool can automatically save any intermediate data generated from Python code's output.\nazure_code_interpreter_spec = AzureCodeInterpreterToolSpec(\n    pool_managment_endpoint=\"your-pool-management-endpoint\",\n    local_save_path=\"local-file-path-to-save-intermediate-data\",\n)This sets up a tool ready to be used with LlamaIndex. You\u2019re now ready to set up your agent:# Import the ReActAgent\nfrom llama_index.core.agent import ReActAgent\n\n# Create the ReActAgent and inject the tools defined in the AzureDynamicSessionsToolSpec\nagent = ReActAgent.from_tools(\n    azure_code_interpreter_spec.to_tool_list(), llm=llm, verbose=True\n)In this example we\u2019re providing only a single tool, but you could provide any other tools you like to your ReAct agent. Now you\u2019ve got an agent, you\u2019re ready to ask it to perform tasks!Dynamic sessions code interpreter in actionIn our first example, we\u2019re going to ask the agent the time in Seattle. This is usually a tricky task for LLMs, which don\u2019t know what time it is anywhere!# Test the agent with simple answers that could leverage Python codes\nprint(agent.chat(\"Tell me the current time in Seattle.\"))The agent generates python code to determine the time and convert it to the correct time zone. It passes this code to Azure Container Apps dynamic sessions, which execute the code and return the answer:Thought: To provide the current time in Seattle, I need to calculate it based on the current UTC time and adjust for Seattle's time zone, which is Pacific Daylight Time (PDT) during daylight saving time and Pacific Standard Time (PST) outside of daylight saving time. PDT is UTC-7, and PST is UTC-8. I can use the code interpreter tool to get the current UTC time and adjust it accordingly.\nAction: code_interpreter\nAction Input: {'python_code': \"from datetime import datetime, timedelta; import pytz; utc_now = datetime.now(pytz.utc); seattle_time = utc_now.astimezone(pytz.timezone('America/Los_Angeles')); seattle_time.strftime('%Y-%m-%d %H:%M:%S %Z%z')\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '2024-05-04 13:54:09 PDT-0700', 'executionTimeInMilliseconds': 120}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The current time in Seattle is 2024-05-04 13:54:09 PDT.\nThe current time in Seattle is 2024-05-04 13:54:09 PDT.\nYou can also use the tool to safely inspect and manipulate data, as in this example where we ask it to open a CSV file and answer questions about it:# Upload a sample temperature file of a day in Redmond Washington and ask a question about it\nres = azure_code_interpreter_spec.upload_file(\n    local_file_path=\"./TemperatureData.csv\"\n)\nif len(res) != 0:\n    print(\n        agent.chat(\"Find the highest temperature in the file that I uploaded.\")\n    )It doesn\u2019t just read data from the CSV, it performs math on it to determine the highest temperature:Thought: I need to use the list_files tool to get the metadata for the uploaded file, and then use python to read the file and find the highest temperature.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: I have the metadata for the file. I need to use python to read the file and find the highest temperature.\nAction: code_interpreter\nAction Input: {'python_code': \"import csv\\n\\nwith open('/mnt/data/TemperatureData.csv', 'r') as f:\\n    reader = csv.reader(f)\\n    next(reader)\\n    highest_temp = float('-inf')\\n    for row in reader:\\n        temp = float(row[1])\\n        if temp > highest_temp:\\n            highest_temp = temp\\nprint(highest_temp)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '12.4\\n', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 26}\nThought: I have the highest temperature. I can answer the question.\nAnswer: The highest temperature in the file is 12.4 degrees.\nThe highest temperature in the file is 12.4 degrees.The tool can also safely modify your uploaded files, as in this example where we get it to sort the data in the CSV:# Ask the LLM to perform modifications on the sample temperature file\nprint(\n    agent.chat(\n        \"Rearrange the temperature data in a descending order and save it back to the original csv file.\"\n    )\n)In this example, you\u2019ll see that the LLM initially produces code with a syntax error, but the agent is smart enough to figure out what went wrong and correct itself:Thought: To proceed, I need to first list the files in the session pool to identify the uploaded temperature data file.\nAction: list_files\nAction Input: {}\nObservation: [RemoteFileMetadata(filename='TemperatureCurve.png', size_in_bytes=35512, file_full_path='/mnt/data/TemperatureCurve.png'), RemoteFileMetadata(filename='TemperatureData.csv', size_in_bytes=514, file_full_path='/mnt/data/TemperatureData.csv')]\nThought: The temperature data file is identified as 'TemperatureData.csv'. I need to read its contents, rearrange the temperature data in descending order, and then save the changes back to the original CSV file.\nAction: code_interpreter\nAction Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\\n'filename': 'TemperatureData.csv'\"}\nObservation: {'$id': '1', 'status': 'Failure', 'stdout': '', 'stderr': \"illegal target for annotation (3407605186.py, line 6)  Cell In[2363], line 6\\n    'filename': 'TemperatureData.csv'\\n    ^\\nSyntaxError: illegal target for annotation\\n\\n\", 'result': None, 'executionTimeInMilliseconds': 9}\nThought: There was a syntax error in the Python code due to an incorrect line. I need to correct the code and execute it again to sort the temperature data and save it back to the CSV file.\nAction: code_interpreter\nAction Input: {'python_code': \"import pandas as pd\\n\\ndf = pd.read_csv('/mnt/data/TemperatureData.csv')\\ndf_sorted = df.sort_values(by='Temperature', ascending=False)\\ndf_sorted.to_csv('/mnt/data/TemperatureData.csv', index=False)\"}\nObservation: {'$id': '1', 'status': 'Success', 'stdout': '', 'stderr': '', 'result': '', 'executionTimeInMilliseconds': 91}\nThought: I can answer without using any more tools. I'll use the user's language to answer.\nAnswer: The temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.\nThe temperature data has been successfully rearranged in descending order and saved back to the original CSV file, 'TemperatureData.csv'.Modifying files would not be useful if you couldn\u2019t retrieve them after modification, which is done like this:# Download the modified file\nazure_code_interpreter_spec.download_file_to_local(\n    remote_file_path=\"TemperatureData.csv\",\n    local_file_path=\"/.../SortedTemperatureData.csv\",\n)Endless possibilitiesThe scope of tasks that you can achieve with sandboxed code execution is as broad as programming itself, and having safe execution guaranteed allows you to confidently hand agents tasks that previously you might have been hesitant about. We think this is an amazing addition to our LLM agent capabilities and we\u2019re excited to see what you build with it.",
            "related_posts": [
                "Secure code execution in LlamaIndex with Azure Container Apps dynamic sessions"
            ],
            "link": "https://www.llamaindex.ai/blog//secure-code-execution-in-llamaindex-with-azure-container-apps-dynamic-sessions"
        },
        {
            "date": "Apr 9, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-04-09",
            "tags": [],
            "content": "Hello, LlamaIndex members! \ud83e\udd99Welcome to another thrilling weekly update from LlamaUniverse! We're excited to present a variety of outstanding updates, including Anthropic's Function Calling, Cookbooks, RankLLM, Guides, Tutorials, and much more.\ud83e\udd29\u00a0The highlights:Anthropic's Claude Function Calling Agent: Enhance QA/RAG and workflow automation with advanced tool calling in an agent framework. Notebook, Tweet.RankLLM Integration: RankLLM is an open-source LLM collection for reranking, surpassing GPT-4 based alternatives is now integrated with LlamaIndex. Notebook, Tweet.LlamaIndex + MistralAI Cookbook Series: Launched a cookbook series with MistralAI for building diverse RAG applications, from basic to advanced, with distinctive methods and abstractions. Cookbooks, Tweet\u2728 Feature Releases and Enhancements:We have introduced the Anthropic\u2019s Claude Function Calling Agent, leveraging advanced tool calling capabilities within an agent framework for enhanced QA/RAG and workflow automation. Notebook, Tweet.RankLLM (by Ronak Pradeep) integration with LlamaIndex - an open-source LLM collection fine-tuned for reranking, offering top-notch results and outperforming GPT-4 based rerankers. Notebook, Tweet.We have launched the LlamaIndex + MistralAI Cookbook Series for creating a range of RAG applications, from simple setups to advanced agents, featuring unique abstractions and techniques. Cookbooks, TweetWe launched create-llama for building full-stack RAG/agent applications with a single CLI command, akin to create-react-app, for a comprehensive chatbot setup including tool use. Tweet.\ud83c\udfa5\u00a0Demos:AutoRAG by Marker-Inc-Korea: Streamline RAG pipeline optimization with an automated three-step process, from data preparation to evaluation and optimal pipeline adoption, enhancing the efficiency of the RAG pipeline using LlamaIndex.\ud83d\uddfa\ufe0f Guides:Guide to Building Advanced RAG with Temporal Filters: Learn how to enhance your RAG pipeline with time-based metadata for more effective financial report analysis using LlamaIndex and KDB.AI vector store.Guide to Adaptive RAG for dynamically selecting RAG strategies based on query complexity, enhancing efficiency across varying question types.\u270d\ufe0f Tutorials:(\u03bbx.x)eranga\u2019s tutorial on the step-by-step process for building RAG with local models (LlamaIndex, Ollama, HuggingFace Embeddings, ChromaDB) and wrapping it all in a Flask server.Ivan Ilin\u2019s video tutorial on iki.ai - an LLM-powered digital library, for organizing, and sharing information within teams or organizations.Tutorial on scaling LLM Applications with Koyeb on deploying a full-stack RAG application globally without infrastructure setup, using Koyeb, LlamaIndex.TS, and TogetherAI.Ankush Singal's tutorial on Building Multi-Document Agents with LlamaIndex covers advanced multi-document agent concepts, where documents serve as sub-agents enabling complex QA, semantic search, and summarization.Rohan\u2019s tutorial on building a Full-Stack RAG application that streams intermediate results to visual UI components with event queues and server-side events.Hanane Dupouy's tutorial on building a Finance Agent using an LLM with Yahoo Finance and LlamaIndex abstractions to analyze financial data for publicly traded companies, covering everything from balance sheets to stock recommendations.\ud83c\udfa5\u00a0Webinars:Webinar with Daniel Huynh ****featuring LaVague - an agent that can navigate the web in your Jupyter/Colab notebook.Webinar with Logan Kelly featuring CallSine that utilizes LlamaIndex abstractions and LLMs for personalized sales outreach.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-04-09"
        },
        {
            "date": "Sep 21, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex Update \u2014 20/09/2023",
            "tags": [
                "llamaindex",
                "llm",
                "openai",
                "ai",
                "nlp"
            ],
            "content": "Hello LlamaIndex Enthusiasts!Welcome to the fifth edition of our LlamaIndex Update series.Most Important Takeaways:We\u2019ve open-sourced SECInsights.ai \u2014 your gateway to the production RAG framework.Replit templates \u2014 kickstart your projects with zero environment setup hassles.Build RAG from scratch and get hands-on with our processes.But wait, there\u2019s more!Feature Releases and EnhancementsFine-Tuning GuidesRetrieval Tips for RAGBuilding RAG from Scratch GuidesTutorialsIntegration with External PlatformsEventsWebinarsSo, let\u2019s embark on this journey together. Dive in and explore the offerings of the fifth edition of the LlamaIndex Update series!Feature Releases and EnhancementsOpen-Sourced RAG Platform: LlamaIndex open-sourced http://secinsights.ai, accelerating RAG app development with chat-based Q&A features. TweetLinear Adapter Fine-Tuning: LlamaIndex enables efficient fine-tuning of linear adapters on any embedding without re-embedding, enhancing retrieval/RAG across various models. Tweet, Docs, BlogPostHierarchical Agents: By structuring LLM agents in a parent-child hierarchy, we enhance complex search and retrieval tasks across diverse data, offering more reliability than a standalone agent. TweetSummaryIndex: We\u2019ve renamed ListIndex to SummaryIndex to make it clearer what its main functionality is. Backward compatibility is maintained for existing code using ListIndex. TweetEvaluation: LlamaIndex\u2019s new RAG evaluation toolkit offers async capabilities, diverse assessment criteria, and a centralized BaseEvaluator for easier developer integrations. Tweet, Docs.Hybrid Search for Postgres/pgvector: LlamaIndex introduces a hybrid search for Postgres/pgvector. Tweet, Docs.Replit Templates: LlamaIndex partners with Replit for easy LLM app templates, including ready-to-use Streamlit apps and full Typescript templates. Tweet, Replit Templates.LlamaIndex.TS:Launches with MongoDBReader and type-safe metadata. Tweet.Launches with chat history, enhanced keyword index, and Notion DB support. Tweet.Fine-Tuning Guides:OpenAI Fine-Tuning: LlamaIndex unveils a fresh guide on harnessing OpenAI fine-tuning to embed knowledge from any text corpus. In short: generate QA pairs with GPT-4, format them into a training dataset, and proceed to fine-tuning. Tweet, Docs.Embedding Fine-Tuning: LlamaIndex has a more advanced embedding fine-tuning feature, enabling complex NN query transformations on any embedding, including custom ones, and offering the ability to save intermediate checkpoints for enhanced model control. Tweet, Docs.Retrieval Tips For RAG:Use references (smaller chunks or summaries) instead of embedding full text.Results in 10\u201320 % improvement.Embeddings decoupled from main text chunks.Smaller references allow efficient LLM synthesis.Deduplication applied for repetitive references.Evaluated using synthetic dataset; 20\u201325% MRR boost.TweetBuilding RAG from Scratch Guides:Build Data Ingestion from scratch. Docs.Build Retrieval from scratch. Docs.Build Vector Store from scratch. Docs.Build Response Synthesis from scratch. Docs.Build Router from scratch. Docs.Build Evaluation from scratch. Docs.Tutorials:Wenqi Glantz tutorial on Fine-Tuning GPT-3.5 RAG Pipeline with GPT-4 Training Data with LlamaIndex fine-tuning abstractions.Wenqi Glantz tutorial on Fine-Tuning Your Embedding Model to Maximize Relevance Retrieval in RAG Pipeline with LlamaIndex.Tutorials from the LlamaIndex Team.Sourabh tutorial on SEC Insights, End-to-End Guide on secinsights.aiAdam\u2019s tutorial on Custom Tools for Data Agents.Logan tutorial on retrieval/reranking, covering Node Parsing, AutoMergingRetriever, HierarchicalNodeParser, node post-processors, and the setup of a RouterQueryEngine.Integrations with External PlatformsIntegration with PortkeyAI: LlamaIndex integrates with PortkeyAI, boosting LLM providers like OpenAI with features like auto fallbacks and load balancing. Tweet, DocumentationCollaboration with Anyscale: LlamaIndex collaborates with anyscalecompute, enabling easy tuning of open-source LLMs using Ray Serve/Train. Tweet, DocumentationIntegration with Elastic: LlamaIndex integrates with Elastic, enhancing capabilities such as vector search, text search, hybrid search models, enhanced metadata handling, and es_filters. Tweet, DocumentationIntegration with MultiOn: LlamaIndex integrates with MultiOn, enabling data agents to navigate the web and handle tasks via an LLM-designed browser. Tweet, DocumentationIntegration with Vectara: LlamaIndex collaborates with Vectara to streamline RAG processes from loaders to databases. Tweet, Blog PostIntegration with LiteLLM: LlamaIndex integrates with LiteLLM, offering access to over 100 LLM APIs and features like chat, streaming, and async operations. Tweet, DocumentationIntegration with MonsterAPI: LlamaIndex integrates with MonsterAPI, allowing users to query data using LLMs like Llama 2 and Falcon. Tweet, Blog PostEvents:Jerry Liu spoke on Production Ready LLM Applications at the Arize AI event.Ravi Theja conducted a workshop at LlamaIndex + Replit Pune Generative AI meetup.Jerry Liu session on Building a Lending Criteria Chatbot in Production with Stelios from MQube.Webinars:Webinar on How to Win an LLM Hackathon by Alex Reibman, Rahul Parundekar, Caroline Frasca, and Yi Ding.Webinar on LLM Challenges in Production with Mayo Oshin, AI Jason, and Dylan.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-update-20-09-2023-86ed66f78bac"
        },
        {
            "date": "Jun 23, 2023",
            "author": "Jerry Liu",
            "title": "Build and Evaluate LLM Apps with LlamaIndex and TruLens",
            "tags": [
                "ai",
                "mlops",
                "large-language-models",
                "nlp"
            ],
            "content": "Authors: Anupam Datta, Shayak Sen, Jerry Liu, Simon SuoSource Link: https://truera.com/build-and-evaluate-llm-apps-with-llamaindex-and-trulens/LlamaIndex is a popular open source framework for building LLM apps. TruLens is an open source library for evaluating, tracking, and iterating on LLM apps to improve their quality. The LlamaIndex and TruLens teams are actively collaborating to enable LLM app developers to rapidly build, evaluate, and iterate on their apps.In the latest release of TruLens, we introduce tracing for LlamaIndex based LLM applications that allow you to evaluate and track your experiments with just a few lines of code. This lets you automatically evaluate a number of different components of the application stack including:App inputs and outputsLLM callsRetrieved context chunks from an indexLatencyCost and Token Counts (coming soon!)Check out this notebook to get started and read along to get a step by step view.How do I actually use this?Build A LlamaIndex AppLlamaIndex lets you connect your data to LLMs and rapidly build applications for a number of different use cases.from llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('llama_index/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()Once you build your app, you can easily query your data:response = query_engine.query(\"What did the author do growing up?\")\nprint(response)And you get an appropriate response.Growing up, the author wrote short stories, programmed on an IBM 1401, and nagged his father to buy him a TRS-80 microcomputer. He wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, but switched to AI after becoming bored with it. He then took art classes at Harvard and applied to art schools, eventually attending RISD.Wrap A LlamaIndex App with TruLensWith TruLens, you can wrap LlamaIndex query engines with a TruLlama wrapper. This wrapper preserves all LlamaIndex behavior, but traces all of the intermediate steps so that they can be individually evaluated.from trulens_eval import TruLlama\nl = TruLlama(query_engine)The wrapped app can now be queried in the exact same way:response = l.query(\"What did the author do growing up?\")\nprint(response)Except, now the details of the query are logged by TruLens.Add Feedback FunctionsNow to evaluate the behavior of your models, we can add feedback functions to your wrapped application. Note that as a developer you only need to add a few lines of code to start using feedback functions in your apps. You can also easily add functions tailored to the needs of your application.Our goal with feedback functions is to programmatically check the app for quality metrics.The first feedback function checks for language match between the prompt and the response. It\u2019s a useful check since a natural user expectation is that the response is in the same language as the prompt. It is implemented with a call to a HuggingFace API that programmatically checks for language match.The next feedback function checks how relevant the answer is to the question by using an Open AI LLM that is prompted to produce a relevance score.Finally, the third feedback function checks how relevant individual chunks retrieved from the vector database are to the question, again using an OpenAI LLM in a similar manner. This is useful because the retrieval step from a vector database may produce chunks that are not relevant to the question and the quality of the final response would be better if these chunks are filtered out before producing the final response.from trulens_eval import TruLlama, Tru, Query, Feedback, feedback\n\n# Initialize Huggingface-based feedback function collection class:\nhugs = feedback.Huggingface()\nopenai = feedback.OpenAI()\n# Define a language match feedback function using HuggingFace.\nf_lang_match = Feedback(hugs.language_match).on_input_output()\n# By default this will check language match on the main app input and main app\n# output.\n\n# Question/answer relevance between overall question and answer.\nf_qa_relevance = Feedback(openai.relevance).on_input_output()\n\n# Question/statement relevance between question and each context chunk.\nf_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n    TruLlama.select_source_nodes().node.text\n).aggregate(np.min)\n\n\nfeedbacks = [f_lang_match, f_qa_relevance, f_qs_relevance]\n\nl = TruLlama(app=query_engine, feedbacks=feedbacks)Explore In DashboardEvery query that is tracked can now be viewed in the TruLens dashboard. After running the feedback functions on a set of records (interactions), you can see the aggregate results of the evaluation on a leaderboard; then drill down into an app version and examine how it is performing on individual records. These steps can help you understand the quality of an app version and its failure modes.In this example, the model is doing fairly well on the relevance and language match feedback evaluations, but seems to be doing poorly on qs_relevance. This can be an indicator that the retrieved chunks are often irrelevant. This can be a significant source of \u201challucinations\u201d in retrieval-augmented generative AI apps.We can now drill down and identify specific instances where this may be an issue:Let\u2019s look at a good example first. \u201cWhat did the author do growing up?\u201dIn this example, we retrieved two chunks from the index both of which were fairly relevant to the the question and as a result the LLM summarizes it into a relevant and factually correct answer.On the other hand, let\u2019s look at an example where this didn\u2019t go so well: \u201cWhere was the author born?\u201d. In this example, the app confidently provides an incorrect answer.In this example, the two pieces of context retrieved had moderate relevance to the question. Further, neither context contained the answer. Even though our relevance feedback function (which doesn\u2019t check for factual correctness) didn\u2019t detect an issue, because the underlying chunks were not very relevant, this was a strong indicator that something was off. Indeed, this is an example of the model hallucinating on a question that is fairly easy to fact check.Iterate on your AppOnce you find issues like this with your app, it can be helpful to iterate on your prompts, models and chunking approaches to optimize your app. As you do this, you can track the performance of each version of your model with TruLens. Here is an example of a dashboard with multiple iterations testing against each other.",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c"
        },
        {
            "date": "Mar 19, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-03-19",
            "tags": [
                "llamaparse",
                "ai",
                "llm",
                "newsletter"
            ],
            "content": "Greetings, LlamaIndex enthusiasts! \ud83e\udd99Welcome to another exciting weekly update from the world of LlamaVerse!We have an amazing news for you from LlamaIndex. We've officially launched LlamaParse, a GenAI-native document parsing solution. With state-of-the-art table and chart extraction, natural language steerable instructions, and compatibility with over a dozen document types, LlamaParse excels in creating accurate RAG applications from complex documents. After a successful private preview with 2k users and 1M pages parsed, it's now ready to transform your document handling. Check out our launch post for all the details!\ud83e\udd29\u00a0The highlights:New observability with Instrumentation: Enhanced developer workflow with a new Instrumentation module for improved observability. Docs, Tweet.LlamaParse accepts natural language parsing instructions: Easily extract math snippets from PDFs into LaTeX with LlamaParse. Blogpost, Tweet.Financial Data Parsing: Transform PowerPoint parsing, utilizing LlamaParse to extract and interpret complex financial data from .pptx files, enabling detailed and accurate financial analysis. Notebook, Tweet.\u2728 Feature Releases and Enhancements:We introduced LlamaIndex v0.10.20, featuring our new Instrumentation module, a leap in observability that simplifies developer workflows by providing a module-level dispatcher, reducing the need for individual callback managers and facilitating comprehensive handler sets across your application. Docs, Tweet.We have launched parsing by prompting feature in LlamaParse to properly extract out any math snippets from PDFs into LaTex which helps you to plug easily into your RAG pipeline. Blogpost, Tweet.We have launched an advanced RAG pipeline for Financial PowerPoints, using LlamaParse to tackle the challenge of parsing .pptx files. Our solution accurately extracts slides, including text, tables, and charts, enabling precise question-answering over complex financial data. Notebook, Tweet.We collaborated with langfuse to launch open-source observability for your RAG pipeline, enhancing your application with integrated tracing, prompt management, and evaluation in just two lines of code. Blogpost, Docs, Tweet.Search-in-the-Chain: a method by Shicheng Xu et al., is now integrated into LlamaIndex, enhancing question-answering with an advanced system that interleaves retrieval and planning. This approach verifies each reasoning step in a chain, allowing for dynamic replanning and application in various agent reasoning contexts. LlamaPack, Tweet\ud83c\udfa5\u00a0Demos:Home AI, a tool created with create-llama, to help home searches by using LLMs to automate the parsing of complex property disclosures, enabling users to filter searches with unprecedented detail and efficiency. Blogpost, Code, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to using LlamaIndex and Mathpix to parse, index, and query complex mathematics within scientific papers, detailing steps from parsing tables and extracting images to indexing in a RAG app and answering questions with precise LaTeX outputs, to showcase hierarchical retrieval technique.\u270d\ufe0f Tutorials:Thomas Reid\u2019s tutorial on using LlamaParse can help properly extract text from a Tesla quarterly filings.Sudarshan Koirala video tutorial on RAG with LlamaParse, Qdrant, and Groq.Kyosuke Morita tutorial showing how to match a candidate to jobs based on their CV with LlamaParse + LlamaIndex.Cobus Greyling tutorial on Agentic RAG: Context-Augmented OpenAI Agents.Roey Ben Chaim\u2019s tutorial on PII Detector: hacking privacy in RAG.\ud83c\udfa5\u00a0Webinars:Webinar with Charles Packer, lead author of MemGPT on Long-Term, Self-Editing Memory with MemGPT\ud83d\udcc5\u00a0Events:We are hosting a RAG meetup in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB",
                "LlamaIndex Newsletter 2024-04-02"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-03-19"
        },
        {
            "date": "Feb 20, 2024",
            "author": "Jerry Liu",
            "title": "Introducing LlamaCloud and LlamaParse",
            "tags": [
                "llamaindex",
                "llm",
                "ai",
                "retrieval-augmented",
                "data"
            ],
            "content": "Today is a big day for the LlamaIndex ecosystem: we are announcing LlamaCloud, a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.Using LlamaCloud as an enterprise AI engineer, you can focus on writing the business logic and not on data wrangling. Process large volumes of production data, immediately leading to better response quality. LlamaCloud launches with the following key components:LlamaParse: Proprietary parsing for complex documents with embedded objects such as tables and figures. LlamaParse directly integrates with LlamaIndex ingestion and retrieval to let you build retrieval over complex, semi-structured documents. You\u2019ll be able to answer complex questions that simply weren\u2019t possible previously.Managed Ingestion and Retrieval API: An API which allows you to easily load, process, and store data for your RAG app and consume it in any language. Backed by data sources in LlamaHub, including LlamaParse, and our data storage integrations.LlamaParse is available in a public preview setting starting today. It can currently handle PDFs and usage is capped for public use; contact us for commercial terms. The managed ingestion and retrieval API is available as a private preview; we are offering access to a limited set of enterprise design partners. If you\u2019re interested, get in touch. (We\u2019ve also launched a new version of our website \ud83e\udd99!)RAG is Only as Good as your DataA core promise of LLMs is the ability to automate knowledge search, synthesis, extraction, and planning over any source of unstructured data. Over the past year a new data stack has emerged to power these context-augmented LLM applications, popularly referred to as Retrieval-Augmented Generation (RAG). This stack includes loading data, processing it, embedding it, and loading into a vector database. This enables downstream orchestration of retrieval and prompting to provide context within an LLM app.This stack is different from any ETL stack before it, because unlike traditional software, every decision in the data stack directly affects the accuracy of the full LLM-powered system. Every decision like chunk size and embedding model affects LLM outputs, and since LLMs are black boxes, you can\u2019t unit test your way to correct behavior.We\u2019ve spent the past year hard at work at the forefront of providing tooling and educating users on how to build high-performing, advanced RAG for various use cases. We crossed the 2M monthly download mark, and are used by large enterprises to startups, including Adyen, T-Systems, Jasper.ai, Weights and Biases, DataStax, and many more.But while getting started with our famous 5-line starter example is easy, building production-grade RAG remains a complex and subtle problem. In our hundreds of user conversations, we learned the biggest pain points:Results aren\u2019t accurate enough: The application was not able to produce satisfactory results for a long-tail of input tasks/queries.The number of parameters to tune is overwhelming: It\u2019s not clear which parameters across the data parsing, ingestion, retrieval.PDFs are specifically a problem: I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it?Data syncing is a challenge: Production data often updates regularly, and continuously syncing new data brings a new set of challenges.These are the problems we set out to solve with LlamaCloud.Data Pipelines to Bring you to ProductionWe built LlamaCloud and LlamaParse as the data pipelines to get your RAG application to production more quickly.LlamaParseLlamaParse is a state-of-the-art parser designed to specifically unlock RAG over complex PDFs with embedded tables and charts. This simply wasn\u2019t possible before with other approaches, and we\u2019re incredibly excited about this technology.LlamaParse Demo. Given a PDF file, returns a parsed markdown file that maintains semantic structure within the document.For the past few months we\u2019ve been obsessed with this problem. This is a surprisingly prevalent use case across a variety of data types and verticals, from ArXiv papers to 10K filings to medical reports.Naive chunking and retrieval algorithms do terribly. We were the first to propose a novel recursive retrieval RAG technique for being able to hierarchically index and query over tables and text in a document. The only challenge that remained was how to properly parse out tables and text in the first place.Comparison of LlamaParse vs. PyPDF over the Apple 10K filing. Full comparisons are here. A green highlight in a cell means that the RAG pipeline correctly returned the cell value as the answer to a question over that cell. A red highlight means that the question was answered incorrectly.This is where LlamaParse comes in. We\u2019ve developed a proprietary parsing service that is incredibly good at parsing PDFs with complex tables into a well-structured markdown format. This representation directly plugs into the advanced Markdown parsing and recursive retrieval algorithms available in the open-source library. The end result is that you are able to build RAG over complex documents that can answer questions over both tabular and unstructured data. Check out the results below for a comparison:Comparison of baseline PDF approach (top) vs. LlamaParse + advanced retrieval (bottom)Results over the Uber 10K Dataset. For more information on our evaluation metrics check out our evaluation page here.This service is available in a public preview mode: available to everyone, but with a usage limit (1k pages per day). It operates as a standalone service that also plugs into our managed ingestion and retrieval API (see below). Check out our LlamaParse onboarding here for more details.from llama_parse import LlamaParse\n\nparser = LlamaParse(\n    api_key=\"llx-...\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n    verbose=True\n)For unlimited commercial use of LlamaParse, get in touch with us.Next StepsOur early users have already given us important feedback on what they\u2019d like to see next. Currently we primarily support PDFs with tables, but we are also building out better support for figures, and and an expanded set of the most popular document types: .docx, .pptx, .html.Managed Ingestion and RetrievalOur other main offering in LlamaCloud is a managed ingestion and retrieval API which allows you to easily declare performant data pipelines for any context-augmented LLM application.Get clean data for your LLM application, so you can spend less time wrangling data and more time writing core application logic. LlamaCloud empowers enterprise developers with the following benefits:Engineering Time Savings: Instead of having to write custom connectors and parsing logic in Python, our APIs allow you to directly connect to different data sources.Performance: we provide good out-of-the-box performance for different data types, while offering an intuitive path for experimentation, evaluation, and improvement.Ease Systems Complexity: Handle a large number of data sources with incremental updates.Let\u2019s do a brief tour through the core components!Ingestion: Declare a managed pipeline to process and transform/chunk/embed data backed by our 150+ data sources in LlamaHub and our 40+ storage integrations as destinations. Automatically handle syncing and load balancing. Define through the UI or our open-source library.Retrieval: Get access to state-of-the-art, advanced retrieval backed by our open-source library and your data storage. Wrap it in an easy-to-use REST API that you can consume from any language.Playground: Interactive UI to test and refine your ingestion/retrieval strategies pre-deployment, with evaluations in the loop.LlamaCloud Playground: configure, evaluate, and optimize your ingestion/retrieval pipeline before deployment.LlamaCloud Retrieval: Access advanced retrieval over your storage system via an API.We are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you\u2019re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come talk to us.Launch Partners and CollaboratorsWe opened up access to LlamaParse at our hackathon we co-hosted with Futureproof Labs and Datastax at the beginning of February. We saw some incredible applications of LlamaParse in action, including parsing building codes for Accessory Dwelling Unit (ADU) planning, parsing real-estate disclosures for home buying, and dozens more.Eric Ciarla, co-founder at Mendable AI, incorporated LlamaParse into Mendable\u2019s data stack: \u201cWe integrated LlamaParse into our open source data connector repo which powers our production ingestion suite. It was easy to integrate and more powerful than any of the alternatives we tried.\u201dWe\u2019re also excited to be joined by initial launch partners and collaborators in the LLM and AI ecosystem, from storage to compute.DataStaxDatastax has incorporated LlamaParse into their RAGStack to bring a privacy-preserving out-of-the-box RAG solution for enterprises: \"Last week one of our customers Imprompt has launched a pioneering 'Chat-to-Everything' platform leveraging RAGStack powered by LlamaIndex to enhance their enterprise offerings while prioritizing privacy.\" said Davor Bonaci, CTO and executive vice president, DataStax. \"We're thrilled to partner with LlamaIndex to bring a streamlined solution to market. By incorporating LlamaIndex into RAGStack, we are providing enterprise developers with a comprehensive Gen AI stack that simplifies the complexities of RAG implementation, while offering long-term support and compatibility assurance.\u201dMongoDB\u201cMongoDB\u2019s partnership with LlamaIndex allows for the ingestion of data into the MongoDB Atlas Vector database, and the retrieval of the index from Atlas via LlamaParse and LlamaCloud, enabling the development of RAG systems and other AI applications,\u201d said Greg Maxson, Global Lead, AI Ecosystems at MongoDB. \u201cNow, developers can abstract complexities associated with data ingestion, simplify RAG pipeline implementations, and more cost effectively develop large language model applications, ultimately accelerating generative AI app development and more quickly bringing apps to market.\u201dQdrantAndr\u00e9 Zayarni, CEO of Qdrant, says \u201cThe Qdrant team is excited to partner with LlamaIndex to combine the power of optimal data preprocessing, vectorization, and ingestion with Qdrant for a powerful fullstack RAG solution.\u201dNVIDIAWe are also excited to collaborate with NVIDIA to integrate LlamaIndex with the NVIDIA AI Enterprise software platform for production AI: \u201cLlamaCloud will help enterprises get generative AI applications from development into production with connectors that link proprietary data to the power of large language models,\u201d said Justin Boitano, vice president, Enterprise and Edge Computing, NVIDIA. \u201cPairing LlamaCloud with NVIDIA AI Enterprise can accelerate the end-to-end LLM pipeline \u2014 including data processing, embedding creation, indexing, and model inference on accelerated computing across clouds, data centers and out to the edge.\u201dFAQIs this competitive with vector databases?No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are working hard to do the following:Integrate LlamaCloud with storage providers of existing design partnersMake LlamaCloud available in a more \u201cself-serve\u201d manner.Next StepsAs mentioned in the above sections, LlamaParse is available for public preview starting today with a usage cap. LlamaCloud is in a private preview mode; we are offering access to a limited set of enterprise design partners. If you\u2019re interested come talk to us!LlamaParse: Repo, Cookbook, Contact UsLlamaCloud: Contact Us",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-llamacloud-and-llamaparse-af8cedf9006b"
        },
        {
            "date": "Jul 10, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex Update \u2014 07/11/2023",
            "tags": [
                "llamaindex",
                "ai",
                "nlp",
                "large-language-models",
                "openai"
            ],
            "content": "Greetings once again, LlamaIndex community!Welcome back to our second installment in the LlamaIndex Update series. In our ongoing commitment to keep you informed and engaged with our rapidly evolving open-source project, this blog post brings you more exciting updates on features, webinars, hackathons, and community events.Building on the foundation of our inaugural post, we will continue to strive to keep both our long-standing contributors and fresh faces synced with our progress. We aim to not just inform but also inspire you to partake in our collective journey towards growth and innovation.Without further delay, let\u2019s delve into the latest happenings in this edition of the LlamaIndex Update.Features And Integrations:LlamaIndex\u2019s partnership with Anyscale uses the Ray platform to boost performance and deployment. It accelerates LlamaIndex\u2019s operations by a factor of ten and streamlines deployment to production servers. The core Ray Distributed Toolkit aids in efficient task parallelization, while Ray Serve ensures easy deployment of query engines to production.\nBlogpost, TweetLlamaIndex enhanced metadata representation in documents. The extra_info and node_info fields are now replaced with a metadata dictionary. This facilitates precise control over data and allows users to exclude metadata keys during embedding or LLM prediction. This boosts LLM and retrieval performance and offers customizable metadata injection, formatting, and template creation.\nDocs, TweetLlamaIndex supports both Text Completion API, involving output parsing and input prompt modification, and Structured API, requiring input function signatures and output conversion. Despite Structured API being easier to use, its limited availability keeps Text Completion API relevant. Both are supported by LlamaIndex\u2019s PydanticProgram.\nDocs, TweetLlamaIndex now collaborates with Chainlit.io, facilitating swift construction of advanced chat UIs for any LLM app. This integration, beyond providing a basic chat interface, also logs intermediate results and sources.\nBlogpost, TweetLlamaIndex now incorporates the DePlot model for interpreting charts and plots in QA/chatbot applications. Primarily effective for simple charts, such as bar charts and time series, DePlot converts these visuals into text format for easy embedding, indexing, and usage in downstream applications. This functionality is now accessible via the LlamaHub data loader, expanding LlamaIndex\u2019s capabilities for diverse applications.\nDocs, TweetLlamaIndex now incorporates the Github Issues reader, which allows for comprehensive loading and querying of issues from any GitHub repository. Additionally, the Sitemap Loader reader enables users to read all webpages from a specified sitemap.\nTweetLlamaIndex introduces the ContextRetrieverOpenAIAgent feature, which enhances tool picking by incorporating more context from user messages. It performs a retrieval step before the LLM call, ensuring increased reliability and better mapping of queries to the right tools, especially in the presence of domain-specific terms. Unlike a \u201cretrieval tool\u201d, this feature guarantees retrieval before any action is taken.\nDocs, TweetLlamaIndex now features code-based extraction for efficient data extraction from arbitrary text. This feature includes a \u201cFit\u201d step to generate functions based on training data, and an \u201cInference\u201d step to run these functions on new data. It offers two versions: DFEvaporateProgram for extracting one value per field from a text, and MultiValueEvaporateProgram for extracting multiple values per field. This feature can be used to extract structured data from raw HTML sources and also offers the ability to identify salient fields in a text given a topic.\nDocs, TweetLlamaIndex has significantly improved its text-to-SQL capabilities, offering a \u201cDefault\u201d SQL query engine and an SQL query engine with an object index for handling large table schemas. These upgrades simplify the process, requiring only a SQL database for the default engine and enabling indexing of large table schemas with the ObjectIndex. Additionally, LlamaIndex now also integrates with duckdb, further enhancing the SQL querying process.\nDocs_SQL, Docs_duckdb, TweetLlamaIndex 0.7.0 enhances modularity for LLM app development. It includes native LLM abstractions for platforms like OpenAI and Hugging Face, a standalone Response Synthesis module, and improved Document Metadata Management. These abstractions can be used independently or integrated into indices/query engines. The Response Synthesis module abstracts away context window limitations, while the Document Metadata Management feature allows deep customization of metadata, potentially boosting retrieval performance.\nBlogpost, TweetLlamaIndex introduces Recursive Retrieval, a concept that utilizes the hierarchical nature of knowledge. A Node in LlamaIndex can contain references to other retrievers or query engines. This process starts with a retriever and recursively explores links to others. For instance, structured tables from a PDF can be extracted, each represented as a data frame. These tables can be referenced by IndexNode objects embedded with other Nodes. During a query, if an IndexNode is among the top-k nodes, it triggers another retriever or query engine, allowing sophisticated querying overall data.\nDocs, TweetLlamaIndex introduces OpenAI agent streaming for efficient function calling and enhances user experience by providing progress bars during index creation for a real-time understanding of the process duration.\nTweetLlamaIndex introduces personalized data interaction through system prompts, callback events for SubQuestionQueryEngine, and a streamlined process for Azure OpenAI integration.\nDocs_AOI, Notebook_personality, TweetLlamaIndex leverages LLM\u2019s to automatically extract metadata, significantly enhancing the relevance and precision of information retrieval. This is achieved through five key MetadataExtractor modules (SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, MetadataFeatureExtractor) that augment text with rich, context-specific details.\nDocs, TweetTutorials:Anyscale tutorial on \u201cHow to Build an LLM Query Engine in 10 Minutes using LlamaIndex.\u201dErika Cardenas tutorial on how to load data into Weaviate and how to connect LlamaIndex to a Weaviate instance using LlamaIndex.Wenqi Glantz tutorial on Refreshing Private Data Sources with LlamaIndex Document Management.Michael Hunger tutorial on Load in data from neo4j, NebulaGraph, and index/query with LlamaIndex using GraphDB Cypher and GraphQL data loaders.Pradip Nichite video tutorial and blogpost on Mastering LlamaIndex: Create, Save & Load Indexes, Customize LLMs, Prompts & Embeddings.Webinars And Podcasts:Webinar on Graph Databases, Knowledge Graphs, and RAG with Wey (NebulaGraph).Webinar with Albus \u2014 a comprehensive Slackbot for enterprise search, xpress.ai \u2014 a low-code solution for building LLM workflows + agents and ImmigrantFirst.ai \u2014 assistant to help immigrants complete their EB-1A/O1 apps more efficiently.Data Exchange Podcast with Ben Lorica on LlamaIndexEvents:Ravi Theja gave talks on \u201cLlamaIndex: Basics To Production\u201d at Accel Partners and Together VC Fund in India.",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-update-07-10-2023-4ceebdab96cb"
        },
        {
            "date": "Jan 11, 2024",
            "author": "Mikayel",
            "title": "Free Advanced RAG Certification course with Activeloop and LlamaIndex",
            "tags": [
                "llamaindex",
                "retrieval-augmented",
                "large-language-models",
                "course",
                "ai"
            ],
            "content": "Master LlamaIndex with our course developed in collaboration with Activeloop, TowardsAI, & Intel. Learn to apply advanced retrieval across industries in 40+ lessons. This is a guest post from Activeloop.LlamaIndex is proud to collaborate with Activeloop, Towards AI, and the Intel Disruptor Initiative to offer a free course on \u201cAdvanced Retrieval Augmented Generation for Production,\u201d a part of the Gen AI 360: Foundational Model Certification series. This comprehensive course takes a hands-on approach to applying RAG techniques across various industries, including legal, biomedical, healthcare, e-commerce, and finance.The free course is designed for practical learning and invites participants to tackle real business challenges, such as developing a multi-modal AI shopping assistant. The course has over 40 lessons, 7 interactive projects, and 2 hours of video content, including from LlamaIndex CEO Jerry Liu. In 20+ hours of learning, the curriculum is geared towards enabling GenAI tinkerers, professionals, and executives to apply LlamaIndex and Deep Lake, Activeloop\u2019s database for AI in production. Participants who complete the course will be awarded a certificate at no cost.In the rapidly evolving business landscape, leveraging Retrieval Augmented Generation (RAG) tools like Llamalndex & Deep Lake by Activeloop is essential for enterprises seeking a competitive edge in GenAI. This course is tailored to quickly upskill your team in GenAI workflows, emphasizing the integration of Activeloop\u2019s advanced features like Deep Memory with Llamalndex for unmatched retrieval accuracy. It\u2019s a strategic investment to enhance your team\u2019s capabilities, ensuring your enterprise stays at the forefront of AI innovation- Jerry Liu, CEO & Co-Founder, LlamaIndexHighlighted projects across several industries:Healthcare: Advanced RAG for Pill Searching. Combine cutting-edge NLP and computer vision techniques to build an AI app that recognizes pills from images, and lists their side effects and instructions to use. This project offers hands-on experience with the latest AI technologies like Segment Anything or GPT-4-vision.Legal: Patent Generation and Search Engine. Gain practical knowledge in constructing a system like PatentPT, which incorporates a fine-tuned LLM to search or create patents. Learn how to build a meta-agent to smartly route user inquiries, ensuring a fluid chat experience with a database of 8 million USPTO patents for comprehensive retrieval and generation capabilities.E-commerce: AI-Powered Shopping Assistant for Outfit Recommendations. Build a multi-modal AI assistant that curates outfit suggestions for any occasion, weather, or budget!What will I learn?Challenges with Naive RAG: We\u2019ll address common issues such as low precision, recall, and suboptimal response generation. Strategies for refining data processing, enhancing embedding models, refining retrieval algorithms, and optimizing prompt usage will be explored to improve RAG system performance.Advanced RAG with LlamaIndex: Delve into basic and advanced RAG methods using LlamaIndex. The course covers the essential aspects of LlamaIndex required for RAG application development, complemented by Activeloop\u2019s Deep Memory module, which natively integrates seamlessly with LlamaIndex to enhance retrieval accuracy by an average of 22%. Topics will range from small to large-scale retrieval, handling structured and unstructured data, and querying techniques.RAG Agents: This module focuses on the creation and application of RAG agents with LlamaIndex, including advanced querying, summarizing databases, and designing AI assistants using various APIs.Production-grade apps: Learn to fine-tune the LlamaIndex RAG pipeline for professional deployment, evaluate RAG systems crafted with LlamaIndex, and ensure your models\u2019 observability and effectiveness.Here\u2019s a brief introduction to the course by Louis Bouchard from TowardsAI team.Who Should Enroll?This course is ideal for aspiring AI professionals, executives, and enthusiasts eager to apply AI in practical scenarios. Whether you want to enhance your organization\u2019s AI capabilities or expand your knowledge, this course offers valuable hands-on experience. A basic understanding of coding and Python is recommended.Complimentary Free Trial of Deep LakeAs a part of the course, all course takers can redeem a free extended trial of one month for the Activeloop Starter and Growth plans by redeeming the GENAI360LLAMA promo code at checkout. Check out the following video to learn more.Join thousands of AI engineers in mastering master Retrieval Augmented Generation with LlamaIndex. Enroll for free today!",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//join-thousands-in-our-free-advanced-rag-certification-created-with-activeloop-ad63f24f27bb"
        },
        {
            "date": "Aug 25, 2023",
            "author": "Jerry Liu",
            "title": "Fine-Tuning Embeddings for RAG with Synthetic Data",
            "tags": [
                "fine-tuning",
                "llm",
                "llamaindex",
                "nlp",
                "embedding"
            ],
            "content": "UPDATE 9/10/2023: We\u2019ve included embedding finetuning abstractions into the LlamaIndex repo, so this repo is technically outdated! Please check out our embedding fine-tuning guides in the core documentation.We\u2019ve created a comprehensive, end-to-end guide showing you how to fine-tune an embedding model to improve performance of Retrieval Augmented Generation (RAG) systems over any unstructured text corpus (no labels required!).The result is a 5\u201310% performance increase in retrieval evaluation metrics \u2014 our finetuned bge model almost reaches text-embedding-ada-002 levels of retrieval performance in terms of hit rate. This enables more accurate retrieval which leads to better RAG systems as a whole.This tutorial is helpful to anyone building RAG systems:If you\u2019re new to finetuning, no problem! We have step by step notebooks walking through the key steps. Simply substitute the file links for your own data, and just run every cell.Finetuning embedding models is lightweight and doesn\u2019t require a GPU. These notebooks were tested on an M2 Macbook Pro.ResourcesRepo: https://github.com/run-llama/finetune-embeddingNotebooks: Dataset Generation, Finetuning, EvaluationBackground/ContextThe Current RAG StackRAG is a popular paradigm for connecting Large Language Models (LLMs) with an external source of data that was not present in its training corpus. It pairs a retrieval model over a knowledge bank with the LLM through its input prompt space. RAG stacks typically look like the following:Indexing: Prepare a corpus of unstructured text, parse/chunk it. Then embed each chunk and put in a vector database.Query-time: Retrieve context from the vector db using top-k embedding similarity lookup, and stuff context into the LLM input space.(Of course RAG can be much more advanced than this, and LlamaIndex provides tools for both simple and advanced RAG)Unfortunately RAG is easy to prototype by cobbling together the different components, but hard to productionize. The simple stack has many failure modes and oftentimes the issue lies with bad retrieval \u2014 if the returned context is irrelevant to the query, then the capability of the LLM is irrelevant; the answer will always be bad.How Can We Make Retrieval Better?We can try more sophisticated retrieval algorithms (e.g. hybrid search, reranking).An insight from our recent production RAG webinar, however, is that the embeddings themselves may not live in an optimal latent space for your data. Embeddings generated by pre-trained models may be close/far from each other based on the pre-training objective, but may not completely align with your own retrieval objective. For instance, if you\u2019re building search over ML ArXiv papers, you may want the embeddings to align semantically with specific ML concepts (e.g. \u201cLLMs\u201d, \u201cNLP\u201d) and not filler words \u201cThis paper is\u2026\u201d).Finetuning is a way to solve that. The concept of finetuning has become increasingly popular in the LLM space, with technological advancements as well as easy-to-use services.In this tutorial, we focus on finetuning the embedding model. We show how finetuning the embedding model can lead to better retrieval performance.Challenges/ConsiderationsWhen you finetune embeddings, you need training examples. In the case of embeddings, this typically means that you have both \u201cpositive\u201d and \u201cnegative\u201d examples \u2014 pairs of texts that should be close to each other and far from each other.An issue is that we don\u2019t have these positive or negative examples apriori. Given a dataset of unstructured text, is it possible to automatically generate these example pairs?With LlamaIndex you can! We use LlamaIndex modules to automatically generate a set of questions from unstructured text chunks. These (question, chunk) pairs are then used as positive examples as training signals for the model (negative examples are randomly sampled across other chunks).The next section shows a full walkthrough across all of our modules.WalkthroughAt a high-level, we do the following:Generating synthetic dataset for training and evaluation (Notebook)Finetuning an opensource embedding model (Notebook)Evaluating the embedding model (Notebook)Generating synthetic dataset for training and evaluationThe key idea here is that we can leverage an LLM to generate hypothetical questions that are best answered by a given piece of context. This allows us to generate synthetic positive pairs of (query, relevant documents) in a scalable way without requiring human labellers.More concretely, we first process the given documents into a corpus of text chunks. We do this with the SimpleNodeParser module in LlamaIndex:parser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\ncorpus = {\n  node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) \n  for node in nodes\n}Then for each text chunk, we use LLM to generate a few hypothetical questions that can be answered with information form that text chunk. The example prompt is shown below as well.prompt_template = prompt_template or \"\"\"\\\n  Context information is below.\n  \n  ---------------------\n  {context_str}\n  ---------------------\n  \n  Given the context information and not prior knowledge.\n  generate only questions based on the below query.\n  \n  You are a Teacher/ Professor. Your task is to setup \\\n  {num_questions_per_chunk} questions for an upcoming \\\n  quiz/examination. The questions should be diverse in nature \\\n  across the document. Restrict the questions to the \\\n  context information provided.\"\n  \"\"\"\n\n# for a given node, extract questions (do this over all nodes in outer loop)\nquery = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\nresponse = llm.complete(query)\n\nresult = str(response).strip().split(\"\\n\")\nquestions = [\n    re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n]\nquestions = [question for question in questions if len(question) &gt; 0]\nFinally, we collect all pairs of questions and text chunks as the dataset. Example query, chunk, and mapping is shown below.\n# example query\nf331640a-b407-4028-8db8-4b8db691dd34: \"What is the market value of Lyft's common stock held by non-affiliates as of June 30, 2021, based on the closing sales price of the Class A common stock on that date?\"\n\n# example corpus\nd5554f3e-cdaf-41d7-ac49-8f0ffe3f5759:\"UNITED STATESSECURITIES AND...\"\n\n# example mapping\nf331640a-b407-4028-8db8-4b8db691dd34: d5554f3e-cdaf-41d7-ac49-8f0ffe3f5759Finetuning an opensource embedding modelWe leverage the high-level model fitting API from sentencetransformers to very easily setup a training process.We use MultipleNegativesRankingLoss as the training object and InformationRetrievalEvaluator as the evaluator during training. Also, we useBAAI/bge-small-en on Hugging Face as the base model and train for a small number of epochs.# define model\nmodel_id = \"BAAI/bge-small-en\"\nmodel = SentenceTransformer(model_id)\n\n...\n\n# define loss\nfrom sentence_transformers import losses\nloss = losses.MultipleNegativesRankingLoss(model)\n\n# define evaluator\nfrom sentence_transformers.evaluation import InformationRetrievalEvaluator\n# define over validation dataset\n...\nevaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)\n\n# run training\n...\nmodel.fit(\n    train_objectives=[(loader, loss)],\n    epochs=EPOCHS,\n    warmup_steps=warmup_steps,\n    output_path='exp_finetune',\n    show_progress_bar=True,\n    evaluator=evaluator, \n    evaluation_steps=50,\n)Evaluating the embedding modelWe compare the finetuned model against the base model, as well as the OpenAI embedding model text-embedding-ada-002 .We evaluate with two main metrics:Hit-rate metric: For each (query, relevant_doc) pair, we retrieve the top-k documents with the query. It\u2019s a hit if the results contain relevant_doc.InformationRetrievalEvaluator from sentence_transformers. This provides a comprehensive suite of metrics such as cosine similarity accuracy, precision, recall at different top-k values.ResultsIn terms of hit-rate metric, the base model gets 78% hit-rate on the validation dataset, and the fine-tuned model gets 84%. text-embedding-ada-002 gets 87%, which means that our fine-tuned model is only 3% off!Hit-rate for `text-embedding-ada-002`, base model, finetuned modelThe InformationRetrievalEvaluator shows a similar improvement across an entire suite of metrics. The fine-tuned model increases evaluation metrics by 5\u201310% compared to the base-model.Evaluation suite from `InformationRetrievalEvaluator`ConclusionWe successfully finetuned an embedding model over unlabeled, unstructured data to give better retrieval performance for downstream RAG systems. We show a 5\u201310% improvement across all metrics!Resources(copied from intro)Repo: https://github.com/run-llama/finetune-embeddingNotebooks: Dataset Generation, Finetuning, Evaluation",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971"
        },
        {
            "date": "Aug 11, 2023",
            "author": "Jerry Liu",
            "title": "Zep and LlamaIndex: A Vector Store Walkthrough",
            "tags": [
                "ai",
                "llamaindex",
                "machine-learning",
                "vector-database",
                "nlp"
            ],
            "content": "Editor\u2019s Note: This article was written by Daniel at Zep for the LlamaIndex blog.Zep is a long-term memory store for LLM applications. With Zep, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Document and chat history storage, embedding, enrichment, and more are taken care of by the Zep service.In this article, we demonstrate how to use Zep\u2019s new Document Vector Store with the (also new) ZepVectorStore for LlamaIndex.Installing Zep and some important conceptsZep is open source and may be installed via Docker, or to Kubernetes and hosting platforms such as Render. SDKs are available for Python and TypeScript, and frameworks such as LangChain and LlamaIndex ship with support for Zep.Zep stores documents in Collections, with the document text, embeddings, and metadata all colocated. This enables hybrid semantic search over a collection, with results filtered by JSONPath queries against document metadata. When using Zep with LlamaIndex, LlamaIndex filters are translated for use by Zep.A document or document chunk is equivalent to a LlamaIndex TextNode or NodeWithEmbedding.Collections can be optionally set to automatically embed texts using a service such as OpenAI or locally using an embedding model of your choice. However, when using Zep with LlamaIndex, we rely on LlamaIndex\u2019s integrations with embedded services and libraries.Creating a ZepVectorStore and Document CollectionYou will need to have installed Zep and have your API URL and, optionally, authentication key handy.from llama_index.vector_stores import ZepVectorStore\nzep_api_url = \"http://localhost:8000\"\nzep_api_key = \"&lt;optional_jwt_token&gt;\"\ncollection_name = \"babbage\" # The name of a new or existing collection\nembedding_dimensions = 1536 # the dimensions of the embedding model you intend to use\nvector_store = ZepVectorStore(\n  api_url=zep_api_url,\n  api_key=zep_api_key,\n  collection_name=collection_name,\n  embedding_dimensions=embedding_dimensions\n)The collection name is a unique identifier for your vector index and should only contain alphanumeric characters. If the collection does not exist, Zep will automatically create one for you.Creating and populating an IndexBelow we\u2019ll use a common LlamaIndex pattern for loading content and adding it to an index. After loading the text data, we create a StorageContext backed by the ZepVectorStore.We then create the index using our loaded documents and Zep-backed storage context.from llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.storage.storage_context import StorageContext\n\ndocuments = SimpleDirectoryReader(\"./babbages_calculating_engine/\").load_data()\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\nquery = \"the sun and stars\"\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\n\n\nprint(str(response))But one of the most signal examples of this kind, of which we are aware, is related by Mr Baily. The catalogue of stars published by the Astronomical Society was computed by two separate and independent persons, and was afterwards compared and examined with great care and attention by Mr Stratford. On examining this catalogue, and recalculating a portion of it, Mr Baily discovered an error in the case of the starFinally, we run a simple text query against the index and print the resulting node\u2019s text.Hybrid search with metadata filtersAs mentioned above, Zep also supports associating rich metadata with documents. This metadata can be an arbitrarily deep JSON structure. When working with LlamaIndex, we currently support filtering on top-level keys in the map.The code below demonstrates running a vector search over an index and filtering on metadata using LlamaIndex\u2019s MetadataFilters. We print the result and the normalized cosine similarity for the matching result.from llama_index.schema import TextNode\nfrom llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n\nnodes = [\n   TextNode(\n       text=\"Not aware that tables of these squares existed, Bouvard, who calculated the tides for Laplace, underwent the labour of calculating the square of each individual sine in every case in which it occurred.\",\n       metadata={\n           \"topic\": \"math\",\n           \"entities\": \"laplace\",\n       },\n   ),\n   TextNode(\n       text=\"Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.\",\n       metadata={\n           \"topic\": \"astronomy\",\n           \"entities\": \"moon\",\n       },\n   ),\n]\n\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n\nfilters = MetadataFilters(filters=[ExactMatchFilter(key=\"topic\", value=\"astronomy\")])\n\n\nretriever = index.as_retriever(filters=filters)\nresult = retriever.retrieve(\"What is the structure of our galaxy?\")\n\n\nfor r in result:\n   print(\"\\n\", r.node.text, r.score)Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.  0.6456785674Summing it upZep offers a single API for vector search over documents and chat history, allowing developers to populate prompts with both forms of long-term memory. LlamaIndex makes it extremely easy to populate Zep with content from a broad set of documents and data sources and query these sources when building prompts and other functionality for LLM apps.Next StepsRead the Zep Quick Start GuideZep and LlamaIndex Walkthrough NotebookGetting Started with LllamaIndex",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//zep-and-llamaindex-a-vector-store-walkthrough-564edb8c22dc"
        },
        {
            "date": "Jun 5, 2023",
            "author": "Jerry Liu",
            "title": "Vellum <> LlamaIndex Integration",
            "tags": [
                "nlp",
                "ai",
                "mlops",
                "observability",
                "prompt-engineering"
            ],
            "content": "Co-Authors:Akash Sharma, founder and CEO, VellumJerry Liu, co-founder and CEO, LlamaIndexAbout UsThe central mission of LlamaIndex is to provide an interface between Large Language Models (LLM\u2019s), and your private, external data. Over the past few months, it has become one of the most popular open-source frameworks for LLM data augmentation (context-augmented generation), for a variety of use cases: question-answering, summarization, structured queries, and more.Vellum is a developer platform to build high quality LLM applications. The platform provides best-in-class tooling for prompt engineering, unit testing, regression testing, monitoring & versioning of in-production traffic and model fine tuning. Vellum\u2019s platform helps companies save countless engineering hours to build internal tooling and instead use that time to build end user facing applications.Why we partnered on this integrationUntil recently, LlamaIndex users did not have a way to do prompt engineering and unit testing pre-production and versioning/monitoring the prompts post production. Prompt engineering and unit testing is key to ensure that your LLM feature is producing reliable results in production. Here\u2019s an example of simple prompt that produces vastly different results between GPT-3, GPT-3.5 and GPT-4:Unit testing your promptsCreating a unit test bank is a proactive approach to ensure prompt reliability \u2014 it\u2019s best practice to run 50\u2013100 test cases before putting prompts in production. The test bank should comprise scenarios & edge cases anticipated in production, think of this as QAing your feature before it goes to production. The prompts should \u201cpass\u201d these test cases based on your evaluation criteria. Use Vellum Test Suites to upload test cases in bulk via CSV upload.Regression testing in productionDespite how well you test before sending a prompt in production, edge cases can appear when in production. This is expected, so no stress! Through the Vellum integration, LlamaIndex users can change prompts and get prompt versioning without making any code changes. While doing that, however, it\u2019s best practice to run historical inputs that were sent to the prompt in production to the new prompt and confirm it doesn\u2019t break any existing behavior. LLMs are sometimes unpredictable, even changing the word \u201cgood\u201d to \u201cgreat\u201d in a prompt can result in differing outputs!Best practices to leverage the integrationHow to access the integrationThis demo notebook goes into detail on how you can use Vellum to manage prompts within LlamaIndex.PrerequisitesSign up for a free Vellum account at app.vellum.ai/signupGo to app.vellum.ai/api-keys and generate a Vellum API key. Note it down somewhere.Auto-Register Prompts & Make Predictions Through VellumIf you import a prompt in LlamaIndex, the VellumPredictor class will used to auto-register a prompt with Vellum to make predictions.By registering a prompt with Vellum, Vellum will create:A \u201cSandbox\u201d \u2014 an environment where you can iterate on the prompt, it\u2019s model, provider, params, etc.; andA \u201cDeployment\u201d \u2014 a thin API proxy between you and LLM providers and offering prompt versioning, request monitoring, and moreYou can use VellumPromptRegistry to retrieve information about the registered prompt and get links to open its corresponding Sandbox and Deployment in Vellum\u2019s UI. More details about Vellum\u2019s Sandbox and Deployment features can be found herePrompt engineering tips in context augmented use casesThink of the Large Language Model as a smart college graduate that needs instructions if the task at hand is not clear. If you\u2019re not getting good results with the default prompt templates, follow these instructions:Add use case specific details to the prompt to guide what the model focuses on.Create 5\u201310 input scenarios to test performanceIterate a few times: (i) Tweak the prompt by adding more specific instructions or examples for the scenarios with bad results, (ii) Evaluate against the target response for each scenarioIn parallel, test out different foundation models and model providers using Vellum\u2019s Sandbox. Maybe Claude or PaLM does better than GPT-4 for your use case.If you would like additional reasoning or explanation, use a more prescriptive approach:Add detailed step by step instructions to the end of the prompt and ask the LLM to walk though those steps when creating it\u2019s answer:e.g. (1) \u2026 (2) \u2026 (3) \u2026 \u2026 (6) Output a JSON with the following typescript schemaThis is convenient because it\u2019s simple to parse out the JSON blob from the LLM outputHowever this causes more tokens to be generated so is slower and costs more, but it\u2019s not nearly as expensive and slow as chaining multiple callsMeasuring prompt quality, before productionOne of the common reasons why evaluating LLM model quality is hard is that there\u2019s no defined framework. The evaluation metric depends on your use case. This blog goes in more detail, but in summary, the evaluation approach depends on type of use case:Classification: accuracy, recall, precision, F score and confusion matrices for a deeper evaluationData extraction: Validate that the output is syntactically valid and the expected keys are present in the generated responseSQL/Code generation: Validate that the output is syntactically valid and running it will return the expected valuesCreative output: Semantic similarity between model generated response and target response using cross-encodersVellum\u2019s Sandbox and Test Suites offer Exact Match, Regex Match, Semantic Similarity & Webhook as evaluation criteria. You get a clear indication of which test cases \u201cpass\u201d, given your evaluation criteriaTesting in Vellum SandboxTesting in Vellum Test SuitesMeasuring prompt quality, once in productionUser feedback is the ultimate source of truth for model quality \u2014 if there\u2019s a way for your users to either implicitly or explicitly tell you whether they the response is \u201cgood\u201d or \u201cbad,\u201d that\u2019s what you should track and improve!Explicit user feedback is collected when your users respond with something like a \ud83d\udc4d or \ud83d\udc4e in your UI when interacting with the LLM output. Asking explicitly may not result in enough volume of feedback to measure overall quality. If your feedback collection rates are low, we suggest using implicit feedback if possible.Implicit feedback is based on how users react to the output generated by the LLM. For example, if you generate a first draft of en email for a user and they send it without making edits, that\u2019s likely a good response! If they hit regenerate, or re-write the whole thing, that\u2019s probably not a good response. Implicit feedback collection may not be possible for all use cases, but it can be a powerful gauge of quality.Use Vellum\u2019s Actuals endpoint to track the quality of each completion and track results in the Completions and Monitoring tabs of your Deployment.",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//vellum-llamaindex-integration-58b476a1e33f"
        },
        {
            "date": "Feb 12, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex v0.10",
            "tags": [
                "llm",
                "llamaindex",
                "upgrade",
                "ai"
            ],
            "content": "Today we\u2019re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date (see this gargantuan PR), and it takes a massive step towards making LlamaIndex a next-generation, production-ready data framework for your LLM applications.LlamaIndex v0.10 contains some major updates:We have created a llama-index-core package, and split all integrations and templates into separate packages: Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still usefrom llama_index.llms.openai import OpenAI for a LLM.LlamaHub will be the central hub for all integrations: the former llama-hub repo itself is consolidated into the main llama_index repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned!ServiceContext is deprecated: Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default.Upgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we\u2019ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10.Check out the below sections for more details, and go to the very last section for resource links to everything.Splitting into `llama-index-core` and integration packagesThe first and biggest change we\u2019ve made is a massive packaging refactor.LlamaIndex has evolved into a broad toolkit containing hundreds of integrations:150+ data loaders35+ agent tools50+ LlamaPack templates50+ LLMs25+ embeddings40+ vector storesand more across the llama_index and llama-hub repos. The rapid growth of our ecosystem has been awesome to see, but it\u2019s also come with growing pains:Many of the integrations lack proper testsUsers are responsible for figuring out dependenciesIf an integration updates, users will have to update their entire llama-index Python package.In response to this, we\u2019ve done the following.Created llama-index-core : This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations.Created separate packages for all integrations/templates: Every integration is now available as a separate package. This includes all integrations, including those on LlamaHub! See our Notion registry page for a full list of all packages.The llama-index package still exists, and it imports llama-index-core and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages (llama-index-llms-openai, llama-index-embeddings-openai, and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in llama-index-readers-file).NOTE: if you don\u2019t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining llama-index-legacy (pinned to the latest release 0.9.48) for the foreseeable future.Revamped Folder StructureWe\u2019ve completely revamped the folder structure in the llama_index repo. The most important folders you should care about are:llama-index-core : This folder contains all core LlamaIndex abstractions.llama-index-integrations: This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.llama-index-packs : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user\u2019s application.Other folders:llama-index-legacy : contains the legacy LlamaIndex code.llama-index-experimental : contains experimental features. Largely unused right now (outside parameter tuning).llama-index-finetuning : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.The sub-directories in integrations and packs represent individual packages. The name of the folder corresponds to the package name. For instance, llama-index-integrations/llms/llama-index-llms-gemini corresponds to the llama-index-llms-gemini PyPI package.Within each package folder, the source files are arranged in the same paths that you use to import them. For example, in the Gemini LLM package, you\u2019ll see a folder called llama_index/llms/gemini containing the source files. This folder structure is what allows you to preserve the top-level llama_index namespace during importing. In the case of Gemini LLM, you would pip install llama-index-llms-gemini and then import using from llama_index.llms.gemini import Gemini.Every one of these subfolders also has the resources needed to packagify it: pyproject.toml, poetry.lock, and a Makefile , along with a script to automatically create a package.If you\u2019re looking to contribute an integration or pack, don\u2019t worry! We have a full contributing guide designed to make this as seamless as possible, make sure to check it out.IntegrationsAll third-party integrations are now under llama-index-integrations. There are 19 folders in here. The main integration categories are:llmsembeddingsmulti_modal_llmsreaderstoolsvector_storesFor completeness here are all the other categories: agent, callbacks, evaluation, extractors, graph_stores, indices, output_parsers, postprocessor, program, question_gen, response_synthesizers, retrievers, storage, tools.The integrations in the most common categories can be found in our temporary Notion package registry page. All integrations can be found in our Github repo. The folder name of each integration package corresponds to the name of the package \u2014 so if you find an integration you like, you now know how to pip install it!We are actively working to make all integrations viewable on LlamaHub. Our vision for LlamaHub is to be the hub for all third-party integrations.If you\u2019re interested in contributing a package, see our contributing section below!Usage ExampleHere is a simple example of installing and using an Anthropic LLM.pip install llama-index-llms-anthropicfrom llama_index.llms.anthropic import Anthropic\nllm = Anthropic(api_key=\"&lt;api_key&gt;\")Here is an example of using a data loader.pip install llama-index-readers-notionfrom llama_index.readers.notion import NotionPageReader\nintegration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\npage_ids = [\"&lt;page_id&gt;\"]\nreader = NotionPageReader(integration_token=integration_token)\ndocuments = reader.load_data(page_ids=page_ids)Here is an example of using a LlamaPack:pip install llama-index-packs-sentence-window-retrieverfrom llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack\n# create the pack\n# get documents from any data loader\nsentence_window_retriever_pack = SentenceWindowRetrieverPack(\n  documents\n)\nresponse = sentence_window_retriever_pack.run(\"Tell me a bout a Music celebritiy.\")Dealing with Breaking ChangesThis update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these:# no more using `llama_index.llms` as a top-level package\nfrom llama_index.llms import OpenAI\n# no more using `llama_index.vector_stores` as a top-level package\nfrom llama_index.vector_stores import PineconeVectorStore\n# llama_hub imports are now no longer supported.\nfrom llama_hub.slack.base import SlackReaderInstead you can do these:from llama_index.llms.openai import OpenAI\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\n# NOTE: no longer import a separate llama_hub package\nfrom llama_index.readers.slack import SlackReaderSee our migration guide (also described below) for more details.LlamaHub as a Central Hub for IntegrationsWith these packaging updates, we\u2019re expanding the concept of LlamaHub to become a central hub of all LlamaIndex integrations to fulfill its vision of becoming an integration site at the center of the LLM ecosystem. This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, embeddings, vector stores, callbacks, and more.This effort is still a WIP. If you go to llamahub.ai today, you\u2019ll see that the site has not been updated yet, and it still contains the current set of integrations (data loaders, tools, LlamaPacks, datasets). Rest assured we\u2019ll be updating the site in a few weeks; in the meantime check out our Notion package registry / repo for a list of all integrations/packages.Sunsetting llama-hub repoSince all integrations have been moved to the llama_index repo, we are sunsetting the llama-hub repo (but LlamaHub itself lives on!). We did the painstaking work of migrating and packaging all existing llama-hub integrations. For all future contributions please submit directly to the llama_index repo!`download` syntaxA popular UX for fetching integrations through LlamaHub has been the download syntax: download_loader , download_llama_pack , and more.This will still work, but have different behavior. Check out the details below:download_llama_pack : Will download a pack under llama-index-packs to a local file on your disk. This allows you to directly use and modify the source code from the template.Every other download function download_loader , download_tool : This will directly run pip install on the relevant integration package.Deprecating ServiceContextLast but not least, we are deprecating our ServiceContext construct and as a result improving the developer experience of LlamaIndex.Our ServiceContext object existed as a general configuration container containing an LLM, embedding model, callback, and more; it was created before we had proper LLM, embedding, prompt abstractions and was meant to be an intermediate user-facing layer to let users define these parameters.Over time however, this object became increasingly difficult to use. Passing in an entire service_context container to any module (index, retriever, post-processor, etc.) made it hard to reason about which component was actually getting used. Since all modules use OpenAI by default, users were getting asked to unnecessarily specify their OpenAI key even in cases where they\u2019d want to use a local model (because the embedding model default was still OpenAI). It was also laborious to import and type out.Another related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the service_context to all modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.Therefore we\u2019ve made the following changes:ServiceContext is now deprecated: You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.You can now define global settings: Define this once, and don\u2019t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.All references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.Usage ExampleTo build a VectorStoreIndex and then query it, you can now pass in the embedding model and LLM directlyfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.callbacks import CallbackManager\n\nembed_model = OpenAIEmbedding()\nllm = OpenAI()\ncallback_manager = CallbackManager()\nindex = VectorStoreIndex.from_documents(\n documents, embed_model=embed_model, callback_manager=callback_manager\n)\nquery_engine = index.as_query_engine(llm=llm)Or you can define a global settings objectfrom llama_index.core.settings import Settings\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.callback_manager = callback_manager\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()Contributing to LlamaIndex v0.10v0.10 makes the llama_index repo the central place for all community contributions, whether you are interested in contributing core refactors, or integrations/packs!If you\u2019re contributing an integration/pack, v0.10 makes it way easier for you to contribute something that can be independently versioned, tested, and packaged.We have utility scripts to make the package creation process for an integration or pack effortless:# create a new pack\ncd ./llama-index-packs\nllamaindex-cli new-package --kind \"packs\" --name \"my new pack\"\n\n# create a new integration\ncd ./llama-index-integrations/readers\nllamaindex-cli new-pacakge --kind \"readers\" --name \"new reader\"Take a look at our updated contributing guide here for more details.Migration to v0.10If you want to use LlamaIndex v0.10, you will need to do two main things:Adjust imports to fit the new package structure for core modules/integrationsDeprecate ServiceContextLuckily, we\u2019ve created a comprehensive migration guide that also contains a CLI tool to automatically upgrade your existing code and notebooks to v0.10!Just dollamaindex-cli upgrade <source-dir>Check out the full migration guide here.Next StepsWe\u2019ve painstakingly revamped all of our README, documentation and notebooks to reflect these v0.10 changes. Check out the below section for a compiled list of all resources.Documentationv0.10 Documentationv0.10 Installation Guidev0.10 QuickstartUpdated Contribution GuideTemporary v0.10 Package Registryv0.10 Migration GuideRepoRepo READMEllama-index-integrationsllama-index-packsExample NotebooksThese are mostly to show our updated import syntax.Sub-Question Query Engine (primarily uses core)Weaviate Vector Store DemoOpenAI Agent over RAG PipelinesBug reportsWe\u2019ll be actively monitoring our Github Issues and Discord. If you run into any issues don\u2019t hesitate to hop into either of these channels!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-v0-10-838e735948f8"
        },
        {
            "date": "Jul 16, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-07-16",
            "tags": [],
            "content": "Hello, Llama Family! \ud83e\udd99Welcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons.Before we get into the details of our newsletter, we\u2019re thrilled to share the beta launch of LlamaCloud. This new data processing layer boosts RAG workflows with sophisticated parsing, indexing, and retrieval functions. Alongside this, we\u2019re also introducing LlamaTrace in partnership with Arize AI, which provides unmatched tracing, observability, and evaluation capabilities for LLM application workflows.Signup here: cloud.llamaindex.ai\ud83e\udd29\u00a0The highlights:LlamaCloud Launch: We\u2019ve launched the beta release of LlamaCloud, a data processing layer designed to enhance RAG workflows with state-of-the-art parsing, indexing, and retrieval capabilities. Blogpost, Tweet.LlamaTrace Launch: In collaboration with Arize AI, we\u2019ve introduced LlamaTrace, offering unmatched tracing, observability, and evaluation capabilities for LLM application workflows. It features detailed call stack tracing, one-click setup through LlamaIndex, and seamless integration with LlamaCloud. Blogpost, Tweet.GraphRAG Implementation: Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation. Notebook, Tweet.Redis Queue Integration with Llama-Agents: We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance. Notebook, Tweet.\u2728 Feature Releases and Enhancements:We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities. Blogpost, Tweet.We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries. Notebook, Tweet.We have integrated Redis Queue with llama-agents to enhance coordination in multi-agent workflows, allowing for robust communication. Notebook, Tweet.We have introduced LlamaTrace in collaboration with Arize AI, offering unparalleled tracing, observability, and evaluation capabilities for LLM application workflows. LlamaTrace stands out for its detailed tracing, which logs the entire call stack, one-click setup through LlamaIndex, and seamless integration with LlamaCloud for easy access and authentication. Blogpost, Tweet.We have integrated NebulaGraph with LlamaIndex, enhancing PropertyGraph capabilities with sophisticated extractors, customizable properties on nodes and edges, and advanced retrieval options. Docs, Tweet.\ud83d\udca1\u00a0Demos:Lyzrai has achieved over $1M ARR using LlamaIndex! This full-stack autonomous AI agent framework enhances AI sales and marketing functions with LlamaIndex\u2019s data connectors and RAG capabilities, boasting rapid revenue growth, high accuracy, and customer satisfaction.\ud83d\uddfa\ufe0f Guides:Guide to Multi-Modal RAG for Document Processing that introduces a multi-modal RAG architecture using LlamaParse, LlamaIndex, and GPT-4o, designed to handle complex slide decks. Tweet.Guide to using LlamaParse and GPT-4o for Financial Report RAG to to effectively parse and synthesize complex financial documents, enhancing clarity and accuracy in data analysis.Guide to Building Agentic RAG with Llama3: Explore our comprehensive cookbooks, created in collaboration with AI at Meta, featuring advanced techniques from routing and tool use to constructing complex agent reasoning loops and multi-document agents using purely local models like Llama3.\u270d\ufe0f Tutorials:1LittleCoder\u2019s video tutorial demonstrates how to deploy self-hosted llama-agents using Arcee AI, MistralAI, and Ollama, including setup, local model integration, and tool development.kingzzm\u2019s tutorial on using LlamaIndex to build advanced RAG flows, detailing how to compose and visualize each step from basic retrieval and prompting to advanced techniques and evaluation with RAGAS.Mervin Praison\u2019s tutorial on using llama-agents, detailing the framework\u2019s purpose, a step-by-step setup guide for multi-agent services, and how it stands out from other frameworks.\ud83c\udfa4\u00a0Events:Join our online hackathon this Friday, 19th, to build AI apps with Llama 3 from Meta and win cash, credits, and prizes from us and our co-hosts TogetherAI, Milvus, and LablabAI.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-07-16"
        },
        {
            "date": "Nov 14, 2023",
            "author": "Laurie Voss",
            "title": "LlamaIndex Newsletter 2023\u201311\u201314",
            "tags": [
                "llm",
                "rag",
                "llamaindex",
                "newsletter",
                "ai"
            ],
            "content": "Hello Llama Friends \ud83e\udd99LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we\u2019re taking a stroll down memory lane on our blog with twelve milestones from our first year. Be sure to check it out.Last week we had a blast with all the new things from OpenAI Dev day to learn and explore at LlamaIndex. There was a special edition newsletter with the things we released the same day as the conference, but this week\u2019s newsletter is full of follow-up releases and explorations \u2014 don\u2019t miss our slide deck summing up all the new features!As always, if you\u2019ve got a cool project or a video to share we\u2019d love to see it! Just drop us a line at news@llamaindex.ai.\ud83e\udd29 First, the highlights:Multi-Modal RAG Stack: we unveiled Multi-Modal RAG ****for complex Q&A on documents and images, with new text/image queries and retrieval solutions. Notebook, Tweet, Blog post.OpenAIAssistantAgent Abstractions: we released new abstractions to connect OpenAI Assistant API with any vector database. Docs, Tweet.Parallel Function Calling: we enhanced our data extraction and tool execution using OpenAI\u2019s parallel function calling. Tweet.MechGPT Project: Prof. Markus J. Buehler\u2019s work merges LLM fine-tuning with knowledge graphs for scientific discovery. Tweet, Paper.Feature Slide Deck: Released a slide deck with 10+ new features and guides post-OpenAI updates.\u2728 Feature Releases and Enhancements:We introduced a multi-modal RAG stack for complex document and image QA, featuring text/image queries, joint text/ image embeddings, and versatile storage and retrieval options. Notebook, Tweet, Blog post.We now offer experimental GPT-4-vision support in chat.llamaindex.ai . Users can now upload images for enhanced chatbot interactions. Tweet.We integrated OpenAI\u2019s parallel function calling for efficient extraction of structured data from unstructured text and improving tool execution with agents. Tweet.We introduced OpenAIAssistantAgent abstractions for seamless connection of OpenAI Assistants API with your chosen vector database. Docs, Tweet.We introduced a new agent leveraging OpenAI Assistants API with features like in-house code interpretation, file retrieval, and function calling for external tools integration. Notebook, Tweet.\ud83c\udfa5 Demos:MechGPT by Professor Markus J. Buehler showcases the integration of LLM fine-tuning and knowledge graph creation with LlamaIndex, leading to interesting insights in cross-disciplinary scientific research and hypothesis generation. Tweet, Paper.\ud83d\uddfa\ufe0f Guides:We released a concise slide deck that aggregates over 10+ newly shipped features, guides, and analyses, complete with links to accompanying notebooks for developer use based on OpenAI\u2019s recent updates.We also released a full cookbook showing how you can build advanced RAG with the Assistants API \u2014 beyond just using the in-house Retrieval tool.We produced a guide on evaluating the OpenAI Assistant API vs RAG with LlamaIndex.Here\u2019s a guide on evaluating How well long-context LLMs (gpt-4-turbo, claude-2) recall specifics in BIG documents? (>= 250k tokens).Here\u2019s another guide that highlights how function calling simplifies structured data extraction, while JSON mode ensures format correctness without schema enforcement.Finally, we released a guide to craft a GPT Builder, enabling an agent to programmatically construct another task-specific agent. This builder streamlines the creation of systems for specific functions. Notebook, Tweet.\u270d\ufe0f Tutorials:Bhavesh Bhat gave us a tutorial on How to Chat with YouTube Videos Using LlamaIndex.David Garnitz\u2019s tutorial blog explores the use of VectorFlow alongside ArizePhoenix, Weaviate, and LlamaIndex to manage large data sets.Harshad Suryawanshi\u2019s tutorial covers Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex.Sudarshan Koirala\u2019s made a tutorial on Creating OpenAI Assistant Agent with LlamaIndex.Our own Ravi Theja released his tutorial on Boosting RAG with Embeddings & Rerankers.\ud83c\udfa5 Webinars:Check out our webinar with Dan Shipper, CEO of every to talk about the implications of OpenAI\u2019s release updates.A second webinar with Victoria Lin, author of the RA-DIT paper on Fine-tuning + RAG.Last but not least, Mayo Oshin\u2019s webinar with Jerry Liu on How to Analyze Tables In Large Financial Reports Using GPT-4.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-11-14-dad06ae4284a"
        },
        {
            "date": "Feb 20, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud",
            "tags": [
                "llamaindex",
                "llamacloud",
                "newsletter",
                "llm",
                "ai"
            ],
            "content": "Hi there, LlamaIndex Enthusiasts \ud83e\udd99,Today marks a milestone for the LlamaIndex ecosystem with the introduction of LlamaCloud, a next-generation suite of managed parsing, ingestion, and retrieval services tailored for production-grade context augmentation in your LLM and RAG applications.As an enterprise AI engineer using LlamaCloud, you can concentrate on crafting the business logic, leaving the heavy lifting of data management to us. Process vast amounts of production data effortlessly, enhancing response quality instantly. LlamaCloud debuts with:LlamaParse: A specialized parsing service for complex documents, including tables and figures, seamlessly integrated with LlamaIndex for handling semi-structured documents. This enables answering intricate queries previously out of reach.Managed Ingestion and Retrieval API: Simplify data loading, processing, and storage for your RAG applications, supported by over 150 data sources via LlamaHub, including LlamaParse, and more than 40 data storage solutions.LlamaParse is now in public preview, with a current focus on PDFs and a usage cap for public users; contact us for commercial terms. The managed API is in private preview, and available to a select group of enterprise partners. Interested? Reach out for more details.Your inventive spirit is our driving force! We look forward to discovering the projects, articles, or videos that excite you. Send your outstanding contributions to news@llamaindex.ai. If you haven\u2019t yet, join our newsletter via our website to get all the newest LlamaIndex news directly in your inbox.\ud83e\udd29 The highlights:Corrective RAG LlamaPack: We have launched LlamaPack with CRAG insights, refining information retrieval for enhanced accuracy and relevance. Tweet, LlamaPack.SELF-DISCOVER LlamaPack: We have launched SELF-DISCOVER paper implementation as LlamaPack, leveraging meta-reasoning in LLMs for adaptive, complex problem-solving. Tweet, LlamaPack.RAG Production Guide: A comprehensive guide to production-ready RAG, featuring insights and strategies from Sisil Mehta at JasperAI.\u2728 Feature Releases and Enhancements:We have integrated insights from the Corrective Retrieval Augmented Generation (CRAG) paper as LlamaPack, enhancing our system\u2019s ability to evaluate and refine retrieved information for more accurate and relevant responses. Tweet, LlamaPack.We have integrated SELF-DISCOVER a novel approach as LlamaPack to empower LLMs with meta-reasoning, allowing them to self-discover and adapt the most suitable reasoning modules from a selection, enabling more versatile and complex problem-solving capabilities. Tweet, LlamaPack.\ud83c\udfa5 Demos:DanswerAI, an out-of-the-box ChatGPT integration for enterprise knowledge, enhances efficiency across sales, IT, engineering, and customer support teams by connecting to common workplace tools like GDrive, Slack, and Jira, powered by Llama Index.GenAI for ADU Planning: A comprehensive app by Rujun Gao that navigates local ADU regulations, analyzes buildable space via satellite imagery, offers floor plan suggestions, and connects users to local contractors, showcasing the power of multi-modal AI automation in enhancing productivity\ud83d\uddfa\ufe0f Guides:Guide to building production-ready RAG covering practical tips and tricks inspired by Sisil Mehta from JasperAI.Guide to Nomic-embed-text-v1.5 with LlamaIndex: Get embeddings of any dimension from 64 to 768, inspired by Matryoshka Representation Learning.Guide to creating a RAG-powered research agent, notebook on building a workflow for scientific investigation, leveraging ArXiv, Wikipedia, textbooks, and more, with capabilities for fetching abstracts, generating ideas, and comprehensive information lookup, powered by Llama Index and LionAGI.\u270d\ufe0f Tutorials:Jerry Liu tutorial on Introduction to LlamaIndex v0.10.Jerry Li tutorial on Building Agents from scratch using Query Pipelines.Ravi Theja tutorial video on Building Multi-Modal applications with Ollama and LlamaIndex.Brett Young tutorial on Building a RAG-Based Digital Restaurant Menu with LlamaIndex and W&B Weave.Raghav tutorial on MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB.Tech With Tim tutorial on querying unstructured data, analyzing tabular data with Pandas, and actioning results in a concise, step-by-step approach.Florian June\u2019s tutorial on using RAGAs and LlamaIndex for RAG evaluation.Davide Gallitelli\u2019s tutorial on Deploying a HuggingFace embedding model to Amazon SageMaker and consuming it with Llama-Index.\ud83c\udfa5 Webinar:Webinar with Henry Heng, co-founder of Flowise on building advanced no-code RAG apps over your data.",
            "related_posts": [
                "LlamaCloud - Built for Enterprise LLM App Builders",
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4"
        },
        {
            "date": "Feb 17, 2024",
            "author": "Raghav Dixit",
            "title": "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB",
            "tags": [
                "ai",
                "generative-ai-tools",
                "vector-database",
                "large-language-models",
                "open-source"
            ],
            "content": "The widespread consumption of videos on platforms like YouTube, Instagram, and others highlights the importance of efficiently processing and analyzing video content. This capability unlocks vast opportunities across various sectors, including media and entertainment, security, and education. However, the main challenge is effectively extracting meaningful information from videos, which are inherently complex and multimodal data streams.This blog post introduces a solution that leverages the LlamaIndex Python API for using the advanced capabilities of OpenAI\u2019s GPT4V, combined with the efficient data management by LanceDB across all data formats, to process videos.\u2026But what does \u2018RAG\u2019 even mean?Retrieval-augmented generation (RAG) is a technique that merges information retrieval with generative AI to produce systems capable of generating precise and contextually relevant responses by tapping into large data repositories.Core Concept of RAGRAG operates in two stages:Retrieval: Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords.Generation: Integrates retrieved information to produce coherent responses, allowing the AI to \u201clearn\u201d from a wide range of content dynamically.RAG ArchitectureThe architecture typically involves a dense vector search engine for retrieval and a transformer model for generation. The process:Performs a semantic search to fetch relevant documents.Processes these documents with the query to create a comprehensive context.The generative model then crafts a detailed response based on this enriched context.Extending to MultimodalityMultimodal RAG integrates various data types (text, images, audio, video) in both retrieval and generation phases, enabling richer information sourcing. For example, responding to queries about \u201cclimate change impacts on polar bears\u201d might involve retrieving scientific texts, images, and videos to produce an enriched, multi-format response.Let\u2019s return to our use case and dive into how it\u2019s all done. Moving forward, you can access the full code on Google Colab.The solution is divided into the following sections. Click on the topic to skip to a specific part:Video DownloadingVideo ProcessingBuilding the Multi-Modal Index and Vector StoreRetrieving Relevant Images and ContextReasoning and Response Generation1. Video DownloadingTo begin, we need to locally download multimodal content from a publicly available source, I used pytube to download a YouTube video by 3Blue1Brown on the Gaussian function.# SET CONFIG\nvideo_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\noutput_video_path = \"./video_data/\"\noutput_folder = \"./mixed_data/\"\noutput_audio_path = \"./mixed_data/output_audio.wav\"\n\nfilepath = output_video_path + \"input_vid.mp4\"\nPath(output_folder).mkdir(parents=True, exist_ok=True)def download_video(url, output_path):\n    \"\"\"\n    Download a video from a given url and save it to the output path.\n\n    Parameters:\n    url (str): The url of the video to download.\n    output_path (str): The path to save the video to.\n\n    Returns:\n    dict: A dictionary containing the metadata of the video.\n    \"\"\"\n  from pytube import YouTube\n\n    yt = YouTube(url)\n    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n    yt.streams.get_highest_resolution().download(\n        output_path=output_path, filename=\"input_vid.mp4\"\n    )\n    return metadata\nRun metadata_vid = download_video(video_url, output_video_path) to invoke the function and store the video locally.2. Video ProcessingWe need to now extract multimodal content \u2014 Images, Text(via Audio). I extracted 1 frame every 5 seconds of the video (~160 frames) using moviepy .def video_to_images(video_path, output_folder):\n    \"\"\"\n    Convert a video to a sequence of images and save them to the output folder.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_folder (str): The path to the folder to save the images to.\n\n    \"\"\"\n    clip = VideoFileClip(video_path)\n    clip.write_images_sequence(\n        os.path.join(output_folder, \"frame%04d.png\"), fps=0.2 #configure this for controlling frame rate.\n    )Following this, we extract the audio component:def video_to_audio(video_path, output_audio_path):\n    \"\"\"\n    Convert a video to audio and save it to the output path.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_audio_path (str): The path to save the audio to.\n\n    \"\"\"\n    clip = VideoFileClip(video_path)\n    audio = clip.audio\n    audio.write_audiofile(output_audio_path)Next, let\u2019s extract text from the audio using the SpeechRecognition library:def audio_to_text(audio_path):\n    \"\"\"\n    Convert an audio file to text.\n\n    Parameters:\n    audio_path (str): The path to the audio file.\n\n    Returns:\n    test (str): The text recognized from the audio.\n\n    \"\"\"\n    recognizer = sr.Recognizer()\n    audio = sr.AudioFile(audio_path)\n\n    with audio as source:\n        # Record the audio data\n        audio_data = recognizer.record(source)\n\n        try:\n            # Recognize the speech\n            text = recognizer.recognize_whisper(audio_data)\n        except sr.UnknownValueError:\n            print(\"Speech recognition could not understand the audio.\")\n        except sr.RequestError as e:\n            print(f\"Could not request results from service; {e}\")\n\n    return textRun the below chunk to complete the extraction and storage process:video_to_images(filepath, output_folder)\nvideo_to_audio(filepath, output_audio_path)\ntext_data = audio_to_text(output_audio_path)\n\nwith open(output_folder + \"output_text.txt\", \"w\") as file:\n    file.write(text_data)\nprint(\"Text data saved to file\")\nfile.close()\nos.remove(output_audio_path)\nprint(\"Audio file removed\")3. Building the Multi-Modal Index and Vector StoreAfter processing the video, we proceed to construct a multi-modal index and vector store. This entails generating embeddings for both textual and visual data using OpenAI\u2019s CLIP model, subsequently storing and managing these embeddings in LanceDB VectorStore via the LanceDBVectorStore class.from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\nfrom llama_index import SimpleDirectoryReader, StorageContext\n\nfrom llama_index import SimpleDirectoryReader, StorageContext\nfrom llama_index.vector_stores import LanceDBVectorStore\n\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n)\n\ntext_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\nimage_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n# Create the MultiModal index\ndocuments = SimpleDirectoryReader(output_folder).load_data()\n\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)4. Retrieving Relevant Images and ContextWith the index in place, the system can then retrieve pertinent images and contextual information based on input queries. This enhances the prompt with precise and relevant multimodal data, anchoring the analysis in the video\u2019s content.Lets set up the engine for retrieving, I am fetching top 5 most relevant Nodes from the vectordb based on the similarity score:retriever_engine = index.as_retriever(\n    similarity_top_k=5, image_similarity_top_k=5\n)A Node object is a \u201cchunk\u201d of any source Document, whether it\u2019s text, an image, or other. It contains embeddings as well as meta information of the chunk of data.By default, LanceDB uses l2 as metric type for evaluating similarity. You can specify the metric type as cosine or dot if required.Next, we create a helper function for executing the retrieval logic:from llama_index.response.notebook_utils import display_source_node\nfrom llama_index.schema import ImageNode\n\n\ndef retrieve(retriever_engine, query_str):\n    retrieval_results = retriever_engine.retrieve(query_str)\n\n    retrieved_image = []\n    retrieved_text = []\n    for res_node in retrieval_results:\n        if isinstance(res_node.node, ImageNode):\n            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n        else:\n            display_source_node(res_node, source_length=200)\n            retrieved_text.append(res_node.text)\n\n    return retrieved_image, retrieved_textdef retrieve(retriever_engine, query_str):\n    retrieval_results = retriever_engine.retrieve(query_str)Lets input the query now and then move on to complete the process by retrieving and visualizing the data :query_str = \"\"\"\nUsing examples from the video, explain all things covered regarding\nthe Gaussian function\n\"\"\"\nimg, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\nimage_documents = SimpleDirectoryReader(\n    input_dir=output_folder, input_files=img\n).load_data()\ncontext_str = \"\".join(txt)\nplot_images(img)You should see something similar to the example below (note that the output will vary depending on your query):Displaying the similar Text objects (nodes)Retrieved ImagesObserve that the node object displayed shows the Id of the data chunk , its similarity score and the source text of the chunk that was matched (for images we get the filepath instead of text).5. Reasoning and Response GenerationThe final step leverages GPT4V to reason about the correlations between the input query and the augmented data. Below is the prompt template :qa_tmpl_str = (\n    \"\"\"\n Given the provided information, including relevant images and retrieved context from the video, \\\n accurately and precisely answer the query without any additional prior knowledge.\\n\"\n    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n    \"---------------------\\n\"\n    \"Context: {context_str}\\n\"\n    \"Metadata for video: {metadata_str} \\n\"\n    \"---------------------\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n\"\"\"\n)The OpenAIMultiModal class from LlamaIndex enables us to incorporate image data directly into our prompt object. Thus, in the final step, we enhance the query and contextual elements within the template to produce the response as follows:from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n)\n\n\nresponse_1 = openai_mm_llm.complete(\n    prompt=qa_tmpl_str.format(\n        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n    ),\n    image_documents=image_documents,\n)\n\npprint(response_1.text)The generated response captures the context pretty well and structures the answer correctly :The video \u201cA pretty reason why Gaussian + Gaussian = Gaussian\u201d by 3Blue1Brown delves into the Gaussian function or normal distribution, highlighting several critical aspects:Central Limit Theorem: It starts with the central limit theorem, illustrating how the sum of multiple random variable copies tends toward a normal distribution, improving with more variables.Convolution of Random Variables: Explains the addition of two random variables as their distributions\u2019 convolution, focusing on visualizing this through diagonal slices.Gaussian Function: Details the Gaussian function, emphasizing the normalization factor for a valid probability distribution, and describes the distribution\u2019s spread and center with standard deviation (\u03c3) and mean (\u03bc).Convolution of Two Gaussians: Discusses adding two normally distributed variables, equivalent to convolving two Gaussian functions, and visualizes this using the graph\u2019s rotational symmetry.Rotational Symmetry and Slices: Shows the rotational symmetry of e^(-x\u00b2) * e^(-y\u00b2) around the origin, a unique Gaussian function property. It explains computing the area under diagonal slices, equating to the functions\u2019 convolution.Resulting Distribution: Demonstrates the convolution of two Gaussian functions yielding another Gaussian, a notable exception in convolutions usually resulting in a different function type.Standard Deviation of the Result: Concludes that convolving two normal distributions with mean 0 and standard deviation (\u03c3) produces a normal distribution with a standard deviation of sqrt(2) * \u03c3.Implications for the Central Limit Theorem: Highlights the convolution of two Gaussians\u2019 role in the central limit theorem, positioning the Gaussian distribution as a distribution space fixed point.The author uses visual examples and explanations throughout to clarify the mathematical concepts related to the Gaussian function and its significance in probability and statistics.ConclusionThe Multimodal RAG architecture offers a powerful and efficient solution for processing and analyzing video content. By leveraging the capabilities of OpenAI\u2019s GPT4V and LanceDB, this approach not only simplifies the video analysis process but also enhances its accuracy and relevance. Whether for content creation, security surveillance, or educational purposes, the potential applications of this technology are vast and varied. As we continue to explore and refine these tools, the future of video analysis looks promising, with AI-driven solutions leading the way towards more insightful and actionable interpretations of video data.Stay tuned for upcoming projects !",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"
        },
        {
            "date": "Dec 6, 2023",
            "author": "Wangda Tan",
            "title": "LlamaIndex + Waii: Combining Structured Data from your Database with PDFs for Enhanced Data Analysis",
            "tags": [
                "llamaindex",
                "text-to-sql",
                "openai",
                "data-analytics",
                "sql"
            ],
            "content": "IntroductionIn many enterprises, data primarily resides in databases and generally, it\u2019s very difficult to combine database data with other forms of data, such as PDFs, when trying to generate actionable insights.We envision the development of an agent that empowers anyone to leverage data from all of these data sources for informed decision-making. Imagine an agent proficient in creating documents by merging data from diverse sources, including JIRA and databases, further enriched with the latest internet-sourced information.At waii.ai, we are committed to delivering an enterprise text-to-SQL API with the most complete and accurate translation of plain English to SQL available. Waii allows companies to build text-to-SQL right into their products as well as enable no-code analytics for their internal data/business teams. Waii works out of the box and can be self-hosted/on-prem.LlamaIndex introduces a remarkable RAG framework, facilitating the connection of various customer data sources, such as PDFs, Notion, and internal knowledge bases, to large language models (LLMs). This advancement simplifies the creation of data-augmented chatbots and analysis agents.This opens up a prime opportunity to develop an enterprise agent that can access data from multiple sources, including your preferred database. We will explore this further in the rest of the blog.Why a New Text-to-SQL LlamaIndex Plugin?To enable the Llama Index agent to utilize text-to-SQL APIs, a plugin is essential. LlamaIndex already has a built-in text-to-SQL plugin, but why did we decide to create a new LlamaHub plugin?The existing text-to-SQL plugin in LlamaIndex has been suitable for handling simple databases (less than 10 tables, 100 columns) with straightforward SQL queries. However, managing medium to large databases, which can include 100s of tables and 1000s of columns, presents a complex challenge. Limitations arise due to the restricted context windows of LLMs, and even those with large context windows, like GPT-4-turbo with its 128K tokens, can suffer from inaccuracies and regression in task retrieval when overloaded with content. This issue is discussed in a LlamaIndex study.In contrast, Waii focuses on making query generation more efficient. We have developed a built-in compiler to deal with compilation errors from LLMs to support multiple dialects. Our internal knowledge graph, created from database metadata, constraints, and query history, aids in table/schema selection. Users can also apply semantic rules to schema/table/column, or integrate with their data catalog services, ensuring the semantic correctness of generated queries, in addition to syntactic correctness.To utilize our service, users simply need to connect their database to Waii and copy a Waii API key to create a LlamaIndex agent.LlamaIndex + Waii AgentWe are thrilled to showcase the integration of Waii with LlamaIndex to create an agent capable of executing various text-to-SQL tasks and validating the data based on a PDF.We\u2019ll be analyzing customers\u2019 top-purchased categories during Christmas time, and compare it with Deloitte\u2019s holiday retail survey report.Architecture of LlamaIndex + WaiiBefore diving into the code example, let\u2019s look at the architecture first:\n\n\n\n\n  The LlamaIndex agent operates on the client side, accompanied by a number of\n  tools: Each tool provides function specifications and allows functions to be\n  selected based on context and the user\u2019s input to\n  chat(\"\u2026\"). For example, if the question indicates information needs to be retrieved\n  from the \u201cinternet\u201d, the Google search tool will be chosen. Internally it uses\n  LLM which returns selected functions with parameters for a given context.\n\n\n  When the Waii tool is chosen, whether for describing a dataset, generating a\n  query, or running a query, it sends the API request to the Waii Service.\n\n\n  The Waii Service can be deployed as a hosted SaaS or as Docker containers\n  running in your on-premises environment. The components of the Waii Service\n  include:\n\n\n\nThe Query Generator: coordinates the entire workflow of\n    query generation and communicates with the LLM for this purpose.\n  \n\nKnowledge Graph / Metadata Management: connects to\n    databases, extracting metadata and query history as a knowledge graph to\n    assist the Query Generator in choosing the right tables and schemas.\n  \n\nSemantic Rules: These aid the Query Generator in producing\n    semantically correct queries.\n  \n\nWaii Compiler: After a query is generated by the LLM, the\n    Waii Compiler patches identified issues in the query. If a compilation issue\n    is not fixable, it regenerates the query with an articulated error message.\n  \n\nCreate LlamaIndex agent with Waii + PDF Loader\n\n  Let\u2019s first create two LlamaHub tools \u2014 Waii and PDF Loader. LlamaHub tools\n  include specs to identify available functions along with their parameters, the\n  agent will select and execute which function to use based on available\n  functions and context.\n\nLet\u2019s start with creating an agent which includes the Waii tool:\nfrom llama_hub.tools.google_search import GoogleSearchToolSpecfrom llama_hub.tools.waii import WaiiToolSpecfrom llama_index.agent import OpenAIAgentfrom llama_index.llms import OpenAIwaii_tool = WaiiToolSpec(    api_key='waii_api_key',    # Connection key of WAII connected database, see     # https://github.com/waii-ai/waii-sdk-py#get-connections    database_key='database_to_use',    verbose=True)\nAnd then create a PDF tool:\nfrom pathlib import Pathfrom llama_index import download_loaderfrom llama_index import VectorStoreIndexPDFReader = download_loader(\"PDFReader\")loader = PDFReader()documents = loader.load_data(file=Path('DI_2023-Deloitte-holiday-retail-survey.pdf'))index = VectorStoreIndex.from_documents(documents)engine = index.as_query_engine(similarity_top_k=5)deloitte_retail_survey_tool = QueryEngineTool(        query_engine=engine,        metadata=ToolMetadata(            name=\"deloitte_retail_survey\",            description=(                \"Provides retail survey report for holiday sales based on Deloitte's data\"                \"Use a detailed plain text question as input to the tool, and output using plain text based on pdf data\"            ),        ),    )\nAnd at last, create an agent which combines Waii and PDF tools:\nagent = OpenAIAgent.from_tools(  [deloitte_retail_survey_tool] + waii_tool.to_tool_list(),   llm=OpenAI(model='gpt-4-1106-preview', temperature=0),   verbose=True)\n\n[deloitte_retail_survey_tool] + waii_tool.to_tool_list()\n  indicate using all functions (such as getting answers from the database,\n  generating a query, executing a query, describing datasets, etc.) provided by\n  Waii and PDF Search.\n\nUnderstand your dataset\n\n  The first step in doing data analysis is to get a better understanding of your\n  dataset.\n\nYou can start asking questions to your agent:\nagent.chat(\"Describe my database\")\nThe output of which is:\nSTARTING TURN 1---------------=== Calling Function ===Calling function: describe_dataset with args:   {\"ask\":\"Can you describe the whole database?\"}...========================STARTING TURN 2---------------The database I have access to consists of several schemas, each with its own set of tables and domains:1. **CRUNCHBASE_2016**: This schema includes tables related to acquisitions,    companies, investments, and funding rounds in the Crunchbase database for   the year 2016. It's useful for analyzing acquisition trends, company    information, investment activity, and funding trends.2. **RETAIL_DATA**: This schema contains tables related to retail operations,    such as call centers, customers, addresses, demographics, inventory,    items, promotions, stores, returns, sales, and warehouses. It can be used    to analyze call center performance, customer demographics, inventory    management, sales performance, and other retail operations.3. ...\n\n  As you can see, the agent understood the request, called\n  describe_dataset function provided by Waii, and generated a\n  summary of the dataset.\n\n\n  I\u2019m interested in the\n  RETAIL_DATA schema, so let me ask more of the schema -\n\nagent.chat(\"What can i do with the retail_data schema\")\nAnd I get this:\nThe RETAIL_DATA schema in the TWEAKIT_PLAYGROUND database is designed to support a wide range of analyses related to retail operations. Here are some of the capabilities and types of analyses you can perform with this schema:1. **Call Center Analysis**: Evaluate the performance of call centers,    understand call volumes, and assess customer service efficiency.2. **Customer Demographics**: Analyze customer profiles, including    demographics, purchasing behaviors, and preferences. This can help    in targeted marketing and customer segmentation.Specific questions that can be addressed using the RETAIL_DATA schema include:- What is the total number of call centers?- How many customers have a preferred customer flag?- What is the average price of items?\nLet me do some more data analysis.\nGenerate an SQL query and run it\n\n  Let\u2019s generate a SQL query (asking top 10 item categories sold during\n  Christmas time):\n\nagent.chat(\"Top 10 item category sold during christmas time across all years\")\nNow it calls get_answer function from Waii tool:\n=== Calling Function ===Calling function: get_answer with args:   {\"ask\":\"What are the top 10 item categories sold during           Christmas time across all years?\"}\nAnd got a SQL like:\nWITH christmas_sales AS (    SELECT        ss_item_sk,        SUM(ss_quantity) AS total_quantity    FROM tweakit_playground.retail_data.store_sales AS ss    INNER JOIN tweakit_playground.retail_data.date_dim AS dd        ON ss_sold_date_sk = d_date_sk    WHERE        d_holiday = 'Y' AND d_date ILIKE '%-12-25'    GROUP BY        ss_item_sk),ranked_categories AS (    SELECT        i_category,        SUM(total_quantity) AS category_quantity    FROM christmas_sales AS cs    INNER JOIN tweakit_playground.retail_data.item AS i        ON ss_item_sk = i_item_sk    GROUP BY        i_category    ORDER BY        category_quantity DESC    LIMIT 10)SELECT    i_category AS category,    category_quantityFROM ranked_categories\nLet me ask it to run the query:\nagent.chat(\"Run it\")\nI got a Dataframe along with its summary:\n\n\n\nThe top 10 item categories sold during Christmas time across all years, based on the total quantity of items, are:1. Women: 1,487,8912. Sports: 1,486,6443. Children: 1,483,5314. Electronics: 1,478,4635. Music: 1,476,0456. Books: 1,472,9117. Home: 1,471,3488. Jewelry: 1,459,0259. Shoes: 1,456,29610. Men: 1,451,285\nUse with a PDF report\n\n  Let\u2019s try to cross-check the result with the\n    holiday retailer report (in PDF form) from Deloitte\n  \u2014 Are Deloitte\u2019s top-purchased gift categories the same as what we saw from\n  the database?\n\n\n\nTop gift categories during the holiday\n\n\n  There\u2019s one page from the pdf we are interested in \u2014 let\u2019s see if we can\n  retrieve this page\n\nagent.chat(\"\"\"Give me top holiday gift categories based on Deloitte's holiday retail survey, and how it is different from the top 10 item category sold during christmas time across all years from my database\"\"\")\n\n  The output looks like the following, which indicates it uses the\n  deloitte_retail_survey tool to obtain insights from the PDF.\n\nSTARTING TURN 1---------------=== Calling Function ===Calling function: deloitte_retail_survey with args: {\"input\": \"What are the top holiday gift categories based on Deloitte's holiday retail survey?\"}...\nIt gives the following summary:\nBased on Deloitte's holiday retail survey, the top holiday gift categories are:1. Clothing & Accessories2. Gift Cards & Other3. Food & Beverage...From your database, the top 10 item categories sold during Christmas time across all years are:1. Women2. Sports3. Children...Comparing the two lists, we can see some differences and similarities:- \"Clothing & Accessories\" from Deloitte's survey could correspond to   \"Women,\" \"Men,\" and possibly \"Children\" from your database.- \"Electronics & Accessories\" is a common category in both lists.- \"Gift Cards & Other\" and \"Food & Beverage\" from Deloitte's survey do   not have a direct match in the top categories from your database....\n\n  Bingo! Now we can compare the results from our database with PDFs. And I love\n  seeing how the agent can correlate the two lists and tell me that my store\n  doesn\u2019t have the \u201cGift Cards & Other\u201d and \u201cFood & Beverage\u201d\n  categories!\n\n\n  You can find the code from the Colab notebook\n  link\n\nWrapping up\n\n  The integration of Waii\u2019s text-to-SQL API with LlamaIndex\u2019s RAG framework\n  marks a significant advancement in enterprise data analytics. This powerful\n  combination enables companies to effortlessly merge and analyze data from\n  various sources, including databases, PDFs, and the Internet. We demonstrated\n  the agent\u2019s capability to generate SQL queries, understand complex datasets,\n  and correlate findings with external reports. This innovation not only\n  simplifies data analysis but also opens new avenues for informed\n  decision-making in the digital era.\n\n\n  To learn more about Waii, please contact us here:\n  https://www.waii.ai/#request-demo\n\n",
            "related_posts": [
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse",
                "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-waii-combining-structured-data-from-your-database-with-pdfs-for-enhanced-data-647a9e66be82"
        },
        {
            "date": "Mar 19, 2024",
            "author": "Uptrain",
            "title": "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
            "tags": [
                "ai",
                "evaluation",
                "rag"
            ],
            "content": "This is a guest post from Uptrain.We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the \u2018vibes\u2019. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance.About UpTrainUpTrain [github || website || docs] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them.Key Highlights:Data Security: As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls).Custom Evaluator LLMs: UpTrain allows for customisation of your evaluator LLM, offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure.Insights that help with model improvement: Beyond mere evaluation, UpTrain performs root cause analysis to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution.Diverse Experimentations: The platform enables experimentation with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case.Compare open-source LLMs: With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality.In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what\u2019s affecting the quality of your responses, allowing you to take appropriate corrective actions.LlamaIndex x UpTrain Callback HandlerWe introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps.If you wish to skip right ahead to the tutorial, check it out here.Evals across the board: From Vanilla to Advanced RAGVanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response.While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses.But as Uncle Ben famously said to Peter Parker in the GenAI universe:\u201cWith increased complexity comes more points of failure.\u201d.Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let\u2019s look at all the evaluations provided by UpTrain.Addressing Points of Failure in RAG Pipelines1. RAG Query Engine EvaluationLet's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response.Context Relevance: However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query.Factual Accuracy: Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context.Response Completeness: Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query.2. Sub-Question Query Engine EvaluationLet's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine.If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs:Sub Query Completeness: It evaluates whether the sub-questions accurately and comprehensively cover the original query.Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries.3. Reranking EvaluationsWe looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores.RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [Lost in the Middle: How Language Model Uses Long Contexts] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block.Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity.We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process:a. Same Number of Nodes Before and After Reranking:If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator.Context Reranking: Checks if the order of reranked nodes is more relevant to the query than the original order.b. Fewer Number of Nodes After Reranking:Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator.Context Conciseness: Examines whether the reduced number of nodes still provides all the required information.Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and EvaluationLet's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance.This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions.Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this quickstart tutorial.ReferencesUpTrain Callback Handler TutorialUpTrain GitHub RepositoryAdvanced RAG Techniques: an Illustrated OverviewLost in the Middle: How Language Models Use Long ContextsUpTrainCallbackHandler documentationUpTrain Website",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"
        },
        {
            "date": "Feb 29, 2024",
            "author": "Raymond Weitekamp",
            "title": "Unlocking the 3rd Dimension for Generative AI (Part 1)",
            "tags": [
                "llm",
                "hackathon"
            ],
            "content": "It would be an understatement to say that generative AI has been taking the world by storm over the past couple of years. While text (1D) and image (2D) models are reaching a level of quality that is truly transforming the way we create digital content, the same cannot be said for 3D models.\n3D generative AI has made significant strides in creating 3D digital assets, with techniques such as Neural Radiance Fields (NeRFs) showing promising results for applications such as video game development. However, the limitations of current generative AI technologies become apparent when it comes to applications beyond the digital realm, especially in the context of engineering and manufacturing. After spending more than 2 years actively engaged with the state-of-the-art tools for 3D generative AI, I personally have not been able to generate a single model that I would actually want to have manufactured into a physical object.\nTo put it simply: the current state of 3D generative AI is not very useful for engineers.\nThe Motivation Behind neThing.xyz\nOur mission at polySpectra is to help engineers make their ideas real. The key insight that led to the invention of neThing.xyz (pronounced \u201canything dot x,y,z\u201d) was that AI is actually quite good at writing code, and by training a \u201ccodegen\u201d AI on domain specific languages for \u201ccode CAD\u201d, our AI can produce code that can be rapidly converted into 3D CAD models.\nFor some quick context, AI code generation tools are now achieving ~95% evaluation benchmarks against human programmers. At the current pace, I wouldn\u2019t be surprised if an AI hits 100% in the next three weeks. (See this leaderboard for more details.)\nIn 3D modeling, there is a growing buzz around \u201ccode CAD\u201d \u2014 a term that signifies a paradigm shift in how we approach computer-aided design. Unlike traditional graphical CAD interfaces, which rely heavily on visual tools and manual user input, code CAD leverages programming to create and manipulate 3D models. This approach offers a more direct and potentially more powerful method for generating complex designs, as it allows for precision and automation that can be difficult to achieve with mouse-driven interfaces.\n A testament to the rising prominence of code CAD was its debut this month in the Too Tall Toby speed CAD competition. In the second match of the video below, Jern competes using the code CAD package Build123d, against \u201cMr. Alex\u201d who is using the traditional CAD tool SolidWorks.\n\n(Jump to 1:18 for competition-grade Code CAD!)\nSo my idea was simple: if AI can code, and code can CAD, why can\u2019t AI CAD?\nWhy RAG?\nTrying to create a 3D generative AI with this code generation approach led us to confront a significant challenge: the need for incredibly long prompts to provide the AI with enough context about our code CAD domain-specific languages. This was essential for it to stand any chance of producing working code, let alone something useful.\nI first studied Retrieval-Augmented Generation (RAG) under the tutelage of Mayo Oshin. (Check out his RAG course!). I knew that RAG was going to be a necessary part of the strategy for making neThing.xyz \u201csmarter\u201d, and through Mayo\u2019s course I had the opportunity to meet Jerry Liu and ask him some questions about the more nuanced elements of retrieval.\nThe initial version of neThing.xyz was ok, but I really had to wrangle the LLM: each query involved about 10,000 tokens. So if a user asks for \u201ca box\u201d, it is not 2 input tokens I\u2019m paying for, it is 10,002 tokens. I knew I needed a more scalable approach\u2026\nEnter the RAG-a-thon\nThe announcement of the \u201cRAG-a-thon\u201d presented the perfect opportunity to quickly integrate RAG into neThing.xyz. For those familiar with the whirlwind nature of hackathons, you\u2019ll understand when I say that time always seems to be in shorter supply than anticipated, often leading one to overestimate what can be accomplished. (For me, I usually overestimate what I can achieve by 3\u201310x!) With this in mind, I tried to set a modest goal for the weekend: add LlamaIndex to neThing.xyz.\nMy ultimate aim was to leverage LlamaIndex to dramatically expand the corpus of documentation available to neThing.xyz. But in the theme of setting a low bar, I started by just breaking down my very large system prompt into a set of documents that LlamaIndex could retrieve from AstraDB, bringing back only the most relevant example code for a given user\u2019s query.\nWith significant assistance from Logan Markewich from the LlamaIndex team, I managed to implement RAG via LlamaIndex and AstraDB in a single day. This immediately reduced the average number of tokens per user query from about 10,000 to roughly 2,000. The impact of this was huge, resulting in an 80% cost reduction in our OpenAI bill in just one day \u2014 a change that was incredibly meaningful for us as a small business.\nWhile I am personally passionate about the myriad ways in which RAG can make LLMs smarter, I want to emphasize the immediate ROI that RAG provided. By simply reorganizing the same set of information for retrieval through LlamaIndex, we achieved significant cost savings with minimal effort. As an entrepreneur, an 80% reduction in costs with just eight hours of work is a deal I\u2019d take any day of the week.\nOn the final Sunday of the RAG-a-thon, I dedicated most of my time to ensuring that my demo would function correctly. The \u201cdemo gods\u201d blessed me that day: I won first place in the \u201ccontinuous innovation\u201d track!\nExamples of neThing.xyz in Action:\nText:\n\nCurves:\n\nThreads:\n\nPipes:\n\nLattices:\n\nWhat\u2019s Next?\nMy goal with neThing.xyz is to make the best 3D generative AI for engineers, with a focus on \u201ctext-to-CAD\u201d. This is a really hard problem, and I shared some of these challenges with LlamaIndex in our recent webinar, and in more detail on Wevolver.\nOur key objectives are to make neThing.xyz faster, smarter, and cheaper. We are leveraging LlamaIndex to orchestrate the entire RAG pipeline, and we are really excited about the amazing pace of developments in this open source community.\nWhat would it take to get a part like this from a natural language prompt?\n\nHonestly, I don\u2019t know how to do it.\nWe are just getting things off the ground and I would be tremendously excited to have you join our community. Our AI will only ever be as smart as the sum of the community that trained it, and we are excited to see what you will create!\nPlease give neThing.xyz a try today and share your honest feedback with us via our new community forum. Your input will play a crucial role in our ongoing development efforts, helping us to refine and improve the tool.\nMake it real.\nRaymond\nP.S. \u2014 This article is titled Part 1 for a reason. What do you want to see in Part 2? Tell me below!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//unlocking-the-3rd-dimension-for-generative-ai-part-1"
        },
        {
            "date": "Nov 7, 2023",
            "author": "Laurie Voss",
            "title": "LlamaIndex news special edition: OpenAI developer day!",
            "tags": [
                "llm",
                "ai",
                "llamaindex",
                "openai"
            ],
            "content": "Hello Llama fans!Yesterday was a big day in the world of LLMs; OpenAI held their developer day conference and there were a slew of new features. The team were all hands on deck to bring support for these features to the library as fast as possible \u2014 which is to say, the same day!In case you missed our tweet about it, if you install the latest build of LlamaIndex you\u2019ll get everything below:Support for two new models released todaygpt-4-1106-preview , aka GPT-4 Turbo, the latest GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and a 128,000 token context windowgpt-4-vision-preview , aka GPT 4 Turbo with vision with long-awaited multimodal support, has the ability to understand images in addition to all the other GPT-4 Turbo capabilities.You can use these models just as you would any other OpenAI model:from llama_index.llms import OpenAI\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\nllm = OpenAI(model=\"gpt-4-1106-preview\")\nservice_context = ServiceContext.from_defaults(llm=llm)\ndocuments = SimpleDirectoryReader(\"data\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)Azure OpenAI endpointsCheck out the OpenAI Azure notebook for examples.New embeddings abstractionsIncluding Azure embeddings.Function callingCheck out our notebook for examples.SEC insightsOur demo of the power of retrieval-augmented generation for financial filings, SEC Insights, has been updated to use the latest version of GPT-4! Watch as you instantly get deeper insights and more relevant responses.Look out for more OpenAI updates soon! Our regular newsletter will also be posted tomorrow.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-news-special-edition-openai-developer-day-e955f16db4e2"
        },
        {
            "date": "Dec 4, 2023",
            "author": "Jerry Liu",
            "title": "Introducing Llama Datasets \ud83e\udd99\ud83d\udcdd",
            "tags": [
                "llamaindex",
                "dataset",
                "evaluation",
                "llm",
                "ai"
            ],
            "content": "(Authors: Andrei Fajardo and Jerry Liu @ LlamaIndex)Today we\u2019re excited to introduce Llama Datasets \ud83e\udd99 \ud83d\udcdd\u2014 a set of community-contributed datasets that allow users to easily benchmark their RAG pipelines for different use cases. A dataset consists of both question-answer pairs as well as source context. To use them, download them from LlamaHub; then evaluate your RAG pipeline using the dataset + a set of evaluation metrics.We\u2019re launching with an initial set of 10 evaluation datasets and we\u2019ll be adding more! We\u2019ve also made it super easy to contribute your own dataset \u2014 upload your source documents + QA pairs (generated manually or synthetically).ContextA big problem in building production RAG is evaluation. Unlike traditional software systems, LLM systems (and ML systems more generally) are stochastic black-boxes designed to model noisy real-world signals. This means that developers can\u2019t easily define unit tests that assert deterministic behavior \u2014 there may always be an input that causes an error. Because developers don\u2019t quite know what goes out given what goes in, they need to define an evaluation dataset that\u2019s reflective of their production use cases, and evaluate their system over this dataset using a set of evaluation metrics.We\u2019ve presented extensively on this topic \u2014 every AI engineer should setup evaluation before trying to optimize their LLM or RAG application with advanced techniques.But we\u2019ve increasingly found that defining the right evaluation dataset is hard and use-case dependent. Evaluating over academic benchmarks, like BEIR and HotpotQA oftentimes fail to generalize to specific use cases. Certain parameters that work well on certain data domains (e.g. SEC filings) may fail on others (e.g. research papers).That\u2019s what inspired us to create Llama Datasets. Instead of being prescriptive on the data you must use, we\u2019ve decided to create a hub where you can easily pick and choose the right datasets for your use case!Llama Datasets on LlamaHubOverviewToday\u2019s launch includes the set of Llama Datasets on LlamaHub, an accompanying RagEvaluatorPack to help compute metrics over a dataset, as well as accompanying dataset abstractions that you can also use on their own.To use a Llama Dataset, download it off LlamaHub and run our RagEvaluatorPack (or run your own evaluation modules).To generate a Llama Dataset, define a LabelledRagDataset with a set of LabelledRagDataExampleobjects.To contribute a Llama Dataset, submit a \u201cdata card\u201d to LlamaHub and upload your raw dataset files to our llama_datasets repository.Check out the below sections for a walkthrough over an example dataset.We\u2019re launching with 10 initial datasets:Blockchain Solana DatasetCoda Help Desk Dataset (with Braintrust)FinanceBench Dataset (Patronus AI)Paul Graham Essay DatasetLlama 2 Paper DatasetUber/Lyft 2021 10K Filings DatasetMini Truthful QA Dataset (Arize AI)Mini Squad V2 Dataset (Arize AI)Origin of COVID-19LLM Survey Paper DatasetExample Llama Dataset pageExample WalkthroughLet\u2019s walk through the different steps of using/contributing a Llama Dataset.1. Downloading and Using a Llama DatasetFollow the full notebook here.Downloading a dataset is simple, do the following command (here we download Paul Graham).from llama_index.llama_dataset import download_llama_dataset\n\n# download and install dependencies\nrag_dataset, documents = download_llama_dataset(\n    \"PaulGrahamEssayDataset\", \"./paul_graham\"\n)This downloads a rag_dataset which contains the QA pairs (+ reference context), and documents which is the source document corpus.Let\u2019s inspect the rag_dataset with to_pandas() :Sample rows from `rag_dataset`Generating predictions over the RAG dataset is straightforward. You can easily plug in any query engine into amake_predictions_with :from llama_index import VectorStoreIndex\n\n# a basic RAG pipeline, uses service context defaults\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine()\n\n# generate prediction dataset\nprediction_dataset = await rag_dataset.amake_predictions_with(\n    query_engine=query_engine, show_progress=True\n)The prediction_dataset is a RagPredictionDataset object that looks like the following:Prediction DatasetGiven the rag_dataset and prediction_dataset , you can use our evaluation modules to measure performance across a variety of metrics (e.g. faithfulness, correctness, relevancy).for example, prediction in tqdm.tqdm(\n    zip(rag_dataset.examples, prediction_dataset.predictions)\n):\n    correctness_result = judges[\"correctness\"].evaluate(\n        query=example.query,\n        response=prediction.response,\n        reference=example.reference_answer,\n    )To eliminate the boilerplate of writing all these evaluation modules, we\u2019ve also provided a LlamaPack that will do all this for you!from llama_index.llama_pack import download_llama_pack\n\nRagEvaluatorPack = download_llama_pack(\"RagEvaluatorPack\", \"./pack\")\nrag_evaluator = RagEvaluatorPack(\n    query_engine=query_engine, rag_dataset=rag_dataset\n)\nbenchmark_df = await rag_evaluator.arun()2. Generating a Llama DatasetFollow the full notebook here.You can use our LabelledRagDataExample and LabelledRagDataset abstractions to create your own dataset.Here\u2019s an example of adding an example manually.from llama_index.llama_dataset import (\n    LabelledRagDataExample,\n    CreatedByType,\n    CreatedBy,\n)\n\n# constructing a LabelledRagDataExample\nquery = \"This is a test query, is it not?\"\nquery_by = CreatedBy(type=CreatedByType.AI, model_name=\"gpt-4\")\nreference_answer = \"Yes it is.\"\nreference_answer_by = CreatedBy(type=CreatedByType.HUMAN)\nreference_contexts = [\"This is a sample context\"]\n\nrag_example = LabelledRagDataExample(\n    query=query,\n    query_by=query_by,\n    reference_contexts=reference_contexts,\n    reference_answer=reference_answer,\n    reference_answer_by=reference_answer_by,\n)from llama_index.llama_dataset.rag import LabelledRagDataset\n\nrag_dataset = LabelledRagDataset(examples=[rag_example, rag_example_2])You can also synthetically generate a dataset over any document corpus with GPT-4:# generate questions against chunks\nfrom llama_index.llama_dataset.generator import RagDatasetGenerator\nfrom llama_index.llms import OpenAI\nfrom llama_index import ServiceContext\n\n# set context for llm provider\ngpt_4_context = ServiceContext.from_defaults(\n    llm=OpenAI(model=\"gpt-4\", temperature=0.3)\n)\n\n# instantiate a DatasetGenerator\ndataset_generator = RagDatasetGenerator.from_documents(\n    documents,\n    service_context=gpt_4_context,\n    num_questions_per_chunk=2,  # set the number of questions per nodes\n    show_progress=True,\n)3. Contributing a Llama DatasetWe\u2019ve provided a ready-made submission notebook template here \u2014 just fill in the blanks with your dataset!If you\u2019re interested in contributing a dataset, we\u2019d love to feature it! You just need to follow these steps:Create the dataset: To create a LabelledRagDataset , you can create it from scratch either manually or with synthetically generated examples, or create it from an existing dataset.Generate a baseline evaluation dataset: Benchmark a basic top-k RAG pipeline over your dataset, and report the numbers. This will serve as a point of reference for others. You can use the RagEvaluatorPack for this purpose.Prepare the dataset card ( card.json ) and README.md : These will be shown on the LlamaHub page for this dataset. If you want to auto-generate this given some inputs, check out our LlamaDatasetMetadataLlamaPack.Submit a PR into llama-hub to register the LlamaDataset .Submit a PR into llama-datasets to upload the LlamaDataset and its source files.You can follow all of these steps in our notebook template above \u2014 simply substitute your own data.ConclusionWe\u2019d love for you to check out our datasets and let us know your feedback! We\u2019d love your contributions as well.ResourcesHere are the resources mentioned in the blog post.Llama Datasets on LlamaHub (make sure to select \u201cLlama Datasets\u201d from the dropdown)Downloading a Llama Dataset NotebookCreating a Llama Dataset NotebookContributing a Llama Dataset Notebook TemplateREADME on Contributing a Llama Dataset",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-llama-datasets-aadb9994ad9e"
        },
        {
            "date": "Sep 6, 2023",
            "author": "Jerry Liu",
            "title": "Fine-Tuning a Linear Adapter for Any Embedding Model",
            "tags": [
                "fine-tuning",
                "embedding",
                "llamaindex",
                "nlp",
                "ai"
            ],
            "content": "We\u2019ve added capabilities in LlamaIndex allowing you to fine-tune a linear adapter on top of embeddings produced from any model (sentence_transformers, OpenAI, and more).This allows you to transform your embedding representations into a new latent space that\u2019s optimized for retrieval over your specific data and queries. This can lead to small increases in retrieval performance that in turn translate to better performing RAG systems.A nice bonus: you do not need to re-embed your documents by using this adapter! Simply transform the query instead.We have a full end-to-end guide showing how you can generate a synthetic dataset, fine-tune the linear adapter, and evaluate its performance.ContextThe concept of fine-tuning your embedding model is powerful. In fact, we were inspired to both add a full example repository / blog post as well as native abstractions in LlamaIndex showing how you can fine-tune a sentence_transformers model over any unstructured text corpus (with our SentenceTransformersFinetuneEngine).However, this approach has some limitations:The SentenceTransformersFinetuneEngine is limited to fine-tuning sentence_transformers models.After finetuning the embedding model, you will need to re-embed your document corpus.During our Finetuning + RAG webinar last Friday, Jo (Vespa) mentioned the exact same problem: fine-tuning the embeddings model requires you to reindex your documents. However, his work with Vespa explored the concept of \u201cfreezing\u201d document embeddings using a foundation model, and instead training a transformation on the query embedding.This inspired us to explore a similar embedding fine-tuning approach that was simultaneously more general but also allowed us to freeze existing document embeddings.ApproachOur brand-new EmbeddingAdapterFinetuneEngine fine-tunes a linear adapter on top of query embeddings produced by any model. The linear adapter is simply a linear transformation that specifically transforms the query embedding while keeping document embeddings fixed.The linear adapter can be used on top of any existing embeddings model: SBERT embeddings, OpenAI embeddings, Cohere embeddings, and more. As a result you can just plug this in on top of any embedding model that you\u2019re already using!Since document embeddings are unchanged, this means that you can always fine-tune this linear adapter after you\u2019ve generated embeddings for your documents. You can choose to arbitrarily re-train this adapter on top of changing data distributions, without needing to re-embed all your documents.Technical DetailsAs mentioned above, the linear adapter simply performs a linear transformation on top of the query embedding while keeping the Document embeddings fixed (with a weight matrix W + bias term b):And that\u2019s it! If document embeddings can be represented as a (n x d) matrix D, where n is number of documents and d is the embedding dimension, then embedding similarity is just measured byThe linear adapter is trained using a similar loss term as the MultipleNegativesRankingLoss function in sentence_transformers \u2014 given a batch of positive (question, context) examples, the function uses cross-entropy loss under the hood to penalize the ground-truth (question, context) pairs for being far apart and swapped pairs for being too close.Additional Notes: We ended up writing the bulk of this fine-tuning logic in plain PyTorch, but taking heavy inspiration from the sentence_transformers source code. We couldn\u2019t use sentence_transformers directly since we take in embeddings as inputs rather than raw text. You can take a look at some of our training code here.Notebook WalkthroughIn this notebook walkthrough, we follow a similar set of steps as our previous blog post on embedding fine-tuning:Generate a synthetic question-context dataset for both training and evaluation.Fine-tuning our linear adapter on top of an existing model (e.g. SBERT)Getting the embedding model, and evaluating it.As with the previous post, we use the UBER and LYFT 10K as example data. We use Lyft to generate our training dataset and Uber to generate our evaluation dataset.The full guide is here: https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.htmlGenerate a Synthetic Dataset for Trraining and EvaluationWe use our helper abstractions, generate_qa_embedding_pairs , to generate our training and evaluation dataset. This function takes in any set of text nodes (chunks) and generates a structured dataset containing (question, context) pairs.from llama_index.finetuning import (\n    generate_qa_embedding_pairs,\n    EmbeddingQAFinetuneDataset,\n)\n\n# generate\ntrain_dataset = generate_qa_embedding_pairs(train_nodes)\nval_dataset = generate_qa_embedding_pairs(val_nodes)\n\n# save\ntrain_dataset.save_json(\"train_dataset.json\")\nval_dataset.save_json(\"val_dataset.json\")\n\n# load \ntrain_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\nval_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")Fine-tuning our Linear AdapterWe then fine-tune our linear adapter on top of an existing embedding model. We import our newEmbeddingAdapterFinetuneEngine abstraction, which takes in an existing embedding model and a set of training parameters.In this example we use the bge-small-en sentence-transformers model, but we can also use any embedding model in LlamaIndex/LangChain.from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\nfrom llama_index.embeddings import resolve_embed_model\nimport torch\n\nbase_embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\n# alternative: use OpenAI\n# from llama_index.embeddings import OpenAIEmbedding\n# openai = OpenAIEmbedding()\n\nfinetune_engine = EmbeddingAdapterFinetuneEngine(\n    train_dataset,\n    base_embed_model,\n    model_output_path=\"&lt;model_output_path&gt;\",\n    epochs=4,\n    verbose=True,\n    # can optionally pass along any parameters that go into `train_model`\n    # optimizer_class=torch.optim.SGD,\n    # optimizer_params={\"lr\": 0.01}\n)We can then call fine-tune to kick off the fine-tuning job. Training a linear model is quite straightforward and doesn\u2019t require heavy machinery \u2014 this can easily run on a Macbook.finetune_engine.finetune()Getting the Embedding Model, and Evaluating itOnce the fine-tuning job is then, we can then fetch our embedding model.We can either directly fetch it from our finetune_engine , or import our new LinearAdapterEmbeddingModel and construct it in a more manual fashion.Option 1:embed_model = finetune_engine.get_finetuned_model()Option 2:from llama_index.embeddings import LinearAdapterEmbeddingModel\n\nembed_model = LinearAdapterEmbeddingModel(base_embed_model, \"&lt;model_output_path&gt;\")The next step is to evaluate it. We compare the fine-tuned model against the base model, as well as against text-embedding-ada-002.We evaluate with two ranking metrics:Hit-rate metric: For each (query, context) pair, we retrieve the top-k documents with the query. It\u2019s a hit if the results contain the ground-truth context.Mean Reciprocal Rank: A slightly more granular ranking metric that looks at the \u201creciprocal rank\u201d of the ground-truth context in the top-k retrieved set. The reciprocal rank is defined as 1/rank. Of course, if the results don\u2019t contain the context, then the reciprocal rank is 0.Some additional comments:We ran with 4 epochs over the Lyft documentsWe used Adam as an optimizer with the default learning rate (we tried SGD and it didn\u2019t work as well)ResultsQuantiative metrics (hit-rate and MRR) for ada, bge, and our fine-tuned modelIn terms of hit-rate, the base model gets 78.7% hit-rate on the validation dataset, and the fine-tuned model gets 79.8%. In the meantime text-embedding-ada-002 gets 87.0%.In terms of MRR, the base model gets 64.3%, and the fine-tuned model gets 66%. text-embedding-ada-002 gets 68.4%.There is some performance bump from the fine-tuned model, though admittedly it is small \u2014 it is smaller than the performance bump gained through fine-tuning sentence_transformers directly on the latest dataset.That said, a performance bump is still a performance bump, and it\u2019s very cheap for you to spin up and try yourself! So you can decide whether or not this would make sense for you.ConclusionWe created a brand-new module in LlamaIndex that allows you fine-tune a linear adapter on top of any embedding model.It can help you eke out some marginal improvement in retrieval metrics; importantly, it allows you to keep document embeddings fixed and only transform the query.ResourcesGuide: https://gpt-index.readthedocs.io/en/latest/examples/finetuning/embeddings/finetune_embedding_adapter.htmlTraining code (if you want to take a look for yourself): https://github.com/jerryjliu/llama_index/blob/main/llama_index/finetuning/embeddings/adapter_utils.py",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//fine-tuning-a-linear-adapter-for-any-embedding-model-8dd0a142d383"
        },
        {
            "date": "Jun 26, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex Update \u2014 06/26/2023",
            "tags": [
                "large-language-models",
                "llamaindex",
                "nlp",
                "ai",
                "openai"
            ],
            "content": "Greetings, LlamaIndex community!We\u2019re excited to introduce our new blog series, the LlamaIndex Update. Recognizing the fast pace of our open-source project, this series will serve as your continual guide, tracking the latest advancements in features, webinars, hackathons, and community events.Our goal is simple: to keep you updated, engaged, and inspired. Whether you\u2019re a long-time contributor or a new joiner, these updates will help you stay in sync with our progress.So, let\u2019s explore the recent happenings in our premier edition of the LlamaIndex Update.Features And Integrations:LLMs with Knowledge Graphs, supported by NebulaGraph. This new stack enables unique retrieval-augmented generation techniques. Our Knowledge Graph index introduces a GraphStore abstraction, complementing our existing data store types.\nDocs, TweetBetter LLM app UX supports in-line citations of its sources, enhancing interpretability and traceability. Our new CitationQueryEngine enables these citations and ensures they correspond with retrieved documents. This feature marks a leap towards improving transparency in LlamaIndex applications.\nDocs, TweetLlamaIndex integrates with Microsoft Guidance to ensure structured outputs from LLMs. It allows direct prompting of JSON keys and facilitates the conversion of Pydantic objects into the Guidance format, enhancing structured interactions. It can be used independently or with the SubQuestionQueryEngine.\nDocs, TweetThe GuidelineEvaluator module allows users to set text guidelines, thereby aiding in the evaluation of LLM-generated text responses. This paves the way toward automated error correction capabilities.\nNotebook, TweetWe now include a simple OpenAIAgent, offering an agent interface capable of sequential tool use and async callbacks. This integration was made possible with the help of the OpenAI function API and the LangChain abstractions.\nTweetOpenAIPydanticProgram in LlamaIndex enhances structured output extraction. This standalone module allows any LLM input to be converted into a Pydantic object, providing a streamlined approach to data structuring.\nDocs, TweetWe now incorporate the FLARE technique for a knowledge-augmented long-form generation. FLARE uses iterative retrieval to construct extended content, deciding to perform retrieval with each sentence. Unlike conventional vector index methods, our FLARE implementation builds a template iteratively, filling gaps with retrieval for more pertinent responses. Please note, this is a beta feature and works best with GPT-4.\nDocs, TweetWe now employ the Maximal Marginal Relevance (MMR) algorithm to enhance diversity and minimize redundancy in retrieved results. This technique measures the similarity between a candidate document and the query while minimizing similarity with previous documents, depending on a user-specified threshold. Please note that careful calibration is necessary to ensure that increased diversity doesn\u2019t introduce irrelevant context. The threshold value is key to balancing diversity and relevance.\nDocs, TweetWe now support recursive Pydantic objects for complex schema extraction. This enhancement, inspired by parsing directory trees, employs a mix of recursive (Node) and non-recursive (DirectoryTree) Pydantic models, facilitating more sophisticated agent-tool interactions.\nTweetWe have developed agents that can perform advanced query planning over data using the Function API and Pydantic. These agents input a full Pydantic graph in the function signature of a query plan tool, which is then executed. This system can work with any tool and has the potential to construct complex query plans. However, it has limitations like difficulty in producing deep nesting and the possibility of outputting invalid responses.\nDocs, Tweet`OpenAIAgent` is capable of advanced data retrieval and analysis, such as auto-vector database retrieval and joint text-to-SQL and semantic search. We have also built a query plan tool interface that allows the agent to generate structured/nested query plans, which can then be executed against any set of tools, enabling advanced reasoning and analysis.\nDocs: OpenAI Agent + Query Engine, Retrieval Augmented OpenAI Agent, OpenAI Agent Query Planning.\nTweetThe new multi-router feature allows for QA over complex data collections, where answers may be spread across multiple sources. It uses a \u201cMultiSelector\u201d object to select relevant choices given a query. The router can pick up to a maximum number of choices. It can use either a raw LLM completion API or the OpenAI Function API. If the Function API is used, schema validity can be enforced. A simple usage example involves a RouterQueryEngine, where the PydanticMultiSelector selects the relevant vector and keyword index to synthesize an answer.\nDocs, TweetWe have made a significant upgrade to our token tracking feature. Users can now easily track prompt, completion, and embedding tokens through the platform\u2019s callback handler. The upgrade aims to make token counting more efficient and user-friendly.\nDocs, TweetWe released a guide that demonstrates how to build a custom retriever that combines vector similarity search with knowledge graphs in LLM RAG systems. It involves constructing a vector index and a knowledge graph index and combining the results from both during query time. This method can improve results by providing additional context for entities. However, it may lead to a slight increase in latency.\nDocs, TweetIn an LLM workflow, managing large amounts of data, including PDFs, agent Tools, SQL table schemas, etc., requires efficient indexing. To handle this, we introduce our Object Index, a wrapper over our existing index data structures. This allows any object to be converted into an indexable text format, providing a unified interface that enhances the functionality of our indices over various data types.\nTweetThe OpenBB Finance Terminal is a great platform for investment research and is completely open-source. It now includes a feature called AskOBB, powered by Llama Index, which allows users to easily access any financial data through natural language.\nTweetThe TruLens team has introduced tracing for LlamaIndex-based LLM applications in its latest release. This new feature allows developers to evaluate and track their experiments more efficiently. It automatically evaluates various components of the application stack, including app inputs and outputs, LLM calls, retrieved-context chunks from an index, and latency. This is part of an ongoing collaboration between the LlamaIndex and TruLens teams to improve the development, evaluation, and iteration of LLM apps.\nNotebook, BlogpostPrem App has successfully integrated with Llama Index, enhancing privacy in AI development. This union allows developers to connect custom data sources to large language models easily, simplifying data ingestion, indexing, and querying. To use this integration, download the Prem App and connect your data sources through the Llama Index platform. This allows for efficient data management and boosts AI application development, providing developers with more control and flexibility.\nNotebook, BlogpostWe now enable the extraction of tabular data frames from unstructured text. This feature, powered by the OpenAI Function API and Pydantic models, simplifies text-to-SQL or text-to-DF conversions within structured data workflows. Note that effective use may require significant prompt optimization.\nDocs, TweetTutorials:James Brigg\u2019s tutorial on using LlamaIndex with Pinecone.Jerry Liu's tutorial on using LlamaIndex with Weaviate.Sophia Yang tutorial on LlamaIndex overview, Use cases, and integration with LangChain.Anil Chandra Naidu is building a course on LlamaIndex. The course presently covers topics such as introduction, fundamentals, and data connectors.OpenAI cookbook by Simon on how to perform financial analysis with LlamaIndex.Webinars And Podcasts:Webinar on Demonstrate-Search-Predict (DSP) with Omar Khattab.Webinar on Practical challenges of building a Legal Chatbot over your PDFs with Sam YuMaML podcast with Jerry Liu.Hackathons:The LlamaIndex team has presented at the UC Berkeley Hackathon and the Stellaris VP Hackathon in India. The community has warmly welcomed LlamaIndex, and teams at these hackathons have developed intriguing use cases \u2014 Customer support during emergency cases, Understanding Legal documents.Events:Jerry Liu spoke on Building and troubleshooting an AI Search & Retrieval System at Arize \u2014 LlamaIndex event.Ravi Theja presented about LlamaIndex and its applications at Together in India.That\u2019s all for this edition of the LlamaIndex Update. We hope you found this information useful and are as excited as we are about the progress we\u2019re making. We\u2019re grateful for the continued support and contributions from our community. Remember, your feedback and suggestions are invaluable to us, so don\u2019t hesitate to reach out.Stay tuned for our next update, where we\u2019ll share more exciting developments from the LlamaIndex project. Until then, happy indexing!",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-update-6-26-2023-ed30a9d45f84"
        },
        {
            "date": "Oct 24, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex newsletter 2023\u201310\u201324",
            "tags": [
                "llm",
                "ai",
                "llamaindex",
                "newsletter",
                "aws"
            ],
            "content": "Hello Llama Fans \ud83e\udd99!Welcome back to our newsletter covering new features, guides, integrations, webinars, tutorials, and more. Got a project, blog, or video you\u2019re proud of? Let\u2019s spotlight it! Contact us at news@llamaindex.ai.Plus, for direct updates in your email, just head to our homepage and subscribe to our newsletter.\ud83e\udd29 First, the highlights:QueryFusionRetriever Launch: Inspired by Adrian Raudaschl\u2019s RAG-Fusion, enhancing multiple query generation with LLMs. Tweet, Docs.Router Fine-Tuning: Our innovative router fine-tuning approach has achieved an outstanding 99% match rate, outpacing both the gpt-3.5\u2019s 65% and the base model\u2019s 12%. Tweet, Docs.Fusion Retriever Guide: Guide on building an advanced Fusion Retriever from scratch. DocsAmazon Bedrock LLMs and AI21 Labs LLMs: We have expanded our LLM compatibility, now seamlessly integrating with both Amazon Bedrock and AI21 Labs models.\u2728 Feature Releases and Enhancements:QueryFusionRetriever: We introduced the QueryFusionRetriever, inspired by Adrian Raudaschl\u2019s work on RAG-Fusion. This retriever allows users to generate multiple queries with LLMs, run various retrieval methods, and apply reciprocal rank fusion for improved results. Tweet, Docs.Router Fine-Tuning: We introduced router fine-tuning (V0) for improved LLM automated decision-making. Our approach achieved a 99% match rate, outperforming gpt-3.5\u2019s 65% and the base model\u2019s 12%. Tweet, Docs.SQLRetriever: We introduce SQLRetriever, merging Text-to-SQL and RAG, enabling a RAG pipeline setup over SQL databases for structured table node retrieval and response synthesis. Tweet, Docs.\ud83d\uddfa\ufe0f Guides:Tutorial guide on Building an Advanced Fusion Retriever from Scratch.\u270d\ufe0f Tutorials:Saurav Joshi\u2019s tutorial on Complex Query Resolution through LlamaIndex Utilizing Recursive Retrieval, Document Agents, and Sub Question Query Decomposition.Greg Loughnane and Chris Alexiuk tutorial on tackling domain-specific fine tuning using LlamaIndex.Vishwas Gowda\u2019s blog post on Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.Emanuel Ferreira\u2019s blog post on the RA-DIT paper and its implementation in LlamaIndex.Yujian Tang\u2019s blog post on Chat with Towards Data Science using LlamaIndex.Sudarshan Koirala tutorial on Chat with documents with Pinecone and LlamaIndex.Sudarshan Koirala tutorial on Combined Text-TO-SQL + Semantic Search with LlamaIndex.PromptEngineer tutorial on building LLM-powered financial analyst with LlamaIndex.\u2699\ufe0f Integrations & Collaborations:Gradient AI: We introduce a collaboration with Gradient AI to easily integrate fine-tuned LLMs into your LlamaIndex RAG pipeline. Tweet, Blogpost.PrivateGPT: PrivateGPT partners with LlamaIndex allowing private document interactions using default or custom integrations. Tweet.VectorFlow & LlamaHub Collaboration: VectorFlow\u2019s open-source vector-embedding pipeline now leverages LlamaHub for data connectors to streamline code and reduce maintenance. Tweet.Amazon Bedrock & AI21 Labs LLMs: We\u2019ve broadened our LLM compatibility range by integrating with Amazon Bedrock LLMs and AI21 Labs LLMs.DashVector: We have introduced an integration with DashVector, a robust, fully-managed vectorDB service.Tencent Cloud: We\u2019ve integrated with Tencent Cloud VectorDB.PGVectorStore within LlamaIndex has been enhanced to support custom Postgres schemas. This facilitates better index management and promotes easy schema-based versioning.We now accommodate custom models that align with the OpenAI-compatible API.\ud83c\udfa5 Webinars:Wenqi Glantz workshop webinar on Evaluation-Driven Development (EDD).Webinar showcasing the winning projects from the recent AGI House hackathon: \u201cBuild, Test, and Launch LLM Apps\u201d. This event was co-sponsored by LlamaIndex, TruEra, and Pinecone.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-10-24-4a76204eeaa3"
        },
        {
            "date": "Mar 20, 2024",
            "author": "Protect AI",
            "title": "Secure RAG with LlamaIndex and LLM Guard by Protect AI",
            "tags": [
                "security",
                "privacy"
            ],
            "content": "This is a guest post by Protect AI.We believe that RAG will be one of the preferred approaches for enterprises when developing LLM applications to generate prompt responses that are more relevant, and accurate, tailored to and based on company-specific content. However, while analyzing web pages with ChatGPT may leave the LLM vulnerable to injections embedded within the webpage, it is crucial to recognize that injections may also be concealed within the vector database or knowledge graph where data is retrieved and injected into the LLM.That is why we\u2019re thrilled to describe how LLM Guard by Protect AI can secure your data sources accessed for context in your LLM application, built with LlamaIndex.LLM Guard is an open source solution by Protect AI designed to fortify the security of Large Language Models (LLMs). It is designed for easy integration and deployment in production environments. It provides extensive security scanners for both prompts and responses of LLMs to detect, redact, and sanitize against adversarial prompt attacks, data leakage, and integrity breaches (e.g. offensive content, hallucination).LLM Guard was built for a straightforward purpose: despite the potential of LLMs, corporate adoption has been hesitant. This reluctance stems from the significant security risks and a lack of control and observability of implementing these technologies. With over 2.5M downloads of its models, and a Google Patch Reward, LLM Guard is the open source standard and market leader in LLM security at inference.Secure RAG with LlamaIndexIn the following example, we showcase a practical approach to improve the security of your RAG application. Specifically, we will explore a RAG application designed to facilitate the automated screening of candidate CVs by HR teams. Within the batch of CVs, there exists a diverse pool of candidates, including one who lacks experience and consequently is not the most suitable candidate. The nature of the attack manifests as an embedded prompt injection within the CV of this particular candidate, concealed in white text, rendering it challenging to detect with the naked eye.In the notebook example, we conducted the attack initially and then repeated the process subsequent to fortifying the application with LLM Guard. With this example, we show how you can use LLM Guard and LlamaIndex for both input and output scanning of documents to detect any malicious content. Although ideally, we should scan documents before the ingestion, for simplicity in the example, we chose to do scanning during retrieval. In real-use cases, it's critical to do the scanning both during retrieval of real-time data from APIs (not vector stores) which we still need to verify as it can contain poisoned sources of information. For output scanning, it can simply be done by taking the results generated by LlamaIndex and running them through LLM Guard.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, output_parser=output_parser)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    transformations=transformations,\n    callback_manager=callback_manager,\n)\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\ninput_scanners = [\n    Anonymize(vault, entity_types=[\"PERSON\", \"EMAIL_ADDRESS\", \"EMAIL_ADDRESS_RE\", \"PHONE_NUMBER\"]), \n    Toxicity(), \n    PromptInjection(),\n    Secrets()\n]\n\nllm_guard_postprocessor = LLMGuardNodePostProcessor(\n    scanners=input_scanners,\n    fail_fast=False,\n    skip_scanners=[\"Anonymize\"],\n)\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=3,\n    node_postprocessors=[llm_guard_postprocessor]\n)\nresponse = query_engine.query(\"I am screening candidates for adult caregiving opportunity. Please recommend me an experienced person. Return just a name\")\nprint(str(response))\nLLM Guard protects your LLM applicationsAs demonstrated in the practical example of securing an HR screening application with LLM Guard, the significance of mitigating potential attacks, cannot be overstated. Besides that, as LLMs evolve rapidly and embed advanced capabilities like agency and multi-modality, the complexity and impact of potential breaches escalate significantly. Thus, prioritizing RAG security becomes not just a necessity but rather fundamental in safeguarding against increasingly sophisticated threats and ensuring the integrity of critical enterprise LLM applications.Try out LLM Guard by going to our library or documentation. Also, join our Slack channel for any questions!",
            "related_posts": [
                "Secure RAG with LlamaIndex and LLM Guard by Protect AI",
                "Retrieving Privacy-Safe Documents Over A Network",
                "PII Detector: hacking privacy in RAG"
            ],
            "link": "https://www.llamaindex.ai/blog//secure-rag-with-llamaindex-and-llm-guard-by-protect-ai"
        },
        {
            "date": "Mar 13, 2024",
            "author": "Roey Ben Chaim",
            "title": "PII Detector: hacking privacy in RAG",
            "tags": [
                "hackathon",
                "llm",
                "privacy"
            ],
            "content": "A couple of days ago at the DataStax HQ, I had the chance to participate at the LlamaIndex RAG-A-THON. Over the span of the weekend, we had to implement a solution that leverages Retrieval Augmented Generation (RAG) technique.Because of my background in cybersecurity, I was leaning towards the security pitfalls and obstacles of the RAG technique. One of the first things that came to mind was the fact that a lot of the unstructured data used is unsanitized and can contain sensitive data.PII: What? Why?PII stands for Personally Identifiable Information. It refers to any information that can be used to identify a specific individual.This can be names, addresses, phone numbers, email addresses, social security numbers, and financial information.There are a couple of reasons why handling PIIs is important:Privacy: PII often includes sensitive and private details (like addresses), so protecting it preserves customers\u2019 privacy.Identity Theft: PII can also lead to identity theft (e.g. one\u2019s social security number gets compromised).Legal Compliance: Protecting PII is also the law. Many countries and regions have enacted laws and regulations that require organizations to protect PII. GDPR (General Data Protection Regulation) in the EU or HIPAA (Health Insurance Portability and Accountability Act) in the United States govern the way we handle PII.Trust and Reputation: A data breach or mishandling of PII will severely damage one\u2019s reputation and trust.Financial Security: PII may include financial information, such as credit card numbers and banking details. Compromised PII can lead to fraudulent transactions.National Security Concerns: All of the above are crucial in sovereign environments.PII in RAGEverything listed is applicable to almost all applications leveraging RAG. Remember that the RAG technique contains two components \u2014 the model and the vector database. For this reason, each of these components need to address PII.ModelLanguage models, are trained on large datasets that may contain real-world data, potentially including PII and customer data. When the models generate text, there is a risk that they\u2019ll produce content that includes PII. This is even more crucial if you\u2019re creating a multi-tenant application, and you want to prevent data leak. This risk can be mitigated by either filtering or anonymizing the response. Training the models on anonymized data that is stripped of any sensitive information is the better approach to prevent leaks of PII.Vector DatabaseVector databases, just like regular databases should not persist sensitive information plainly. This kind of information should only be persisted using encryption, hashing, salt and access controls. Having said that, one should also make sure that the similarity search returned by the Database won\u2019t retrieve personal data.On top of that, various regulations such as GDPR and HIPAA still apply here. So, if the original data contain PII, you might need to add another instance in Europe or any additional region in accordance with regulations. Persisted data should be encrypted or hashed (and additionally salted).Introducing: PresidioPresidio is an open-source library maintained by Microsoft (see our GitHub repo). It\u2019s derived from the Latin word praesidium which means \u201cprotection\u201d or garrison.It enables organizations to preserve privacy using a unified SDK.It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.Disclaimer: Nothing is bulletproof. It\u2019s your responsibility to make sure sensitive data is anonymized.How Does Presidio Work?Predefined or custom PII recognizers leverage Named Entity Recognition (NER), regular expressions, rule-based logic and checksum (e.g. bitcoin address validation).It\u2019s extensible, so you can add your own entities and your own detection mechanisms.It\u2019s customizable, so you can create your own anonymizers, and exclude/include certain entities (e.g. exclude anonymization of geographical locations).LlamaIndex Post ProcessorsThere was already some PII integration using NER models and LLMs! These were implemented as post processors that run in the end of the pipeline:from llama_index.postprocessor import NERPIINodePostprocessor\nfrom llama_index import ServiceContext\nfrom llama_index.schema import TextNode\n\ntext = \"\"\"\nMy name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in Amsterdam.\nHave you been to a P\u00e1lmi Einarsson concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? Bob, it's Bob.\nMy great great grandfather was called Yulan Peres, \nand my great great grandmother was called Jennifer Holst\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.FilmFranchise.dk/\n\"\"\"\n\nnode = TextNode(text=text)\n\nservice_context = ServiceContext.from_defaults()\nprocessor = NERPIINodePostprocessor(service_context=service_context)\n\nfrom llama_index.schema import NodeWithScore\n\nnew_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\nprint(new_nodes[0].node.get_text())Running the above code resulted in the following:My name is [PER_12] and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in [LOC_123].\nHave you been to a [PER_153] concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? [PER_286], it's [PER_286].\nMy great great grandfather was called [PER_339], \nand my great great grandmother was called [PER_395]\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.[ORG_521].dk/As can be seen in this example, while NER models do a decent job in detecting PII, they might miss some entities such as IBAN code, credit card numbers, emails, medical license and more.Presidio detects more out of the box entities than traditional models. This is possible because Presidio leverages a couple of methods in detecting PII \u2014 from NER models to regular expressions and rule-based logic.Integrating Presidio with LlamaIndexI ended up integrating PresidioPIINodePostprocessor that got the text as an input and masked it. Doing this was possible using Presidio\u2019s analyzer and anonymizer:from presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\n\nanalyzer = AnalyzerEngine(supported_languages=[\"en\"])\nresults = analyzer.analyze(text=text, language='en')\nengine = AnonymizerEngine()\nnew_text = engine.anonymize(text=text, analyzer_results=results)This was pretty fun and simple. However, given the input text \u201cAlice and Bob are friends\u201d, the output would be: \u201c<PERSON> and <PERSON> are friends\u201d. I could not have that.So, I added a counter and mapped the original values with the masked values, making sure that whenever an entity was seen again, the previously asked value was used:def anonymize_function(origin, entity_type):\n    nonlocal pii_counter\n    nonlocal inverted_mapping\n    nonlocal mapping\n    if entity_type not in inverted_mapping:\n        inverted_mapping[entity_type] = {}\n    typed_mapping = inverted_mapping[entity_type]\n    if origin in typed_mapping:\n        return typed_mapping[origin]\n    new_value = f\"<{entity_type}_{pii_counter}>\"\n    typed_mapping[origin] = new_value\n    mapping[new_value]=origin\n    pii_counter+=1\n    return typed_mapping[origin]\n\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_anonymizer import AnonymizerEngine\nfrom presidio_anonymizer.entities import OperatorConfig\n\nanalyzer = AnalyzerEngine(supported_languages=[\"en\"])\nresults = analyzer.analyze(text=text, language='en')\nengine = AnonymizerEngine()\nnew_text = engine.anonymize(text=text, analyzer_results=results, \n                            operators={\"DEFAULT\": OperatorConfig(\"custom\", \n                            params={\"lambda\": anonymize_function})})Note: Currently presidio doesn\u2019t contain the entity type as an input parameter in the lambda function, so I had to add this functionality.Test and BenchmarkOnce this was all up and running, I was able to call the newly added presidio post processor with the text from the previous run:from llama_index.postprocessor import PresidioPIINodePostprocessor\nfrom llama_index import ServiceContext\nfrom llama_index.schema import TextNode\n\ntext = \"\"\"\nMy name is Roey Ben Chaim and my credit card number is 4095-2609-9393-4932. \nMy email is robo@presidio.site and I live in Amsterdam.\nHave you been to a P\u00e1lmi Einarsson concert before?\nWhat is the limit for card 4158112277712? My IBAN is GB90YNTU67299444055881. \nWhat's your last name? Bob, it's Bob.\nMy great great grandfather was called Yulan Peres, \nand my great great grandmother was called Jennifer Holst\nI can't browse to your site, keep getting address 179.177.214.91 blocked error\nJust posted a photo https://www.FilmFranchise.dk/\n\"\"\"\n\nnode = TextNode(text=text)\n\nservice_context = ServiceContext.from_defaults()\nprocessor = PresidioPIINodePostprocessor(service_context=service_context)\n\nfrom llama_index.schema import NodeWithScore\n\nnew_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\nprint(new_nodes[0].node.get_text())Running the above code resulted in the following:My name is <PERSON_12> and my credit card number is <CREDIT_CARD_11>. \nMy email is <EMAIL_ADDRESS_10> and I live in <LOCATION_9>.\nHave you been to a <PERSON_8> concert before?\nWhat is the limit for card <CREDIT_CARD_7>? My IBAN is <IBAN_CODE_6>. \nWhat's your last name? <PERSON_5>, it's <PERSON_5>.\nMy great great grandfather was called <PERSON_4>, \nand my great great grandmother was called <PERSON_3>\nI can't browse to your site, keep getting address <IP_ADDRESS_2> blocked error\nJust posted a photo <URL_1>Overall Presidio detected 12 entities while the other NER solution detected 8. Notice that credit card numbers, email address, IBAN, IP address and the URL (at least some of it) weren\u2019t detected.I was curious to see how the parsing of these strings would work on the LLM, so I populated the index and queried the following:from llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex([n.node for n in new_nodes])\nresponse = index.as_query_engine().query(\n    \"What is my name?\"\n)\nprint(response)Which resulted in:Your name is <PERSON_12>.How It EndedAnyway, this project won the 3rd place (in the continuous track) in the RAG-A-THON.Note: this picture doesn\u2019t contain PIIUpdatePresidio is now fully integrated into LlamaIndex as a post processor, follow this notebook to learn how to use Presidio for PII masking. The next steps would be to add more customization and anonymization options.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Secure RAG with LlamaIndex and LLM Guard by Protect AI",
                "Retrieving Privacy-Safe Documents Over A Network"
            ],
            "link": "https://www.llamaindex.ai/blog//pii-detector-hacking-privacy-in-rag"
        },
        {
            "date": "Nov 28, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201311\u201328",
            "tags": [
                "llm",
                "llamaindex",
                "newsletter",
                "ai",
                "rag"
            ],
            "content": "Hello to Our Llama Community! \ud83e\udd99Hope your Thanksgiving was delightful! We\u2019re thrilled to announce a major milestone: LlamaIndex has hit 1 million monthly downloads on our Python package! A big thank you to everyone for your support, feedback, and contributions that have fueled our journey. Stay tuned for more exciting new products and features coming your way.If you have a fascinating project or video you\u2019d like to share, we\u2019d love to see it! Feel free to send it to us at news@llamaindex.ai. And remember to subscribe to our newsletter on our website to stay in the loop. We can\u2019t wait to connect with you there!\ud83e\udd29 First, the highlights:Launched Llama Packs: Prepackaged modules and templates streamlining LLM app development. Blog, Tweet.RAGs Project: Build your own retrieval augmented generation app just by talking. Project, Tweet.Introduced FuzzyCitationEnginePack: Precisely aligns LLM responses to source sentences via fuzzy matching, available as an easy-to-implement LlamaPack. Docs, Tweet.Coming up this week: on Thursday 30th our very own Yi Ding will be giving a workshop on Building an Open Source RAG Application Using LlamaIndex. Sign up for free here\u2728 Feature Releases and Enhancements:We introduced Llama Packs \ud83e\udd99\ud83d\udce6, a series of prepackaged modules and templates designed to jumpstart your LLM app development. These packs eliminate the need for assembling and tuning custom components for each use case. Blog, Tweet.We have introduced the RAGs project for programming AI agents using natural language, inspired by the interest in OpenAI\u2019s GPTs. Our approach involves a \u2018Builder Agent\u2019 that crafts a \u2018Custom Agent\u2019 tailored to specific tasks, incorporating tools for system prompt setting, data loading, model configuration, and RAG parameter adjustments. Project, Tweet.We introduced a LlamaPack that enables the setup of a fully local RAG pipeline with just one line of code. This pack includes Zephyr-7b as the LLM and bge-base as the embedding model. Docs, Tweet.We introduced FuzzyCitationEnginePack that maps parts of an LLM-generated response from a RAG pipeline to the exact sentences in the source context using fuzzy matching. This innovation elevates citation accuracy and is now available as a LlamaPack for easy implementation with just one line of code. Docs, Tweet.\ud83d\udc40 Demo:AI-Einblick Prompt is a JupyterLab extension that uses OpenAI\u2019s GPT 3.5 and 4, powered by LlamaIndex, to assist in data science workflows by generating, modifying, and fixing code, creating charts, and building models, seamlessly integrated within the JupyterLab environment. Project, Tweet.Ranya Khemiri uploaded a research paper to RAGs to help with a school assignment and observed results better than file retrieval with ChatGPT. Blog, Tweet.\ud83e\udd1d Integrations:CogniSwitch introduced a fusion RAG approach combining vectors, knowledge graphs, and rules for streamlined ingestion and retrieval. This allows for flexible usage, either as an independent query engine or as an integrated tool within an agent with LlamaIndex. Docs, Tweet.\ud83d\uddfa\ufe0f Guides:Guide on shipping your RAG application to production with create-llama.Guide on multi-modal models: Our comparison tables detail differences in image reasoning, embeddings, and synthesis capabilities. We also provide insights into multi-modal support for vector stores, focusing on image support with future audio/video integration.Guide on getting started with AI in your enterprise from Gradient AI. This introductory guide explains retrieval-augmented generation (RAG), its relevance for businesses, and how to balance fine-tuning, prompt engineering, and RAG for optimal results, along with strategies for RAG optimization.\u270d\ufe0f Tutorials:Ankush k Singal made a tutorial on Document Extraction with Zephyr 7b LLM using LlamaIndex.Wenqi Glantz made a tutorial on Automating Hyperparameter Tuning with LlamaIndex.Tonic AI analysis on OpenAI Assistant API vs LlamaIndex RAG.Pradip Nichite made ****a ****video tutorial on using RAGs which provides easy-to-follow instructions on how to build or customize a chatbot capable of advanced summarization over your data, making it accessible even for non-developers.\ud83c\udfa5 Webinars:Jerry Liu presented a webinar with Arize AI on LLM Retrieval Evaluations.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-11-28-a31be430a786"
        },
        {
            "date": "Nov 3, 2023",
            "author": "Ravi Theja",
            "title": "Boosting RAG: Picking the Best Embedding & Reranker models",
            "tags": [
                "embedding",
                "llm",
                "openai",
                "search",
                "llamaindex"
            ],
            "content": "UPDATE: The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the JinaAI-v2-base-en with bge-reranker-largenow exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and withCohereRerank exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?In this blog post, we\u2019ll use the Retrieval Evaluation module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in!Let\u2019s first start with understanding the metrics available in Retrieval EvaluationUnderstanding Metrics in Retrieval Evaluation:To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: Hit Rate and Mean Reciprocal Rank (MRR). Let\u2019s delve into these metrics to understand their significance and how they operate.Hit Rate:Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it\u2019s about how often our system gets it right within the top few guesses.Mean Reciprocal Rank (MRR):For each query, MRR evaluates the system\u2019s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it\u2019s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it\u2019s second, the reciprocal rank is 1/2, and so on.Now that we\u2019ve established the scope and familiarized ourselves with the metrics, it\u2019s time to dive into the experiment. For a hands-on experience, you can also follow along using our Google Colab NotebookSetting Up the Environment!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdfSetting Up the Keysopenai_api_key = 'YOUR OPENAI API KEY'\ncohere_api_key = 'YOUR COHEREAI API KEY'\nanthropic_api_key = 'YOUR ANTHROPIC API KEY'\nopenai.api_key = openai_api_keyDownload the DataWe will use Llama2 paper for this experiment. Let\u2019s download the paper.!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\"Load the DataLet\u2019s load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.This data was then parsed by converted to nodes, which represent chunks of data we\u2019d like to retrieve. We did use chunk_size as 512.documents = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)Generating Question-Context Pairs:For evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.Let\u2019s initialize a prompt template to generate question-context pairs.# Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\"llm = Anthropic(api_key=anthropic_api_key)\nqa_dataset = generate_question_context_pairs(\n    nodes, llm=llm, num_questions_per_chunk=2\n)Function to filter out sentences such as \u2014 Here are 2 questions based on provided context# function to clean the dataset\ndef filter_qa_dataset(qa_dataset):\n    \"\"\"\n    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n    the filtered data.\n\n    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n    \"\"\"\n\n    # Extract keys from queries and relevant_docs that need to be removed\n    queries_relevant_docs_keys_to_remove = {\n        k for k, v in qa_dataset.queries.items()\n        if 'Here are 2' in v or 'Here are two' in v\n    }\n\n    # Filter queries and relevant_docs using dictionary comprehensions\n    filtered_queries = {\n        k: v for k, v in qa_dataset.queries.items()\n        if k not in queries_relevant_docs_keys_to_remove\n    }\n    filtered_relevant_docs = {\n        k: v for k, v in qa_dataset.relevant_docs.items()\n        if k not in queries_relevant_docs_keys_to_remove\n    }\n\n    # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data\n    return EmbeddingQAFinetuneDataset(\n        queries=filtered_queries,\n        corpus=qa_dataset.corpus,\n        relevant_docs=filtered_relevant_docs\n    )\n\n# filter out pairs with phrases `Here are 2 questions based on provided context`\nqa_dataset = filter_qa_dataset(qa_dataset)Custom Retriever:To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base VectorIndexRetriever. Upon retrieving the nodes, we then introduce a reranker to further refine the results. It\u2019s worth noting that for this particular experiment, we\u2019ve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with OpenAIEmbedding, please refer to the notebook for code with other embeddings.embed_model = OpenAIEmbedding()\nservice_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)\nvector_index = VectorStoreIndex(nodes, service_context=service_context)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10)class CustomRetriever(BaseRetriever):\n    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n    ) -&gt; None:\n        \"\"\"Init params.\"\"\"\n\n        self._vector_retriever = vector_retriever\n\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\"\"\"\n\n    retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n    if reranker != 'None':\n      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n       else:\n          retrieved_nodes = retrieved_nodes[:5]\n         \n       return retrieved_nodes\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        \"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"\n        return self._retrieve(query_bundle)\n\n    async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return await self._aretrieve(str_or_query_bundle)\n\ncustom_retriever = CustomRetriever(vector_retriever)Evaluation:To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics:retriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n)\neval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)Results:We put various embedding models and rerankers to the test. Here are the models we considered:Embedding Models:OpenAI EmbeddingVoyage EmbeddingCohereAI Embedding (v2.0/ v3.0)Jina Embeddings (small/ base)BAAI/bge-large-enGoogle PaLM EmbeddingRerankers:CohereAIbge-reranker-basebge-reranker-largeIt\u2019s worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on.The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR):Analysis:Performance by Embedding:OpenAI: Showcases top-tier performance, especially with the CohereRerank (0.926966 hit rate, 0.86573 MRR) and bge-reranker-large (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools.bge-large: Experiences significant improvement with rerankers, with the best results from CohereRerank (0.876404 hit rate, 0.822753 MRR).llm-embedder: Benefits greatly from reranking, particularly with CohereRerank (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost.Cohere: Cohere\u2019s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR.Voyage: Has strong initial performance that is further amplified by CohereRerank (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking.JinaAI: Very strong performance, sees notable gains with bge-reranker-large (0.938202 hit rate, 0.868539 MRR) and CohereRerank (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance.Google-PaLM: The model demonstrates strong performance, with measurable gains when using the CohereRerank(0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results.Impact of Rerankers:WithoutReranker: This provides the baseline performance for each embedding.bge-reranker-base: Generally improves both hit rate and MRR across embeddings.bge-reranker-large: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the CohereRerank.CohereRerank: Consistently enhances performance across all embeddings, often providing the best or near-best results.Necessity of Rerankers:The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs.Rerankers, especially CohereRerank, have demonstrated their capability to transform any embedding into a competitive one.Overall Superiority:When considering both hit rate and MRR, the combinations of OpenAI + CohereRerank and JinaAI-Base + bge-reranker-large/ CohereRerank emerge as top contenders.However, the consistent improvement brought by the CohereRerank/ bge-reranker-large rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use.In summary, to achieve the peak performance in both hit rate and MRR, the combination of OpenAI or JinaAI-Base embeddings with the CohereRerank/bge-reranker-large reranker stands out.Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them.Conclusions:In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions.Embeddings: The OpenAI and JinaAI-Base embeddings, especially when paired with the CohereRerank/bge-reranker-large reranker, set the gold standard for both hit rate and MRR.Rerankers: The influence of rerankers, particularly CohereRerank/bge-reranker-large, cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.Foundation is Key: Choosing the right embedding for the initial search is essential; even the best reranker can\u2019t help much if the basic search results aren\u2019t good.Working Together: To get the best out of retrievers, it\u2019s important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"
        },
        {
            "date": "Oct 2, 2023",
            "author": "Laurie Voss",
            "title": "LlamaIndex + Laurie Voss: an alpaca joins the llamas",
            "tags": [
                "llamaindex",
                "hires"
            ],
            "content": "Hi, I\u2019m Laurie, and today is my first day as VP of Developer Relations at LlamaIndex!Quick background on me: I started my career 27 years ago as a web developer, founded a couple of companies including npm Inc., and have always been about talking to developers about the state of the development world and how we fit into it, whether that\u2019s on my personal site, in conference talks, or in big community surveys.I wrote last week about why LLMs are the future of software. To summarize that post: until now computers have been very good at ingesting, sorting, and transmitting data, but understanding what they were working with was beyond them. Very recently, with tools like GPT-4 and Llama 2 that threshold has been crossed. Software can now read, summarize, and make novel connections within arbitrarily large sets of data. It can write software, it can use tools, it can generate text, images and music. A huge new set of capabilities have been unlocked.The last time I saw a shift this big in the technological landscape was probably the original iPhone. Suddenly, everybody who was doing everything needed to also do it for mobile devices. You sell real estate? Now you do it with an app. You run a dating site? Now you do it with an app. There was a gigantic rush as everyone who did everything in software suddenly saw huge benefits from moving onto a new platform.But the most interesting applications that platform shift enabled were the ones that couldn\u2019t have existed before. Uber relies on everyone \u2014 drivers as well as customers \u2014 already owning a piece of mobile hardware that can connect to the Internet and use GPS to locate them. Prior to the mobile revolution, that would have been an absurd business plan. After it happened, it was taken for granted. So in the same way, I\u2019m especially interested in seeing what the applications of LLMs are that were simply impossible before.All of which is why I\u2019m delighted to be at a company at the center of the LLM universe. If you\u2019re new to LlamaIndex, it\u2019s a Python and JavaScript framework that lets you quickly put together totally customizable, production-class applications that use LLMs. We let you ingest data from dozens of sources and use any model you want, and we make it simple to link everything together.The team at LlamaIndex is absolutely amazing and I am looking forward to working with Jerry, Simon and Yi. Yi will be focusing more on partnerships.Want to see a demo? Our SEC Insights app lets you ingest regulatory documents from major corporations and then ask questions about them. Want to dive and build your own? Get started in Python or JavaScript!P.S. If you\u2019re wondering \u201cwhy an alpaca?\u201d it\u2019s because my personal mascot is an alpaca and has been for years. Alpacas and llamas are a great match!",
            "related_posts": [
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse",
                "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-laurie-voss-an-alpaca-joins-the-llamas-9cae1081adff"
        },
        {
            "date": "Nov 20, 2023",
            "author": "LlamaIndex",
            "title": "Shipping your Retrieval-Augmented Generation app to production with create-llama",
            "tags": [
                "llm",
                "llamaindex",
                "deployment",
                "rag",
                "ai"
            ],
            "content": "It\u2019s a llama on a ship, geddit?Last week we released create-llama, a command-line tool to generate a full-stack LlamaIndex application for Retrieval-Augmented Generation (RAG). The response was fantastic, so we\u2019ll be following up with more templates and more features. We also wanted to show you just how easy it is to get your generated app all the way to production. So here\u2019s a step by step guide, for each of the three backends we currently support: Next.js serverless, Express, and Python.Next.js backendThe serverless full-stack Next.js application is the simplest version to deploy as you only have one artifact to deploy. Because it\u2019s a Next.js app we\u2019ll be deploying to Vercel, the home of Next.js.Step 1: run create-llamaFirst run create-llama to generate your app. We strongly recommend generating a new OpenAI API key and supplying it at generation time (create-llama apps can be customized to use other LLMs but that\u2019s out of scope for this tutorial).Step 2: create a GitHub repository and push your app to itThe easiest way to deploy on Vercel is from a linked GitHub repository. Your generated app is already set up as a git repo, so all you have to do after creating a new empty repo is follow the instructions to push it up. This should give you a repo that looks a bit like this:Step 3: import your repo into VercelSelect the option to create a new project from a git repo:and select the repo you just created after authorizing:Step 4: configure your projectBecause this is a Next.js app and this is Vercel, there\u2019s very little you need to do! The only thing you need to remember is to click \u201cenvironment variables\u201d and create a variable called OPENAI_API_KEY with your key.Step 5: Deploy!That\u2019s it! Deploying a Next.js app to Vercel is pretty easy.Your deployed app should look like this:Congratulations, you\u2019ve deployed a full-stack RAG application!Express backendIf you chose to generate an Express backend with a Next.js frontend instead, let\u2019s get you into production with those. We\u2019ll be deploying both frontend and backend to Render, a fantastic service for both static sites and dynamic web applications.There will be 3 big things to do here:Deploy the static frontendDeploy the backend, and give the frontend permission to call itTell the frontend where the backend is locatedWe promise you\u2019ll be production in no time.Step 1: run create-llamaThis is just like the same step in Next.jsStep 2: push the code to a new GitHub repoLike Vercel, the easiest way to push a site to production is from a linked git repo. Your generated app already has a git repo initiated with frontend and backend folders, so you can go ahead and push them both to a single GitHub repository just as in the Next.js backend.Step 3: Start a new static siteWe\u2019ll be deploying your frontend first as a static site. After authorizing, select the repository where you pushed your frontend and backend; we\u2019ll specify that we\u2019re deploying the frontend in the next step.Step 4: configure your static siteThere are several changes you need to make to the default configuration to successfully publish your static frontend:Name your site something memorable, it will become the URL of your site once it\u2019s deployedSet your root directory to frontendSet your build command to npm install; npm run buildSet your publish directory to outFinally, click \u201cAdvanced\u201d and set an environment variable called NODE_VERSION to 20 . The default on Render is a much older version of Node so don\u2019t skip this step!Step 5: deploy your static frontendClick the Deploy button and watch your site build! You should now have a site live at a URL something like frontend-name-you-picked.onrender.com .Step 6: start a new web service",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//shipping-your-retrieval-augmented-generation-app-to-production-with-create-llama-7bbe43b6287d"
        },
        {
            "date": "Jun 30, 2023",
            "author": "Jerry Liu",
            "title": "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)",
            "tags": [
                "hackathons",
                "ai",
                "large-language-models",
                "berkeley",
                "nlp"
            ],
            "content": "We had an awesome time at the Berkeley Hackathon two weeks ago (6/17\u20136/18). The attendance stats were impressive:1200 hackers262 submitted projects2 real-life llamas \ud83e\udd99LlamaIndex sponsored a \u201cBest Knowledge-Intensive LLM App\u201d prize series at the hackathon. The criteria was an app that leveraged a knowledge base of custom data to build innovative new application experiences.We announced three prize winners along with an honorable mention. We are excited to feature each project in a special highlight below. In each highlight, the creators describe the project mission and what it solves, the implementation+tech stack, challenges, and future directions. Check it out! \ud83d\udc47First Prize Winner: Helmet AICreators: Jaiveer Singh, Devin Mui, Ethan Mehta, Manav RathodDevpost: https://devpost.com/software/helmet-aiIntroductionIn today\u2019s rapidly evolving business landscape, staying ahead of the competition is paramount for success. However, the deluge of information and the ever-changing market dynamics can make it challenging for business leaders to make informed decisions. In this blog post, we introduce Helmet AI, a cutting-edge market intelligence tool designed to empower leadership teams with real-time insights and a competitive edge. Join us as we explore the capabilities, technology stack, and future prospects of Helmet AI.Unveiling Helmet AIHelmet AI is an innovative market intelligence tool that harnesses the power of advanced technologies to provide leaders with actionable insights and an unparalleled understanding of the global business landscape. With its context-aware Ingestion Engine and Insight Extractor powered by OpenAI\u2019s GPT models, Helmet AI offers a comprehensive solution for tracking breaking news, uncovering hidden relationships, and extracting valuable, personalized insights from vast amounts of data. For ease of use, Helmet AI displays these insights in a familiar, Twitter-like \u201cFeed\u201d interface. Additionally, Helmet AI offers a Chat interface for users to ask questions about a particular news story to Helmet\u2019s knowledgeable chat agent.Key Features and Technology StackContext-Aware Ingestion Engine:Helmet AI\u2019s Ingestion Engine continuously monitors the vast landscape of breaking news and global events. By leveraging techniques such as subscribing to RSS feeds for up to date news data and processing documents with LlamaIndex and LangChain, the engine builds a complete understanding of real-time events and their implications on various user profiles. Embeddings are stored in a Pinecone Vector Database.Insight Extractor with OpenAI\u2019s GPT Models:The Insight Extractor component of Helmet AI utilizes the power of OpenAI\u2019s GPT models to identify and concisely explain the intricate relationships between seemingly disparate topics surfaced in your feed. By transforming raw data into actionable insights with intelligent explanations, leaders can make informed decisions based on an understanding of market trends and complex dynamics.Scalable Infrastructure:Helmet AI is built on Azure\u2019s robust infrastructure, utilizing a range of services such as App Services, a PostgreSQL Database, and Github Actions for orchestrating Deployments. The implementation also incorporates GraphQL for efficient data retrieval and processing.Challenges Overcome and AccomplishmentsDuring the development of Helmet AI, our team encountered various challenges, including integrating MindsDB with Azure and overcoming limitations with Gmail authentication. However, we were able to overcome these obstacles and successfully implemented Helmet AI in just 36 hours during the Berkeley AI Hackathon. Additionally, we established a seamless deployment process using GitHub Actions, automating manual service orchestration. The experience was particularly rewarding for the first-time hackers on the team.Key LearningsThroughout the development process, our team gained valuable insights. We discovered the importance of setting up deployment flows early on to reduce stress during crunch time. Embracing best practices in software engineering proved crucial. Furthermore, we realized the potential of leveraging advanced language models as implicit knowledge graphs, expanding their applications beyond traditional embeddings.Future ProspectsLooking ahead, Helmet AI aims to scale up the Ingestion Engine to handle the entirety of the web, leveraging technologies like AnyScale. The team plans to collaborate with enterprise business development teams to initiate pilot programs and gather feedback for further refinement. With a solid foundation in place, Helmet AI hopes to have an impact on the way leaders gather insights and make strategic decisions.ConclusionHelmet AI represents a solid attempt at a game-changing solution for business leaders seeking to stay ahead in today\u2019s fast-paced business world. By leveraging cutting-edge technologies, including AI-powered insight extraction and explanation and real-time data analysis, Helmet AI empowers leaders to confidently navigate market challenges and seize emerging opportunities. As the tool continues to evolve and expand its capabilities, the future of market intelligence looks promising. Stay tuned for more updates on Helmet AI\u2019s journey towards transforming the way we approach gathering information and strategic decision-making.Winner: SplitCreators: Aditya Ariyur, Nikhil Patel, Ronit NagarapuDevpost: https://devpost.com/software/split-pv4hn7Background/MotivationWe wanted to develop an easy-to-use workflow that allowed users to generate personalized emails with the assistance of AI, while retaining the user\u2019s unique writing style and emotion inflections.What It IsOur product learns from your previous emails and trains a custom LLM that will draft emails that sound like you, not like a robot. It learns from your writing style and how you respond to specific people. Then, it generates emails from user prompts that match that style.How We Built ItWe used the Google API and LlamaIndex to parse through a user\u2019s old emails and develop an LLM model built on OpenAI\u2019s text-davinci-003. Then, we use Hume to understand the user\u2019s tone and emotion in their emails, and associate it with specific subjects and recipients so future\nemails can be fine-tuned to fit the user\u2019s emailing habits. The current interface was developed using React.js for the website and a Flask API to interact with the backend LLM model.Challenges + What We LearnedIt was quite difficult to get all of the different aspects of our model working together in unison, especially establishing the connection between the parsed emails and Hume emotion tags to the LlamaIndex model. We had to experiment with many different tools and prompt styles to get an accurate email generation. However, with a lot of dedication and troubleshooting, we were able to develop a working model to demonstrate our concept and its potential functionality. We learned how rewarding it was to train our own LLM using LlamaIndex. Base LLMs like ChatGPT are already so powerful, so the functionality of training a custom LLM based on your\nown data unlocks endless possibilities.What\u2019s NextWe hope to completely integrate the code and workflow into a Google plugin or extension so users can easily implement it into their daily emailing. We want to ensure the privacy and security of the user\u2019s data, so we want to experiment with methods to reduce how much data is\nsent to third-party services like OpenAI. We also want to dedicate further development to the emotion training, as this could boost the effectiveness of our product and add to our main value proposition of personalized, user-specific email generation.Winner: Prosper AICreators: Alan Yang, Ashay Changwani, Punit Sai Arani, Vedant TapadiaDevpost: https://devpost.com/software/prosper-aiVercel Demo / YouTube VideoOverviewProsper AI is a trailblazer in utilizing Artificial Intelligence to unlock your full financial potential. It serves as an accessible and smart virtual financial advisor, armed with precise insights and personalized advice. Our mission is to democratize financial expertise. By bridging the resource gap, Prosper AI aims to level the playing field for all.The Genesis of Prosper AIThe spark that ignited Prosper AI was a simple observation of the wealth disparity among different social classes. The rich have always had access to knowledge and resources that help in growing and safeguarding their wealth. In contrast, those from modest backgrounds often lack the necessary knowledge and tools to utilize what they earn effectively. Many resort to social media for financial advice, which is often generic and occasionally unreliable as it comes from unqualified influencers. Hiring a financial advisor, on the other hand, could be exorbitant and impractical for those with a limited budget.This is where Prosper AI steps in. We embraced the challenge to develop an innovative solution utilizing state-of-the-art technology and models to help digest and simplify complex financial data.The Prosperity Engine: How Prosper AI WorksProsper AI sources your financial data from any number and type of bank or investment account to provide qualified financial advice that adheres to regulatory policies, to give you personalized tips, advice and explanations.Prosper AI achieves this by leveraging an open finance provider such as Plaid to source users financial information. Then Prosper AI will ask a series of financial goal questions to help contextualize the ideal outcomes for the user. Using this combination of personal financial data and goals, Prosper AI will provide a set of optimal and personalized recommendations on how to achieve these goals.The beauty of Prosper AI lies in its interactivity and support. Users have the liberty to pose questions at any juncture if they find something perplexing. This is particularly invaluable for demystifying complicated charts or financial jargon. Furthermore, Prosper AI goes beyond just answering questions about the current recommendations. It\u2019s like having an expert financial advisor at your beck and call, ready to generate insights, charts, and suggestions for any aspect of your financial landscape. Whether it\u2019s planning for retirement, optimizing investments, or understanding tax liabilities, Prosper AI stands ready to guide users with precision and personalized insights to cultivate financial acumen and empower smarter financial decision-making.The Building Blocks of Prosper AI: A Look into Our Tech StackBackend: Our backend, the engine that powers Prosper AI, is written in Python and based on a FastAPI server. We chose Python because of its agility and the vast availability of open-source libraries that expedite the development process. Additionally, Python\u2019s native packages provided by OpenAI and Plaid seamlessly integrate with our backend, ensuring both development and runtime efficiency.One of the cornerstones of Prosper AI\u2019s backend is a powerful prompting pipeline which simulates a fine-tuned model. To achieve this, we tap into the capabilities of OpenAI\u2019s GPT-4, enhanced with function calling, and interlink it with Pinecone\u2019s vector database using additional tools like LlamaIndex. This fusion forges a streamlined yet powerful interface.Frontend: When we started out, especially during the hackathon phase, we developed the web application frontend using Next.js, which was our comfort zone. However, as we progressed and aimed for higher benchmarks, we recognized the need to migrate to a more performant framework. We decided on SvelteKit, which stands out for its simplicity and performance, significantly accelerating the development process.One of our key objectives is to make Prosper AI accessible and user-friendly. We crafted a minimalist user interface, which declutters the screen while maintaining the essence of information. Moreover, we supplemented this with visualizations, which are crucial in translating complex financial data into understandable and actionable insights for the user. Through this combination of a robust backend and an intuitive frontend, Prosper AI is poised to revolutionize personal financial management.Overcoming Challenges: The Journey of Prosper AI\u2019s DevelopmentThe primary challenge we encountered during the initial stages was the creation of a pipeline to ingest and process years of financial data analytically. The sheer volume of data was not just overwhelming to handle all at once, but it was also crucial to process it responsibly and meaningfully.To tackle this, we had to design a system that dissected the vast financial data into digestible segments, structuring it in an orderly manner that enabled logical understanding and actionable insights. Although crafting such a system under time pressure was strenuous, it offered us a valuable insight into the magnitude of data we were dealing with. It further emphasized the significance of our mission: to efficiently and comprehensively process such vast data for the benefit of our users.Another demanding task was incorporating the complexities of tax code into our platform. Thousands of pages of tax regulations had to be converted into intelligent code, capable of offering savvy financial suggestions. Despite the enormous effort this task required, it was crucial in creating a comprehensive wealth management system. The result is a platform that delivers an optimized, personalized financial plan tailored to each user\u2019s specific goals and needs, as well as future plans. Our platform not only identifies the type of accounts and the cash flow strategies that would minimize tax liabilities but also charts a roadmap for maximizing net worth growth over the next 30 years.This is the essence of Prosper AI \u2014 using technology to simplify complex financial management and facilitate the path towards prosperity.The Road Ahead for Prosper AIAs we set our sights on the future, the Prosper AI team is more determined than ever to make strides in revolutionizing personal wealth management. Our immediate focus is to transition into full-time startup mode, which entails delving deeper into the development of feature functionalities and solidifying the foundation of our platform.A key milestone on our roadmap is engaging in pilot use cases with our initial group of customers who have eagerly joined our waitlist. This phase is critical, as it allows us to validate the effectiveness and impact of Prosper AI in real-world scenarios. Through feedback and insights gathered from this initial group, we\u2019ll be able to refine and enhance the platform to ensure it not only meets but surpasses the expectations of our users.But we won\u2019t stop there. The learnings from the pilot phase will serve as the springboard for subsequent developments and innovations. As we continue to harness cutting-edge technology and data analytics, Prosper AI aims to democratize access to financial knowledge and tools that can empower individuals to unlock their financial potential.Stay tuned as Prosper AI embarks on this exciting journey towards transforming the landscape of personal finance, making it more accessible, intelligent, and personalized for all.Together with Prosper AI, let\u2019s cultivate the seeds of financial growth and harvest the fruits of prosperity.Video/screenshots/links to material.Learn more and join our waitlist for a chance to win a $50 Amazon voucher:Unleash The Power of ComparisonTap into Prosper AI, your intelligent sidekick for personalized & optimal financial advice.prosperai.vercel.apphttps://www.youtube.com/watch?v=_-v0BhFPjAQ",
            "related_posts": [
                "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05"
            ],
            "link": "https://www.llamaindex.ai/blog//special-feature-berkeley-hackathon-projects-llamaindex-prize-winners-c135681bb6f0"
        },
        {
            "date": "Dec 13, 2023",
            "author": "Jerry Liu",
            "title": "LlamaIndex + Gemini",
            "tags": [
                "llamaindex",
                "gemini",
                "llm",
                "ai",
                "multimodal"
            ],
            "content": "(co-authored by Jerry Liu, Haotian Zhang, Logan Markewich, and Laurie Voss @ LlamaIndex)Today is Google\u2019s public release of its latest AI model, Gemini. We\u2019re excited to be a day 1 launch partner for Gemini, with support immediately available in LlamaIndex today!As of 0.9.15, LlamaIndex offers full support for all currently released and upcoming Gemini models (Gemini Pro, Gemini Ultra). We support both a \u201ctext-only\u201d Gemini variant with a text-in/text-out format as well as a multimodal variant that takes in both text and images as input, and outputs text. We\u2019ve made some fundamental multi-modal abstraction changes to support the Gemini multi-modal interface, which allows users to input multiple images along with text. Our Gemini integrations are also feature-complete: they support (non-streaming, streaming), (sync, async), and (text completion, chat message) formats \u2014 8 combinations in total.In addition, we also support the brand-new Semantic Retriever API, which bundles storage, embedding models, retrieval, and LLM in a RAG pipeline. We show you how it can be used on its own, or decomposed+bundled with LlamaIndex components to create advanced RAG pipelines.Huge shoutout to the Google Labs and Semantic Retriever teams for helping us get setup with early access.Google Labs: Mark McDonald, Josh Gordon, Arthur SorokenSemantic Retriever: Lawrence Tsang, Cher HuThe below sections contain a detailed walkthrough of both our brand-new Gemini and Semantic Retriever abstractions in LlamaIndex. If you don\u2019t want to read that now, make sure you bookmark our detailed notebook guides below!Gemini (text-only) GuideGemini (multi-modal) GuideSemantic Retriever GuideGemini Release and SupportThere\u2019s been a ton of press around Gemini, which boasts impressive performance at a variety of benchmarks. The Ultra variants (which are not yet publicly available) outperform GPT-4 on benchmarks from MMLU to Big-Bench Hard to math and coding tasks. Their multimodal demos demonstrate joint image/text understanding from domains like scientific paper understanding to literature review.Let\u2019s walk through examples of using Gemini in LlamaIndex. We walk through both the text model (from llama_index.llms import Gemini) as well as the multi-modal model (from llama_index.multi_modal_llms.gemini import GeminiMultiModal)Text ModelFull Notebook Guide HereWe start with the text model. In the code snippet below, we show a bunch of different configurations, from completion to chat to streaming to async.from llama_index.llms import Gemini\n\n# completion\nresp = Gemini().complete(\"Write a poem about a magic backpack\")\n# chat\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = Gemini().chat(messages)\n# streaming (completion)\nllm = Gemini()\nresp = llm.stream_complete(\n    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n)\n# streaming (chat)\nllm = Gemini()\nmessages = [\n    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n    ChatMessage(\n        role=\"user\", content=\"Help me decide what to have for dinner.\"\n    ),\n]\nresp = llm.stream_chat(messages)\n# async completion\nresp = await llm.acomplete(\"Llamas are famous for \")\nprint(resp)\n# async streaming (completion)\nresp = await llm.astream_complete(\"Llamas are famous for \")\nasync for chunk in resp:\n    print(chunk.text, end=\"\")The Gemini class of course has parameters that can be set. This includes model_name, temperature, max_tokens, and generate_kwargs.As an example, you can do:llm = Gemini(model=\"models/gemini-ultra\")Multi-modal ModelFull Notebook Guide HereIn this notebook, we test out the gemini-pro-vision variant that features multi-modal inputs. It contains the following features:supports both complete and chat capabilitiessupports streaming and asyncSupports feeding in multiple images in addition to text in the completion endpointFuture work: multi-turn chat interleaving text and images is supported within our abstraction, but is not yet enabled for gemini-pro-vision.Let\u2019s walk through a concrete example. Let\u2019s say we are given a picture of the following scene:Scene from a street in New York CityWe can then initialize our Gemini Vision model, and ask it a question: \u201cIdentify the city where this photo was taken\u201d:from llama_index.multi_modal_llms.gemini import GeminiMultiModal\nfrom llama_index.multi_modal_llms.generic_utils import (\n    load_image_urls,\n)\n\nimage_urls = [\n    \"&lt;https://storage.googleapis.com/generativeai-downloads/data/scene.jpg&gt;\",\n    # Add yours here!\n]\nimage_documents = load_image_urls(image_urls)\ngemini_pro = GeminiMultiModal(model=\"models/gemini-pro\")\ncomplete_response = gemini_pro.complete(\n    prompt=\"Identify the city where this photo was taken.\",\n    image_documents=image_documents,\n)Our response is the following:New York CityWe can insert multiple images too. Here\u2019s an example with an image of Messi and the Colosseum.image_urls = [\n    \"&lt;https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg&gt;\",\n    \"&lt;https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg&gt;\",\n]\nimage_documents_1 = load_image_urls(image_urls)\nresponse_multi = gemini_pro.complete(\n    prompt=\"is there any relationship between those images?\",\n    image_documents=image_documents_1,\n)\nprint(response_multi)Multi-Modal Use Cases (Structured Outputs, RAG)Full Notebook Guide HereWe\u2019ve created extensive resources about different multi-modal use cases, from structured output extraction to RAG.Thanks to Haotian Zhang, we have examples for both these use cases with Gemini. Please see our extensive notebook guides for more details. In the meantime here\u2019s the final results!Structured Data Extraction with Gemini Pro VisionScreenshot of a Google Maps Restaurant ListingOutput:('restaurant', 'La Mar by Gaston Acurio')\n('food', 'South American')\n('location', '500 Brickell Key Dr, Miami, FL 33131')\n('category', 'Restaurant')\n('hours', 'Open \u22c5 Closes 11 PM')\n('price', 4.0)\n('rating', 4)\n('review', '4.4 (2,104)')\n('description', 'Chic waterfront find offering Peruvian & fusion fare, plus bars for cocktails, ceviche & anticucho.')\n('nearby_tourist_places', 'Brickell Key Park')Multi-Modal RAGWe run our structured output extractor on multiple restaurant images, index these nodes, and then ask a question \u201cRecommend a Orlando restaurant for me and its nearby tourist places\u201dI recommend Mythos Restaurant in Orlando. It is an American restaurant located at 6000 Universal Blvd, Orlando, FL 32819, United States. It has a rating of 4 and a review score of 4.3 based on 2,115 reviews. The restaurant offers a mythic underwater-themed dining experience with a view of Universal Studios' Inland Sea. It is located near popular tourist places such as Universal's Islands of Adventure, Skull Island: Reign of Kong, The Wizarding World of Harry Potter, Jurassic Park River Adventure, Hollywood Rip Ride Rockit, and Universal Studios Florida.Semantic RetrieverThe Generative Language Semantic Retriever offers specialized embedding models for high-quality retrieval, and a tuned LLM for producing grounded-output with safety settings.It can be used out of the box (with our GoogleIndex) or decomposed into different components (GoogleVectorStore and GoogleTextSynthesizer) and combined with LlamaIndex abstractions!Our full semantic retriever notebook guide is here.Out of the Box ConfigurationYou can use it out of the box with very few lines of setup. Simply define the index, insert nodes, and then get a query engine:from llama_index.indices.managed.google.generativeai import GoogleIndex\n\nindex = GoogleIndex.from_corpus(corpus_id=\"&lt;corpus_id&gt;\")\nindex.insert_documents(nodes)\nquery_engine = index.as_query_engine(...)\nresponse = query_engine.query(\"&lt;query&gt;\")A cool feature here is that Google\u2019s query engine supports different answering styles as well as safety settings.Answering Styles:ABSTRACTIVE (succinct but abstract)EXTRACTIVE (brief and extractive)VERBOSE (extra details)Safety SettingsYou can specify safety settings in the query engine, which let you define guardrails on whether the answer is explicit in different settings. See the generative-ai-python library for more information.Decomposing into Different ComponentsThe GoogleIndex is built upon two components: a vector store (GoogleVectorStore) and the response synthesizer (GoogleTextSynthesizer). You can use these as modular components in conjunction with LlamaIndex abstractions to create advanced RAG.The notebook guide highlights three advanced RAG use cases:Google Retriever + Reranking: Use the Semantic Retriever to return relevant results, but then use our reranking modules to process/filter results before feeding it to response synthesis.Multi-Query + Google Retriever: Use our multi-query capabilities, like our MultiStepQueryEngine to break a complex question into multiple steps, and execute each step against the semantic retriever.HyDE + Google Retriever: HyDE is a popular query transformation technique that hallucinates an answer from a query, and uses the hallucinated answer for embedding lookup. Use that as a step before the retrieval step from the Semantic Retriever.ConclusionThere\u2019s a lot in here, and even then the blog post doesn\u2019t even cover half of what we\u2019ve released today.Please please make sure to check out our extensive notebook guides! Linking the resources again below:Gemini (text-only) GuideGemini (multi-modal) GuideSemantic Retriever GuideAgain, huge shoutout to the Google teams and Haotian Zhang, Logan Markewich from the LlamaIndex team for putting together everything for this release.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-gemini-8d7c3b9ea97e"
        },
        {
            "date": "Jun 30, 2023",
            "author": "Michael Hunger",
            "title": "Enriching LlamaIndex Models with GraphQL and Graph Databases",
            "tags": [
                "llm",
                "graphql",
                "graph-database",
                "llamaindex",
                "python"
            ],
            "content": "In this article I wanted to share the process of adding new data loaders to LlamaIndex. First we\u2019ll look at what LlamaIndex is and try a simple example of providing additional context to an LLM query using a simple CSV loader. Then we look at how easy it is to add a new loader for graph databases to LlamaIndex. And lastly we try that new loader and another loader for GraphQL APIs that I added in practice and see how their extra context can help an LLM answer questions better.Background/ContextI was listening to the \"This Week in ML\" (twiml) Podcast where Jerry Liu from LlamaIndex (previously GPT-Index) explained the ideas behind the library to enrich query contexts to LLMs with data from any number of sources.LlamaIndex is a toolkit to augment LLMs with your own (private) data using in-context learning. It takes care of selecting the right context to retrieve from large knowledge bases. To achieve that it utilizes a number of connectors or loaders (from LlamaHub) and data structures (indices) to efficiently provide the pre-processed data as Documents.Each type of index stores documents in different ways, e.g via embeddings for vector search, as a simple list or graph or tree structure. Those indices are used as query interface to the LLM, transparently embedding the relevant context.Besides the higher quality response from the LLM, you get also the documents returned that have been used to construct the answer. LlamaIndex also allows chain of thought reasoning, compare/contrast queries, and natural language querying of databases.See also this presentation from Jerry:All the code for the blog post is available in this Colab Notebook.Using a Basic CSV LoaderHere is an example of using a basic CSV loader to provide documents for LlamaIndex.In our Notebook we download the countries.csv via the Countries List Project (MIT) (raw source).Our dependencies are llama-index and python-dotenv.!pip install llama-index==0.6.19 python-dotenvWe need to provide our OpenAI-api key, to avoid accidentally leaking it in the notebook, I uploaded an openai.env file and use the dotenv library to load the contents as environment variables.In the next step we load the env file and prepare the OpenAI ChatGPTLLMPredictor (using gpt-3.5-turbo by default) and add it to the ServiceContext .import os\nfrom pathlib import Path\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext, GPTListIndex\nfrom llama_index.llm_predictor.chatgpt import ChatGPTLLMPredictor\nfrom dotenv import load_dotenv\nfrom llama_index import download_loader\n\nload_dotenv(\"openai.env\")\n\nllm_predictor = ChatGPTLLMPredictor()\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)Now we can use the loader to load the CSV and turn it into documents, create an an GPT Index (VectorStoreIndex in this case), which LlamaIndex can then use to retrieve the relevant information to pass along in the context to the LLM.Initializing CSV Loader and GPTVectorStoreIndexSimpleCSVReader = download_loader(\"SimpleCSVReader\")\nloader = SimpleCSVReader(concat_rows=False)\ndocuments = loader.load_data(file=Path('./countries.csv'))\n\nprint(documents)\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)Documents from the CSV Loader[Document(text='country, capital, type', doc_id='67c30c68-7d9f-4906-945b-9affc96f95d2', embedding=None, doc_hash='3a506ebea9c04655b51406d79fdf5e3a87c3d8ff5b5387aace3e5a79711a21b8', extra_info=None),\nDocument(text='Abkhazia, Sukhumi, countryCapital', doc_id='6e6be4b5-051f-48e0-8774-6d48e0444785', embedding=None, doc_hash='ea387d0eab94cc6c59f98c473ac1f0ee64093901673b43e1c0d163bbc203026e', extra_info=None),\n...]The CSV loader didn\u2019t create one Document per CSV row by default, but only one for the whole document, but you could configure it so that it turned the CSV into one document per row.LlamaIndex supports much more involved setups of different kinds of indexes, allows to chain them and even conditionally select one or the other. Here we just do the bare minimum to demonstrate our loaders.After setting up the indices with the appropriate loaders, and connected indexes, we now can use the index as an LLM query engine and execute our user query.To demonstrate that the LLM still is able to use its world knowledge, we can ask in a mix of English (System), German (Question) and French (requested Answer).queryEngine = index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nProvide the answer in French.\nQuestion: Was ist die Hauptstadt von Albanien?\n\"\"\")As you can see in the response below it doesn\u2019t just answer our question correctly in French La capitale de l\u2019Albanie est Tirana., but also provides which documents it used to generate the answer.Response(response=\"La capitale de l'Albanie est Tirana.\", \nsource_nodes=[NodeWithScore(node=Node(text='              &lt;td&gt;Albania&lt;/td&gt;', doc_id='3decbee1-98cc-4650-a071-ed25cd3e00d5', embedding=None, doc_hash='7d9d85082095471a9663690742d2d49fc37b2ec37cc5acf4e99e006a68a17742', extra_info=None, \nnode_info={'start': 0, 'end': 30, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, \nrelationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: '7b6c861f-2c2f-4905-a047-edfc25f7df19'}), score=0.7926356007369129), \nNodeWithScore(node=Node(text='              &lt;td&gt;Algiers&lt;/td&gt;', doc_id='8111b737-9f45-4855-8cd8-f958d4eb0ccd', embedding=None, doc_hash='8570a02a057a6ebbd0aff6d3f63c9f29a0ee858a81d913298d31b025101d1e44', \nextra_info=None, node_info={'start': 0, 'end': 30, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: '22e11ac6-8375-4d0c-91c6-4750fc63a375'}), score=0.7877589022795918)], extra_info={'3decbee1-98cc-4650-a071-ed25cd3e00d5': None, '8111b737-9f45-4855-8cd8-f958d4eb0ccd': None})LlamaIndex LoadersThe number of existing data sources in LlamaHub is impressive, I counted 100+ integrations in the repository. You can find anything from Google docs, to GitHub, to relational databases.LlamaHub, screenshot by AuthorBut I was missing two of my favorite technologies: GraphQL - the API query language open sourced by Facebook and Graph databases like Neo4j, the best way to store and manage large amounts of connected data, for example in Knowledge Graphs.So I thought: \"How hard can it be to add them :)\"Adding the new loadersAdding new loaders is really straightforward. There is a script in the llama-hub repository to help with adding a new loader. Running ./add-loader.sh <folder> added the skeleton files.To get familiar with the existing implementations I looked at the Databases (relational) and MongoDB integrations, the former for the Graph Database and the latter for the GraphQL.It was easy enough, we only needed the requirements for our loader, implement the base.py with an straightforward API and a README.md` with an explanation and a code example.The main difference my loaders have from the existing ones, is that they don\u2019t use hard-coded field names for extracting the relevant value from the query result, but instead turn the result into YAML.I picked YAML not because I like it, but because it was closest to a textual representation of a nested tree of key-value pairs that a user would write as nested bullet lists.Below is the example code for the Graph Database implementation (the GraphQL one is similar).Adding the Graph Database LoaderI added the requirements for the neo4j dependency, a Cypher query language over Bolt protocol python driver, that also works with Memgraph and AWS Neptune.Then I added the code for __init__ to take in a database server URI, database name and credentials to connect and create a driver instance.The load_data method takes in the query to run and optional parameters. It\u2019s implemented by calling the driver\u2019s execute_query method.Each row of results is mapped into a LlamaIndex Document with the text being the YAML representation of the results.\"\"\"Graph Database Cypher Reader.\"\"\"\n\nfrom typing import Dict, List, Optional\n\nfrom llama_index.readers.base import BaseReader\nfrom llama_index.readers.schema.base import Document\n\nimport yaml\n\nclass GraphDBCypherReader(BaseReader):\n    \"\"\"Graph database Cypher reader.\n\n    Combines all Cypher query results into the Document type used by LlamaIndex.\n\n    Args:\n        uri (str): Graph Database URI\n        username (str): Username\n        password (str): Password\n\n    \"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n        username: str,\n        password: str,\n        database: str\n    ) -&gt; None:\n        \"\"\"Initialize with parameters.\"\"\"\n        try:\n            from neo4j import GraphDatabase, basic_auth\n\n        except ImportError:\n            raise ImportError(\n                \"`neo4j` package not found, please run `pip install neo4j`\"\n            )\n        if uri:\n            if uri is None:\n                raise ValueError(\"`uri` must be provided.\")\n            self.client = GraphDatabase.driver(uri=uri, auth=basic_auth(username, password))\n            self.database = database\n\n    def load_data(\n        self, query: str, parameters: Optional[Dict] = None\n    ) -&gt; List[Document]:\n        \"\"\"Run the Cypher with optional parameters and turn results into documents\n\n        Args:\n            query (str): Graph Cypher query string.\n            parameters (Optional[Dict]): optional query parameters.\n\n        Returns:\n            List[Document]: A list of documents.\n\n        \"\"\"\n        if parameters is None:\n            parameters = {}\n\n        records, summary, keys = self.client.execute_query(query, parameters, database_ = self.database)\n\n        documents = [Document(yaml.dump(entry.data())) for entry in records]\n\n        return documentsYou\u2019re now ready to start using the data loader. If you want to start using this in your code, simply import `GraphDBCypherReader` from the relevant file and follow the steps below.If you wish to submit the loader on LlamaHub, the process is fairly straightforward. After adding an example to the readme which uses an always-on demo server with StackOverflow data, I was ready to create a pull request. After a short discussion the PR was quickly merged.Thanks a lot Jerry for the smooth experience.Now let\u2019s see how to use our two loaders.Using the Graph Database LoaderThe GraphDB Cypher loader, connects to graph databases, which are specialized databases that store data not in tables but in entities (Nodes) and their Relationships. Because they are schema free, you can store real-world knowledge without compromising on richness.Image for \u201cNetwork Graph\u201d generated by Midjourney by AuthorRelationships can also hold attributes, which can represent time, weights, costs or whatever defines the concrete relationship. Any node can have as many or as few attributes or relationships as needed.To query a graph database you can use the Cypher query language, a pattern based language that expresses those relationships in visual ascii-art patterns. You encircle nodes in parentheses () and draw relationships as arrows --> with additional constraints put in square brackets. Otherwise Cypher provides many features known from SQL and also supports many graph operations as well as handling data structures like nested documents, of lists and dicts.Let\u2019s use a movie graph database and ask the LLM a question about common action movie plots.Setting up the ServiceContext and the ChatGPTLLMPredictor is the same as before.Then we get the GraphDBCypherReader and connect it to our database (with an small example movie graph from TheMovieDB with permission).GraphDBCypherReader = download_loader('GraphDBCypherReader')\n\nreader = GraphDBCypherReader(uri = \"neo4j+s://demo.neo4jlabs.com\", \\\n    username = \"recommendations\", password = \"recommendations\", database = \"recommendations\")Then we define our query to the graph database with a parameter of year that allows us to pick more recent movies. When loading the data, each row of results should turn into one Document where the text property of the document is the YAML representation of the row.query = \"\"\"\n    MATCH (m:Movie)-[rel:ACTED_IN|DIRECTED|IN_GENRE]-(other)\n    WHERE $year &lt; m.year and m.imdbRating &gt; $rating\n    WITH m, type(rel) as relation, collect(other.name) as names\n    RETURN m.title as title, m.year as year, m.plot as plot, relation, names\n    ORDER BY m.year ASC\n\"\"\"\n\ndocuments = reader.load_data(query, parameters = {\"year\":1990,\"rating\":8})\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\nprint(len(documents))\nprint(documents[0:5])The output will look similar to the following:829\n[Document(text='names:\\n- Saifei He\\n- Li Gong\\n- Jingwu Ma\\n- Cuifen Cao\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: ACTED_IN\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='782d9a63-251b-4bb8-aa3d-5d8f6d1fb5d2', embedding=None, doc_hash='f9fd966bc5f2234e94d09efebd3be008db8c891f8666c1a364abf7812f5d7a1c', extra_info=None), Document(text='names:\\n- Yimou Zhang\\nplot: A young woman becomes the fourth wife of a wealthy lord, and must learn to live\\n  with the strict rules and tensions within the household.\\nrelation: DIRECTED\\ntitle: Raise the Red Lantern (Da hong deng long gao gao gua)\\nyear: 1991\\n', doc_id='2e13caf6-b9cf-4263-a264-7121bc77d1ee', embedding=None, doc_hash='e1f340ed1fac2f1b8d6076cfc2c9e9cb0109d5d11e5dcdbf3a467332f5995cb1', extra_info=None), ...]Now we can use our index to run a LLM query to answer the questions we wanted to pose.queryEngine= index.as_query_engine()\n\nqueryEngine.query(\"\"\"\nWhat are the most common plots in action movies?\n\"\"\")The answer shows that the LLM can utilize the inputs, understands the genre \"action movies\" and can summarize their plots. Here is its answer.Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies.Response(response='Based on the given context information, it appears that the most common plots in action movies are heists and battles against controlling forces. However, it is important to note that this conclusion is based on a limited sample size and may not be representative of all action movies.',\n\n\nsource_nodes=[NodeWithScore(node=Node(text='names:\\n- Action\\n- Crime\\n- Thriller\\nplot: A group of professional bank robbers start to feel the heat from police when\\n  they unknowingly leave a clue at their latest heist.\\nrelation: IN_GENRE\\ntitle: Heat\\nyear: 1995\\n', doc_id='bb117618-1cce-4cec-bd9b-8645ab0b50a3', embedding=None, doc_hash='4d493a9f33eb7a1c071756f61e1975ae5c313ecd42243f81a8827919a618468b', extra_info=None, node_info={'start': 0, 'end': 215, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: 'dbfffdae-d88c-49e2-9d6b-83dad427a3f3'}), score=0.8247381316731472), NodeWithScore(node=Node(text='names:\\n- Thriller\\n- Sci-Fi\\n- Action\\nplot: A computer hacker learns from mysterious rebels about the true nature of his\\n  reality and his role in the war against its controllers.\\nrelation: IN_GENRE\\ntitle: Matrix, The\\nyear: 1999\\n', doc_id='c4893c61-32ee-4d05-b559-1f65a5197e5e', embedding=None, doc_hash='0b6a080bf712548099c5c8c1b033884a38742c73dc23d420ac2e677e7ece82f4', extra_info=None, node_info={'start': 0, 'end': 227, '_node_type': &lt;NodeType.TEXT: '1'&gt;}, relationships={&lt;DocumentRelationship.SOURCE: '1'&gt;: '6c8dea11-1371-4f5a-a1a1-7f517f027008'}), score=0.8220633045996049)], extra_info={'bb117618-1cce-4cec-bd9b-8645ab0b50a3': None, 'c4893c61-32ee-4d05-b559-1f65a5197e5e': None})Using the GraphQL LoaderThe GraphQL loader is similarly easy to use.GraphQL is not a database query language, but an API query language that is based on strict schema expressed in \"type definitions\". There you express your entities, their attributes (fields) both for scalar datatypes as well as object datatypes pointing to other entities.What is GraphQL from GraphQL.org, Screenshot by AuthorGraphQL itself is a tree based query language, that expresses a nested structure of data that you want to fetch starting from a root query. The fields of every entity returned from that query can be selected and for object fields you can further select fields from the referred entity and so on, almost ad-infinitum (API-Limits apply).There are a number of GraphQL libraries, most notably the JavaScript reference implementation, but also gql for python, and also integrations with databases like Hasura, Prisma or the Neo4j-GraphQL-Library. Several larger projects now provide GraphQL APIs including GitHub, Spotify, Twitter.The demo is similar to our first one. We use a public GraphQL endpoint (https://countries.trevorblades.com/), that provides a structure of continent\u2192country\u2192capital. (Licensed under MIT)A subset of the type-definition is here.type Query {\n    continent(code: ID!): Continent\n    continents(filter: ContinentFilterInput = {}): [Continent!]!\n    countries(filter: CountryFilterInput = {}): [Country!]!\n    country(code: ID!): Country\n    language(code: ID!): Language\n    languages(filter: LanguageFilterInput = {}): [Language!]!\n}\n\ntype Continent {\n    code: ID!\n    countries: [Country!]!\n    name: String!\n}\n\ntype Country {\n    awsRegion: String!\n    capital: String\n    code: ID!\n    continent: Continent!\n    currencies: [String!]!\n    currency: String\n    emoji: String!\n    emojiU: String!\n    languages: [Language!]!\n    name(lang: String): String!\n    native: String!\n    phone: String!\n    phones: [String!]!\n    states: [State!]!\n    subdivisions: [Subdivision!]!\n}\n...In our demo, we again define the ServiceContext with the ChatGPTLLMPredictor as before. Then we get the GraphQLReader loader and point it to the URL of the endpoint. You can also provide additional HTTP-Headers, e.g. for authentication.GraphQLReader = download_loader('GraphQLReader')\nreader = GraphQLReader(uri = \"https://countries.trevorblades.com/\", headers = {})query = \"\"\"\nquery getContinents {\n  continents {\n    name\n    countries {\n      name\n      capital\n    }\n  }\n}\n\"\"\"\ndocuments = reader.load_data(query, variables = {})\nprint(len(documents))\nprint(documents)We see that it finds 7 continents with countries and capitals, each of the root results (continent) is turned into a document7\n[Document(text='countries:\\n- capital: Luanda\\n  name: Angola\\n- capital: Ouagadougou\\n  name: Burkina Faso\\n- capital: Bujumbura\\n  name: Burundi\\n- capital: Porto-Novo\\n  name: Benin\\n- capital: Gaborone\\n  name: Botswana\\n- capital: Kinshasa\\n  name: Democratic Republic of the Congo\\n- capital: Bangui\\n  name: Central African Republic\\n....',doc_id='b82fec36-5e82-4246-b7ab-f590bf6741ab', embedding=None, doc_hash='a4caa760423d6ca861b9332f386add3c449f1683168391ae10f7f73a691a2240', extra_info=None)]Again we stress the LLM only a little bit by asking it in German, \"Which capitals are in North America\".index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\nqueryEngine= index.as_query_engine()\n\nresponse = queryEngine.query(\"\"\"\nQuestion: Welche Hauptst\u00e4dte liegen in Nordamerika?\nAnswer:\n\"\"\")\n\nresponse.responseI was surprised, as I had only expected a hand-full of countries and cities. But we get 27 countries that are in North America. This shows how our perception is skewed by the western worldview.Die Hauptst\u00e4dte, die in Nordamerika liegen, sind Ottawa, San Jos\\xE9, Havana, Willemstad, Roseau, Santo Domingo, St. George's, Nuuk, Guatemala City, Tegucigalpa, Port-au-Prince, Kingston, Basseterre, George Town, Castries, Marigot, Fort-de-France, Plymouth, Mexico City, Managua, Panama City, Saint-Pierre, San Juan, San Salvador, Philipsburg, Cockburn Town, Port of Spain, Washington D.C., Kingstown und Road Town.We could also flip the GraphQL query around and then get 250 countries with their respective capitals and continents.query = \"\"\"\nquery getCountries {\n  countries {\n    name\n    capital\n    continent {\n        name\n    }\n  }\n}\n\"\"\"\ndocuments = reader.load_data(query, variables = {})\nprint(len(documents))\nprint(documents)Both document lists should work equally well, but let\u2019s see.This time the answer from the LLM was much more limited. I\u2019m not sure if that was because the index fed the LLM fewer documents to pick from.index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\nqueryEngine= index.as_query_engine()\n\nresponse = queryEngine.query(\"\"\"\nQuestion: Which capitals are in North America?\nAnswer:\n\"\"\")\n\nresponse.responseWashington D.C. and Mexico City are in North America.ConclusionIt was really smooth to add new data loaders to LlamaHub, thanks a lot to Jerry Liu for making it so easy. Please let me know what you\u2019re doing with these loaders and if you have any feedback.If I find time in the next weeks I also want to look into the KnowledgeGraphIndex and see if my graph database loader can nicely populate that one.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//enriching-llamaindex-models-from-graphql-and-graph-databases-bcaecec262d7"
        },
        {
            "date": "Oct 17, 2023",
            "author": "Vishwas Gowda",
            "title": "How I built the Streamlit LLM Hackathon winning app \u2014 FinSight using LlamaIndex.",
            "tags": [],
            "content": "In this article, we\u2019ll dive deep into the world of LLM app development and take a closer look at my journey of building the Streamlit LLM hackathon-winning app FinSight \u2014 Financial Insights At Your Fingertips. This article covers the entire process from ideation to execution, along with code snippets and snapshots.\nIntroduction\nA use case for LLMs in finance\n\n  One fascinating use case for LLMs in finance is to use them on company annual\n  reports (10-K form). These reports are publicly available information that\n  pretty much every portfolio manager, financial analyst, and shareholder uses\n  regularly to make informed decisions.\n\n\n  However reading, understanding, and assessing these reports, especially for\n  multiple companies can be tedious and time-consuming. Hence, using LLMs on\n  annual reports to extract insights and summarize would solve a lot of problems\n  and save valuable time.\n\n\n  When the\n  Streamlit LLM Hackathon\n  was, announced I thought this was the best time to explore this idea. And\n  that\u2019s how\n  FinSight\n  came into existence.\n\nHow does FinSight work?\n\n\nA small Demonstration\n\n\n  FinSight has two main features called Annual Report Analyzer and Finance\n  Metric Review, but for this blog post, we will be concentrating on the former.\n\n\n  Annual Report Analyzer is a RAG(Retrieval Augmented Generation) based feature,\n  which means that the LLM will be generating insights based on the information\n  in a knowledge base (which in this case is a company\u2019s annual report). Here\u2019s\n  how it works behind the scenes:\n\n\n\nRAG pipeline for Annual Report Analyzer\n\n\n  While this is a basic representation of the architecture, we will be doing a\n  deep dive into the importance of each of these components and how they work.\n\nSetup\n\n  In case you want to refer the code to the app:\n  Repo\n\n\n  We will use\n  LlamaIndex\n  to build the knowledge base and to query it using an LLM (gpt-4 is the best\n  suited). LlamaIndex is a simple, flexible data framework for connecting custom\n  data sources to large language models.\n\n\n  For the front end,\n  Streamlit\n  is the most convenient tool to build and share web apps.\n\n\nClone Repository\n\ngit clone https://github.com/vishwasg217/finsight.gitcd finsight\n2. Setup Virtual Environment\n# For macOS and Linux:python3 -m venv venv# For Windows:python -m venv venv\n3. Activate Virtual Environment\n# For macOS and Linux:source venv/bin/activate# For Windows:.\\venv\\Scripts\\activate\n4. Install Required Dependencies:\npip install -r requirements.txt\n5. Set up the Environment Variables:\n# create directorymkdir .streamlit# create toml filetouch .streamlit/secrets.toml\n\n  You can get your API keys here:\n  AlphaVantage,\n  OpenAI,\n\n# Add the following API keysav_api_key = \"ALPHA_VANTAGE API KEY\"openai_api_key = \"OPEN AI API KEY\"\nDocument Loading, Indexing, and Storage\n\n  Although LlamaIndex has its own set of data connectors to read PDFs, we still\n  need to write a small function\n  process_pdf()to load the PDFs since we are doing it through\n  Streamlit.\n\nfrom pypdf import PdfReaderfrom llama_index.schema import Documentdef process_pdf(pdf):    file = PdfReader(pdf)    text = \"\"    for page in file.pages:        text += str(page.extract_text())            doc = Document(text=text)    return [doc]\n\n  The next step is to ingest, index, and store this document in a vector\n  database. In this case, we will use FAISS DB, as we require in an in-memory\n  vector database. FAISS is also very convenient to use. Hence, we write a\n  function called\n  get_vector_index()to do exactly that.\n\n\n  In case you\u2019re interested in checking out other vector DB options, you read\n  can\n  this.\n\nfrom llama_index.llms import OpenAIfrom llama_index import VectorStoreIndex, ServiceContext, StorageContextfrom llama_index.vector_stores import FaissVectorStoredef get_vector_index(documents):    llm = OpenAI(OPENAI_API_KEY)    faiss_index = faiss.IndexFlatL2(d)    vector_store = FaissVectorStore(faiss_index=faiss_index)    storage_context = StorageContext.from_defaults(vector_store=vector_store)    service_context = ServiceContext.from_defaults(llm=llm)     index = VectorStoreIndex.from_documents(documents,         service_context=service_context,        storage_context=storage_context    )       return index\n\nServiceContext() and StorageContext() are used to\n  set the configurations for the vector store. Using\n  VectorStoreIndex.from_documents()\n  we ingest, index, and store the document as vector embeddings in the FAISS DB.\n\n# Calling the functions through streamlit frontendimport streamlit as stif \"index\" not in st.session_state:  st.session_state.index = Noneif \"process_doc\" not in st.session_state:        st.session_state.process_doc = Falseif st.sidebar.button(\"Process Document\"):        with st.spinner(\"Processing Document...\"):            documents = process_pdf(pdfs)            st.session_state.index = get_vector_index(documents)            st.session_state.process_doc = True  st.toast(\"Document Processsed!\")\nQuery Tools and Engines\n\n  Now that we have our knowledge base ready, it\u2019s time to build a mechanism to\n  query it.\n\nindex = get_vector_index(documents)engine = index.as_query_engine()query = \"How has Microsoft performed in this fiscal year?\"response = engine(query)\n\n  Ideally, the above code should have been enough to query and synthesize a\n  response from the information in the vector DB. However, the response wouldn't\n  be comprehensive and detailed enough, especially for such open-ended\n  questions. We need to develop a better mechanism that allows us to break down\n  a query into more detailed questions and retrieve context from multiple parts\n  of the vector DB.\n\ndef get_query_engine(engine):    query_engine_tools = [        QueryEngineTool(            query_engine=engine,            metadata=ToolMetadata(                name=\"Annual Report\",                description=f\"Provides information about the company from its annual report.\",            ),        ),    ]    s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)    return s_engineindex = get_vector_index(documents)engine = index.as_query_engine()s_engine = get_query_engine(engine)\n\n  Let\u2019s break the above function down. The\n  QueryEngineTool module wraps around the engine and\n  helps provide context and metadata to the engine. This is especially useful\n  when you have more than one engine and you want to provide context to the LLM\n  as to which one to use for a given query.\n\nHere\u2019s what that would look like:\n# example for multiple query engine toolsquery_engine_tools = [    QueryEngineTool(        query_engine=sept_engine,        metadata=ToolMetadata(            name=\"sept_22\",            description=\"Provides information about Uber quarterly financials ending September 2022\",        ),    ),    QueryEngineTool(        query_engine=june_engine,        metadata=ToolMetadata(            name=\"june_22\",            description=\"Provides information about Uber quarterly financials ending June 2022\",        ),    )]\n\n  You can read more about the tools available in LlamaIndex\n  here.\n\nHowever, we\u2019re currently sticking to just one QueryEnginerTool for now.\n\n  The SubQuestionQueryEngine module breaks down a complex query\n  into many sub-questions and their target query engine for execution. After\n  executing all sub-questions, all responses are gathered and sent to a response\n  synthesizer to produce the final response. Using this module is essential\n  because generating insights from annual reports requires complex queries that\n  need to retrieve information from multiple nodes within the vector DB.\n\n\n\nSubQuestionQueryEngine at work\n\nPrompt Engineering\n\n  Prompt engineering is essential to the entire process mainly for two reasons:\n\n\n\n    To provide clarity to the agent as to what it needs to retrieve from the\n    vector DB by writing precise and relevant queries\n  \n\n    And then control the quality of the output generated from the retrieved\n    context by providing a structure and description for the output to be\n    generated.\n  \n\n\n  Both these points are handled by using\n  PromptTemplate and PydanticOutputParser module in\n  langchain .\n\n\n  Using the PydanticOutputParser we write the description for the\n  different sections of the insights to be generated. After having a few\n  conversations with finance experts, I concluded generating insights for these\n  4 sections: different sections: Fiscal Year Highlights, Strategic Outlook and\n  Future Direction, Risk Management, Innovation and R&D. Now let\u2019s write the\n  pydantic class for these sections:\n\nfrom pydantic import BaseModel, Fieldclass FiscalYearHighlights(BaseModel):    performance_highlights: str = Field(..., description=\"Key performance metrics and financial stats over the fiscal year.\")    major_events: str = Field(..., description=\"Highlight of significant events, acquisitions, or strategic shifts that occurred during the year.\")    challenges_encountered: str = Field(..., description=\"Challenges the company faced during the year and, if and how they managed or overcame them.\")class StrategyOutlookFutureDirection(BaseModel):    strategic_initiatives: str = Field(..., description=\"The company's primary objectives and growth strategies for the upcoming years.\")    market_outlook: str = Field(..., description=\"Insights into the broader market, competitive landscape, and industry trends the company anticipates.\")class RiskManagement(BaseModel):    risk_factors: str = Field(..., description=\"Primary risks the company acknowledges.\")    risk_mitigation: str = Field(..., description=\"Strategies for managing these risks.\")class InnovationRnD(BaseModel):    r_and_d_activities: str = Field(..., description=\"Overview of the company's focus on research and development, major achievements, or breakthroughs.\")    innovation_focus: str = Field(..., description=\"Mention of new technologies, patents, or areas of research the company is diving into.\")\n\nNote: These sections and their description are for generic use cases. They\n    can be changed to suit your particular needs.\n\n\n  These pydantic classes will provide the format and description for each\n  section to the prompt. So let\u2019s write a function that allows us to plug in any\n  pydantic class to a prompt:\n\nfrom langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParserprompt_template = \"\"\"You are given the task of generating insights for {section} from the annual report of the company. Given below is the output format, which has the subsections.Must use bullet points.Always use $ symbol for money values, and round it off to millions or billions accordinglyIncase you don't have enough info you can just write: No information available---{output_format}---\"\"\"def report_insights(engine, section, pydantic_model):    parser = PydanticOutputParser(pydantic_object=pydantic_model)    prompt_template = PromptTemplate(        template=prompt_template,        input_variables=[\"section\"],        partial_variables={\"output_format\": parser.get_format_instructions()}    )    formatted_input = prompt_template.format(section=section)    response = engine.query(formatted_input)    parsed_response = parser.parse(response.response)    return parsed_response\n\nPromptTemplate plugs in all the values such as\n  section and output_format into the prompt template.\n  PydanticOutputParser converts the pydantic class into a format\n  that is readable to the LLM. The response generated will be in string format,\n  hence we use the parser.parse() function to parse the response\n  and get a structured output.\n\n# calling the function in streamlit frontendif st.session_state.process_doc:    if st.button(\"Analyze Report\"):        engine = get_query_engine(st.session_state.index.as_query_engine(similarity_top_k=3))        with st.status(\"**Analyzing Report...**\"):            st.write(\"Fiscal Year Highlights...\")            st.session_state.fiscal_year_highlights = report_insights(engine, \"Fiscal Year Highlights\", FiscalYearHighlights)            st.write(\"Strategy Outlook and Future Direction...\")            st.session_state.strategy_outlook_future_direction = report_insights(engine, \"Strategy Outlook and Future Direction\", StrategyOutlookFutureDirection)            st.write(\"Risk Management...\")            st.session_state.risk_management = report_insights(engine, \"Risk Management\", RiskManagement)                        st.write(\"Innovation and R&D...\")            st.session_state.innovation_and_rd = report_insights(engine, \"Innovation and R&D\", InnovationRnD)# displaying the generated insights  if st.session_state.fiscal_year_highlights:                with tab1:            st.write(\"## Fiscal Year Highlights\")            st.write(\"### Performance Highlights\")            st.write(st.session_state.fiscal_year_highlights.performance_highlights)            st.write(\"### Major Events\")            st.write(st.session_state.fiscal_year_highlights.major_events)            st.write(\"### Challenges Encountered\")            st.write(st.session_state.fiscal_year_highlights.challenges_encountered)            st.write(\"### Milestone Achievements\")            st.write(str(st.session_state.fiscal_year_highlights.milestone_achievements))    if st.session_state.strategy_outlook_future_direction:        with tab2:            st.write(\"## Strategy Outlook and Future Direction\")            st.write(\"### Strategic Initiatives\")            st.write(st.session_state.strategy_outlook_future_direction.strategic_initiatives)            st.write(\"### Market Outlook\")            st.write(st.session_state.strategy_outlook_future_direction.market_outlook)            st.write(\"### Product Roadmap\")            st.write(st.session_state.strategy_outlook_future_direction.product_roadmap)    if st.session_state.risk_management:        with tab3:            st.write(\"## Risk Management\")            st.write(\"### Risk Factors\")            st.write(st.session_state.risk_management.risk_factors)            st.write(\"### Risk Mitigation\")            st.write(st.session_state.risk_management.risk_mitigation)    if st.session_state.innovation_and_rd:        with tab4:            st.write(\"## Innovation and R&D\")            st.write(\"### R&D Activities\")            st.write(st.session_state.innovation_and_rd.r_and_d_activities)            st.write(\"### Innovation Focus\")            st.write(st.session_state.innovation_and_rd.innovation_focus)\n\n  You can find the complete code Annual Report Analyzer\n  here\n\nUpcoming Features\n\n\n    Select and Store Insights: I\u2019ve been working on a feature that allows the\n    user to select any insight needed and also save it into the user\u2019s account\n  \n\n    Adding more profession-specific insights: Currently, the insight works well\n    for generic purposes. However, different professions use annual reports\n    differently, so naturally I need to create a different set of insights based\n    on the user\u2019s use case.\n  \n\nPandasQueryEngine Module for querying financial statements:\n    Using this module, the LLM will be able to extract better insights from\n    financial statements which are typically in a structured format.\n  \n\nConclusion\n\n  In summary, FinSight\u2019s Annual Report Analyzer makes financial analysis easier\n  and more insightful by harnessing the power of LLMs. It\u2019s a valuable tool for\n  portfolio managers, financial analysts, and shareholders, saving time and\n  improving decision-making. While the core pipeline remains consistent, note\n  that our deployed app code might evolve to incorporate upgrades and enhanced\n  features, ensuring ongoing improvements.\n\n\n  Big thanks to\n  LlamaIndex\n  for helping me make FinSight a reality. No other framework is as advanced in\n  making RAG-based tools.\n\n\n  If you like what you\u2019ve read, please do leave a clap for me, and also show\n  some love to\n  FinSight. You can check out the GitHub repo\n  here.\n\n\n  You connect with me on\n  LinkedIn\n  and\n  Twitter\n\n",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//how-i-built-the-streamlit-llm-hackathon-winning-app-finsight-using-llamaindex-9dcf6c46d7a0"
        },
        {
            "date": "May 28, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-05-28",
            "tags": [],
            "content": "Greetings, LlamaIndex Family! \ud83e\udd99Welcome to your latest weekly update from LlamaIndex! We're excited to present a variety of outstanding integration updates, detailed guides, demos, educational tutorials, and informative webinars this week.\ud83e\udd29\u00a0The highlights:Secure Code Execution with AzureCodeInterpreterTool: Securely run LLM-generated code with Azure Container Apps, integrated with LlamaIndex for safe code execution.Build Automated Email Agents: Create email agents with MultiOn and LlamaIndex that autonomously read, index, and respond to emails.LlamaFS for Organized Files: Alex Reibman's team developed LlamaFS to automatically structure messy file directories, enhanced by Llama 3 and Groq Inc.'s API.RAGApp's No-Code Chatbots: Deploy RAG chatbots easily with RAGApp's no-code interface, fully open-source and cloud-compatible.\u2728 Feature Releases and Enhancements:We have launched Azure Container Apps dynamic sessions to securely run LLM-generated code in a sandbox. Integrated into LlamaIndex, this feature ensures safe execution of complex code tasks by your agents. Set up a session pool on Azure, add the AzureCodeInterpreterTool to your agent, and you\u2019re ready to go. Blogpost, Tweet.We have integrated with the open source Nomic embed, now fully operable locally. This integration allows for completely local embeddings and introduces a dynamic inference mode that optimizes embedding latency. The system automatically selects between local and remote embeddings based on speed, ensuring optimal performance. Docs, Tweet.We have integrated the Vespa vector store, supporting hybrid search with BM25. Docs, Tweet.We have integrated with MyMagic AI to facilitate batch data processing for GenAI applications. This setup allows you to pre-process large datasets with an LLM, enabling advanced analysis and querying capabilities. Docs, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to building an automated Email Agent with MultiOn and LlamaIndex that can autonomously read and index emails for easy retrieval and draft responses using advanced browsing capabilities.Guide to building Full-Stack Job Search Assistant by Rishi Raj Jain using Gokoyeb, MongoDB, and LlamaIndex. This guide takes you through setting up MongoDB Atlas, crafting a Next.js application, developing UI components, and deploying your app on Koyeb, complete with real-time response streaming and continuous job updates.\ud83d\udda5\ufe0f\u00a0Demos:LlamaFS, a project developed by Alex Reibman and his team, automatically organizes messy file directories into neatly structured folders with interpretable names. Enhanced by Llama 3 and supported by Groq Inc.'s API, Ollama's fully local mode and LlamaIndex, this tool significantly improves file management efficiency. Code, Tweet.RAGApp, a project developed by Marcus Schiesser, offers a no-code interface for configuring RAG chatbots as simply as GPTs by OpenAI. This fully open-source docker container can be deployed on any cloud platform, allowing users to set up the LLM, define system prompts, upload knowledge bases, and launch chatbots via UI or API. Code, Tweet.\u270d\ufe0f Tutorials:Phil Chirchir\u2019s tutorial on DSPy RAG with LlamaIndex. It demonstrates how to integrate DSPy bootstrapping models with a LlamaIndex RAG pipeline powered by LlamaParse.Pavan Kumar\u2019s tutorial on advanced image indexing for RAG demonstrates how to combine image embeddings with structured annotations using multimodal models. It details how to enhance image search with LlamaIndex and Qdrant Engine\u2019s capabilities.Jayita Bhattacharyya\u2019s tutorial on Building a RAG Chatbot using Llamaindex, Groq with Llama3 & Chainlit.\ud83d\udcf9\u00a0Webinar:Webinar with OpenDevin team to learn how to build an Open-Source Coding Assistant using OpenDevin.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-05-28"
        },
        {
            "date": "Jan 8, 2024",
            "author": "Jerry Liu",
            "title": "Introducing Query Pipelines",
            "tags": [
                "llamaindex",
                "retrieval-augmented",
                "llm",
                "ai"
            ],
            "content": "Today we introduce Query Pipelines, a new declarative API within LlamaIndex that allows you to concisely orchestrate simple-to-advanced query workflows over your data for different use cases (RAG, structured data extraction, and more).At the core of all this is ourQueryPipeline abstraction. It can take in many LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It can create a computational graph over these modules (e.g. a sequential chain or a DAG). It has callback support and native support with our observability partners.The end goal is that it\u2019s even easier to build LLM workflows over your data. Check out our comprehensive introduction guide, as well as our docs page for more details.Example `QueryPipeline` setup for an advanced RAG pipelineContextOver the past year AI engineers have developed customized, complex orchestration flows with LLMs to solve a variety of different use cases. Over time some common patterns developed. At a top-level, paradigms emerged to query a user\u2019s data \u2014 this includes RAG (in a narrow definition) to query unstructured data, and text-to-SQL to query structured data. Other paradigms emerged around use cases like structured data extraction (e.g. prompt the LLM to output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and agents that could interact with external services (combine prompt chainingThere is a lot of query orchestration in RAG. Even within RAG itself there can be a lot of work to build an advanced RAG pipeline optimized for performance. Starting from the user query, we may want to run query understanding/transformations (re-writing, routing). We also may want to run multi-stage retrieval algorithms \u2014 e.g. top-k lookup + reranking. We may also want to use prompts + LLMs to do response synthesis in different ways. Here\u2019s a great blog on advanced RAG components.Source: \u201cAdvanced RAG Techniques: an Illustrated Overview\u201d by Ivan IlinRAG has become more modular: Instead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the RAG Survey paper by Gao et al.This leads to creative new patterns like DSP, Rewrite-Retrieve-Read, or interleaving retrieval+generation multiple times.Previous State of LlamaIndexLlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes letting users setup different RAG pipelines, and has been at the forefront of establishing advanced RAG patterns.We\u2019ve also exposed low-level modules such as LLMs, prompts, embeddings, postprocessors and easy subclassability of core components like retrievers and query engines so that users can define their own workflows.But up until now, we didn\u2019t explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively.Query PipelineAs a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity.You can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code.It has the following benefits:Express common query workflows with fewer lines of code/boilerplate: Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module!Greater readability: Reduced boilerplate leads to greater readability.End-to-end observability: Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations.[In the future] Easy Serializability: A declarative interface allows the core components to be serialized/redeployed on other systems much more easily.[In the future] Caching: This interface also allows us to build a caching layer under the hood, allowing input re-use.Visualization of our advanced RAG QueryPipeline using `networkx` and `pyvis`UsageThe QueryPipeline allows you to a DAG-based query workflow using LlamaIndex modules. There are two main ways to use it:As a sequential chain (easiest/most concise)As a full DAG (more expressive)See our usage pattern guide for more details.Sequential ChainSome simple pipelines are purely linear in nature \u2014 the output of the previous module directly goes into the input of the next module.Some examples:Prompt \u2192 LLM \u2192 Output parsingRetriever \u2192Response synthesizerHere\u2019s the most basic example, chaining a prompt with LLM. Simply initialize QueryPipeline with the chain parameter.# try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)Setting up a DAG for an Advanced RAG WorkflowGenerally setting up a query workflow will require using our lower-level functions to build a DAG.For instance, to build an \u201cadvanced RAG\u201d consisting of query rewriting/retrieval/reranking/synthesis, you\u2019d do something like the following.from llama_index.postprocessor import CohereRerank\nfrom llama_index.response_synthesizers import TreeSummarize\nfrom llama_index import ServiceContext\n\n# define modules\nprompt_str = \"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\nretriever = index.as_retriever(similarity_top_k=3)\nreranker = CohereRerank()\nsummarizer = TreeSummarize(\n    service_context=ServiceContext.from_defaults(llm=llm)\n)\n\n# define query pipeline\np = QueryPipeline(verbose=True)\np.add_modules(\n    {\n        \"llm\": llm,\n        \"prompt_tmpl\": prompt_tmpl,\n        \"retriever\": retriever,\n        \"summarizer\": summarizer,\n        \"reranker\": reranker,\n    }\n)\n# add edges \np.add_link(\"prompt_tmpl\", \"llm\")\np.add_link(\"llm\", \"retriever\")\np.add_link(\"retriever\", \"reranker\", dest_key=\"nodes\")\np.add_link(\"llm\", \"reranker\", dest_key=\"query_str\")\np.add_link(\"reranker\", \"summarizer\", dest_key=\"nodes\")\np.add_link(\"llm\", \"summarizer\", dest_key=\"query_str\")In this code block we 1) add modules, and then 2) define relationships between modules. Note that by source_key and dest_key are optional and are only required if first module has more than one output / the second module has more than one input respectively.Running the PipelineIf the pipeline has one \u201croot\u201d node and one output node, use run . Using the previous example,output = p.run(topic=\"YC\")\n# output type is Response\ntype(output)If the pipeline has multiple root nodes and/or multiple output nodes, use run_multi .output_dict = p.run_multi({\"llm\": {\"topic\": \"YC\"}})\nprint(output_dict)Defining a Custom Query ComponentIt\u2019s super easy to subclass CustomQueryComponent so you can plug it into the QueryPipeline.Check out our walkthrough for more details.Supported ModulesCurrently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own!LLMs (both completion and chat) ( LLM )Prompts ( PromptTemplate )Query Engines ( BaseQueryEngine )Query Transforms ( BaseQueryTransform )Retrievers ( BaseRetriever )Output Parsers ( BaseOutputParser )Postprocessors/Rerankers ( BaseNodePostprocessor)Response Synthesizers ( BaseSynthesizer )Other QueryPipelineobjectsCustom components ( CustomQueryComponent )Check out the module usage guide for more details.Walkthrough ExampleMake sure to check out our Introduction to Query Pipelines guide for full details. We go over all the steps above with concrete examples!The notebook guide also logs traces through Arize Phoenix. You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider.Related WorkThe idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include Haystack as well as the LangChain Expression Language. Other related works include pipelines that are setup in the no-code/low-code setting such as Langflow / Flowise.Our main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There\u2019s a lot of optimizations/guides to be done here!FAQWhat\u2019s the difference between a QueryPipeline and IngestionPipeline ?Great question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there\u2019s potentially some shared abstractions we\u2019ll develop for both!Conclusion + ResourcesThat\u2019s it! As mentioned above we\u2019ll be adding a lot more resources and guides soon. In the meantime check out our current guides:Query Pipelines GuideQuery Pipelines WalkthroughQuery Pipeline Usage PatternQuery Pipelines Module Usage Guide",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-query-pipelines-025dc2bb0537"
        },
        {
            "date": "May 23, 2024",
            "author": "MultiOn",
            "title": "Automate online tasks with MultiOn and LlamaIndex",
            "tags": [
                "automation",
                "agents"
            ],
            "content": "IntroductionMultiOn is an AI agents platform designed to facilitate the autonomous completion of tasks in any web environment. It empowers developers to build AI agents that can manage online activities from start to finish, handling everything from simple data retrieval to complex interactions.LlamaIndex complements this by providing an orchestration framework that bridges the gap between private and public data essential for building applications with Large Language Models. It facilitates data ingestion, indexing, and querying, making it indispensable for developers looking to leverage generative AI.In this article, we'll demonstrate how MultiOn's capabilities can be seamlessly integrated within the LlamaIndex framework, showcasing a practical application that leverages both technologies to automate and streamline web interactions.Technical walkthrough: Integrating MultiOn with LlamaIndexLet\u2019s explore a practical example where MultiOn and LlamaIndex work in tandem to manage email interactions and web browsing.Step 1: Setting Up the Environment We begin by setting up our AI agent with the necessary configurations and API keys:import openai\nfrom llama_index.agent.openai import OpenAIAgent\nopenai.api_key = \"sk-your-key\"\n\nfrom llama_index.tools.multion import MultionToolSpec\nmultion_tool = MultionToolSpec(api_key=\"your-multion-key\")Step 2: Integrating Gmail Search Tool Next, we integrate a Gmail search tool to help our agent fetch and analyze emails, providing the necessary context for further actions:from llama_index.tools.google import GmailToolSpec\nfrom llama_index.core.tools.ondemand_loader_tool import OnDemandLoaderTool\n\ngmail_tool = GmailToolSpec()\ngmail_loader_tool = OnDemandLoaderTool.from_tool(\n    gmail_tool.to_tool_list()[1],\n    name=\"gmail_search\",\n    description=\"\"\"\n         This tool allows you to search the users gmail inbox and give directions for how to summarize or process the emails\n\n        You must always provide a query to filter the emails, as well as a query_str to process the retrieved emails.\n        All parameters are required\n        \n        If you need to reply to an email, ask this tool to build the reply directly\n        Examples:\n            query='from:adam subject:dinner', max_results=5, query_str='Where are adams favourite places to eat'\n            query='dentist appointment', max_results=1, query_str='When is the next dentist appointment'\n            query='to:jerry', max_results=1, query_str='summarize and then create a response email to jerrys latest email'\n            query='is:inbox', max_results=5, query_str='Summarize these emails'\n    \"\"\"\n)Step 3: Initialize agentInitialise the agent with tools and a system promptagent = OpenAIAgent.from_tools(\n    [*multion_tool.to_tool_list(), gmail_loader_tool],\n    system_prompt=\"\"\"\n\t    You are an AI agent that assists the user in crafting email responses based on previous conversations.\n\t    \n\t    The gmail_search tool connects directly to an API to search and retrieve emails, and answer questions based on the content.\n\t    The browse tool allows you to control a web browser with natural language to complete arbitrary actions on the web.\n\t    \n\t    Use these two tools together to gain context on past emails and respond to conversations for the user.\n    \"\"\"\n)Step 4: Agent Execution Flow With our tools integrated, the agent is now equipped to perform a series of tasks:1. Search and Summarize Emails: The agent uses LlamaIndex's Gmail tool to fetch relevant emails and summarize the content, providing a basis for drafting a response.print(agent.chat(\"browse to the latest email from Julian and open the email\"))Added user message to memory: browse to the latest email from Julian and open the email\n=== Calling Function ===\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Browse to the latest email from Julian and open the email\"}\nPlease visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=1054044249014.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.compose+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=JSdsfdsi990sddsd&access_type=offline\nGot output: Open the email from Julian to view the latest communication.\n========================\n \nI have opened the latest email from Julian for you to view. If you need any specific information or action to be taken, please let me know.2. Generate Response: Based on the summarized information, the agent crafts an appropriate response to the email chain.print(agent.chat(\n\t\"Summarize the email chain with julian and create a response to the last email that confirms all the details\"\n))Added user message to memory: Summarize the email chain with julian and create a response to the last email that confirms all the details\n=== Calling Function ===\nCalling function: gmail_search with args: {\"query\":\"from:Julian\",\"max_results\":1,\"query_str\":\"Summarize the email chain with Julian and create a response to the last email confirming all the details\"}\nGot output: The email chain with Julian involved a change in an event scheduled for Friday, August 6, 2021, from 15:30 to 16:00 United Kingdom Time on Google Meet. The instructions for joining were provided in the description. The email also included contact information for joining the meeting. Julian and Nassar were listed as attendees, with Julian being the organizer. The email was authenticated and passed SPF and DKIM checks.\n\nIn response to the last email, I would confirm all the details of the event change, reiterating the date, time, platform (Google Meet), and any specific instructions provided. I would express gratitude for the update and confirm attendance at the revised event timing.\n========================\n\nBased on the email chain with Julian, here is a summary:\n- The event scheduled for Friday, August 6, 2021, has been changed from 15:30 to 16:00 United Kingdom Time on Google Meet.\n- Instructions for joining the meeting were provided in the email.\n- Attendees included Julian and Nassar, with Julian as the organizer.\n- The email passed SPF and DKIM checks.\n\nTo respond and confirm all the details, you can mention the revised event date and time, the platform (Google Meet), and express gratitude for the update. Confirm your attendance at the new timing. Let me know if you would like me to draft the response email for you.3. Send Email through MultiOn: Finally, the generated response is passed to the MultiOn agent, which manages the action of sending the email through the web browser.print(agent.chat(\n\t\"pass the entire generated email to the browser and have it send the email as a reply to the chain\"\n))Added user message to memory: pass the entire generated email to the browser and have it send the email as a reply to the chain\n=== Calling Function ===\nCalling function: browse with args: {\"cmd\": \"Compose a reply email to Julian confirming the event change to Fri 6 Aug 2021 from 15:30 to 16:00 UK Time on Google Meet. Express readiness to attend and thank Julian for the details.\"}\nGot output: Email response sent to Julian\n========================Next StepsMultiOn is an officially supported tool on LlamaHub, the central page for all LlamaIndex integrations (from tools to LLMs to vector stores). Check out the LlamaHub page here.If you\u2019re interested in running through this tutorial on building a browser + Gmail-powered agent yourself, check out our notebook.The integration of MultiOn and LlamaIndex offers a powerful toolkit for developers aiming to automate and streamline online tasks. As these technologies evolve, they will continue to unlock new potentials in AI application, significantly impacting how developers interact with digital environments and manage data.",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS"
            ],
            "link": "https://www.llamaindex.ai/blog//automate-online-tasks-with-multion-and-llamaindex"
        },
        {
            "date": "Mar 5, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-03-05",
            "tags": [
                "llm",
                "newsletter",
                "ai"
            ],
            "content": "Greetings, LlamaIndex devotees! \ud83e\udd99It was another fun week to be at the center of the LLM universe, and we have tons to share!\ud83e\udd29\u00a0The highlights:We shared our thoughts on the future of long-context RAG. As LLMs with context windows over 1M tokens begin to appear, what changes about RAG, and how will LlamaIndex evolve? Tweet, Blog postllama-index-networks lets you build a super-RAG application by combining answers from independent RAG apps over the network. Tweet, Blog post, repoPeople loved our release of LlamaParse, a world-beating PDF parsing service, so we made it even better! Tweet, blog post\u2728 Feature Releases and Enhancements:We released a new llama-index-networks feature that lets you combine multiple independent RAG applications over the network, allowing you to run a single query across all the applications and get a single, combined answer. Tweet, Blog post, repoInference engine Groq wowed us and the world with their incredibly fast query times and we were delighted to introduce first-class support for their LLM APIs. Tweet, notebookUsers love LlamaParse, the world-beating PDF parsing service we released last week. We pushed improved parsing and OCR support for 81+ languages! We also increased the usage cap from 1k to 10k pages per day. Tweet, blog postWe migrated our blog off of Medium, we hope you like the new look and the absence of nag screens!RAPTOR is a new tree-structured technique for advanced RAG; we turned the paper into a LlamaPack, allowing you to use the new technique in one line of code. Tweet, package, notebook, original paper\ud83c\udfa5\u00a0Demos:The Koda Retriever is a new retrieval concept: hybrid search where the alpha parameter controlling the importance of vector search vs. keyword search is tuned on a per-query basis by the LLM itself, based on a few-shot examples. Tweet, notebook, package, blog postMixedbread.ai released some state-of-the-art rerankers that perform better than anything seen before; we whipped up a quick cookbook to show you how to use them directly in LlamaIndex. Tweet, Notebook, blog post\ud83d\uddfa\ufe0f Guides:Function-calling cookbook with open source models shows you how to use Fireworks AI\u2019s OpenAI-compatible API to use all native LlamaIndex support for function calling. Notebook, Tweet.We released a best practices cookbook showing how to use LlamaParse, our amazing PDF parser. Tweet, notebookA comprehensive guide to semantic chunking for RAG by Florian June covers embedding-based chunking, BERT-based chunking techniques, and LLM-based chunking for everything you need to know about this highly effective technique to improve retrieval quality. Tweet, Blog post\u270d\ufe0f Tutorials:Our own Andrei presented a notebook on building Basic RAG with LlamaIndex at Vector Institute\u2019s RAG bootcamp. Tweet, NotebookClickHouse presented an in-depth tutorial using LlamaIndex to query both structured and unstructured data, and built a bot that queries Hacker News to find what people are saying about the most popular technologies. Tweet, blog postPOLM (Python, OpenAI, LlamaIndex, MongoDB) is a new reference architecture for building RAG applications and MongoDB has a beautiful, step-by-step tutorial for building it out. Tweet, blog post\ud83c\udfa5\u00a0Webinar:Our CEO Jerry Liu will do a joint webinar with Adam Kamor of Tonic.ai about building fully-local RAG applications with Ollama and Tonic. People love local models! Tweet, Registration pageJerry also did a webinar with Traceloop on leveling up your LLM application with observability. Tweet, YouTubeOur hackathon at the beginning of February was a huge success! Check out this webinar in which we invited the winners to come and talk about their projects. Tweet, YouTube.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-03-05"
        },
        {
            "date": "Jun 27, 2023",
            "author": "Jerry Liu",
            "title": "Build and Scale a Powerful Query Engine with LlamaIndex and Ray",
            "tags": [
                "nlp",
                "distributed-systems",
                "ai",
                "large-language-models",
                "parallel-computing"
            ],
            "content": "Co-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at Anyscale)(note: this is cross-posted from the original blog post on Anyscale\u2019s website. Check it out here!)In this blog, we showcase how you can use LlamaIndex and Ray to build a query engine to answer questions and generate insights about Ray itself, given its documentation and blog posts.We\u2019ll give a quick introduction of LlamaIndex + Ray, and then walk through a step-by-step tutorial on building and deploying this query engine. We make use of both Ray Datasets to parallelize building indices as well as Ray Serve to build deployments.IntroductionLarge Language Models (LLMs) offer the promise of allowing users to extract complex insights from their unstructured text data. Retrieval-augmented generation pipelines have emerged as a common pattern for developing LLM applications allowing users to effectively perform semantic search over a collection of documents.Example of retrieval augmented generation. Relevant context is pulled from a set of documents and included in the LLM input prompt.However, when productionizing these applications over many different data sources, there are a few challenges:Tooling for indexing data from many different data sourcesHandling complex queries over different data sourcesScaling indexing to thousands or millions of documentsDeploying a scalable LLM application into productionHere, we showcase how LlamaIndex and Ray are the perfect setup for this task.LlamaIndex is a data framework for building LLM applications, and solves Challenges #1 and #2. It also provides a comprehensive toolkit allowing users to connect their private data with a language model. It offers a variety of tools to help users first ingest and index their data \u2014 convert different formats of unstructured and structured data into a format that the language model can use, and query their private data.Ray is a powerful framework for scalable AI that solves Challenges #3 and #4. We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud.More specifically, we showcase a very relevant use case \u2014 highlighting Ray features that are present in both the documentation as well as the Ray blog posts!Data Ingestion and Embedding PipelineWe use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below.Code for this part of the blog is available here.Sequential pipeline with \u201cingest\u201d, \u201cparse\u201d and \u201cembed\u201d stages. Files are processed sequentially resulting in poor hardware utilization and long computation time.Parallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized.Load DataWe start by ingesting these two sources of data. We first fetch both data sources and download the HTML files.We then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API\u2019s, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by Unstructured.from typing import Dict, List\nfrom pathlib import Path\n\nfrom llama_index import download_loader\nfrom llama_index import Document\n\n# Step 1: Logic for loading and parsing the files into llama_index documents.\nUnstructuredReader = download_loader(\"UnstructuredReader\")\nloader = UnstructuredReader()\n\ndef load_and_parse_files(file_row: Dict[str, Path]) -&gt; Dict[str, Document]:\n    documents = []\n    file = file_row[\"path\"]\n    if file.is_dir():\n        return []\n    # Skip all non-html files like png, jpg, etc.\n    if file.suffix.lower() == \".html\":\n        loaded_doc = loader.load_data(file=file, split_documents=False)\n        loaded_doc[0].extra_info = {\"path\": str(file)}\n        documents.extend(loaded_doc)\n    return [{\"doc\": doc} for doc in documents]Unstructured offers a robust suite of parsing tools on top of various files. It is able to help sanitize HTML documents by stripping out information like tags and formatting the text accordingly.Scaling Data IngestSince we have many HTML documents to process, loading/processing each one serially is inefficient and slow. This is an opportunity to use Ray and distribute execution of the `load_and_parse_files` method across multiple CPUs or GPUs.import ray\n\n# Get the paths for the locally downloaded documentation.\nall_docs_gen = Path(\"./docs.ray.io/\").rglob(\"*\")\nall_docs = [{\"path\": doc.resolve()} for doc in all_docs_gen]\n\n# Create the Ray Dataset pipeline\nds = ray.data.from_items(all_docs)\n\n# Use `flat_map` since there is a 1:N relationship.\n# Each filepath returns multiple documents.\nloaded_docs = ds.flat_map(load_and_parse_files)Parse FilesNow that we\u2019ve loaded the documents, the next step is to parse them into Node objects \u2014 a \u201cNode\u201d object represents a more granular chunk of text, derived from the source documents. Node objects can be used in the input prompt as context; by setting a small enough chunk size, we can make sure that inserting Node objects do not overflow the context limits.We define a function called `convert_documents_into_nodes` which converts documents into nodes using a simple text splitting strategy.# Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.data_structs import Node\n\ndef convert_documents_into_nodes(documents: Dict[str, Document]) -&gt; Dict[str, Node]:\n    parser = SimpleNodeParser()\n    document = documents[\"doc\"]\n    nodes = parser.get_nodes_from_documents([document]) \n    return [{\"node\": node} for node in nodes]Run Parsing in ParallelSince we have many documents, processing each document into nodes serially is inefficient and slow. We use Ray `flat_map` method to process documents into nodes in parallel:\n# Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\nnodes = loaded_docs.flat_map(convert_documents_into_nodes)Generate EmbeddingsWe then generate embeddings for each Node using a Hugging Face Sentence Transformers model. We can do this with the help of LangChain\u2019s embedding abstraction.Similar to document loading/parsing, embedding generation can similarly be parallelized with Ray. We wrap these embedding operations into a helper class, called `EmbedNodes`, to take advantage of Ray abstractions.# Step 3: Embed each node using a local embedding model.\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nclass EmbedNodes:\n    def __init__(self):\n        self.embedding_model = HuggingFaceEmbeddings(\n            # Use all-mpnet-base-v2 Sentence_transformer.\n            # This is the default embedding model for LlamaIndex/Langchain.\n            model_name=\"sentence-transformers/all-mpnet-base-v2\", \n            model_kwargs={\"device\": \"cuda\"},\n            # Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.\n            # Remove the \"device\": \"cuda\" to use CPU instead.\n            encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n            )\n\n    def __call__(self, node_batch: Dict[str, List[Node]]) -&gt; Dict[str, List[Node]]:\n        nodes = node_batch[\"node\"]\n        text = [node.text for node in nodes]\n        embeddings = self.embedding_model.embed_documents(text)\n        assert len(nodes) == len(embeddings)\n\n        for node, embedding in zip(nodes, embeddings):\n            node.embedding = embedding\n        return {\"embedded_nodes\": nodes}Afterwards, generating an embedding for each node is as simple as calling the following operation in Ray:# Use `map_batches` to specify a batch size to maximize GPU utilization.\n# We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. \n\n# This state can be reused for multiple batches.\nembedded_nodes = nodes.map_batches(\n    EmbedNodes, \n    batch_size=100, \n    # Use 1 GPU per actor.\n    num_gpus=1,\n    # There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.\n    compute=ActorPoolStrategy(size=4))\n\n# Step 5: Trigger execution and collect all the embedded nodes.\nray_docs_nodes = []\nfor row in embedded_nodes.iter_rows():\n    node = row[\"embedded_nodes\"]\n    assert node.embedding is not None\n    ray_docs_nodes.append(node)Data IndexingThe next step is to store these nodes within an \u201cindex\u201d in LlamaIndex. An index is a core abstraction in LlamaIndex to \u201cstructure\u201d your data in a certain way \u2014 this structure can then be used for downstream LLM retrieval + querying. An index can interface with a storage or vector store abstraction.The most commonly used index abstraction within LlamaIndex is our vector index, where each node is stored along with an embedding. In this example, we use a simple in-memory vector store, but you can also choose to specify any one of LlamaIndex\u2019s 10+ vector store integrations as the storage provider (e.g. Pinecone, Weaviate, Chroma).We build two vector indices: one over the documentation nodes, and another over the blog post nodes and persist them to disk. Code is available here.from llama_index import GPTVectorStoreIndex\n\n# Store Ray Documentation embeddings\nray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\nray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n\n# Store Anyscale blog post embeddings\nray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\nray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\")That\u2019s it in terms of building a data pipeline using LlamaIndex + Ray Data!Your data is now ready to be used within your LLM application. Check out our next section for how to use advanced LlamaIndex query capabilities on top of your data.Data QueryingLlamaIndex provides both simple and advanced query capabilities on top of your data + indices. The central abstraction within LlamaIndex is called a \u201cquery engine.\u201d A query engine takes in a natural language query input and returns a natural language \u201coutput\u201d. Each index has a \u201cdefault\u201d corresponding query engine. For instance, the default query engine for a vector index first performs top-k retrieval over the vector store to fetch the most relevant documents.These query engines can be easily derived from each index:ray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n\nray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)LlamaIndex also provides more advanced query engines for multi-document use cases \u2014 for instance, we may want to ask how a given feature in Ray is highlighted in both the documentation and blog. `SubQuestionQueryEngine` can take in other query engines as input. Given an existing question, it can decide to break down the question into simpler questions over any subset of query engines; it will execute the simpler questions and combine results at the top-level.This abstraction is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.For instance, given the following question \u201cWhat is Ray?\u201d, we can break this into sub-questions \u201cWhat is Ray according to the documentation\u201d, and \u201cWhat is Ray according to the blog posts\u201d over the document query engine and blog query engine respectively.# Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)Have a look at deploy_app.py to review the full implementation.Deploying with Ray ServeWe\u2019ve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.To do this, you just need to do the following steps:Define an outer class that can \u201cwrap\u201d a query engine, and expose a \u201cquery\u201d endpointAdd a `@ray.serve.deployment` decorator on this classDeploy the Ray Serve applicationIt will look something like the following:from ray import serve\n\n@serve.deployment\nclass QADeployment:\n    def __init__(self):\n self.query_engine = ...\n\n    def query(self, query: str):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str = \"\"\n            for i in range(len(source_nodes)):\n                node = source_nodes[i]\n                source_str += f\"Sub-question {i+1}:\\n\"\n                source_str += node.node.text\n                source_str += \"\\n\\n\"\n            return f\"Response: {str(response)} \\n\\n\\n {source_str}\\n\"\n\n    async def __call__(self, request: Request):\n        query = request.query_params[\"query\"]\n        return str(self.query(query))\n\n# Deploy the Ray Serve application.\ndeployment = QADeployment.bind()Have a look at the deploy_app.py for full implementation.Example QueriesOnce we\u2019ve deployed the application, we can query it with questions about Ray.We can query just one of the data sources:Q: \"What is Ray Serve?\"\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard for monitoring applications.But, we can also provide complex queries that require synthesis across both the documentation and the blog posts. These complex queries are easily handled by the subquestion-query engine that we defined.Q: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n\nResponse: \nThe Ray docs and the Ray blogs both present Ray Serve as a web interface\nthat provides metrics, charts, and other features to help Ray users\nunderstand and debug Ray applications. However, the Ray docs provide more\ndetailed information, such as a Quick Start guide, user guide, production\nguide, performance tuning guide, development workflow guide, API reference,\nexperimental Java API, and experimental gRPC support. Additionally, the Ray\ndocs provide a guide for migrating from 1.x to 2.x. On the other hand, the\nRay blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\nhelp users get started and understand the features of Ray Serve.\nAdditionally, the Ray blogs provide examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\n\n---\n\nSub-question 1\n\nSub question: How does the Ray docs present Ray Serve\n\nResponse: \nThe Ray docs present Ray Serve as a web interface that provides metrics,\ncharts, and other features to help Ray users understand and debug Ray\napplications. It provides a Quick Start guide, user guide, production guide,\nperformance tuning guide, and development workflow guide. It also provides\nan API reference, experimental Java API, and experimental gRPC support.\nFinally, it provides a guide for migrating from 1.x to 2.x.\n\n---\n\nSub-question 2\n\nSub question: How does the Ray blogs present Ray Serve\n\nResponse: \nThe Ray blog presents Ray Serve as a framework for distributed applications\nthat enables users to handle HTTP requests, scale and allocate resources,\ncompose models, and more. It provides a Quick Start guide, a User Guide, and\nAdvanced Guides to help users get started and understand the features of Ray\nServe. Additionally, it provides examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.ConclusionIn this example, we showed how you can build a scalable data pipeline and a powerful query engine using LlamaIndex + Ray. We also demonstrated how to deploy LlamaIndex applications using Ray Serve. This allows you to effortlessly ask questions and synthesize insights about Ray across disparate data sources!We used LlamaIndex \u2014 a data framework for building LLM applications \u2014 to load, parse, embed and index the data. We ensured efficient and fast parallel execution by using Ray. Then, we used LlamaIndex querying capabilities to perform semantic search over a single document, or combine results across multiple documents. Finally, we used Ray Serve to package the application for production use.Implementation in open source, code is available on GitHub: LlamaIndex-Ray-appWhat\u2019s next?Visit LlamaIndex site and docs to learn more about this data framework for building LLM applications.Visit Ray docs to learn more about how to build and deploy scalable LLM apps.Join our communities!Join Ray community on Slack and Ray #LLM channel.You can also join the LlamaIndex community on discord.We have our Ray Summit 2023 early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"
        },
        {
            "date": "Jan 30, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201301\u201330",
            "tags": [
                "llamaindex",
                "newsletter",
                "llm",
                "ai",
                "rags"
            ],
            "content": "Hello LlamaIndex Adventurers \ud83e\udd99,Welcome to another thrilling week at LlamaIndex! It\u2019s brimming with community contributions and a wealth of educational content that will take your LlamaIndex experience to new heights. Dive into our latest features, comprehensive tutorials, insightful guides, and interactive demos, all designed to supercharge your journey with LlamaIndex.But first, let\u2019s ignite your excitement with a reminder about our upcoming first-ever in-person hackathon, happening February 2nd-4th. Don\u2019t miss this incredible chance to mingle with fellow RAG aficionados, collaborate on exciting projects, and vie for a share of over $16,000 in prizes!Your creations inspire us! Whether it\u2019s a project, article, or video that you\u2019re proud of, we\u2019d love to see it. Share your brilliance with us at news@llamaindex.ai. And for those who haven\u2019t yet, make sure to subscribe to our newsletter on our website \u2014 it\u2019s your gateway to all the latest and greatest from LlamaIndex, delivered directly to your inbox.\ud83e\udd29 The highlights:RAG CLI: Easy-to-use tool for local file indexing and search, with advanced integration and customization features. Docs, Tweet.JSONalyze: Efficiently summarizes large JSON datasets, transforming them into SQLite for detailed SQL queries. Docs, Tweet.OpenAI Embeddings: We now support the latest OpenAI text-embedding-3-small and text-embedding-3-large embeddings for improved accuracy and cost-effectiveness in data retrieval. Docs, Tweet.ReAct Agent Guide: From scratch guide for building ReAct agents, covering all key aspects from setup to memory management.Slack Bot: Step-by-step guide for developing a learning Slack bot, integrated with advanced data engines and deployment tools.\u2728 Feature Releases and Enhancements:We have launched RAG CLI: A straightforward command-line tool for indexing and searching any local file, featuring integration with IngestionPipeline, QueryPipeline, and ChromaDB, with support for local models and customizable logic. Docs, Tweet.We have introduced JSONalyze, a query engine that swiftly summarizes large JSON datasets. It transforms JSON data into an SQLite table, enabling precise SQL queries for efficient data analysis, combining LlamaIndex\u2019s capabilities with text-to-SQL technology. Docs, Tweet.We have launched day 0 support for OpenAI\u2019s latest embedding models featuring cost-effective text-embedding-3-small and high-performance text-embedding-3-large, both with customizable dimensions for enhanced retrieval accuracy in Python and TypeScript versions of LlamaIndex. Docs, Tweet.We have launched Infer-Retrieve-Rerank as a LlamaPack, a technique developed by Karel Doostrlnck, as a simple yet effective LLM-based approach for tackling complex classification challenges with numerous categories, applicable in areas like medical diagnosis and job skill assessment. LlamaPack, Tweet.We have launched LlamaPack with Vanna AI: An advanced text-to-SQL tool using RAG for storing, indexing, and generating SQL queries. LlamaPack.We have integrated with Zilliz Cloud Pipeline in partnership with Zilliz Universe. This fully managed, scalable retrieval service supports multi-tenancy. Blog, Tweet.We have partnered with Exa which created an advanced RAG-powered web search, designed for LLMs and now integrated with Llama Index agents, enhancing workflow automation and data source combination. Notebook, Tweet.We have integrated with Neutrino, offering GPT-4 level performance at significantly reduced costs by smartly allocating queries to the most suitable model from a diverse range. Docs, Twitter.\ud83d\uddfa\ufe0f Guides:Guide to Building a ReAct Agent from Scratch and cookbook detailing the essential components for creating your agents, including reasoning prompts, output parsing, tool selection, and memory management.Guide to Building Slack Bot: Create and deploy an intelligent Slack bot that learns from conversations and accurately answers organizational queries, featuring integration with Qdrant Engine and Render.\u270d\ufe0f Tutorials:Marco Bertelli tutorial on Empowering Your Chatbot: Unveiling Dynamic Knowledge Sources with Advanced Integration.Tonic Validate tutorial on Implementing integration tests for LlamaIndex.Chia Jeng Yang tutorial on Injecting Knowledge Graphs in different RAG stages.Wenqi Glantz tutorial on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI.\ud83c\udfa5 WebinarLlamaIndex Webinar on Efficient Parallel Function Calling Agents with LLMCompiler with Sehoon Kim and Amir Gholami.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-01-30-0d01eb0d8cef"
        },
        {
            "date": "Jan 15, 2024",
            "author": "Ravi Theja",
            "title": "Building Multi-Tenancy RAG System with LlamaIndex",
            "tags": [
                "llamaindex",
                "multitenancy",
                "llm",
                "openai",
                "ai"
            ],
            "content": "Introduction:The concept of Multi-Tenancy in RAG (Retriever-Augmented Generation) systems has become increasingly vital, especially when it comes to data security and privacy. Multi-Tenancy, in simple terms, refers to a system\u2019s ability to serve multiple users (\u2018tenants\u2019) independently and securely.Consider this scenario: In a RAG system, there are two users, User-1 and User-2. Both have their own set of documents which they have indexed into the system. The critical aspect of Multi-Tenancy here is that when User-1 queries the system, they should only receive answers from the documents they have indexed, and not from the documents indexed by User-2, and vice versa. This separation is crucial for maintaining data confidentiality and security, as it prevents the accidental or unauthorized cross-referencing of private information between different users.In the context of Multi-Tenancy in RAG systems, this means designing a system that not only understands and retrieves information effectively but also strictly adheres to user-specific data boundaries. Each user\u2019s interaction with the system is isolated, ensuring that the retriever component of the RAG pipeline accesses only the information relevant and permitted for that particular user. This approach is important in scenarios where sensitive or proprietary data is involved, as it safeguards against data leaks and privacy breaches.In this blog post, we will look into Building a Multi-Tenancy RAG System with LlamaIndex.Solving Multi-Tenancy ChallengesThe key to managing Multi-Tenancy lies within the metadata. When indexing documents, we incorporate user-specific information into the metadata before adding it to the index. This ensures that each document is uniquely tied to an individual user.During the query phase, the retriever uses this metadata to filter and only access documents associated with the querying user. Subsequently, it performs a semantic search to retrieve the most relevant information segments, or \u2018top_k chunks\u2019, for that user. By implementing this approach, we effectively prevent the unauthorized cross-referencing of private information between different users, upholding the integrity and confidentiality of each user\u2019s data.Now that we\u2019ve discussed the concept, let\u2019s dive into the construction of a Multi-Tenancy RAG system. For an in-depth step-by-step guide, feel free to follow along with the subsequent instructions in our Google Colab Notebook.Download Data:We will use An LLM Compiler for Parallel Function Calling and Dense X Retrieval: What Retrieval Granularity Should We Use? papers for the demonstrations.!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\"Load Data:We will load the data of LLMCompiler paper for user Jerry and Dense X Retrieval paper for user Ravireader = SimpleDirectoryReader(input_files=['dense_x_retrieval.pdf'])\ndocuments_jerry = reader.load_data()\n\nreader = SimpleDirectoryReader(input_files=['llm_compiler.pdf'])\ndocuments_ravi = reader.load_data()Create An Empty Index:We will initially create an empty index to which we can insert documents, each tagged with metadata containing the user information.index = VectorStoreIndex.from_documents(documents=[])Ingestion Pipeline:The IngestionPipeline is useful for data ingestion and performing transformations, including chunking, metadata extraction, and more. Here we utilize it to create nodes, which are then inserted into the index.pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=20),\n    ]\n)Update Metadata and Insert Documents:We will update the metadata of the documents with each user and insert the documents into the index.# For user Jerry\nfor document in documents_jerry:\n    document.metadata['user'] = 'Jerry'\n\nnodes = pipeline.run(documents=documents_jerry)\n# Insert nodes into the index\nindex.insert_nodes(nodes)\n\n# For user Ravi\nfor document in documents_ravi:\n    document.metadata['user'] = 'Ravi'\n\nnodes = pipeline.run(documents=documents_ravi)\n# Insert nodes into the index\nindex.insert_nodes(nodes)Define Query Engines:We will define query engines for both the users with necessary filters.# For Jerry\njerry_query_engine = index.as_query_engine(\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(\n                key=\"user\",\n                value=\"Jerry\",\n            )\n        ]\n    ),\n    similarity_top_k=3\n)\n\n# For Ravi\nravi_query_engine = index.as_query_engine(\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(\n                key=\"user\",\n                value=\"Ravi\",\n            )\n        ]\n    ),\n    similarity_top_k=3\n)Querying:# Jerry has Dense X Rerieval paper and should be able to answer following question.\nresponse = jerry_query_engine.query(\n    \"what are propositions mentioned in the paper?\"\n)The paper mentions propositions as an alternative retrieval unit choice. Propositions are defined as atomic expressions of meanings in text that correspond to distinct pieces of meaning in the text. They are minimal and cannot be further split into separate propositions. Each proposition is contextualized and self-contained, including all the necessary context from the text to interpret its meaning. The paper demonstrates the concept of propositions using an example about the Leaning Tower of Pisa, where the passage is split into three propositions, each corresponding to a distinct factoid about the tower.# Ravi has LLMCompiler paper\nresponse = ravi_query_engine.query(\"what are steps involved in LLMCompiler?\")LLMCompiler consists of three key components: an LLM Planner, a Task Fetching Unit, and an Executor. The LLM Planner identifies the execution flow by defining different function calls and their dependencies based on user inputs. The Task Fetching Unit dispatches the function calls that can be executed in parallel after substituting variables with the actual outputs of preceding tasks. Finally, the Executor executes the dispatched function calling tasks using the associated tools. These components work together to optimize the parallel function calling performance of LLMs.# This should not be answered as Jerry does not have information about LLMCompiler\nresponse = jerry_query_engine.query(\"what are steps involved in LLMCompiler?\")I\u2019m sorry, but I couldn\u2019t find any information about the steps involved in LLMCompiler in the given context.As demonstrated, if Jerry queries the system regarding a document indexed by Ravi, the system does not retrieve any answers from that document.What\u2019s Next?We have included a MultiTenancyRAGPack within the LlamaPacks and Replit template which offers a Streamlit interface for hands-on experience. Be sure to explore it.References:Multi-Tenancy RAG with Qdrant and LlamaIndex.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b"
        },
        {
            "date": "Dec 14, 2023",
            "author": "Anil Chandra Naidu Matcha",
            "title": "How to train a custom GPT on your data with EmbedAI + LlamaIndex",
            "tags": [
                "machine-learning",
                "artificial-intelligence",
                "llamaindex",
                "customgpt",
                "python"
            ],
            "content": "ChatGPT, developed by OpenAI, has changed the way we interact online. Being a general purpose chatbot, ChatGPT is limited to answering generic queries. But it becomes even more useful if you can get it to answer your questions specific to your business. To do that, you need to train ChatGPT on your data.EmbedAI is a no-code platform for creating AI chatbots trained on your business data. This includes data sourced from web pages, PDFs, Notion documents, or YouTube videos, allowing EmbedAI to adapt to a wide range of information sources.In this blog post, we\u2019ll show you how we used LlamaIndex with EmbedAI to enable us to train ChatGPT on your own data, helping you create a customized and effective AI chatbot tailored for your business needs.Chat with your data use-casesThere\u2019s a variety of ways that a chatbot trained on your data could be helpful, including:Customer Support Bot: Manages frequently asked questions about a product, addressing customer support inquiries efficiently.Company Search Engine: Finds internal company documents and information fast, boosting workplace efficiency.Personalized Learning Assistant: Offers tailored educational support and study guidance based on specific course content.Technical Support assistant: Provides in-depth help for complex software issues, from troubleshooting to usage tips.Healthcare Assistant: Gives general health advice and information, based on medical literature and FAQs.Finance Chatbot: Assists with financial queries, offering advice on products, market trends, and investment strategies by training on financial dataLet\u2019s delve into creating our own chat apps that integrate with various data sources like PDFs, Notion documents, videos, webpages, and more.Case 1: Custom ChatGPT for your siteTo train ChatGPT on your website content, we need to scrape the content from all the relevant webpages. The steps to do this are:Extract all the URLs from your website, such as from your sitemapInclude only relevant URLs which you need to train onUse SimpleWebPageReader from LlamaIndex to download the content from these URLsHere\u2019s some sample code to do that:Once the data is ready, an AI chatbot can be trained on these documents by using LlamaIndex\u2019s VectorStoreIndex class.To create a ChatGPT chatbot on your website without coding you can use EmbedAI as outlined below which uses LlamaIndex internally:Case 2: Custom ChatGPT for your PDF documentsIf your business specific data is stored in PDF documents and you wish to create a chatbot that can surface the information in them we can do that with LlamaIndex using the PDFMiner library. This time the steps are:Upload your PDFs and store them in the cloudInstall the PDFMiner libraryFetch the uploaded PDFs and extract the document text using LlamaIndex loaderHere\u2019s the code for creating an AI chatbot trained on PDF documents with LlamaIndexIf you want to create a ChatGPT chatbot on your PDF content without coding you can use EmbedAI as in the demo below which uses LlamaIndex internallyCase 3: Custom ChatGPT for your videosOften, valuable information is embedded in videos, which isn\u2019t as accessible for users searching for information. However, by training an AI chatbot with this content, it can become an incredibly rich resource for your users, significantly enhancing their experience.Let\u2019s see how we can fetch the information from our youtube videos to train an AI chatbot using LlamaIndex. The steps are:Find your Channel IDInstall scrapetube and pass it your channel ID to get your list of videosInstall the Youtube transcript api and pass the video URLs from above to LlamaIndex loader to get a list of documentsThe code looks like this:Now you can train an AI chatbot on these documents by using SimpleVectorIndex from LlamaIndex to create a ChatGPT bot trained on your youtube videos, and as before, you can use EmbedAI to create a chatbot with no code.Case 4: Custom ChatGPT for NotionIn many modern companies, a significant portion of their content is stored in Notion. As this content grows, quickly locating specific information becomes increasingly challenging. To address this, we can develop a chatbot for Notion to streamline the process of finding the necessary information.Steps to prepare the data:Fetch an access token from Notion following their instructionsUsing the Notion API, parse data from Notion and generate LlamaIndex documentsTrain a chatbot on these using VectorStoreIndexIf you prefer a No-code way to train a chatbot on your Notion documents, you can use EmbedAI as in the demo below which uses LlamaIndex internally:This doesn\u2019t stop here. With EmbedAI, you can connect data from even more sources like Google Docs, Shopify or even use Zapier to connect with 6000+ tools and chat with their data. You can achieve this by choosing your specific data connector from LlamaHubChallenges while building EmbedAIIn EmbedAI, while connecting with a data source like Notion, the data can keep changing regularly which needs to be auto-refreshed. So the data needs a periodic refresh to add new documents or edit existing documents which needs to be handled internally. Likewise, when indexing website data it can be refreshed regularly. LlamaIndex makes it easy to handle these scenarios. LlamaIndex has a guide to handling continuous ingestion.Querying over tabular data in EmbedAI is a major issue when dealing with PDF content containing tables. Naive chunking can give sub-optimal results and even hallucinations. LlamaIndex provides a guide on how to deal with PDFs containing both text and tables and achieve optimal results while querying.Shopify integration in EmbedAI needed hybrid search, as we needed to search not only on product description but also on product metadata. Thus a combination of semantic search and keyword search is needed to obtain optimal results. LlamaIndex provides a simple framework to build a hybrid search application, such as in this example.Custom trained chatbots can help your businessTraining ChatGPT with your own data provides a significant advantage for your business. From enhancing customer support with bots trained on specific product knowledge to creating sophisticated company search engines, the applications are as diverse as they are impactful. LlamaIndex provides a lot of abstractions to help with building a custom chatbot trained on your data, and we use them heavily at EmbedAI. For those seeking a no-code solution to develop an AI chatbot tailored to their data, starting with EmbedAI is a straightforward option and we encourage you to try it out.",
            "related_posts": [
                "Querying a network of knowledge with llama-index-networks",
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse"
            ],
            "link": "https://www.llamaindex.ai/blog//how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070"
        },
        {
            "date": "Nov 16, 2023",
            "author": "Ravi Theja",
            "title": "Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex",
            "tags": [
                "llamaindex",
                "ai",
                "llm",
                "cohere",
                "fine-tuning"
            ],
            "content": "Introduction:Achieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is heavily dependent on robust retrieval performance. As we explored in our previous blog post, rerankers have a significant impact on boosting retrieval performance. But what if we could take it a step further? What if our reranker was not just any reranker, but one tuned specifically to our domain or dataset? Could this specialization enhance the retrieval performance even more?To answer these questions, we turn to CohereAI\u2019s beta release of fine-tuning reranker(Custom reranker) models. By integrating these with LlamaIndex, we now offer the ability to build your very own Cohere custom reranker using our streamlined process.In this blog post, we\u2019ll guide you through the steps to create a Cohere custom reranker with LlamaIndex and evaluate the retrieval performance.For a hands-on walkthrough, you can follow the tutorial on Google Colab Notebook.Let\u2019s start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex.NOTE: This is a guide for fine-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. We suggest experimenting with your dataset and various parameters before deciding to incorporate it into your RAG pipeline.Setting Up the Environment!pip install llama-index cohere pypdfSetting Up the Keysopenai_api_key = 'YOUR OPENAI API KEY'\ncohere_api_key = 'YOUR COHEREAI API KEY'\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\nos.environ[\"COHERE_API_KEY\"] = cohere_api_keyDownload the DataWe will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC Filings for evaluation.!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'Load the Datalyft_docs = SimpleDirectoryReader(input_files=['./data/10k/lyft_2021.pdf']).load_data()\nuber_docs = SimpleDirectoryReader(input_files=['./data/10k/uber_2021.pdf']).load_data()Data CurationCreate Nodes.The documentation mentions that Query + Relevant Passage/ Query + Hard Negatives should be less than 510 tokens. To accommodate that we limit chunk_sizeto 400 tokens. (Each chunk will eventually be treated as a Relevant Passage/ Hard Negative)# Limit chunk size to 400\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=400)\n\n# Create nodes\nlyft_nodes = node_parser.get_nodes_from_documents(lyft_docs)\nuber_nodes = node_parser.get_nodes_from_documents(uber_docs)We will use gpt-4 to create questions from chunks.llm = OpenAI(api_key=openai_api_key, temperature=0, model='gpt-4')Prompt to generate questions from each Node/ chunk.# Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\"It expects a minimum of 256 (Query + Relevant passage) pairs with or without hard negatives for training and 64 pairs for validation. Please note that the validation is optional.Training: We use the first 256 nodes from Lyft for creating training pairs.Validation: We will use the next 64 nodes from Lyft for validation.Testing: We will use the first 150 nodes from Uber.# Training dataset\nqa_dataset_lyft_train = generate_question_context_pairs(\n    lyft_nodes[:256], llm=llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n# Save [Optional]\nqa_dataset_lyft_train.save_json(\"lyft_train_dataset.json\")\n\n# Validation dataset\nqa_dataset_lyft_val = generate_question_context_pairs(\n    lyft_nodes[257:321], llm=llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n# Save [Optional]\nqa_dataset_lyft_val.save_json(\"lyft_val_dataset.json\")\n\n# Testing dataset\nqa_dataset_uber_val = generate_question_context_pairs(\n    uber_nodes[:150], llm=llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n# Save [Optional]\nqa_dataset_uber_val.save_json(\"uber_val_dataset.json\")Now that we have compiled questions from each chunk, we will format the data according to the specifications required for training and validation.Data Format and RequirementsFor both training and validation, it currently accepts data in the format of triplets, every row should have the followingquery: This represents the question or target.relevant_passages: This represents a list of documents or passages that contain information that answers the query. For every query, there must be at least one relevant_passagehard_negatives: This represents chunks or passages that don\u2019t contain answers for the query. It should be noted that Hard negatives are optional but providing at least ~5 hard negatives will lead to meaningful improvement.You can check the documentation for more details.We need to have an embedding model for creating hard negatives with a cosine similarity approach.# Initialize the Cohere embedding model which we use it for creating Hard Negatives.\nembed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)Let\u2019s create 3 datasets.Dataset without hard negatives.Dataset with hard negatives selected at random.Dataset with hard negatives selected based on cosine similarity.# Train and val datasets without hard negatives.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    finetune_dataset_file_name = \"train.jsonl\"\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    finetune_dataset_file_name = \"val.jsonl\"\n)\n\n# Train and val datasets with hard negatives selected at random.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"train_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"val_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\n# Train and val datasets with hard negatives selected based on cosine similarity.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"train_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"val_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)Fine-tuning Reranker (Custom Reranker)With our training and validation datasets ready, we\u2019re set to proceed with the training process. Be aware that this training is expected to take approximately 25 to 45 minutes.# Reranker model with 0 hard negatives.\nfinetune_model_no_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train.jsonl\",\n    val_file_name=\"val.jsonl\",\n    model_name=\"lyft_reranker_0_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n    api_key = cohere_api_key\n)\nfinetune_model_no_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected at random\nfinetune_model_random_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_random.jsonl\",\n    val_file_name=\"val_5_random.jsonl\",\n    model_name=\"lyft_reranker_5_random_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_random_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected based on cosine similarity\nfinetune_model_cosine_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_cosine_similarity.jsonl\",\n    val_file_name=\"val_5_cosine_similarity.jsonl\",\n    model_name=\"lyft_reranker_5_cosine_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_cosine_hard_negatives.finetune()Once the jobs are submitted, you can check the training status in the models section of the dashboard. You can check the status of the job in the dashboard and you should see an image something similar to the following one.You then need to get the Cohere Reranker model for testing.reranker_base = CohereRerank(top_n=5)\nreranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(\n    top_n=5\n)\nreranker_model_5_random = (\n    finetune_model_random_hard_negatives.get_finetuned_model(top_n=5)\n)\nreranker_model_5_cosine = (\n    finetune_model_cosine_hard_negatives.get_finetuned_model(top_n=5)\n)TestingWe will conduct tests on the first 150 nodes from Uber using the following different rerankers.Without Reranker.Cohere Reranker.Fine-tuned reranker (Custom reranker) without hard negatives.Fine-tuned reranker (Custom reranker) with hard negatives selected at random.Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity.Let\u2019s define the rerankers.RERANKERS = {\n    \"WithoutReranker\": \"None\",\n    \"CohereRerank\": reranker_base,\n    \"CohereRerank_0\": reranker_model_0,\n    \"CohereRerank_5_random\": reranker_model_5_random,\n    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n}Create an Index and Retriever for evaluation purposes.# Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\nindex_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)\n\nquery_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n)\n\nservice_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\nservice_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n\nvector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query)Define a function to display the resultsdef display_results(embedding_name, reranker_name, eval_results):\n    \"\"\"Display results from evaluate.\"\"\"\n\n    metric_dicts = []\n    for eval_result in eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[\"hit_rate\"].mean()\n    mrr = full_df[\"mrr\"].mean()\n\n    metric_df = pd.DataFrame(\n        {\"Embedding\": [embedding_name], \"Reranker\": [reranker_name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n    )\n\n    return metric_dfLoop over different rerankers and evaluate retrieval performance using Custom Retriever.results_df = pd.DataFrame()\n\nembed_name = 'CohereEmbedding'\n\n# Loop over rerankers\nfor rerank_name, reranker in RERANKERS.items():\n\n    print(f\"Running Evaluation for Reranker: {rerank_name}\")\n\n    # Define Retriever\n    class CustomRetriever(BaseRetriever):\n        \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n\n        def __init__(\n            self,\n            vector_retriever: VectorIndexRetriever,\n        ) -&gt; None:\n            \"\"\"Init params.\"\"\"\n\n            self._vector_retriever = vector_retriever\n\n        def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n            \"\"\"Retrieve nodes given query.\"\"\"\n\n            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n            if reranker != 'None':\n                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n            else:\n                retrieved_nodes = retrieved_nodes[:5]\n\n            return retrieved_nodes\n\n        async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n            \"\"\"Asynchronously retrieve nodes given query.\n            \"\"\"\n            return self._retrieve(query_bundle)\n\n        async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n            if isinstance(str_or_query_bundle, str):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            return await self._aretrieve(str_or_query_bundle)\n\n    custom_retriever = CustomRetriever(vector_retriever)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n        [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n    )\n    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n\n    current_df = display_results(embed_name, rerank_name, eval_results)\n    results_df = pd.concat([results_df, current_df], ignore_index=True)Results:From the above table (1- without reranker, 2 \u2014 with base cohere reranker, 3\u20135: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It\u2019s crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker.Summary:In this blog post, we\u2019ve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community\u2019s use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"
        },
        {
            "date": "Aug 21, 2023",
            "author": "Jerry Liu",
            "title": "LlamaIndex + Metaphor: Towards Automating Knowledge Work with LLMs",
            "tags": [
                "search",
                "llm",
                "agents",
                "nlp",
                "llamaindex"
            ],
            "content": "(co-authored by Jerry Liu, CEO of LlamaIndex, Jeffrey Wang, co-founder at Metaphor, and Adam Hoffman, Software Engineer at Hypotenuse Labs)We\u2019re incredibly excited to launch an integration between LlamaIndex and Metaphor: combine the capabilities of LlamaIndex data agents with Metaphor as a native LLM search tool to enable knowledge workers capable of answering any question over any data, no matter how recent or complex.We provide a deeper overview of Metaphor and the LlamaIndex integration below. We also walk through our example notebook to showcase how they can be combined.Background/ContextState-of-the art large language models (LLMs) such as ChatGPT, GPT-4, Claude 2 have incredible reasoning capabilities that unlock a wide variety of use cases \u2014 from insight extraction to question-answering to general workflow automation. Yet they are limited in their abilities to retrieve contextually relevant information. A popular stack that has emerged is to setup a retrieval-augmented generation (RAG) system, which combines LLMs with external storage solutions over a static knowledge source. Frameworks such as LlamaIndex provide a variety of tools to setup both simple and complex RAG systems.Yet even this is not the complete picture. LLMs should ideally be able to dynamically search and retrieve information from the external world, not just depend on a static source of knowledge. This would allow them to fulfill a more general set of tasks and not only perform search/retrieval, but perform actions as well.To do this well, we need two core components:General abstractions that allow LLMs to intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d fashionA good search engine tailored for LLM useLlamaIndex data agent abstractions help to satisfy the first core component. A complete data agent consists of both a reasoning loop as well as a set of Tools. These tools can be interfaces for search/retrieval or more generally any external API. Given a query, the agent will execute its reasoning loop and dynamically figure out the set of Tools it will need to fulfill the task at hand.Data agents have access to a rich set of Tools offered on LlamaHub \u2014 these range from Gmail API, to a SQL db API, to a basic tool in the form of Bing search. We\u2019ve shown that they are capable of e2e tasks from sending emails, scheduling meetings, to automating custom support insight extraction. Yet there has never been a tool tailored for LLM use.Overview of MetaphorThe Metaphor API is designed to connect your LLM to the internet. It allows you to perform fully neural, highly semantic searches over the Internet and also get clean, HTML content from the results.Metaphor was trained to predict links on the internet, given how people talk about things on the Internet. For example, someone might post about a great article they read like this:Found an amazing article I read about the history of Rome\u2019s architecture: [LINK]By training a model to predict these links given how people talk about them, the end result is a totally different way to search the internet \u2014 search as if you\u2019re about to share the link you want. While a little unintuitive at first, searching this way can return extremely high quality results. But for the purposes of LlamaIndex, you won\u2019t need to worry about this because by default, queries will be converted into Metaphor prompts.Why would you use Metaphor Search over Bing/Google? There are 3 main reasons:You can search fully semantically, for instance with feelings or complex descriptors.You can search only for the type of entity that you want. Companies, articles, people.You can find content that Google simply doesn\u2019t surface well, maybe because keywords aren\u2019t the right tool or maybe just because Google doesn\u2019t care about returning good results for that type of content.To learn more, you can read the full Metaphor API blog post.Integration DetailsThe Metaphor Tool Spec in LlamaHub is an API interface that consists of 5 tools that an agent can use.Search: The entrypoint to Metaphor \u2014 allows an agent to pass a natural language query that will then be passed to the Metaphor search engine. This endpoint also contains some additional parameters, such as the number of results, domains to include/exclude, and a date filter.Retrieve Documents: This will retrieve the content of a set of documents given IDs. These ids are returned as part of the results from the search endpoint above.Search and Retrieve Documents: This is a convenience endpoint that combines the functionality of `search` and `retrieve_documents`.Find Similar: This directly calls an endpoint offered by Metaphor, which will return a list of documents similar to a given URL.Current Date: This is a convenience function that returns the current date. On its own it is unrelated to Metaphor\u2019s API, but may be called beforehand to figure out the right date filters to pass to some of Metaphor\u2019s endpoints.In the next section, let\u2019s walk through how a data agent can make use of these endpoints through various use cases.Example WalkthroughLet\u2019s walk through our example notebook showing how LlamaIndex data agents can be used with Metaphor.Testing the Metaphor ToolsThe first step is to import the Metaphor tool spec.# Set up Metaphor tool\nfrom llama_hub.tools.metaphor.base import MetaphorToolSpec\nmetaphor_tool = MetaphorToolSpec(\napi_key='your-key',\n)\n# convert tool spec to a list of tools\nmetaphor_tool_list = metaphor_tool.to_tool_list()\nfor tool in metaphor_tool_list:\nprint(tool.metadata.name)In this walkthrough, we make use of all of the tools. But you\u2019re free to pick and choose to use specific tools if you want to define a more custom workflow and restrict the agent action space.We can play around with the set of tools before defining our agent. All of our Metaphor tools make use of the `AutoPrompt` option where Metaphor will pass a query through an LLM to refine and improve the query.Example input:metaphor_tool.search('machine learning transformers', num_results=3)Example output:[{'title': 'On the potential of Transformers in Reinforcement Learning',\n'url': 'https://lorenzopieri.com/rl_transformers/',\n'id': 'ysJlYSgeGW3l4zyOBoSGcg'},\n{'title': 'Transformers: Attention in Disguise',\n'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n{'title': 'Transformers in Computer Vision: Farewell Convolutions!',\n'url': 'https://towardsdatascience.com/transformers-in-computer-vision-farewell-convolutions-f083da6ef8ab?gi=a1d0a9a2896c',\n'id': 'kX1Z89DdjSvBrH1S1XLvwg'}]The notebook also contains examples of us playing around with the other endpoints: retrieve_documents, find_similar, search_and_retrieve_documents.Setting up an OpenAI Function Calling Agent with MetaphorWe can create an agent with access to all of the above tools and start testing it out:from llama_index.agent import OpenAIAgent\n# We don't give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools\nagent = OpenAIAgent.from_tools(\n  metaphor_tool_list,\n  verbose=True,\n)That\u2019s it in terms of setup! Let\u2019s try giving an example query:print(agent.chat('What are the best restaurants in toronto?\"))We walk through the execution trace of this agent to see how it is interacting with the Metaphor tool.=== Calling Function ===\nCalling function: search with args: {\n  \"query\": \"best restaurants in Toronto\"\n}\n[Metaphor Tool] Autoprompt string: Here's a link to the best restaurant in Toronto:\nGot output: [{'title': 'Via Allegro Ristorante - Toronto Fine Dining Restaurant', 'url': 'https://viaallegroristorante.com/', 'id': 'EVlexzJh-lzkVr4tb2y_qw'}, {'title': 'The Senator \u2013 Home', 'url': 'https://thesenator.com/', 'id': 'dA3HVr5P8E0Bs7nH2gH7ZQ'}, {'title': 'Home - The Rushton', 'url': 'https://therushton.com/', 'id': '6Je-igG-i-ApqISC5XXmGQ'}, {'title': 'Location', 'url': 'https://osteriagiulia.ca/', 'id': 'HjP5c54vqb3n3UNa3HevSA'}, {'title': 'StockYards | Stockyards Toronto', 'url': 'https://www.thestockyards.ca/', 'id': 'Pffz-DQlOepqVgKQDmW5Ig'}, {'title': 'Select A Restaurant', 'url': 'https://www.torontopho.com/', 'id': 'DiQ1hU1gmrIzpKnOaVvZmw'}, {'title': 'Home | Kit Kat Italian Bar &amp; Grill', 'url': 'http://www.kitkattoronto.com/', 'id': 'kdAcLioBgnwzuHyd0rWS1w'}, {'title': 'La Fenice', 'url': 'https://www.lafenice.ca/', 'id': 'M-LHQZP6V40V81fqLFAQxQ'}, {'title': 'Le Ph\u00e9nix', 'url': 'https://www.lephenixto.com/', 'id': 'spCTcFr0GHlFUTzyngfRVw'}, {'title': 'ITALIAN, INSPIRED.', 'url': 'https://figotoronto.com/', 'id': 'OvBcTqEo1tCSywr4ATptCg'}]\n========================\nHere are some of the best restaurants in Toronto:\n\n1. [Via Allegro Ristorante](https://viaallegroristorante.com/)\n2. [The Senator](https://thesenator.com/)\n3. [The Rushton](https://therushton.com/)\n4. [Osteria Giulia](https://osteriagiulia.ca/)\n5. [Stockyards](https://www.thestockyards.ca/)\n6. [Toronto Pho](https://www.torontopho.com/)\n7. [Kit Kat Italian Bar &amp; Grill](http://www.kitkattoronto.com/)\n8. [La Fenice](https://www.lafenice.ca/)\n9. [Le Ph\u00e9nix](https://www.lephenixto.com/)\n10. [Figo](https://figotoronto.com/)\n\nYou can visit their websites for more information. Enjoy your dining experience in Toronto!The execution trace shows that the agent is simply calling the `search` endpoint with \u201cbest restaurants in Toronto\u201d, and returning that as a list of dictionaries representing the search results.Note that we can ask a followup question as well:print(agent.chat('tell me more about Osteria Giulia'))And we get the following result (note: we truncate some of the intermediate output):=== Calling Function ===\nCalling function: retrieve_documents with args: {\n\"ids\": [\"HjP5c54vqb3n3UNa3HevSA\"]\n}\nGot output: [\u2026]\n========================\nOsteria Giulia is a restaurant located at 134 Avenue Road in Toronto, Ontario. You can contact them at 416.964.8686 or via email at info@osteriagiulia.ca (for general inquiries only, no reservation requests via email).\nThe restaurant's operating hours are from Monday to Saturday, from 5:00pm to 11:00pm. On Sundays, the restaurant is available for private bookings.\nParking is available on Avenue Road and Davenport Road.\nYou can follow Osteria Giulia on Instagram [@osteriagiulia](https://www.instagram.com/osteriagiulia). They also have a sister restaurant called Giulietta, which you can visit at [giu.ca](https://giu.ca) or on Instagram [@giulietta972](https://www.instagram.com/giulietta972).\nPlease note that the information provided is based on the available document and may be subject to change. It is recommended to visit their official website or contact them directly for the most up-to-date information.Since \u201cOsteria Giulia\u201d is in the agent conversation history, the agent now knows to call the `retrieve` endpoint to return more information about the relevant search result.Advanced: Avoiding Context Window IssuesOne issue with using retrieve is that the content can be quite long. If the content is naively appended to the conversation history and dumped into the LLM context window, then we may run into context window limitations.LlamaIndex offers tool abstractions to help deal with this. Our LoadAndSearchToolSpec wraps any given tool that may return a large amount of data, and it splits it into two tools: a load tool that will dynamically store the data in an index, and a search tool that allows for search over that index.On the Metaphor side, this is also where we define a search_and_retrieve_documents endpoint that combines search and retrieve. This allows the agent to make a single query to retrieve a large number of documents, which when combined with the LoadAndSearchToolSpec will get directly stored within an index. If the agent were to call search and retrieve separately, then it would both take longer and consume more tokens to write the search results to conversation history, and then passing that into the prompt again to call retrieve over all document IDs.Creating the LoadAndSearchToolSpec:from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n# The search_and_retrieve_documents tool is the third in the tool list, as seen above\nwrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n  metaphor_tool_list[2],\n)Now let\u2019s walk through a full execution example:# Just pass the wrapped tools and the get_date utility\nagent = OpenAIAgent.from_tools(\n  [*wrapped_retrieve.to_tool_list(), metaphor_tool_list[4]],\n  verbose=True,\n)\nprint(agent.chat('Can you summarize everything published in the last month regarding news on superconductors'))The output here shows that the agent calls multiple tools in succession to get the right answer.=== Calling Function ===\nCalling function: current_date with args: {}\nGot output: 2023-08-20\n========================\n=== Calling Function ===\nCalling function: search_and_retrieve_documents with args: {\n  \"query\": \"superconductors\",\n  \"start_published_date\": \"2023-07-20\",\n  \"end_published_date\": \"2023-08-20\"\n}\n[Metaphor Tool] Autoprompt: \"Here is an interesting article about superconductors:\nGot output: Content loaded! You can now search the information using read_search_and_retrieve_documents\n========================\n=== Calling Function ===\nCalling function: read_search_and_retrieve_documents with args: {\n  \"query\": \"superconductors\"\n}\nGot output: \nSuperconductors are materials that can perfectly conduct electricity. They are used in a variety of applications, such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains. However, so far, no superconductor has been proven to work at ambient pressures and temperatures. On July 22, scientists in South Korea published research claiming to have solved this problem with a material called LK-99, which has an electrical resistivity that drops to near zero at 30 degrees Celsius (86 degrees Fahrenheit).\n========================\nIn the last month, there have been developments in the field of superconductors. Scientists in South Korea have published research on a material called LK-99, which has the ability to conduct electricity with near-zero resistance at a temperature of 30 degrees Celsius (86 degrees Fahrenheit). This breakthrough could potentially lead to the development of superconductors that work at ambient pressures and temperatures, opening up new possibilities for various applications such as particle accelerators, nuclear fusion devices, MRI machines, and maglev trains.The agent used the get_date tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling search. It then loaded the documents using retrieve_documents and read them using read_retrieve_documents.ConclusionAs shown above, the integration between LlamaIndex data agents + Metaphor search has the potential to bypass existing limitations with LLMs and even RAG systems. We\u2019re excited to continue exploring this further in future blog posts.We encourage you to play around with the notebook \u2014 make sure to check it out!Resources:Notebook: https://github.com/emptycrown/llama-hub/blob/main/llama_hub/tools/notebooks/metaphor.ipynbLlamaHub: https://llamahub.ai/l/tools-metaphorMetaphor: https://platform.metaphor.systems/Metaphor API Docs: https://docs.metaphor.systems/",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "LlamaIndex Newsletter 2024-04-02"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f"
        },
        {
            "date": "Jul 12, 2023",
            "author": "Jerry Liu",
            "title": "Data Agents",
            "tags": [
                "llamaindex",
                "nlp",
                "ai",
                "agents",
                "data"
            ],
            "content": "Today we\u2019re incredibly excited to announce the launch of a big new capability within LlamaIndex: Data Agents.Data Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are capable of the following:Perform automated search and retrieval over different types of data \u2014 unstructured, semi-structured, and structured.Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use.Storing conversation history.Using all of the above to fulfill both simple and complex data tasks.We\u2019ve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Today\u2019s launch consists of the following key components:General Agent/Tool Abstractions: a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition.LlamaHub Tool Repository: A brand-new section within LlamaHub that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to community contributions!See below for full details. We show you how to build a Gmail agent that\u2019s able to automatically create/send emails in <10 lines of code!ContextOur core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define \u201cstate\u201d (how to parse/structure your data), and \u201ccompute\u201d (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality beyond that.A lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the \u201creasoning\u201d capabilities of these query engines. As a result some of our existing query capabilities contain \u201cagent-like\u201d components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.But there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both \u201cread\u201d and \u201cwrite\u201d capabilities \u2014 the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source.Some existing services, toolkits, and research papers have already demonstrated the possibilities of LLM-powered \u201cagents\u201d that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.Core Components of Data AgentsBuilding a data agent requires the following core components:A reasoning loopTool abstractionsAt a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool. The \u201cloop\u201d can conceptually be very simple (a one-step tool selection process), or complex (a multi-step selection process, where a multitude of tools are picked at each step).These components are described in more detail below.Agent Abstraction + Reasoning LoopWe have support for the following agents:OpenAI Function agent (built on top of the OpenAI Function API)a ReAct agent (which works across any chat/text completion endpoint).You can use them as the following:from llama_index.agent import OpenAIAgent, ReActAgent\nfrom llama_index.llms import OpenAI\n\n# import and define tools\n...\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n# initialize ReAct agent\nagent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n# use agent\nresponse = agent.chat(\"What is (121 * 3) + 42?\")Each agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task \u2014 chat and query. Note that these are the core methods used in our ChatEngine and QueryEngine respectively. In fact that our base agent class (BaseAgent) simply inherits from BaseChatEngine and BaseQueryEngine. chat allows the agent to utilize previously stored conversation history, whereas query is a stateless call - history/state is not preserved over time.The reasoning loop depends on the type of agent. The OpenAI agent calls the OpenAI function API in a while loop, since the tool decision logic is baked into the function API. Given an input prompt and previous chat history (which includes previous function calls), the function API will decide whether to make another function call (pick a Tool), or return an assistant message. If the API returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. If the API returns an assistant message, then the loop is complete (we assume the task is solved).The ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str \u2192 output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this:...\nYou have access to the following tools:\n{tool_desc}\n\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names})\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n```\nPlease use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, you will receive a response in the following format:\n\n```\nObservation: tool response\n```\n...We implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message.Note: the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API \u2014 the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting.Tool AbstractionsHaving proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a single Tool as well as a \u201cToolSpec\u201d containing a series of functions under the hood.We describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools.Base Tool AbstractionThe base tool defines a very generic interface. The __call__ function can take in any series of arguments, and return a generic ToolOutput container that can capture any response. A tool also has metadata containing its name, description, and function schema.@dataclass\nclass ToolMetadata:\n    description: str\n    name: Optional[str] = None\n    fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n\nclass BaseTool:\n    @property\n    @abstractmethod\n    def metadata(self) -&gt; ToolMetadata:\n        pass\n    @abstractmethod\n    def __call__(self, input: Any) -&gt; ToolOutput:\n        passFunction ToolA function tool allows users to easily convert any function into a Tool. It takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface. It can also \u201cauto-infer\u201d the function schema if it isn\u2019t specified beforehand.Our ToolSpec classes make use of this FunctionTool abstraction to convert functions defined in the tool spec into a set of agent tools (see below).Here\u2019s a trivial example of defining a FunctionTool.from llama_index.tools.function_tool import FunctionTool\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)QueryEngineToolOf course, we also provide Tool abstractions to wrap our existing query engines. This provides a seamless transition from working on query engines to working on agents. Our query engines can be thought of \u201cconstrained\u201d agents meant for the read/write setting and centered around retrieval purposes. These query engines can be used in an overall agent setting.from llama_index.tools import QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=query_engine, \n        metadata=ToolMetadata(\n            name='&lt;tool_name&gt;', \n            description=\"Queries over X data source.\"\n        )\n    ),\n ...\n]Tool SpecsA tool spec is a Python class that represents a full API specification that an agent can interact with, and a tool spec can be converted into a list of tools that an agent can be initialized with.This class allows users to define entire services, not just single tools that perform individual tasks. Each tool spec may contain read/write endpoints that allow an agent to interact with a service in meaningful ways. For instance, a Slack tool spec could allow the user to both read existing messages and channels (load_data, fetch_channels) as well as write messages (send_message). It would be roughly defined as the following:class SlackToolSpec(BaseToolSpec):\n    \"\"\"Slack tool spec.\"\"\"\n    spec_functions = [\"load_data\", \"send_message\", \"fetch_channels\"]\n\n    def load_data(\n          self,\n          channel_ids: List[str],\n          reverse_chronological: bool = True,\n      ) -&gt; List[Document]:\n          \"\"\"Load data from the input directory.\"\"\"\n          ...\n      def send_message(\n          self,\n          channel_id: str,\n          message: str,\n      ) -&gt; None:\n          \"\"\"Send a message to a channel given the channel ID.\"\"\"\n          ...\n      def fetch_channels(\n          self,\n      ) -&gt; List[str]:\n          \"\"\"Fetch a list of relevant channels.\"\"\"\n          ...If a tool spec is initialized, it can be converted into a list of tools that can be fed into an agent with to_tool_list. For instance,tool_spec = SlackToolSpec()\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True)Defining a tool spec is not that different than defining a Python class. Each function becomes converted into a tool, and by default the docstring for each function gets used as the tool description (though you can customize names/description in to_tool_list(func_to_metadata_mapping=...).We also made the intentional choice that the input arguments and return types can be anything. The primary reason is to preserve the generality of the tool interface for subsequent iterations of agents. Even if current iterations of agents expect tool outputs to be in string format, that may change in the future, and we didn\u2019t want to arbitrarily restrict the types of tool interface.LlamaHub Tool RepositoryA huge component of our launch is a brand-new addition to LlamaHub: a Tool Repository. The Tool Repository consists of 15+ Tool Specs that an agent can use. These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.Among others, they include the following specs:Gmail SpecZapier SpecGoogle Calendar SpecOpenAPI SpecSQL + Vector Database SpecWe also provide a list of utility tools that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.For instance, our Gmail Tool Spec allows an agent to search existing emails, create drafts, update drafts, and send emails. Our Zapier Spec allows an agent to perform any natural language query to Zapier through their Natural Language Actions interface.Best of all, you don\u2019t need to spend a lot of time figuring out how to use these tools \u2014 we have 10+ notebooks showing how you can build agents for each service, or even build agents that use a combination of services (e.g. Gmail, Google Calendar, and Search).Example WalkthroughLet\u2019s take a look at a few examples! We initialize an OpenAIAgent with the Gmail Spec. As mentioned above, the spec consists of tools to search emails, create/update drafts, and send emails.Now let\u2019s give the agent a sequence of commands so that it can create an email draft, make a few edits to it, and then send it off.First, let\u2019s create an initial email draft. Note that the agent chooses the create_draft tool, which takes in the \u201cto\u201d, \u201csubject\u201d, and \u201cmessage\u201d parameters. The agent is able to infer the parameters simultaneously while picking the tool.Next, let\u2019s update the draft with a slight modification:Next, let\u2019s show the current state of the draft.Finally, let\u2019s send the email!This is a good start, but this is just the beginning. We are actively working on contributing more tools to this repository, and we\u2019re also opening this up to community contributions. If you\u2019re interested in contributing a Tool to LlamaHub, please feel free to open a PR in this repo.Utility ToolsOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).To tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in the core LlamaIndex repo \u2014 utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.Let\u2019s walk through our two main utility tools below.OnDemandLoaderToolThis tool turns any existing LlamaIndex data loader ( BaseReader class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger load_data from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con-demand\u201d. All three of these steps happen in a single tool call.Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.A usage example is given below:from llama_hub.wikipedia.base import WikipediaReader\nfrom llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n reader,\n name=\"Wikipedia Tool\",\n description=\"A tool for loading data and querying articles from Wikipedia\"\n)LoadAndSearchToolSpecThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements to_tool_list , and when that function is called, two tools are returned: a load tool and then a search tool.The load Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The search Tool execution would take in a query string as input and call the underlying index.This is helpful for any API endpoint that will by default return large volumes of data \u2014 for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.Example usage is shown below:from llama_hub.tools.wikipedia.base import WikipediaToolSpec\nfrom llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n# Get the search wikipedia tool\ntool = wiki_spec.to_tool_list()[1]\n# Create the Agent with load/search tools\nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose=True\n)This is the output when we run an input promptagent.chat('what is the capital of poland')Output:=== Calling Function ===\nCalling function: search_data with args: {\n  \"query\": \"capital of Poland\"\n}\nGot output: Content loaded! You can now search the information using read_search_data\n========================\n=== Calling Function ===\nCalling function: read_search_data with args: {\n  \"query\": \"What is the capital of Poland?\"\n}\nGot output: \nThe capital of Poland is Warsaw.\n========================\nAgentChatResponse(response='The capital of Poland is Warsaw.', sources=[])Note that the agent figures out that it first needs to first call the \u201cload\u201d tool (denoted by the original name of the tool, \u201csearch_data\u201d). This load tool will load the Wikipedia page and index under the hood. The output just mentions that the \u201ccontent is loaded, and tells the agent that the next step is to use read_search_data. The agent then reasons that it needs to call the read_search_data tool, which will query the index for the right answer.FAQShould I use Data Agents for search and retrieval, or continue to use Query Engines?Short answer: both are possible. Query engines give you the ability to define your own workflows over your data, in both a constrained reasoning fashion as well as unconstrained fashion. For instance, you may want to define a specific workflow over text-to-SQL with our NLStructStoreQueryEngine (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our SubQuestionQueryEngine to decompose a question among sub-documents (even less constrained).By default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities \u2014 for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too.How are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)?Most of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our \u201cdata agents\u201d design, we\u2019ve tried our best to answer the following key questions well:How do we effectively index/query and retrieve data beforehand?How do we effectively index/query and retrieve data on the fly?How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?How do we properly get sources in citations?Our goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data \u2014 these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to \u201ccache\u201d API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.Can I use Tools with LangChain agents? \nYou can easily use any of our tools with LangChain agents as well.tools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool() for t in tools]See our tools usage guide for more details!ConclusionIn summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository.ResourcesWe\u2019ve written a comprehensive section in the docs \u2014 take a look here: https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.htmlTake a look at our LlamaHub Tools section: https://llamahub.ai/Notebook Tutorials for LlamaHub Tools: https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooksIf you have questions, please hop on our Discord: https://discord.gg/dGcwcsnxhU",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"
            ],
            "link": "https://www.llamaindex.ai/blog//data-agents-eed797d7972f"
        },
        {
            "date": "Jul 17, 2024",
            "author": "Laurie Voss",
            "title": "Building a multi-agent concierge system",
            "tags": [
                "llama-agents",
                "agents"
            ],
            "content": "Why build this?Interactive chat bots are by this point a familiar solution to customer service, and agents are a frequent component of chat bot implementations. They provide memory, introspection, tool use and other features necessary for a competent bot.We have become interested in larger-scale chatbots: ones that can complete dozens of tasks, some of which have dependencies on each other, using hundreds of tools. What would that agent look like? It would have an enormous system prompt and a huge number of tools to choose from, which can be confusing for an agent.Imagine a bank implementing a system that can:Look up the price of a specific stockAuthenticate a userCheck your account balanceWhich requires the user be authenticatedTransfer money between accountsWhich requires the user be authenticatedAnd also that the user checks their account balance firstEach of these top-level tasks has sub-tasks, for instance:The stock price lookup might need to look up the stock symbol firstThe user authentication would need to gather a username and a passwordThe account balance would need to know which of the user's accounts to checkComing up with a single primary prompt for all of these tasks and sub-tasks would be very complex. So instead, we designed a multi-agent system with agents responsible for each top-level task, plus a \"concierge\" agent that can direct the user to the correct agent.What we builtWe built a system of agents to complete the above tasks. It's open-source! There are four basic \"task\" agents:A stock lookup agent (which takes care of sub-tasks like looking up symbols)An authentication agent (which asks for username and password)An account balance agent (which takes care of sub-tasks like selecting an account)A money transfer agent (which takes care of tasks like asking what account to transfer to, and how much)There are also three \"meta\" agents:A concierge agent: this agent is responsible for interacting with the user when they first arrive, letting them know what sort of tasks are available, and providing feedback when tasks are complete.An orchestration agent: this agent never provides output directly to the user. Instead, it looks at what the user is currently trying to accomplish, and responds with the plain-text name of the agent that should handle the task. The code then routes to that agent.A continuation agent: it's sometimes necessary to chain agents together to complete a task. For instance, to check your account balance, you need to be authenticated first. The authentication agent doesn't know if you were simply trying to authenticate yourself or if it's part of a chain, and it doesn't need to. When the authentication agent completes, the continuation agent checks chat history to see what the original task was, and if there's more to do, it formulates a new request to the orchestration agent to get you there without further user input.A global state keeps track of the user and their current state, shared between all the agents.The flow of the the system looks something like this:The system in actionTo get a sense of how this works in practice, here's sample output including helpful debug statements. Output that would be ordinarily shown to the user has two >>, while user input has one >.At the beginning of the conversation nothing's happened yet, so you get routed to the concierge:No current speaker, asking orchestration agent to decide\n\nConcierge agent selected\n\n>> Hi there! How can I assist you today? Here are some things I can help you with:\n\n>> Looking up a stock price\n\n>> Authenticating you\n\n>> Checking an account balance (requires authentication first)\n\n>> Transferring money between accounts (requires authentication and checking an account balance first)\n\n>> What would you like to do?\n\n> Transfer moneyThe \"transfer money\" task requires authentication. The orchestration agent checks if you're authenticated while deciding how to route you (it does this twice for some reason, it's a demo!):No current speaker, asking orchestration agent to decide\n\nOrchestrator is checking if authenticated\n\nOrchestrator is checking if authenticated\n\nAuth agent selectedIt correctly determines you're not authenticated, so it routes you to the authentication agent:>> To transfer money, I need to authenticate you first. Could you please provide your username and password?\n\n> seldoThis is a fun part: you've provided input, but it's not sufficient to complete the task (you didn't give a password). So when the flow goes back to the orchestration agent, the global state indicates that the \"authenticate\" agent is already running and hasn't completed yet, so it routes back to the authentication agent, and does that again for the password:There's already a speaker: authenticate\n\nAuth agent selected\n\nRecording username\n\n>> Thank you! Now, could you please provide your password?\n\n> monkey\n\nThere's already a speaker: authenticate\n\nAuth agent selected\n\nLogging in seldo\n\nChecking if authenticated\n\nAuthentication is completeNow the auth agent has called a done() function that indicates to the global state that it has completed its task. So the flow now goes to the continuation agent, which looks at the chat history and sees that the user was trying to transfer money. So it generates a prompt, as if spoken by the user, and sends that to the orchestration agent:>> You have been successfully authenticated. Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nOrchestrator checking if account has a balance\n\nOrchestrator checking if account has a balance\n\nAccount balance agent selectedNow you're authenticated, but you haven't checked your balance yet, which the orchestration agent knows is necessary for transferring money. So it routes you to the account balance agent (after checking twice for some reason):>> Before you can transfer money, you need to check your account balance. Let's start by looking up your account balance. Could you please provide the name of the account you're interested in?\n\n> Checking\n\nThere's already a speaker: account_balance\n\nAccount balance agent selected\n\nLooking up account ID for Checking\n\nLooking up account balance for 1234567890\n\nAccount balance lookup is complete\n\n>> Your Checking account has a balance of $1000. Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nTransfer money agent selectedThe account balance agent asks you which account, uses a tool to get the ID for that account, and then marks itself as done. The continuation agent kicks in again and sees that you still haven't completed your original task of transferring money, so it prompts the orchestrator agent again. Unfortunately the orchestrator gets a little confused, and loops twice before finally routing you to the transfer money agent:Money transfer is complete\n\n>> Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nTransfer money agent selected\n\nMoney transfer is complete\n\n>> Another agent will assist you with transferring money.\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said \"I would like to transfer money.\"\n\nNo current speaker, asking orchestration agent to decide\n\nOrchestrator checking if account has a balance\n\nTransfer money agent selected\n\n>> You have already checked your account balance. Please provide the following details to proceed with the money transfer:\n\n>> 1. The account ID to which you want to transfer the money.\n\n>> 2. The amount you want to transfer.\n\n> To account ID 1234324\n\nThere's already a speaker: transfer_money\n\nTransfer money agent selected\n\n>> How much would you like to transfer to account ID 1234324?\n\n> 500\n\nThere's already a speaker: transfer_money\n\nTransfer money agent selected\n\nChecking if balance is sufficient\n\nTransferring 500 from 1234567890 account 1234324\n\nMoney transfer is complete\n\n>> The transfer of $500 to account ID 1234324 has been successfully completed. If you need any further assistance, feel free to ask!\n\nAsking the continuation agent to decide what to do next\n\nContinuation agent said no_further_tasksWe've reached the end of the task! The continuation agent sees that there are no further tasks, and routes you back to the concierge.The codeNow let's look at some highlights of the code that gets all of this done. The core of the system is a central loop that runs forever. At the core of that is a very simple block that simply asks the orchestration agent who should speak next, and sets the next_speaker value which is contained in the state object that is passed between all the agents. Note that if there's already a sub-agent speaking, that agent gets to keep speaking.current_history = root_memory.get()\n\n# who should speak next?\nif (state[\"current_speaker\"]):\n  print(f\"There's already a speaker: {state['current_speaker']}\")\n  next_speaker = state[\"current_speaker\"]\nelse:\n  print(\"No current speaker, asking orchestration agent to decide\")\n  orchestration_response = orchestration_agent_factory(state).chat(\n    user_msg_str, \n    chat_history=current_history\n  )\n  next_speaker = str(orchestration_response).strip()The orchestration agent has a very strict prompt; its output only goes to other machines. It includes a natural-language summary of the dependencies between agents:    system_prompt = (f\"\"\"\n        You are on orchestration agent.\n        Your job is to decide which agent to run based on the current state of the user and what they've asked to do. Agents are identified by short strings.\n        What you do is return the name of the agent to run next. You do not do anything else.\n        \n        The current state of the user is:\n        {pprint.pformat(state, indent=4)}\n\n        If a current_speaker is already selected in the state, simply output that value.\n\n        If there is no current_speaker value, look at the chat history and the current state and you MUST return one of these strings identifying an agent to run:\n        * \"{Speaker.STOCK_LOOKUP.value}\" - if they user wants to look up a stock price (does not require authentication)\n        * \"{Speaker.AUTHENTICATE.value}\" - if the user needs to authenticate\n        * \"{Speaker.ACCOUNT_BALANCE.value}\" - if the user wants to look up an account balance\n            * If they want to look up an account balance, but they haven't authenticated yet, return \"{Speaker.AUTHENTICATE.value}\" instead\n        * \"{Speaker.TRANSFER_MONEY.value}\" - if the user wants to transfer money between accounts (requires authentication and checking an account balance first)\n            * If they want to transfer money, but is_authenticated returns false, return \"{Speaker.AUTHENTICATE.value}\" instead\n            * If they want to transfer money, but has_balance returns false, return \"{Speaker.ACCOUNT_BALANCE.value}\" instead\n        * \"{Speaker.CONCIERGE.value}\" - if the user wants to do something else, or hasn't said what they want to do, or you can't figure out what they want to do. Choose this by default.\n\n        Output one of these strings and ONLY these strings, without quotes.\n        NEVER respond with anything other than one of the above five strings. DO NOT be helpful or conversational.\n    \"\"\")A simple if-else block takes the output of the orchestration agent and uses it to instantiate the sub-agent to run next. This is when the state object gets passed to each sub-agent:        if next_speaker == Speaker.STOCK_LOOKUP:\n            print(\"Stock lookup agent selected\")\n            current_speaker = stock_lookup_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.AUTHENTICATE:\n            print(\"Auth agent selected\")\n            current_speaker = auth_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.ACCOUNT_BALANCE:\n            print(\"Account balance agent selected\")\n            current_speaker = account_balance_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.TRANSFER_MONEY:\n            print(\"Transfer money agent selected\")\n            current_speaker = transfer_money_agent_factory(state)\n            state[\"current_speaker\"] = next_speaker\n        elif next_speaker == Speaker.CONCIERGE:\n            print(\"Concierge agent selected\")\n            current_speaker = concierge_agent_factory(state)\n        else:\n            print(\"Orchestration agent failed to return a valid speaker; ask it to try again\")\n            is_retry = True\n            continueAnd then the full chat history is passed as part of a regular chat message to the newly-instantiated agent:response = current_speaker.chat(user_msg_str, chat_history=current_history)The agent reads its prompt and the user input and decides what to say. As we saw in our very first block of code, if the speaker is already selected, then the loop will keep talking to the current sub-agent. This continues until the sub-agent has completed its task, at which point its prompt instructs it to call the done() function:    def done() -> None:\n        \"\"\"When you complete your task, call this tool.\"\"\"\n        print(\"Money transfer is complete\")\n        state[\"current_speaker\"] = None\n        state[\"just_finished\"] = TrueThis modifies the state, setting the current speaker to none. This triggers the outer loop to run the continuation agent, to see if there's anything else to do:        elif state[\"just_finished\"] == True:\n            print(\"Asking the continuation agent to decide what to do next\")\n            user_msg_str = str(continuation_agent_factory(state).chat(\"\"\"\n                Look at the chat history to date and figure out what the user was originally trying to do.\n                They might have had to do some sub-tasks to complete that task, but what we want is the original thing they started out trying to do.                                                                      \n                Formulate a sentence as if written by the user that asks to continue that task.\n                If it seems like the user really completed their task, output \"no_further_task\" only.\n            \"\"\", chat_history=current_history))\n            print(f\"Continuation agent said {user_msg_str}\")\n            if user_msg_str == \"no_further_task\":\n                user_msg_str = input(\">> \").strip()\n            state[\"just_finished\"] = FalseThe continuation agent's prompt instructs it to reply as if it were the user asking to perform a task, or to output no_further_task if there's no more to do. If there's a new task, the output of the continuation agent becomes the input to the orchestrator, which selects a new speaker. If there's no further task, the loop pauses for more user input.And that's the full system! The sub-agents can be arbitrarily complicated, multi-turn systems in themselves, and the outer loop doesn't need to know how they work, just how they depend on each other.What's nextWe think there's some novel stuff in here: coordinating multiple agents \"speaking\" simultaneously, creating implicit \"chains\" of agents through natural language instructions, using a \"continuation\" agent to manage those chains, and using a global state this way. We're excited to see what you do with the patterns we've laid out here. Don't forget to check out the open-source repo!",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS"
            ],
            "link": "https://www.llamaindex.ai/blog//building-a-multi-agent-concierge-system"
        },
        {
            "date": "Mar 1, 2024",
            "author": "Jerry Liu",
            "title": "Towards Long Context RAG ",
            "tags": [
                "llm"
            ],
            "content": "Google recently released Gemini 1.5 Pro with a 1M context window, available to a limited set of developers and enterprise customers. Its performance has caught the imagination of AI Twitter. It achieves 99.7% recall in the \u201cNeedle in a Haystack\u201d experiment popularized by Greg Kamradt. Early users have shared results feeding dozens of research papers, financial reports at once and report impressive results in terms of its ability to synthesize across vast troves of information.Naturally, this begs the question - is RAG dead? Some folks think so, while others disagree. Those in the first camp make valid points. Most small data use cases can fit within a 1-10M context window. Tokens will get cheaper and faster to process over time. Having an LLM natively interleave retrieval/generation via attention layers leads to a higher response quality compared to the one-shot retrieval present in naive RAG.We were fortunate to have a preview of Gemini 1.5 Pro\u2019s capabilities, and through playing around with it developed a thesis for how context-augmented LLM applications will evolve. This blog post clarifies our mission as a data framework along with our view of what long-context LLM architectures will look like. Our view is that while long-context LLMs will simplify certain parts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG architectures to handle the new use cases that long-context LLMs bring along. No matter what new paradigms emerge, our mission at LlamaIndex is to build tooling towards that future.Our Mission Goes Beyond RAGThe goal of LlamaIndex is very simple: enable developers to build LLM applications over their data. This mission goes beyond just RAG. To date we have invested a considerable amount of effort in advancing RAG techniques for existing LLMs, and we\u2019ve done so because it\u2019s enabled developers to unlock dozens of new use cases such as QA over semi-structured data, over complex documents, and agentic reasoning in a multi-doc setting.But we\u2019re also excited about Gemini Pro, and we will continue to advance LlamaIndex as a production data framework in a long-context LLM future.An LLM framework is intrinsically valuable. As an open-source data framework, LlamaIndex paves the cowpaths towards building any LLM use case from prototype to production. A framework makes it easier to build these use cases versus building from scratch. We enable all developers to build for these use cases, whether it\u2019s setting up the proper architecture using our core abstractions or leveraging the hundreds of integrations in our ecosystem. No matter what the underlying LLM advancements are and whether RAG continues to exist in its current form, we continue to make the framework production-ready, including watertight abstractions, first-class documentation, and consistency.We also launched LlamaCloud last week. Our mission for LlamaCloud remains building the data infra enabling any enterprise to make their vast unstructured, semi-structured, and structured data sources production-ready for use with LLMs.Initial Gemini 1.5 Pro ObservationsDuring our initial testing we played around with some PDFs: SEC 10K Filings, ArXiv papers, this monster Schematic Design Binder, and more. We will do a lot more deeper analyses once the APIs are available, but in the meantime we share observations below.Gemini results are impressive and consistent with what we\u2019ve seen in the technical report and on socials:Gemini has impressive recall of specific details: We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing.Gemini has impressive summarization capabilities. The model can analyze large swaths of information across multiple documents and synthesize answers.This figure shows a question-response pair from Gemini over the 2019 Uber 10K filing. The question and answer is shown at the top and the source table is shown at the bottom. Gemini is able to return the correct answer.There are some parts where we noticed Gemini struggles a bit.Gemini doesn\u2019t read all tables and charts correctly. Gemini Pro still has a hard time being able to read figures and complex tables.Gemini can take a long time. Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds.Gemini can hallucinate page numbers. When asked to give a summary but also with page number citations, Gemini hallucinated the sources.An example where Gemini 1.5 Pro still hallucinates. The model hallucinates a number when asked about the total number of gross bookings across all segments - the number is visible in the chart and can also be pieced together from the table. Directionally though it\u2019s an exciting glimpse of the future and warrants a bigger discussion on which RAG paradigms will fade and new architectures that will emerge. See below!Long Contexts Resolve Some Pain Points, but some Challenges RemainGemini 1.5 Pro is just the first of many long-context LLMs to emerge, which will inevitably change how users are building RAG.Here are some existing RAG pain points that we believe long-context LLMs will solve:Developers will worry less about how to precisely tune chunking algorithms. We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages.Developers will need to spend less time tuning retrieval and chain-of-thought over single documents. An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer.Summarization will be easier. This is related to the above statement. A lot of summarization strategies over big documents involve \u201chacks\u201d such as sequential refinement or hierarchical summarization (see our response synthesis modules as a reference guide). This can now be alleviated with a single LLM call.Personalized memory will be better and easier to build: A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction).There are, however, some lingering challenges:10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge. 1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many \u201csmall\u201d document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context.Embedding models are lagging behind in context length. So far the largest context window we\u2019ve seen for embeddings are 32k from together.ai. This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller.Cost and Latency. Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that Yao Fu brought up is that a KV Cache can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below.A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies. We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document.Towards New RAG ArchitecturesProper usage of long-context LLMs will necessitate new architectures to best take advantage of their capabilities, while working around their remaining constraints. We outline some proposals below.1. Small to Big Retrieval over DocumentsTo the extent that long-context LLMs need retrieval augmentation over big knowledge bases (e.g. in the gigabytes), we will need small-to-big retrieval: index and retrieve small chunks, but have each chunk link to big chunks that will ultimately be fed to LLMs during synthesis.This architecture already exists in LlamaIndex in different forms (sentence window retriever and recursive retrieval over chunk sizes), but can be scaled up even more for long-context LLMs - embed document summaries, but link to entire documents.One reason we want to embed and index smaller chunks is due to the fact that current embedding models are not keeping up with LLMs in terms of context length. Another reason is that there can actually be retrieval benefits in having multiple granular embedding representations compared to a single document-level embedding for a document. If there is a single embedding for a document, then that embedding has the burden of encoding all information throughout the entire document. On the other hand, we\u2019ve found that embedding many smaller chunks and having each small chunk link to a bigger chunk, will lead to better retrieval of the relevant information.Check out the diagram above for an illustration of two flavors of small-to-big retrieval. One is indexing document summaries and linking them to documents, and the other is indexing smaller chunks within a document and linking them to the document. Of course, you could also do both - a general best practice for improving retrieval is to just try out multiple techniques at once and fuse the results later.2. Intelligent Routing for Latency/Cost TradeoffsThe arrival of long-context LLMs will inevitably raise questions on the amount of context that is suitable for each use case. Injecting LLMs with long context comes with real cost and latency tradeoffs and isn\u2019t suitable for every use case or even every question. Although cost and latency will decrease in the future, we anticipate users will need to think carefully about this tradeoff for the next year or two.Certain questions that are asking about specific details are well suited for existing RAG techniques of top-k retrieval and synthesis.More complex questions require more context from disparate pieces of different documents, and in those settings it is less clear how to correctly answer these questions while optimizing for latency and cost:Summarization questions require going over entire documents.Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt.We imagine an intelligent routing layer that operates on top of multiple RAG and LLM synthesis pipelines over a knowledge base. Given a question, the router can ideally choose an optimal strategy in terms of cost and latency in terms of retrieving context to answer the question. This ensures that a single interface can handle different types of questions while not becoming prohibitively expensive.3. Retrieval Augmented KV CachingAn optimization that Google and other companies are certainly working on is resolving latency and cost concerns through a KV Cache. At a high-level, a KV cache stores activations from pre-existing key and query vectors in an attention layer, preventing the need to recompute activations across the entire text sequence during LLM generation (we found this to be a nice intro reference to how a KV Cache works).Using a KV Cache to cache all document tokens within the context window prevents the need to recompute activations for these tokens on subsequent conversations, bringing down latency and cost significantly.But this leads to interesting retrieval strategies on how to best use the cache, particularly for knowledge corpuses that exceed the context length. We imagine a \u201cretrieval augmented caching\u201d paradigm emerging, where we want to retrieve the most relevant documents that the user would want to answer, with the expectation that they will continue to use the documents that are in the cache.This could involve interleaving retrieval strategies with traditional caching algorithms such as LRU caching. But a difference with existing KV cache architectures is that the position matters, since the cached vector is a function of all tokens leading up to that position, not just the tokens in the document itself. This means that you can\u2019t just swap out a chunk from the KV cache without affecting all cached tokens that occur after it positionally.In general the API interface for using a KV Cache is up in the air. It\u2019s also up in the air as to whether the nature of the cache itself will evolve or algorithms will evolve to best leverage the cache.What\u2019s NextWe believe the future of LLM applications is bright, and we are excited to be at the forefront of this rapidly evolving field. We invite developers and researchers to join us in exploring the possibilities of long-context LLMs and building the next generation of intelligent applications.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//towards-long-context-rag"
        },
        {
            "date": "May 23, 2023",
            "author": "Jerry Liu",
            "title": "Dumber LLM Agents Need More Constraints and Better Tools",
            "tags": [
                "artificial-intelligence",
                "large-language-models",
                "nlp",
                "machine-learning"
            ],
            "content": "SummaryIn this article, we compare how well LLM-powered agents with different degrees of complexity perform over practical data tasks (financial analysis). We compare the performance of agents with more complex, unrestrained interaction behavior (ReAct) with agents that contain simpler, more constrained interactions (routing). We specifically analyze how much complexity can be added to the agent layer vs. the tool layer.We find that the choice of the language model matters a lot. ReAct agents that are powered by \u201cdumber\u201d models (in a tongue-in-cheek fashion we are referring to any non GPT-4 model as \u201cdumb\u201d) struggle to return relevant results over data. We find that constraining agent interaction behavior, and giving them access to more tools that can more explicitly perform complex actions, can help improve query performance over these less sophisticated LLMs. In contrast, more sophisticated models (GPT-4) can more reliably utilize the ReAct loop to execute a variety of complex data queries.This blog post is quite detailed; we provide a lot of experiments and results below. Best of all, you can run this all yourself with our example notebook!Overview of AgentsBuilding LLM-powered agents have gotten increasingly popular in the past few months. Frameworks like LangChain have made it much easier to create these agents according to a set of common abstractions.At a high-level, an \u201cagent\u201d is essentially an automated decision engine, that can be used to interact with an external environment. The core agent loop looks something like the following:The agent has access to a set of \u201ctools\u201d, which are generic functions that it can perform. It has an awareness of each tool through some attached metadata, and it can call each tool (either as a function call or structured API).User feeds in a natural language input to the agent.Given the input, the agent interacts with the set of tools in some fashion and returns the response.There\u2019s a variety of ways to perform agent-tool interaction.The most popular is probably ReAct: the agent reasons over the next action, constructs an action command, executes the action. It repeats these steps in an iterative loop until the task is complete.There are other interaction modes too. Recently there was a paper on Plan-and-solve Prompting, which generates a plan beforehand (to decompose a complex task into simpler ones). Before ReAct there have also been related techniques on Self-Ask and Chain of Thought Prompting.\u201cComplex\u201d vs. \u201cSimple\u201d Agent Interaction TechniquesWe classify techniques like ReAct are more complex and unconstrained: this is because they perform iterative reasoning and also break the input into smaller steps. Complicated agent interaction loops allow for more freedom of behavior, and create an increased burden on the LLM being used. The pro of complex interaction frameworks is that they can be more general and handle a broader class of queries over simple tools. The con is that if the LLM is not up to par, then these frameworks are prone to making mistakes; unconstrained behavior can lead to unexpected results.On the other end of the spectrum, you can imagine a simple and constrained agent interaction mechanism, where the agent does one-step selection of the underlying tool to use, and returns the response from the tool. The agent essentially just acts as a router from the query to Tool. There are no steps to break down the question into smaller ones, and no iterative chain-of-thought loops. The pro here is that the model will likely make fewer errors. The con here is that the interaction technique allows for less freedom and imposes more constraints on behavior.Investigating Agent Interaction Techniques for Data QueryingWe at LlamaIndex are interested in how agents can help augment data tasks. More specifically, we are interested in how agents can help perform complex user queries over a diverse range of data sources. This includes not only asking questions over a single document, but being able to synthesize insights across multiple documents and return that to the user.LlamaIndex query engines can be used as Tools within an agent construct to query your data (we provide seamless integrations with LangChain). These Tools can vary in complexity. For instance, a simple Tool could be our vector store query engine, which does top-k embedding retrieval from a vector store. A more advanced tool could be a query engine over our graph data structure, which can be setup to explicitly provide compare/contrast capabilities over any subset of documents. The tool itself can contain \u201cagent-like\u201d decision-making capabilities under the hood. LlamaIndex provides a variety of modules around routing, query decomposition, and multi-part query planning.In this blog post, we are interested in comparing the following approaches to designing agents and tools to see which approach can provide good answers to different user queries in a robust fashion:more complex and unconstrained agent interaction (ReAct) over a set of simple Toolsmore simple and constrained agent interaction (simple routing) that uses more complex ToolsComplex Agents with Simple Tools, Simple Agents with Complex ToolsEssentially what we are interested in is how much complexity can be pushed to the agent interaction layer vs. being left in the Tool layer. We explore the following concrete example: let\u2019s say the user query is to compare/contrast two different documents (a relatively complex query). If the set of Tools are all just vector indices over different documents, could the agent interaction loop figure out how to execute that query reliably against the vector indices? On the other hand, if we push the complexity down to the Tool layer, then we could explicitly have a Tool that can perform \u201ccompare/contrast\u201d over your Documents. Then the burden on the agent is to simply call this Tool instead of interacting with a set of other tools in a more complex fashion.High-Level FindingsThe high-level finding is that less sophisticated agents need more constraints. More specifically, we found that using a GPT-3 powered agent in a ReAct loop did not provide good results over complex queries; it was not able to figure out the proper interaction pattern over the provided set of Tools in order to surface the results. Instead, by adding more constraints to the agent behavior and providing more sophistication in the Tool itself, we were able to get a GPT-3 agent to produce better results.Smarter agents require fewer constraints. We did find that GPT-4 agents with ReAct were able to provide better query results than GPT-3 agents when presented with a set of simple Tools over the data. This implies that more powerful agents may not need as many tools to \u201cexplicitly\u201d perform tasks when much of that logic can be handled in the agent interaction loop.SetupOur data consists of three Uber 10-Q filings (quarterly financial reports) in 2022: March, June, and September. We wish to execute different queries over this data; the bulk of these queries are around comparing different bits of information between these documents.march_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_march_2022.pdf\"]).load_data()\njune_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_june_2022.pdf\"]).load_data()\nsept_2022 = SimpleDirectoryReader(input_files=[\"../data/10q/uber_10q_sept_2022.pdf\"]).load_data()We use LlamaIndex to define a vector index over each document, which just stores the document chunks + embeddings in a vector store. We can then query each vector index using a simpleQueryEngine . We create a tool for each of theseQueryEngine objects.# define indices\nmarch_index = GPTVectorStoreIndex.from_documents(march_2022)\njune_index = GPTVectorStoreIndex.from_documents(june_2022)\nsept_index = GPTVectorStoreIndex.from_documents(sept_2022)\n\n# define query engine\nmarch_engine = march_index.as_query_engine(similarity_top_k=3)\njune_engine = june_index.as_query_engine(similarity_top_k=3)\nsept_engine = sept_index.as_query_engine(similarity_top_k=3)We also define a ComposableGraph over these three documents. The composable graph roughly follows the guide described here. This graph is explicitly setup to perform compare/contrast queries over these three documents.graph = ComposableGraph.from_indices(\n    GPTListIndex,\n    children_indices=[march_index, june_index, sept_index],\n    index_summaries=[\n        \"Provides information about Uber quarterly financials ending March 2022\",\n        \"Provides information about Uber quarterly financials ending June 2022\",\n        \"Provides information about Uber quarterly financials ending September 2022\"\n    ]\n)The graph can be queried with a ComposableGraphQueryEngine :# define decompose_transform\ndecompose_transform = DecomposeQueryTransform(verbose=True)\n\n# define custom query engines\ncustom_query_engines = {}\nfor index in [march_index, june_index, sept_index]:\n    query_engine = index.as_query_engine(service_context=service_context)\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\n\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    service_context=service_context,\n    streaming=True,\n)\n\n# define graph\ng_engine = graph.as_query_engine(\n    custom_query_engines=custom_query_engines\n)We try the following agent setups:GPT-3 ReAct agent: A zero-shot GPT-3 ReAct agent with three Tools: each Tool corresponds to the vector index over a 10-Q filing.GPT-4 ReAct agent: Same as above but using GPT-4 instead.Simple Router agent: A simple router \u201cagent\u201d with four Tools: the three Tools listed above + the ComposableGraphQueryEngine explicitly setup to perform compare/contrast queries.The code snippets for initializing these agents are below. For the simple router agent, we use the native RouterQueryEngine within LlamaIndex, though you should also be able to achieve similar results in LangChain through either the zero-shot agent (with tweaked settings) or the router chain.GPT-3/GPT-4 ReAct Agent Setup# initializing zero-shot ReAct agent\n\nuber_config_sept = IndexToolConfig(\n    query_engine=sept_engine, \n    name=f\"Uber 10Q September 2022\",\n    description=f\"Provides information about Uber quarterly financials ending September 2022\",\n    tool_kwargs={\"return_direct\": False}\n)\nuber_config_june = IndexToolConfig(\n    query_engine=june_engine, \n    name=f\"Uber 10Q June 2022\",\n    description=f\"Provides information about Uber quarterly financials ending June 2022\",\n    tool_kwargs={\"return_direct\": False}\n)\nuber_config_march = IndexToolConfig(\n    query_engine=march_engine, \n    name=f\"Uber 10Q March 2022\",\n    description=f\"Provides information about Uber quarterly financials ending March 2022\",\n    tool_kwargs={\"return_direct\": False}\n)\n\ntoolkit = LlamaToolkit(\n    index_configs=[uber_config_sept, uber_config_june, uber_config_march],\n)\n\n# this is a light wrapper around `initialize_agent` in langchain (which defaults to zero-shot)\nagent_chain = create_llama_agent(\n    toolkit,\n    llm, # can be GPT-3 or GPT-4 \n    verbose=True\n)Simple Router Agent Setup\nquery_tool_sept = QueryEngineTool.from_defaults(\n    query_engine=sept_engine,\n    description=f\"Provides information about Uber quarterly financials ending September 2022\",\n)\nquery_tool_june = QueryEngineTool.from_defaults(\n    query_engine=june_engine,\n    description=f\"Provides information about Uber quarterly financials ending June 2022\",\n)\nquery_tool_march = QueryEngineTool.from_defaults(\n    query_engine=march_engine,\n    description=f\"Provides information about Uber quarterly financials ending March 2022\",\n)\nquery_tool_graph = QueryEngineTool.from_defaults(\n    query_engine=g_engine,\n    description=f\"Provides comparisons between Uber financials across quarters in 2022. Can be used to answer \"\n                 \"any questions that require analysis across multiple quarters.\",\n)\n\n# our \"router\" query engine is effectively a simple agent that can only perform routing\nquery_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(),\n    query_engine_tools=[\n        query_tool_sept,\n        query_tool_june,\n        query_tool_march,\n        query_tool_graph\n    ]\n)Now that we\u2019ve described the setup, let\u2019s take a look at the results below!Findings and ExperimentsAt a high-level, we find using GPT-3 in ReAct agents produces suboptimal results over these queries. They tend to exhibit the following characteristics:Unpredictability in the set of chosen tools: The set of tools chosen can differ even if the questions are semantically similar, leading to variability in the responses.Lack of coverage in the set of chosen tools: Oftentimes we expect that a given question is able to make use of all three 10-Q statements, but only a subset of them are picked.Erroneous chain-of-thought processing: Sometimes the agent uses tools throughout the CoT process that are irrelevant to the question.In contrast, we find that GPT-4 ReAct agents provide answers that are more relevant, predictable, and exhibit fewer errors in intermediate results.Finally, we find that using a simpler routing-only GPT-3 agent with access to an explicit \u201ccompare/contrast\u201d tool allows the agent to perform better.As a reminder, full results are in the notebook: https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharingGPT-3 ReAct Agent ResultsQuery 1agent_chain.run(input=\"Analyze Uber revenue growth over the last few quarters\")Response:We see that only the September 10-Q filing is chosen to answer the question. The September 10-Q does contain some information about revenue growth compared to the same period in 2021, but that doesn\u2019t explicitly answer the question, which is about revenue growth the past few quarters.Query 2agent_chain.run(input=\"Analyze changes in risk factors for Uber\")Response:The September and June 10-Q filings are chosen, but not March. Moreover, the answer is vague and doesn\u2019t provide much detail regarding concrete risk factors for Uber (and also mentions that the risk factors \u201chave changed over the past three quarters\u201d even though it\u2019s only using two Tools).Query 3In this query, we more explicitly showcase how slight changes in prompts can induce different chain-of-thought paths through different Tools, and as a result produce different answers.# Prompt variation 1 \nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over time\")Response:# Prompt variation 2\nagent_chain.run(input=\"Analyze Uber revenue growth and risk factors over quarters\")The main difference between these two queries is \u201cover time\u201d versus \u201cover quarters.\u201d As we can see, not only are the selected Tools different between the two variations, but the inputs are different as well \u2014 in the first it\u2019s \u201cfinancials\u201d, and in the second it\u2019s \u201cRevenue growth and risk factors.\u201dSince the Tool input in the first variant is unrelated to the question, the answer is similarly vague: \u201cUber\u2019s revenue growth and risk factors can be analyzed by comparing the financials\u2026\u201dQuery 4:Here instead of asking a compare/contrast question let\u2019s just ask a question about a given statement.agent_chain.run(input=\"How much cash did Uber have in sept 2022?\")We see that the agent makes two errors 1) it is not able to supply an action input to each Tool, and 2) ends up looking through the June and March filings which are irrelevant to the question.GPT-4 ReAct Agent ResultsGPT-4 ReAct agents perform a lot better than GPT-3 agents. They comprehensively go through the set of available Tools, and provide much more detailed observation extraction and response synthesis.We won\u2019t go through all of these examples, but they can be found in the example notebook!Query 1:agent_chain_gpt4.run(input=\"Analyze Uber revenue growth over the last few quarters\")Response:Unlike the GPT-3 agent, here the GPT-4 agent at least goes through every filing and synthesizes the result.Query 2agent_chain_gpt4.run(input=\"Analyze changes in risk factors for Uber\")Response:Here the GPT-4 agent still only looks at September and June (and skips March), but the response is way more detailed, and references concrete facts within the report.GPT-3 Router Agent ResultsReminder: the router agent doesn\u2019t do any CoT and has additional access to our ComposableGraph query engine, which can explicitly perform compare/contrast queries. Let\u2019s take a look at how this agent does.Query 1response = query_engine.query(\"Analyze Uber revenue growth over the last few quarters\")Response (intermediate steps):Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n> Current query: Analyze Uber revenue growth over the last few quarters\n> New query:  What was Uber's revenue growth from the last quarter ending March 2022 compared to the previous quarter?\n> Current query: Analyze Uber revenue growth over the last few quarters\n> New query:  What was Uber's revenue growth from the last quarter ending June 2022 compared to the previous quarter?\n> Current query: Analyze Uber revenue growth over the last few quarters\n> New query:  What was Uber's revenue growth from the last few quarters ending September 2022?\nFinal ResponseBased on the limited context information provided, we can only analyze Uber's revenue growth for two specific quarters:\n\n1. For the quarter ending March 2022, Uber's revenue growth increased compared to the first quarter of 2021. However, the exact percentage of growth is not provided.\n\n2. For the quarter ending September 2022, Uber's revenue growth was 72%.\n\nUnfortunately, we do not have enough data to analyze the revenue growth for the quarters ending June 2022 or to compare it to the previous quarter. To provide a more comprehensive analysis, additional data on Uber's revenue growth for other quarters would be needed.To be fair, this response does not cover the month of June. But the fault of that lies with the implementation of the Tool itself rather than the agent interaction step. The agent\u2019s only job is to pick the right Tool to solve the task at hand, and it does that job correctly; it decides to choose the Tool corresponding to our ComposableGraphQueryEngine abstraction (which can perform compare/contrast queries).Query 2response = query_engine.query(\"Analyze changes in risk factors for Uber\")Response (intermediate steps):INFO:llama_index.query_engine.router_query_engine:Selecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\nSelecting query engine 3: Provides comparisons between Uber financials across quarters in 2022. Can be used to answer any questions that require analysis across multiple quarters..\n&gt; Current query: Analyze changes in risk factors for Uber\n&gt; New query:  What are the risk factors for Uber in the quarter ending March 2022?\n&gt; Current query: Analyze changes in risk factors for Uber\n&gt; New query:  What are the risk factors for Uber in the quarter ending June 2022?\n&gt; Current query: Analyze changes in risk factors for Uber\n&gt; New query:  What are the risk factors for Uber in the quarter ending September 2022?Final ResponseFrom the context information provided, we can analyze the changes in risk factors for Uber across the three quarters as follows:\n\n1. COVID-19 pandemic: The ongoing pandemic remains a constant risk factor across all quarters, with its impact on driver and courier supply constraints, consumer demand, and government restrictions. However, the specific concerns related to the pandemic may evolve, such as the emergence of new virus variants or changes in vaccination rates.\n\n2. Driver and courier supply constraints: This risk factor is present in the quarter ending March 2022 but is not explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially if the pandemic continues to impact driver availability.\n\n3. Changes in consumer demand: This risk factor is mentioned in the quarter ending March 2022, with an imbalance between Mobility and Delivery services. It is not explicitly mentioned in the subsequent quarters, but fluctuations in revenue due to changes in Gross Bookings and Mobility Trip volumes in the quarter ending June 2022 could be related to changes in consumer demand.\n\n4. Government restrictions and regulations: This risk factor is present in the quarter ending March 2022, with the impact of restrictions on Mobility offerings. In the quarter ending June 2022, the focus shifts to the potential impact of regulatory changes and legal claims, such as driver classification in the UK. In the quarter ending September 2022, the risk factor of driver classification as employees, workers, or quasi-employees becomes more prominent.\n\n5. Uncertainty around future developments: This risk factor is mentioned in the quarter ending March 2022 but is not explicitly mentioned in the subsequent quarters. However, it may still be an underlying concern, especially with the ongoing impact of COVID-19 and potential permanent changes to end-users' behavior.\n\n6. Accounting pronouncements: This risk factor is introduced in the quarter ending June 2022, with the adoption and impact of recently issued accounting pronouncements potentially affecting Uber's consolidated financial statements.\n\n7. Integration and performance of acquired businesses: This risk factor is introduced in the quarter ending June 2022, with the acquisition of Transplace contributing to the increase in Freight revenue.\n\n8. Competition: This risk factor is introduced in the quarter ending September 2022, highlighting the highly competitive nature of the mobility, delivery, and logistics industries, and the need to lower fares, offer incentives, and promotions to remain competitive.\n\n9. Significant losses and increasing operating expenses: This risk factor is introduced in the quarter ending September 2022, emphasizing the financial challenges Uber faces in achieving profitability.The job of the agent is once again simple: it chooses the graph query engine that is optimized to perform \u201ccompare/contrast\u201d queries. The result is much more detailed than the result obtained from the GPT-3 ReAct agent (and is comparable to the GPT-4 ReAct agent). It contains a detailed analysis of all risk factors in bullet points, and offers a comparison across time for all bullet points.Query 3response = query_engine.query(\"Analyze Uber revenue growth and risk factors over the last few quarters\")Response (intermediate steps):> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending March 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What is Uber's revenue growth and risk factors for the quarter ending June 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What were Uber's total revenues for the quarter ending September 2022?\n> Current query: Analyze Uber revenue growth and risk factors over quarters\n> New query:  What were Uber's total revenues for the quarter ending September 2022?Final ResponseUber's revenue growth has been impressive over the past few quarters, with the quarter ending March 2022 showing a 136% year-over-year growth and the quarter ending June 2022 showing a 105% year-over-year growth. The revenue for the quarter ending June 2022 was $8,343 million. \n\nThe risk factors for Uber's business have been largely related to the ongoing impacts of the COVID-19 pandemic, including reduced global demand for Mobility rides, supply constraints, and potential permanent changes to end-user behavior. These risk factors have been present in both the quarter ending March 2022 and the quarter ending June 2022, though the specific risks have varied slightly.Concluding ThoughtsReAct-based agents offer a powerful, general reasoning loop, and have the potential to solve complex tasks over your data. But they tend to only work reliably with more powerful language models such as GPT-4. Less sophisticated models (e.g. GPT-3) will make more unpredictable and erroneous decisions, leading to subpar query performance over your data sources.Agents implemented with \u201cdumber\u201d models need more interaction constraints in order to make more reliable, less erroneous decisions. We find that if we explicitly constrain the agent interface and push the complexity down to the Tool layer, we can still create agents that offer good performance over your data.Of course, this is just an initial analysis and there\u2019s a few caveats/limitations:You may be able to \u201cprompt hack\u201d the default ReAct loop to get more consistent results, and we did not try that.We only tested this over a set of three financial documents. There\u2019s a lot more work that needs to be done if we want to test this out on thousands of docs.We only compared GPT-3 and GPT-4, there\u2019s so many more models to compare/benchmark, e.g ChatGPT, any open-source model, Anthropic Claude, etc.We did not test out other agent interaction patterns besides ReAct: \u201cplan and solve\u201d agents (though we do have similar formulations in LlamaIndex), AutoGPT-like task management, and more.Whether you\u2019ve run into similar findings or you disagree with our analysis, let us know! We\u2019d love to facilitate this discussion on our Discord.Notebook WalkthroughYou can find the full notebook walkthrough here: https://colab.research.google.com/drive/1uP38k4nr8OPmXbY4dLoKKQW0F29WtNuY?usp=sharing",
            "related_posts": [
                "Querying a network of knowledge with llama-index-networks",
                "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS",
                "Agentic RAG With LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12"
        },
        {
            "date": "Jan 3, 2024",
            "author": "Sherlock Xu",
            "title": "Building An Intelligent Query-Response System with LlamaIndex and OpenLLM",
            "tags": [
                "openllm",
                "llamaindex",
                "ai",
                "bentoml",
                "open-source"
            ],
            "content": "Over the past year, Large Language Models (LLMs) like GPT-4 have not only transformed how we interact with machines but also have redefined the possibilities within the realm of natural language processing (NLP). A notable trend in this evolution is the increasing popularity of open-source LLMs like Llama 2, Falcon, OPT and Yi. Some may prefer them over their commercial counterparts in terms of accessibility, data security and privacy, customization potential, cost, and vendor dependency. Among the tools gaining increasing traction in the LLM space are OpenLLM and LlamaIndex \u2014 two powerful platforms that, when combined, unlock new use cases for building AI-driven applications.OpenLLM is an open-source platform for deploying and operating any open-source LLMs in production. Its flexibility and ease of use make it an ideal choice for AI application developers seeking to harness the power of LLMs. You can easily fine-tune, serve, deploy, and monitor LLMs in a wide range of creative and practical applications.LlamaIndex provides a comprehensive framework for managing and retrieving private and domain-specific data. It acts as a bridge between the extensive knowledge of LLMs and the unique, contextual data needs of specific applications.OpenLLM\u2019s support for a diverse range of open-source LLMs and LlamaIndex\u2019s ability to seamlessly integrate custom data sources provide great customization for developers in both communities. This combination allows them to create AI solutions that are both highly intelligent and properly tailored to specific data contexts, which is very important for query-response systems.In this blog post, I will explain how you can leverage the combined strengths of OpenLLM and LlamaIndex to build an intelligent query-response system. This system can understand, process, and respond to queries by tapping into a custom corpus.Setting up the environmentThe first step is to create a virtual environment in your machine, which helps prevent conflicts with other Python projects you might be working on. Let\u2019s just call it llamaindex-openllm and activate it.python -m venv llamaindex-openllm\nsource llamaindex-openllm/bin/activateInstall the required packages. This command installs OpenLLM with the optional vllm component (I will explain it later).pip install \"openllm[vllm]\" llama-index llama-index-llms-openllm llama-index-embeddings-huggingfaceFor handling requests, you need to have an LLM server. Here, I use the following command to start a Llama 2 7B local server at http://localhost:3000. Feel free to choose any model that fits your needs. If you already have a remote LLM server, you can skip this step.openllm start meta-llama/Llama-2-7b-chat-hf --backend vllmOpenLLM automatically selects the most suitable runtime implementation for the model. For models with vLLM support, OpenLLM uses vLLM by default. Otherwise, it falls back to PyTorch. vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. According to this report, you can achieve 23x LLM inference throughput while reducing P50 latency using vLLM.Note: To use the vLLM backend, you need a GPU with at least the Ampere architecture (or newer) and CUDA version 11.8. This demo uses a machine with an Ampere A100\u201380G GPU. If your machine has a compatible GPU, you can also choose vLLM. Otherwise, simply install the standard OpenLLM package (pip install openllm) in the previous command.v1: Creating a simple completion serviceBefore building a query-response system, let\u2019s get familiar with the integration of OpenLLM and LlamaIndex and use it to create a simple completion service.The integration offers two APIs for interactions with LLMs:1. OpenLLM: This can be used to initiate a local LLM server directly without the need for starting a separate one using commands like openllm start. Here\u2019s how you can use it:from llama_index.llms.openllm import OpenLLM\nllm = OpenLLM('meta-llama/Llama-2-7b-chat-hf')2. OpenLLMAPI: This can be used to interact with a server hosted elsewhere, like the Llama 2 7B model I started previously.Let\u2019s try the complete endpoint and see if the Llama 2 7B model is able to tell what OpenLLM is by completing the sentence \u201cOpenLLM is an open source tool for\u201d.from llama_index.llms.openllm import OpenLLMAPI\n\nremote_llm = OpenLLMAPI(address=\"http://localhost:3000\")\n\ncompletion_response = remote_llm.complete(\"OpenLLM is an open source tool for\", max_new_tokens=1024)\nprint(completion_response)Run this script and here is the output:learning lifelong learning models. It is designed to be easy to use, even for those without extensive knowledge of machine learning. OpenLLM allows users to train, evaluate, and deploy lifelong learning models using a variety of datasets and algorithms.\n\nOpenLLM provides a number of features that make it useful for learning lifelong learning models. Some of these features include:\n\n1. Easy-to-use interface: OpenLLM provides an easy-to-use interface that makes it simple to train, evaluate, and deploy lifelong learning models.\n2. Support for a variety of datasets: OpenLLM supports a variety of datasets, including images, text, and time-series data.\n3. Support for a variety of algorithms: OpenLLM supports a variety of algorithms for lifelong learning, including neural networks, decision trees, and support vector machines.\n4. Evaluation tools: OpenLLM provides a number of evaluation tools that allow users to assess the performance of their lifelong learning models.\n5. Deployment tools: OpenLLM provides a number of deployment tools that allow users to deploy their lifelong learning models in a variety of environments.\n\nOpenLLM is written in Python and is available under an open source license. It is designed to be used in a variety of settings, including research, education, and industry.\n\nSome potential use cases for OpenLLM include:\n\n1. Training lifelong learning models for image classification: OpenLLM could be used to train a lifelong learning model to classify images based on their content.\n2. Training lifelong learning models for natural language processing: OpenLLM could be used to train a lifelong learning model to process and analyze natural language text.\n3. Training lifelong learning models for time-series data: OpenLLM could be used to train a lifelong learning model to predict future values in a time-series dataset.\n4. Deploying lifelong learning models in a production environment: OpenLLM could be used to deploy a lifelong learning model in a production environment, such as a recommendation system or a fraud detection system.\n\nOverall, OpenLLM is a powerful tool for learning lifelong learning models. Its ease of use, flexibility, and support for a variety of datasets and algorithms make it a valuable resource for researchers and practitioners in a variety of fields.Obviously, the model couldn\u2019t correctly explain OpenLLM with some hallucinations \ud83e\udd23. Nevertheless, the code works well as the server outputs a response for the request. This is a good start as we proceed with building our system.v2: Enhancing with a query-response systemThe initial version revealed a key limitation: the model\u2019s lack of specific knowledge about OpenLLM. One solution is to feed the model with domain-specific information, allowing it to learn and respond according to topic-specific queries. This is where LlamaIndex comes into play, enabling you to build a local knowledge base with pertinent information. Specifically, you create a directory (for example, data) and build an index for all the documents in the folder.Create a folder and let\u2019s import the GitHub README file of OpenLLM into the folder:mkdir data\ncd data\nwget https://github.com/bentoml/OpenLLM/blob/main/README.mdGo back to the previous directory and create a script called starter.py like the following:from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama_index.llms.openllm import OpenLLMAPI\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# Change the address to your OpenLLM server\nllm = OpenLLMAPI(address=\"http://localhost:3000\")\n\n# Break down the document into manageable chunks (each of size 1024 characters, with a 20-character overlap)\ntext_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n\n# Create a ServiceContext with the custom model and all the configurations\nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=\"local\",\n    text_splitter=text_splitter,\n    context_window=8192,\n    num_output=4096,\n)\n\n# Load documents from the data directory\ndocuments = SimpleDirectoryReader(\"data\").load_data()\n\n# Build an index over the documents using the customized LLM in the ServiceContext\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n# Query your data using the built index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is OpenLLM?\")\nprint(response)To improve the quality of your response, I recommend you define a SentenceSplitter to provide finer control over the input processing, leading to better output quality.In addition, you can set streaming=True to stream your response:query_engine = index.as_query_engine(streaming=True)\nresponse = query_engine.query(\"What is OpenLLM?\")\nresponse.print_response_stream()Your directory structure should look like this now:\u251c\u2500\u2500 starter.py\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 README.mdRun starter.py to test the query-response system. The output should be consistent with the content of the OpenLLM README. Here is the response I received:OpenLLM is an open-source platform for deploying and managing large language models (LLMs) in a variety of environments, including on-premises, cloud, and edge devices. It provides a comprehensive suite of tools and features for fine-tuning, serving, deploying, and monitoring LLMs, simplifying the end-to-end deployment workflow for LLMs.ConclusionThe exploration in this article underscores the importance of customizing AI tools to fit specific needs. By using OpenLLM for flexible deployment of LLMs and LlamaIndex for data management, I have demonstrated how to create an AI-powered system. It not only understands and processes queries but also delivers responses based on a unique knowledge base. I hope this blog post has inspired you to explore more capabilities and use cases of OpenLLM and LlamaIndex. Happy coding! \u2328\ufe0f",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//building-an-intelligent-query-response-system-with-llamaindex-and-openllm-ff253a200bdf"
        },
        {
            "date": "May 18, 2023",
            "author": "Jerry Liu",
            "title": "Build a ChatGPT with your Private Data using LlamaIndex and MongoDB",
            "tags": [
                "nlp",
                "ai",
                "machine-learning",
                "large-language-models",
                "mongodb"
            ],
            "content": "Co-authors:Prakul Agarwal \u2014 Senior Product Manager, Machine Learning at MongoDBJerry Liu \u2014 co-founder at LlamaIndexUpdate (6/22/2023): The preferred way to use LlamaIndex + MongoDB is now with our MongoDBAtlasVectorSearch class. Take a look at our guide here: https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/MongoDBAtlasVectorSearch.htmlSummaryLarge Language Models (LLMs) like ChatGPT have revolutionized the way users can get answers to their questions. However, the \u201cknowledge\u201d of LLMs is restricted by what they were trained on, which for ChatGPT means publicly available information on the internet till September 2021. How can LLMs answer questions using private knowledge sources like your company\u2019s data and unlock its true transformative power?This blog will discuss how LlamaIndex and MongoDB can enable you to achieve this outcome quickly. The attached notebook provides a code walkthrough on how to query any PDF document using English queries.BackgroundTraditionally, AI has been used to analyze data, identify patterns and make predictions based on existing data. The recent advancements have led to AI becoming better at generating new things (rather than just analyzing existing things). This is referred to as Generative AI. Generative AI is powered mainly by machine learning models called Large Language Models (LLM). LLMs are pre-trained on large quantities of publicly available text. There are various proprietary LLMs from companies like OpenAI, Cohere, AI21, as well as a lot of emerging open-source LLMs like Llama, Dolly, etc.There are 2 main scenarios where the knowledge of LLMs falls short:Private data such as your company\u2019s internal knowledge base spread across PDFs, Google Docs, Wiki pages, and applications like Salesforce and SlackNewer data than when the LLMs were last trained. Example question: Who is the most recent UK prime minister?There are 2 main paradigms currently for extending the amazing reasoning and knowledge generation capabilities of LLMs: Model finetuning and in-context learning.Model Finetuning can be more complex and expensive to operationalize. There are also some open questions like how to delete information from a fine-tuned model to ensure you comply with local laws (ex. GDPR in Europe), and for changing data you need to fine-tune again constantly.In-context learning requires inserting the new data as part of the input prompts to the LLM. To perform this data augmentation in a secure, high performance and cost-effective manner is where tools like LlamaIndex and MongoDB Developer Data Platform can help.Introduction to LlamaIndexLlamaIndex provides a simple, flexible interface to connect LLMs with external data.Offers data connectors to various data sources and data formats (APIs, PDFs, docs, etc).Provides indices over the unstructured and structured data for use with LLMs.Structures external information so that it can be used with the prompt window limitations of any LLM.Exposes a query interface which takes in an input prompt and returns a knowledge-augmented output.MongoDB as the DatastoreIt is effortless to store the ingested documents (i.e. Node objects), index metadata, etc to MongoDB using the inbuilt abstractions in LlamaIndex. There is an option to store the \u201cdocuments\u201d as an actual collection in MongoDB using MongoDocumentStore. There is an option to persist the \u201cIndexes\u201d using the MongoIndexStore .Storing LlamaIndex\u2019s documents and indexes in a database becomes necessary in a couple of scenarios:Use cases with large datasets may require more than in-memory storage.Ingesting and processing data from various sources (for example, PDFs, Google Docs, Slack).The requirement to continuously maintain updates from the underlying data sources.Being able to persist this data enables processing the data once and then being able to query it for various downstream applications.MongoDB AtlasMongoDB offers a free forever Atlas cluster in the public cloud service of your choice. This can be accomplished very quickly by following this tutorial. Or you can get started directly here.Use of LLMsLlamaIndex uses LangChain\u2019s (another popular framework for building Generative AI applications) LLM modules and allows for customizing the underlying LLM to be used (default being OpenAI\u2019s text-davinci-003 model). The chosen LLM is always used by LlamaIndex to construct the final answer and is sometimes used during index creation as well.The workflowConnect private knowledge sources using LlamaIndex connectors (offered through LlamaHub).Load in the Documents. A Document represents a lightweight container around the data source.Parse the Documents objects into Node objects. Nodes represent \u201cchunks\u201d of source Documents (ex. a text chunk). These node objects can be persisted to a MongoDB collection or kept in memory.Construct Index from Nodes. There are various kinds of indexes in LlamaIndex like \u201cList Index\u201d (this stores nodes as Sequential chain), \u201cVector Store Index\u201d (this stores each node and a corresponding embedding in a vector store). Depending on the type of Index, these indexes can be persisted into a MongoDB collection or a Vector Database.Finally query the index. This is where the the query is parsed, relevant Nodes retrieved through the use of indexes, and provided as an input to a \u201cLarge Language Model\u201d (LLM). Different types of queries can use different indexes.LlamaIndex + MongoDB Workflow DiagramGetting questions answered over your private dataWe want to query the \u201cGPT-4 Technical Report\u201d published by OpenAI in March 2023. This was a PDF document with 100 pages. This is a recent publication, so was not included as part of the original ChatGPT training data.Here\u2019s the summary of the various queries we can ask the PDF.Query\u201cHow does GPT4 do on the bar exam?\u201dResponse\u201cGPT-4 performs well on the Uniform Bar Exam, with a score in the top 10% of test takers (Table 1, Figure 4).\u201dLLM token usageTotal embedding token usage: 18 tokens\nTotal LLM token usage: 1889 tokensQuery\u201cHow much better is GPT-4 in reducing hallucinations over GPT-3.5?\u201dResponse\u201cGPT-4 improves on the latest GPT-3.5 model by 19 percentage points, with significant gains across all topics.\u201dQuery\u201cWhat issues were observed after fine-tuning GPT-4 with RHLF??\u201dResponse\u201cAfter fine-tuning GPT-4 with RHLF, issues observed included the model becoming overly cautious in certain ways, such as refusing innocuous requests and excessively hedging or \u201coverrefusing\u201d. Additionally, the model was still quite brittle and sometimes exhibited undesired behaviors based on prompts where instructions to labelers were underspecified.\u201dQuery\u201cWhat is RBRM?\u201dResponse\u201cRBRM stands for Rule-Based Reward Model. It is a technique used to provide an additional reward signal to the GPT-4 policy model during PPO fine-tuning on a subset of training prompts. The RBRM takes three things as input: the prompt (optional), the output from the policy model, and a human-written rubric (e.g., a set of rules in multiple-choice style) for how this output should be evaluated. The RBRM then classifies the output based on the rubric.\u201dThe screenshots below show how the PDF document is converted into \u201cLlamaIndex nodes\u201d and \u201cLlamaIndex indices\u201d and persisted into MongoDB.Relevant ResourcesFurther details can be found here. Also check out the reference notebook below!Reading data from MongoDB: linkVarious Indexes in LlamaIndex: linkReference Notebookhttps://colab.research.google.com/drive/1SNIeLW38Nvx6MtL3-_LPS2XTIzqD4gS6?usp=sharing",
            "related_posts": [
                "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05"
            ],
            "link": "https://www.llamaindex.ai/blog//build-a-chatgpt-with-your-private-data-using-llamaindex-and-mongodb-b09850eb154c"
        },
        {
            "date": "Nov 22, 2023",
            "author": "Jerry Liu",
            "title": "Introducing Llama Packs",
            "tags": [
                "llamaindex",
                "nlp",
                "ai",
                "llm",
                "llamahub"
            ],
            "content": "Today we\u2019re excited to introduce Llama Packs \ud83e\udd99\ud83d\udce6\u2014 a community-driven hub of prepackaged modules that you can use to kickstart your LLM application. Import them for a wide variety of use cases, from building a Streamlit app to building advanced retrieval over Weaviate to a resume parser that does structured data extraction. Just as important, inspect and customize them to your liking.They\u2019re available on LlamaHub: we\u2019ve launched 16+ templates with our launch partners already, and we\u2019re going to be adding a lot more!(To those of you in the states, Happy Thanksgiving \ud83e\udd83)ContextThere are so many choices when building an LLM app that it can be daunting to get started building for a specific use case. Even for RAG the user needs to make the following decisions:Which LLM should I use? Embedding model?Vector database?Chunking/parsing strategyRetrieval AlgorithmWrapping in surrounding applicationEvery use case requires different parameters, and LlamaIndex as a core LLM framework offers a comprehensive set of unopinionated modules to let users compose an application.But we needed a way for users to get started more easily for their use case. And that\u2019s exactly where Llama Packs comes in.OverviewLlama Packs can be described in two ways:On one hand, they are prepackaged modules that can be initialized with parameters and run out of the box to achieve a given use case (whether that\u2019s a full RAG pipeline, application template, and more). You can also import submodules (e.g. LLMs, query engines) to use directly.On another hand, LlamaPacks are templates that you can inspect, modify, and use.They can be downloaded either through our llama_index Python library or the CLI in one line of code:CLI:llamaindex-cli download-llamapack <pack_name> --download-dir <pack_directory>Pythonfrom llama_index.llama_pack import download_llama_pack\n\n# download and install dependencies\nVoyageQueryEnginePack = download_llama_pack(\n  \"&lt;pack_name&gt;\", \"&lt;pack_directory&gt;\"\n)Llama Packs can span abstraction levels \u2014 some are full prepackaged templates (full Streamlit / Gradio apps), and some combine a few smaller modules together (e.g. our SubQuestionQueryEngine with Weaviate). All of them are found in LlamaHub \ud83d\udc47. You can filter by packs by selecting \u201cLlama Packs\u201d from the dropdown.Llama Packs on LlamaHubWe\u2019re excited to partner with the following companies/contributors for our launch, featuring 16+ templates. We highlight some examples below:Streamlit / Snowflake (Caroline F.): Streamlit ChatbotArize (Mikyo K., Xander S.): Arize PhoenixActiveLoop / DeepLake (Mikayel H., Adhilkhan S.): DeepMemory Pack, Multi-modal RetrievalWeaviate (Erika C.): Sub Question Query Engine, Retry Query EngineVoyage AI (Hong L.): Voyage AI PackTruEra (Josh R.): TruLens Eval Pack (this is 3 packs in one)Timescale (Matvey A.): Timescale Vector AutoRetrievalWenqi G.: LLaVa Completion PackThere\u2019s not enough room in this blog post to feature every template, we\u2019ll be running features on every pack in the next few days.Special thanks to Logan Markewich and Andrei Fajardo on the LlamaIndex team for getting Llama Packs up and running.Example WalkthroughThe best way to highlight LlamaPack features is to showcase an example. We\u2019ll walk through a simple Llama Pack that gives the user a RAG pipeline setup with Voyage AI embeddings.Voyage AI Pack. Every Pack has a detailed README on how to use / modules.First, we download and initialize the Pack over a set of documents:from llama_index.llama_pack import download_llama_pack\n\n# download pack\nVoyageQueryEnginePack = download_llama_pack(\"VoyageQueryEnginePack\", \"./voyage_pack\")\n# initialize pack (assume documents is defined)\nvoyage_pack = VoyageQueryEnginePack(documents)Every Llama Pack implements a get_modules() function allowing you to inspect/use the modules.modules = voyage_pack.get_modules()\ndisplay(modules)\n\n# get LLM, vector index\nllm = modules[\"llm\"]\nvector_index = modules[\"index\"]The Llama Pack can be run in an out of the box fashion. By calling run , we\u2019ll execute the RAG pipeline and get back a response. In this setting, you don\u2019t need to worry about the internals.# this will run the full pack\nresponse = voyage_pack.run(\"What did the author do growing up?\", similarity_top_k=2)\nprint(str(response))\nThe author spent his time outside of school mainly writing and programming. He wrote short stories and attempted to write programs on an IBM 1401. Later, he started programming on a TRS-80, creating simple games and a word processor. He also painted still lives while studying at the Accademia.The second important thing is that you have full access to the code of the Llama Pack. This allows you to customize the Llama Pack, rip out code, or just use it as reference to build your own app. Let\u2019s take a look at the downloaded pack in voyage_pack/base.py , and swap out the OpenAI LLM for Anthropic:from llama_index.llms import Anthropic\n...\n\nclass VoyageQueryEnginePack(BaseLlamaPack):\n    def __init__(self, documents: List[Document]) -&gt; None:\n        llm = Anthropic()\n        embed_model = VoyageEmbedding(\n            model_name=\"voyage-01\", voyage_api_key=os.environ[\"VOYAGE_API_KEY\"]\n        )\n        service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n        self.llm = llm\n        self.index = VectorStoreIndex.from_documents(\n            documents, service_context=service_context\n        )\n\n    def get_modules(self) -&gt; Dict[str, Any]:\n        \"\"\"Get modules.\"\"\"\n        return {\"llm\": self.llm, \"index\": self.index}\n\n    def run(self, query_str: str, **kwargs: Any) -&gt; Any:\n        \"\"\"Run the pipeline.\"\"\"\n        query_engine = self.index.as_query_engine(**kwargs)\n        return query_engine.query(query_str)You can re-import the module directly and run it again:from voyage_pack.base import VoyageQueryEnginePack\n\nvoyage_pack = VoyageQueryEnginePack(documents)\nresponse = voyage_pack.run(\"What did the author do during his time in RISD?\")\nprint(str(response))ConclusionTry it out and let us know what you think!ContributingNot on here yet? We\u2019d love to feature you! If you have any templates with LlamaIndex, adding it is almost as simple as copying/pasting your existing code over into aBaseLlamaPack subclass. Take a look at this folder for a full set of examples: https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packsResourcesAll Llama Packs can be found on LlamaHub: https://llamahub.ai/The full notebook walkthrough is here: https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_hub/llama_packs_example.ipynb",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-llama-packs-e14f453b913a"
        },
        {
            "date": "May 8, 2023",
            "author": "Jerry Liu",
            "title": "A New Document Summary Index for LLM-powered QA Systems",
            "tags": [
                "information-retrieval",
                "large-language-models",
                "gpt-4",
                "ai",
                "machine-learning"
            ],
            "content": "In this blog post, we introduce a brand new LlamaIndex data structure: a Document Summary Index. We describe how it can help offer better retrieval performance compared to traditional semantic search, and also walk through an example.BackgroundOne of the core use cases of Large Language Models (LLMs) is question-answering over your own data. To do this, we pair the LLM with a \u201cretrieval\u201d model that can perform information retrieval over a knowledge corpus, and perform response synthesis over the retrieved texts using the LLM. This overall framework is called Retrieval-Augmented Generation.Most users building LLM-powered QA systems today tend to do some form of the following:Take source documents, split each one into text chunksStore text chunks in a vector dbDuring query-time, retrieve text chunks by embedding similarity and/or keyword filters.Perform response synthesisFor a variety of reasons, this approach provides limited retrieval performance.Limitations of Existing ApproachesThere are a few limitations of embedding retrieval using text chunks.Text chunks lack global context. Oftentimes the question requires context beyond what is indexed in a specific chunk.Careful tuning of top-k / similarity score thresholds. Make the value too small and you\u2019ll miss context. Make the value too big and cost/latency might increase with more irrelevant context.Embeddings don\u2019t always select the most relevant context for a question. Embeddings are inherently determined separately between text and the context.Adding keyword filters are one way to enhance the retrieval results. But that comes with its own set of challenges. We would need to adequately determine the proper keywords for each document, either manually or through an NLP keyword extraction/topic tagging model. Also we would need to adequately infer the proper keywords from the query.Document Summary IndexA diagram for the Document Summary IndexWe propose a new index in LlamaIndex that will extract/index an unstructured text summary for each document. This index can help enhance retrieval performance beyond existing retrieval approaches. It helps to index more information than a single text chunk, and carries more semantic meaning than keyword tags. It also allows for a more flexible form of retrieval: we can do both LLM retrieval and embedding-based retrieval.How It WorksDuring build-time, we ingest each document, and use a LLM to extract a summary from each document. We also split the document up into text chunks (nodes). Both the summary and the nodes are stored within our Document Store abstraction. We maintain a mapping from the summary to the source document/nodes.During query-time, we retrieve relevant documents to the query based on their summaries, using the following approaches:LLM-based Retrieval: We present sets of document summaries to the LLM, and ask the LLM to determine which documents are relevant + their relevance score.Embedding-based Retrieval: We retrieve relevant documents based on summary embedding similarity (with a top-k cutoff).Note that this approach of retrieval for document summaries (even with the embedding-based approach) is different than embedding-based retrieval over text chunks. The retrieval classes for the document summary index retrieve all nodes for any selected document, instead of returning relevant chunks at the node-level.Storing summaries for a document also enables LLM-based retrieval. Instead of feeding the entire document to the LLM in the beginning, we can first have the LLM inspect the concise document summary to see if it\u2019s relevant to the query at all. This leverages the reasoning capabilities of LLM\u2019s which are more advanced than embedding-based lookup, but avoids the cost/latency of feeding the entire document to the LLMAdditional InsightsDocument retrieval with summaries can be thought of as a \u201cmiddle ground\u201d between semantic search and brute-force summarization across all docs. We look up documents based on summary relevance with the given query, and then return all *nodes* corresponding to the retrieved docs.Why should we do this? This retrieval method gives user more context than top-k over a text-chunk, by retrieving context at a document-level. But, it\u2019s also a more flexible/automatic approach than topic modeling; no more worrying about whether your text has the right keyword tags!ExampleLet\u2019s walk through an example that showcases the document summary index, over Wikipedia articles about different cities.The rest of this guide showcases the relevant code snippets. You can find the full walkthrough here (and here\u2019s the notebook link).We can build the GPTDocumentSummaryIndex over a set of documents, and pass in a ResponseSynthesizer object to synthesize summaries for the documents.from llama_index import (\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n    ResponseSynthesizer\n)\nfrom llama_index.indices.document_summary import GPTDocumentSummaryIndex\nfrom langchain.chat_models import ChatOpenAI\n\n# load docs, define service context\n...\n\n# build the index\nresponse_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\", use_async=True)\ndoc_summary_index = GPTDocumentSummaryIndex.from_documents(\n    city_docs, \n    service_context=service_context,\n    response_synthesizer=response_synthesizer\n)Once the index is built, we can get the summary for any given document:summary = doc_summary_index.get_document_summary(\"Boston\")Next, let\u2019s walk through an example LLM-based retrieval over the index.from llama_index.indices.document_summary import DocumentSummaryIndexRetriever\n\nretriever = DocumentSummaryIndexRetriever(\n    doc_summary_index,\n    # choice_select_prompt=choice_select_prompt,\n    # choice_batch_size=choice_batch_size,\n    # format_node_batch_fn=format_node_batch_fn,\n    # parse_choice_select_answer_fn=parse_choice_select_answer_fn,\n    # service_context=service_context\n)\nretrieved_nodes = retriever.retrieve(\"What are the sports teams in Toronto?\")\nprint(retrieved_nodes[0].score)\nprint(retrieved_nodes[0].node.get_text())The retriever will retrieve a set of relevant nodes for a given index.Note that the LLM returns relevance scores in addition to the document text:8.0\nToronto ( (listen) t\u0259-RON-toh; locally [t\u0259\u02c8\u0279\u0252\u027e\u0303\u0259] or [\u02c8t\u0279\u0252\u027e\u0303\u0259]) is the capital city of the Canadian province of Ontario. With a recorded population of 2,794,356 in 2021, it is the most populous city in Canada...We can also use the index as part of an overall query engine, to not only retrieve the relevant context, but also synthesize a response to a given question. We can do this through both the high-level API as well as lower-level API.High-level APIquery_engine = doc_summary_index.as_query_engine(\n  response_mode=\"tree_summarize\", use_async=True\n)\nresponse = query_engine.query(\"What are the sports teams in Toronto?\")\nprint(response)Lower-level API# use retriever as part of a query engine\nfrom llama_index.query_engine import RetrieverQueryEngine\n\n# configure response synthesizer\nresponse_synthesizer = ResponseSynthesizer.from_args()\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\n# query\nresponse = query_engine.query(\"What are the sports teams in Toronto?\")\nprint(response)Next StepsThe approach of autosummarization over any piece of text is really exciting. We\u2019re excited to develop extensions in two areas:Continue exploring autosummarization in different layers. Currently it\u2019s at the doc-level, but what about summarizing a big text chunk into a smaller one? (e.g. a one-liner).Continue exploring LLM-based retrieval, which summarization helps to unlock.Also we\u2019re sharing the example guide/notebook below in case you missed it above:Document Summary GuideNotebook Link",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"
        },
        {
            "date": "Nov 16, 2023",
            "author": "LlamaIndex",
            "title": "Evaluating Multi-Modal Retrieval-Augmented Generation",
            "tags": [
                "llm",
                "ai",
                "evaluation",
                "multimodal",
                "llamaindex"
            ],
            "content": "A few days ago, we published a blog on Multi-Modal RAG (Retrieval-Augmented Generation) and our latest (still in beta) abstractions to help enable and simplify building them. In this post, we now go over the important topic of how one can sensibly evaluate Multi-Modal RAG systems.A natural starting point is to consider how evaluation was done in traditional, text-only RAG and then ask ourselves how this ought to be modified to suit the multi-modal scenario (e.g., in text-only RAG, we use an LLM, but in multi-modal RAG we require a Large Multi-Modal Model or LMM for short). This is exactly what we\u2019ll do next and as you\u2019ll see, the overarching evaluation framework stays the same as it was in the text-only RAG, requiring only a few additions and modifications in order to make it more multi-modal appropriate.Primer: Multi-Modal RAG vs Text-Only RAGIllustration of text-only RAG versus multi-modal RAG. In multi-modal RAG, images modality can show up in the user query, the retrieved context, as well as the final answer.Let\u2019s consider the main differences between multi-modal and text-only RAG. Below are two tables that describe the RAG build considerations as well as query-time pipeline and compares and contrasts multi-modal and text-only cases against them.Table 1: Build considerations for RAG systems and how they differ text-only versus multi-modal scenarios.Table 2: The pipeline for querying a RAG and how they differ text-only versus multi-modal scenarios.Evaluation Of Text-Only RAGFor text-only RAG, the standard approach is to separately consider the evaluation of two stages: Retrieval and Generation.Retriever Evaluation: are the retrieved documents relevant to the user query?Some of the more popular metrics for retrieval evaluation include recall, hit rate, mean reciprocal rank, mean average precision, and normalized discounted cumulative gain. The first two of these metrics recall and hit rate, don\u2019t consider the position (or ranking) of the relevant documents, whereas all the others do in their own respective ways.Generator Evaluation: does the response use the retrieved documents to sufficiently answer the user query?In abstractive question-answering systems, like the kinds we\u2019re talking about in this blog, measuring the generated response is made more tricky due to the fact that there isn\u2019t just one way to sufficiently answer a query in written language \u2014 there\u2019s plenty!So, in this case, our measurement relies on subjective judgement, which can be performed by humans, though this is costly and unscalable. An alternative approach, is to use an LLM judge to measure things like relevancy and faithfulness.Relevancy: considers textual context and evaluates how much the generated response matches the query.Faithfulness: evaluates how much the generated response matches the retrieved textual context.For both of these, the retrieved context as well as the query and generated response are passed to the LLM judge. (This pattern of using an LLM to judge the responses has been termed by some researchers of the space as LLM-As-A-Judge (Zheng et al., 2023).)Currently, the llama-index (v0.9.2) library supports hit-rate and mean reciprocal rank for retrieval evaluation, as well as relevancy, faithfulness and a few others for generator evaluation. (Check out our Evaluation guides in our docs!).Evaluation Of Multi-Modal RAGFor the multi-modal case, the evaluation can (and should) still be carried out with respect to the different stages of retrieval and generation.Separating Out Retrieval Evaluation For Text and Image ModalitiesNow that retrieved documents can come in two forms, it would seem most sensible to consider computing the usual retrieval evaluation metrics separately for images and text. In this way, you have more knowledge as to which aspect of the multi-modal retriever is working well and what isn\u2019t. One can then apply a desired weighting scheme to establish a single aggregated retrieval score per metric.Hit Rate Mean Reciprocal Rank Text 0.95 0.88 Images 0.88 0.75Table 3: Retrieval evaluation in multi-modal scenario.Using Multi-Modal LLMs For Generator Evaluations (LMM-As-A-Judge)Multi-modal models (i.e., LMMs) like OpenAI\u2019s GPT-4V or open-source alternatives like LLaVA are able to take in both input and image context to produce an answer the user query. As in text-only RAG, we are also concerned about the \u201crelevancy\u201d and \u201cfaithfulness\u201d of these generated answers. But in order to be able to compute such metrics in the multi-modal case, we would need a judge model that is also able to take in the context images and text data. Thus, in the multi-modal case, we adopt the LMM-As-A-Judge pattern in order to compute relevancy and faithfulness as well as other related metrics!Relevancy (multi-modal): considers textual and visual context and evaluates how much the generated response matches the query.Faithfulness (multi-modal): evaluates how much the generated response matches the retrieved textual and visual context.If you want to test these out, then you\u2019re in luck as we\u2019ve recently released our beta Multi-Modal Evaluator abstractions! See the code snippet below for how one can use these abstractions to perform their respective evaluations on a generated response to a given query.from llama_index.evaluation.multi_modal import (\n\tMultiModalRelevancyEvaluator,\n\tMultiModalFaithfulnessEvaluator\n)\nfrom llama_index.multi_modal_llm import OpenAIMultiModal\n\nrelevancy_judge = MultiModalRelevancyEvaluator(\n    multi_modal_llm=OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\",\n        max_new_tokens=300,\n    )\n)\n\nfaithfulness_judge = MultiModalRelevancyEvaluator(\n    multi_modal_llm=OpenAIMultiModal(\n        model=\"gpt-4-vision-preview\",\n        max_new_tokens=300,\n    )\n)\n\n# Generated response to a query and its retrieved context information\nquery = ...\nresponse = ...\ncontexts = ...  # retrieved text contexts\nimage_paths = ...  # retrieved image contexts\n\n# Evaluations\nrelevancy_eval = relevancy_judge.evaluate(\n query=query,\n response=response,\n contexts=contexts,\n image_paths=image_paths\n)\n\nfaithfulness_eval = faithfulness_judge.evaluate(\n query=query,\n response=response,\n contexts=contexts,\n image_paths=image_paths\n)A Few Important RemarksFirst, it is worth mentioning that using LLMs or LMMs to judge generated responses has its drawbacks. These judges are generative models themselves and can suffer from hallucinations and other inconsistencies. Though studies have shown that strong LLMs can align to human judgments at a relatively high rate (Zheng et al., 2023), using them in production systems should be handled with higher standards of care. At time of writing, there has no been study to show that strong LMMs can also align well to human judgements.Secondly, the evaluation of a generator touches mostly on the evaluation of its knowledge and reasoning capabilities. There are other important dimensions on which to evaluate LLMs and LMMs, including Alignment and Safety \u2014 see Evaluating LMMs: A Comprehensive Survey for more information.Go forth and evaluateIn this post, we covered how evaluation can be performed on multi-modal RAG systems. We believe that separating out the retrieval evaluations per modalities for increased visibility as well as the LMM-As-A-Judge represent a sensible extension of the evaluation framework for text-only RAG. We encourage you to check out our practical notebook guides as well as docs for more information on how you can not only build Multi-Modal RAGs but also adequately evaluate them!Notebook guide for evaluating Multi-Modal RAG systems with LlamaIndexIntro to Multi-Modal RAGDocs/guides on Multi-Modal Abstractions",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"
            ],
            "link": "https://www.llamaindex.ai/blog//evaluating-multi-modal-retrieval-augmented-generation-db3ca824d428"
        },
        {
            "date": "Jan 30, 2024",
            "author": "Cobus Greyling",
            "title": "Agentic RAG With LlamaIndex",
            "tags": [
                "ai",
                "artificial-intelligence",
                "large-language-models",
                "conversational-ai",
                "prompt-engineering"
            ],
            "content": "\n  The topic of Agentic RAG explores how agents can be incorporated into existing\n  RAG pipelines for enhanced, conversational search and retrieval.\n\nIntroduction\n\nConsidering the architecture below, it is evident how Agentic RAG creates\n      an implementation which easily scales. New documents can be added with\n      each new set being managed by a sub-agent.\n\n\n\n\n\n  The basic structure of LlamaIndex\u2019s approach called Agentic RAG is shown in\n  the diagram below where a large set of documents are ingested, in this case it\n  was limited to 100.\n\n\n  The large corpus of data is broken up into smaller documents. An agent is\n  created for each document, and each of the numerous document agents have the\n  power of search via embeddings and to summarise the response.\n\n\n  A top-level agent is created over the set of document agents. The meta-agent /\n  top-level agent performs tool retrieval and then uses Chain-of-Thought to\n  answer the user\u2019s question.\n\n\n  The Rerank endpoint computes a relevance score for the query and each\n  document, and returns a sorted list from the most to the least relevant\n  document.\n\n\n\n\nNotebook Example\n\n  Here you will find a Colab\n  notebook\n  with a fully working and executed example of this\n  implementation.\n\nTo run the notebook, you will need an OpenAI and Cohere API key\u2026\nimport osimport openaiimport cohereos.environ[\"OPENAI_API_KEY\"] = \"Your API Key goes here\"os.environ[\"COHERE_API_KEY\"] = \"Your API Key goes here\"\nFor each document agent, the system prompt is:\nYou are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\nThe system prompt for the meta-agent /top-agent:\n You are an agent designed to answer queries about the documentation. Please always use the tools provided to answer a question. Do not rely on prior knowledge\nWorking Example\nI asked the agent the following question:\n\nTell me about LlamaIndex connectors\n\nBelow is the detailed answer generated by the agent:\n\nThought: I need to use a tool to help me answer the question.\n\n\nAction: tool_latest_search Action Input: {\u2018input\u2019: \u2018LlamaIndex\n      connectors\u2019}\n\nAdded user message to memory: LlamaIndex connectors\n\n  === Calling Function === Calling function: vector_tool_latest_search\n  with args: { \u201cinput\u201d: \u201cLlamaIndex connectors\u201d } Got output:\n  The LlamaIndex connectors can be found on the\n  /content/docs.llamaindex.ai/en/latest/search.html page.\n  ========================\n\n\nObservation: The LlamaIndex connectors are not specifically mentioned in\n      the `latest_search.html` part of the LlamaIndex docs. You may need to\n      check other sections of the documentation for detailed information about\n      LlamaIndex connectors.\n\n\nThought: I need to search for information about LlamaIndex connectors in\n      a different tool.\n\n\nAction: tool_latest_index Action Input: {\u2018input\u2019: \u2018LlamaIndex\n      connectors\u2019}\n\nAdded user message to memory: LlamaIndex connectors\n\n  === Calling Function === Calling function: vector_tool_latest_index with\n  args: { \u201cinput\u201d: \u201cLlamaIndex connectors\u201d }\n\n\n  Got output: LlamaIndex provides data connectors that allow you to ingest your\n  existing data from various sources and formats. These connectors can work with\n  APIs, PDFs, SQL, and more. By using LlamaIndex connectors, you can easily\n  bring your data into the LlamaIndex ecosystem and make it accessible for\n  natural language access and retrieval. ========================\n\n\nObservation: LlamaIndex offers data connectors that enable you to import\n      your existing data from a variety of sources and formats. These connectors\n      are compatible with APIs, PDFs, SQL, and more. Utilizing LlamaIndex\n      connectors allows you to seamlessly integrate your data into the\n      LlamaIndex ecosystem, making it readily available for natural language\n      access and retrieval.\n\n\nThought: I can answer without using any more tools.\n\n\nAnswer: LlamaIndex connectors are used to import existing data from\n      various sources and formats into the LlamaIndex ecosystem. These\n      connectors are compatible with APIs, PDFs, SQL, and more, allowing\n      seamless integration of data for natural language access and\n      retrieval.\n\nBelow is a snipped from the Colab notebook:\n\n\n\nSource\n\n\n\n  This complex implementation from LlamaIndex is an example of multi-document\n  agents which can:\n\n\nSelect documents relevant to a user query\n\n    Execute an agentic loop over the documents relevant to the query; including\n    chain-of-thought, summarisation and reranking.\n  \n\nIn Conclusion\nThis implementation by LlamaIndex illustrates a few key principles\u2026\n\n\n    Agentic RAG, where an agent approach is followed for a RAG implementation\n    adds resilience and intelligence to the RAG implementation.\n  \nIt is a good illustration of multi-agent orchestration.\n\n    This architecture serves as a good reference framework of how scaling an\n    agent can be optimised with a second tier of smaller worker-agents.\n  \n\n    Agentic RAG is an example of a controlled and well defined\n    autonomous\nagent\n    implementation.\n  \n\n    One of the most sought-after enterprise LLM implementation types are RAG,\n    Agentic RAG is a natural progression of this.\n  \n\n    It is easy to envisage how this architecture can grow and expand over an\n    organisation with more sub bots being added.\n  \n\n\n\u2b50\ufe0f Follow me on LinkedIn for updates on Large Language Models \u2b50\ufe0f\n\n",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//agentic-rag-with-llamaindex-2721b8a49ff6"
        },
        {
            "date": "Dec 5, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201312\u201305",
            "tags": [
                "llamaindex",
                "llm",
                "rag",
                "newsletter",
                "ai"
            ],
            "content": "Hello Llama Community \ud83e\udd99,We are excited to collaborate with DeepLearningAI and TruEraAI to launch an extensive course on advanced Retrieval-Augmented Generation (RAG) and its evaluations. The course includes Sentence Window Retrieval, Auto-merging Retrieval, and Evaluations with TruLensML, providing practical tools for enhanced learning and application. To make the most of this learning opportunity, we invite you to take the course.We appreciate your support and are always excited to see your projects and videos. Feel free to share them at news@llamaindex.ai. Also, remember to subscribe to our newsletter on our website for the latest updates and to connect with our vibrant community.\ud83e\udd29 First, the highlights:Launch of Seven Advanced Retrieval LlamaPacks: Simplifies building advanced RAG systems to nearly a single line of code, offering techniques like Hybrid Fusion and Auto-merging Retriever. Tweet.Introduction of the OpenAI Cookbook: A comprehensive guide for evaluating RAG systems with LlamaIndex, covering system understanding, building, and performance evaluation. Blog, NotebookSpeed Enhancement in Structured Metadata Extraction: Achieved 2x to 10x faster processing in extracting structured metadata from text, boosting RAG performance. Docs, Tweet.We launched versions 3 of RAGs, our project that lets you use natural language to generate a RAG bot customized to your needs. This version incorporates web search, so your bot can incorporate answers fresh from the web. Tweet.Core guide for Full-Stack LLM App Development: Simplifies complex app development with tools like \u2018create-llama\u2019 for full-stack apps, \u2018SEC Insights\u2019 for multi-document processing, and \u2018LlamaIndex Chat\u2019 for chatbot customization.\u2728 Feature Releases and Enhancements:We\u2019ve launched seven advanced retrieval LlamaPacks, serving as templates to easily build advanced RAG systems. These packs simplify the process to almost a single line of code, moving away from the traditional notebook approach. The techniques include Hybrid Fusion, Query Rewriting + Fusion, Retrieval with Embedded Tables, Auto-merging Retriever, Sentence Window Retriever, Node Reference Retriever, and Multi-Document Agents for handling complex queries. Tweet.We introduce new abstractions for structured output extraction in multi-modal settings, enabling the transformation of images into structured Pydantic objects. This enhancement is particularly useful for applications like product reviews, restaurant listings, and OCR. Notebook, Tweet.We introduce the OpenAI Cookbook, a guide focused on evaluating RAG systems using LlamaIndex. It encompasses understanding RAG systems, building them with LlamaIndex, and evaluating their performance in retrieval and response generation. Blog, Notebook, Tweet.We launched RAGs v3 \u2014 a bot that transcends traditional limits by incorporating web search capabilities. This bot, designed to operate in natural language rather than code, offers an enhanced experience compared to the combination of ChatGPT and Bing. Leveraging our integration with Metaphor Systems \u2014 a search engine tailored for Large Language Models (LLMs) \u2014 the bot can retrieve relevant text from the internet to provide answers beyond its internal corpus. Additionally, users can now view the tools the agent uses, with the web search feature exclusively accessible to our OpenAI agent. Repo, Tweet.We have significantly improved the speed of extracting structured metadata (like titles and summaries) from text to enhance RAG performance. Our new implementation offers 2x to 10x faster processing, overcoming the limitations of previous slower methods. Docs, Tweet.We have made it incredibly easy to set up a RAG + Streamlit app, now possible with just a single line of code using our StreamlitChatPack. This pack provides a ready-to-use RAG pipeline and a Streamlit chat interface, customizable in terms of data sources and retrieval algorithms. Docs, Tweet.\ud83d\udc40 Demo:AInimal Go \u2014 an innovative multi-modal app inspired by Pokemon-Go. This interactive application, developed by Harshad Suryawanshi, lets users capture or upload images of animals, classify them using the ResNet-18 model, and engage in conversations with the animals, augmented by a knowledge base of over 200 Wikipedia articles. Notably, the app employs a targeted ResNet model for classification, offering enhanced speed and cost efficiency, instead of using GPT-4V.Blog, Repo, HuggingFace Space, Tweet.\ud83d\uddfa\ufe0f Guides:We introduce a core guide within the LlamaIndex ecosystem, designed to simplify \u201cfull-stack\u201d app development, which is notably more complex than notebook development. This includes \u2018create-llama\u2019 for building full-stack apps with advanced templates, \u2018SEC Insights\u2019 for multi-document handling of over 10,000 filings, and \u2018LlamaIndex Chat\u2019 for a customizable chatbot experience. All tools are open-source with full guides and tutorials available.Guide on using the Table Transformer model with GPT-4V for advanced RAG applications in parsing tables from PDFs: Our method involves CLIP for page retrieval, Table Transforms for table image extraction, and GPT-4V for answer synthesis. This approach is compared with three other multi-modal table understanding techniques, including using CLIP for whole page retrieval, text extraction and indexing with GPT-4V, and OCR on table images for context.Guide on analyzing various multi-modal models for their ability to extract structured data from complex product images on an Amazon page. The models compared include GPT-4V, Fuyu-8B, MiniGPT4, CogVLM-4, and LLaVa-13B. Key findings reveal that all models incorrectly identified the number of reviews (correct answer: 5685), only GPT-4V and Fuyu accurately determined the price, each model\u2019s product description varied from the original, and Mini-GPT4 incorrectly assessed the product rating.\u270d\ufe0f Tutorials:Jo Kristian Bergum blog post on Hands-On RAG guide for personal data with Vespa and LLamaIndex.Wenqi Glantz made a tutorial on Llama Packs: The Low-Code Solution to Building Your LLM Apps.Liza Shulyayeva\u2019s in-depth tutorial on building and deploying a retrieval-augmented generation (RAG) app to conversationally query the contents of your video library\ud83c\udfa5 Webinars:Webinar on PrivateGPT \u2014 Production RAG with Local Models.\ud83c\udfc6 Hackathons:Your reminder that there\u2019s still time to join the TruEra Challenge, an online hackathon from Dec 1st to 8th, and explore AI observability with technology from TruEra AI and Google Vertex AI. Use the LlamaIndex framework to enhance your LLM-based app. Participants receive $30 in Google Cloud credits, plus an additional $100 upon solution submission. Winners share a $9,000 cash prize pool and $14,000 in Google Cloud credits.We partnered with Zilliz Universe to participate in their Advent of Code event. This December, explore 25 open-source projects, with daily challenges to build something in 30 minutes or less. It\u2019s a great opportunity to learn new skills and have winter fun. For tips, tutorials, and resources, visit the Advent of Code channel in Discord each day.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-12-05-faf5ab930264"
        },
        {
            "date": "Feb 24, 2024",
            "author": "Sharon Tan",
            "title": "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
            "tags": [
                "llamaindex",
                "mental-health",
                "retrieval-augmented",
                "generative-ai-use-cases",
                "llm"
            ],
            "content": "Co-authored by: Riya Jagetia,\u00a0Tarun Malik,\u00a0Divija N,\u00a0Sharon Tan,\u00a0Zehra Rizvi, Amanda PiyapaneeAt the recent LlamaIndex RAG-a-thon [1], our team\u2019s \u201cCounselor Copilot\u201d won 2nd prize in the Traditional track and 1st prize in the Datastax/AstraDB category. More details can be found on our DevPost [2] writeup.IntroductionAgainst the backdrop of growing strain on mental health services [3, 4], non-profit organizations like The Trevor Project [5] are a critical part of the care ecosystem. Focusing on helping LGBTQ+ youth who are contemplating suicide, The Trevor Project provides accessible crisis services including via TrevorText, an online chat service with trained volunteer counselors.Problem: The Dual Challenge Faced by Crisis CounselorsHowever, TrevorText counselors face significant challenges. Not only is there high demand for counselors during busy times like holidays and night shifts, but also, counselors have to juggle a number of administrative tasks such as sifting through forms, responding to messages across multiple chats, and locating relevant local resources. This not only increases the risk of counselors burning out but also hampers their ability to provide timely and effective care.In light of these challenges, there\u2019s a pressing need for innovative solutions to bridge the gap between the demand and supply of crisis services.While our hackathon project focused on augmenting TrevorText, our product can be easily extended to general crisis chat alternatives as well.The Winning Solution: An AI Copilot for Crisis CounselorsCounselor Copilot is a real-time assistant for crisis counselors that takes into account contact context and chat history to suggest replies so that counselors can focus on what they do best: providing care. There is no prompting that is needed from counselors; the copilot works seamlessly in the background.Further, the copilot never directly replies to contacts; instead, replies are suggested and can be edited.Counselor copilot takes into account contact context and chat history to provide real-time reply suggestions to the counselorsSpecifically, the copilot automates counselor tasks that include but are not limited to:Retrieving and synthesizing contact data from complex PDFs in real-time. This also provides counselors context on their contacts when conversations are initiated.Assessing from the chat context if emergency intervention is required. If so, suggesting escalation to a supervisor.Using existing resources and guidelines from the organization to suggest appropriate replies.4. Searching for location-specific resources for contacts, and quickly sharing those resources via email.5. Completing case forms in a CRM for contacts, including summarizing the interaction.While these tasks are important and necessary, they pull attention away from conversations with youth in crisis and take up precious time.With Counselor copilot, these tasks are completed when they are required and without any prompting from counselors, providing more bandwidth for counselors and ultimately leading to higher-quality conversations with patients.Below is a demo of our solution:How we built itWhen the chat is initiated, the Counselor Copilot gets the contact\u2019s data from the CRM, which is stored in complex PDFs. We used LlamaParse to extract relevant contact data in real-time and then provide a summary of that data to counselors as context at the beginning of each conversation.Further, we used a LlamaIndex ReAct Agent to monitor the conversation and \u2014 based on the chat history and contact context \u2014 deploy the right tool. Tools at the ReAct Agent\u2019s disposal include:Escalating the conversation to a supervisorSuggesting a response and related resources based on The Trevor Project\u2019s guidelinesSearching the web for location-specific resources and sending the resources to the contactFor tool #2, we created a vector database that contains The Trevor Project\u2019s documents, which highlight key guidelines for counselors based on different scenarios and situations that they may face. We used RAG to retrieve resources relevant to the conversation, and GPT4 to draft a response for the counselor based on those resources, both of which are essential due to the sensitive nature of the conversation.Lastly, we used the conversation content to fill out a form with key Salesforce fields (e.g. name, age, city, state), as well as to summarize the conversation.Possible ExtensionsWe\u2019re excited by the potential for others to build on our work [6] and extend Counselor Copilot further. Some ideas include:Reduce costs and improve quality of suggested responses: Fine-tune a state-of-the-art open-source LLM on extracts of chat conversations conducted by counselorsMore targeted conversation management: Add a tool for the agent to assess the stage of the conversation, given that there are recommended styles and questions for each stage (e.g. establishing rapport, risk assessment)Closed-loop feedback cycle: Allow counselors to thumbs-up or thumbs-down selected responses, as a natural way to collect human feedback which can be used for further model or agent trainingConclusion: A Step Toward Efficient and Effective Crisis CareOur AI copilot for crisis counselors represents a significant step toward more efficient and effective crisis care. By automating administrative tasks, it frees up counselors to focus on their core mission of providing youth in crisis a safe place to talk. This not only enhances the quality of care provided but also addresses the pressing issue of counselor shortage by maximizing the impact of existing resources. As we continue to refine and expand this technology, we envision a future where crisis counseling is more accessible, responsive, and impactful for all those in need\u200b\u200b.Referenceshttps://rag-a-thon.devpost.com/https://devpost.com/software/counselor-copilothttps://www.mhanational.org/issues/state-mental-health-americahttps://www.aamc.org/news/growing-psychiatrist-shortage-enormous-demand-mental-health-serviceshttps://www.thetrevorproject.org/https://github.com/zrizvi93/trevorhack",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3"
        },
        {
            "date": "Apr 16, 2024",
            "author": "Apr 16, 2024",
            "title": "LlamaIndex Newsletter 2024-04-16",
            "tags": [],
            "content": "Hello, LlamaIndex Family! \ud83e\udd99Welcome to another thrilling weekly update from LlamaGalaxy! We're excited to bring you a variety of outstanding updates, including the Chain of Abstraction LlamaPack, create-tsi, demos, guides, tutorials, and much more.Before we delve into these updates, we have an exciting tutorial series on Agents and Tools for you to check out. Perfect for beginners, this series covers everything from advanced QA/RAG implementations to step-wise execution. By the end, you\u2019ll have gained a deeper understanding of how to use agent reasoning with tool use to build simple applications. Check them out:OverviewReAct agentsFunction Calling agentsRetrieval-Augmented agentControlling tool outputsAgents with step-by-step execution\ud83e\udd29\u00a0The highlights:Chain of Abstraction LlamaPack: Chain of Abstraction technique as llamapack a method enabling multi-step reasoning for enhanced tool use introduced by Silin Gao's team. LlamaPack, Tweet.Create-tsi Toolkit: Launched a toolkit for building full-stack RAG applications with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers. Code, Tweet.Improved Agent Control: return_direct feature in tools allows direct output returns, reducing costs and enhancing response efficiency. Docs, Tweet.\u2728 Feature Releases and Enhancements:We have introduced the Chain of Abstraction Technique Developed by Silin Gao, and team as LlamaPack, this new method enables LLMs to generate multi-step reasoning chains for efficient sequence planning, enhancing tool use beyond single-shot functions. LlamaPack, Tweet.We have launched create-tsi: A toolkit in collaboration with T-Systems and Marcus Schiesser to generate GDPR-compliant, full-stack AI applications via a CLI interface. Build enterprise-grade RAG bots with customizable features like web crawling, local file indexing, and multilingual support, all hosted in EU data centers. Code, Tweet.We have introduced return_direct feature in tools that enhances agent controllability by allowing direct output returns as final responses. This optimizes for reduced latency and costs, and effectively halts the agent after crucial actions like booking confirmations or answering queries. Docs, Tweet.\ud83c\udfa5\u00a0Demos:RAG-enhanced MetaGPT: A robust multi-agent framework that features structured team dynamics for problem-solving, now supercharged with domain-specific knowledge from LlamaIndex modules. This framework supports diverse data inputs, sophisticated retrieval options, and efficient data management for enhanced agent performance.\ud83d\uddfa\ufe0f Guides:Guide to Building and Evaluating Advanced RAG by Hamza Gharbi for setting up a basic RAG pipeline, defining custom evaluation functions, and optimizing retrieval techniques.Paper by Prof. Markus J. Buehler: Using LLM-Generated Knowledge Graphs to Accelerate Biomaterials Discovery - This study showcases how a comprehensive knowledge graph from over 1000 scientific papers reveals novel insights and connections, driving innovation in biomaterials through art as inspiration. The KG construction was done with the help of LlamaIndex modules.Guide to Full-Stack RAG Application with AWS Bedrock: Set up Bedrock embeddings, use LlamaIndex for PDF retrieval, and build an interactive Streamlit interface, an ideal resource for enterprises starting with AWS services.Guide to Building a Lightweight ColBERT Retrieval Agent: Learn how to create an agent capable of advanced document retrieval and maintaining conversation memory, without the complexity of heavyweight agent frameworks.Guide to the Best RAG Techniques: 'ARAGOG' paper by Matous Eibich is a comprehensive evaluation survey exploring various RAG methods from classic vector databases to LlamaIndex's advanced techniques. Key findings highlight the effectiveness of HyDE, LLM reranking, and sentence window retrieval for improving precision and answer similarity.\u270d\ufe0f Tutorials:Akash Mathur\u2019s tutorial on Data Management in LlamaIndex: Featuring LlamaCloud and its open-source counterpart, this tutorial showcases efficient live data handling to minimize costs and latency in LLM applications.Leonie\u2019s interactive tutorial to create an app that lets you converse with code from a GitHub repository.kingzzm\u2019s tutorial on enhancing RAG Performance to overcome the issue of 'broken' context in RAG construction by dynamically creating contiguous chunks with auto-merging retrieval.Activeloop\u2019s tutorial on Multimodal RAG for Pill Search teaches how to identify pills using images and text. This helps in identifying unknown pills, checking drug interactions and side effects, and confirming proper dosage amounts.Fanghua Yu's tutorial on using LlamaParse for Knowledge Graph Creation from Documents.\ud83c\udfa5\u00a0Webinars:Webinar with Tianjun Zhang and Shishir Patil, the two lead co-authors of RAFT, where they presented RAFT and discussed fine-tuning and RAG.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-04-16"
        },
        {
            "date": "Jan 30, 2024",
            "author": "bstadt",
            "title": "Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex",
            "tags": [],
            "content": "What is a Retriever?Recently, retrieval augmented generation (RAG) has enabled language models to reduce hallucinations, improve response quality, and maintain up-to-date knowledge of the world without requiring retraining of the model itself. This is done by equipping a language model with a retriever and a database. At inference time, a RAG system uses the retriever to select relevant documents from the database, and passes them to the language model context window.Today, the most popular type of retriever is based on an embedding model. This embedding model converts all of the documents in the database to a vector representation. Then, at inference time, it converts the query to a vector representation, and retrieves the most similar documents to the query vector from the database.In this post, we are going to show you how to build a fully open source retriever using LlamaIndex and Nomic Embed, the first fully open source embedding model to exceed OpenAI Ada performance on both short and long context benchmarks.Why Open Source?As AI becomes deployed in increasingly high impact domains, such as defense, medicine, and finance, end-to-end auditability of the entire system becomes a key component of safe AI deployment. Unfortunately, the closed source embedding models used in most RAG systems today have deliberately obfuscated training protocols and cannot be audited.Further, as organizations adopting AI begin to mature, reliance on closed source embedding models will result in vendor lock-in and a limited ability to modify the embedding model to suit the needs of the business.Luckily, fully open source embedding models like Nomic Embed offer end-to-end auditability of the training process as well as a strong basis for further improvements and modifications of the model.How ToTo build an open source retriever with LlamaIndex and Nomic Embed, we will start by importing the relevant librariesfrom llama_index.embeddings import NomicEmbedding\nfrom llama_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)Next, we need to download some data for our database. For this example, we are going to use an essay by Paul Graham, which we download from here and place into a directory named ./data/paul_graham.Now, it\u2019s time to get the vectors for the documents in our database. To do this, we are going to use the LlamaIndex SimpleDirectoryReader and Nomic\u2019s hosted inference service. You\u2019ll have to replace <NOMIC_API_KEY> with your Nomic API key, which you can get after signing up for Nomic Atlas here.documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nnomic_api_key = \"&lt;NOMIC_API_KEY&gt;\"\nembed_model = NomicEmbedding(\n    api_key=nomic_api_key,\n    model_name=\"nomic-embed-text-v1\",\n    task_type=\"search_document\"\n)\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model, chunk_size=1024,\n)\nindex = VectorStoreIndex.from_documents(\n    documents=documents, service_context=service_context, show_progress=True\n)Notice that we set task_type to search_document in NomicEmbedding. Nomic Embed supports many different types of tasks, and search_document is optimized for building representations of documents for RAG databases.Once our database is set up, we are ready to build our retriever. Using LlamaIndex, this is as simple as a few lines of python:embed_model = NomicEmbedding(\n    api_key=nomic_api_key,\n    model_name=\"nomic-embed-text-v1\",\n    task_type=\"search_query\"\n)\n\nservice_context = ServiceContext.from_defaults(\n    embed_model=embed_model\n)\n\nsearch_query_retriever = index.as_retriever(service_context=service_context, similarity_top_k=1)Again, notice that we are using a new NomicEmbedding model with task_type set to search_query. This task type is optimized for embedding queries for search over a retrieval database.Finally, we can use our retriever to surface relevant documents given user queries! As an example:retrieved_nodes_nomic = retriever_nomic.retrieve(\n    \"What software did Paul write?\"\n)returns a document that describes Paul\u2019s first programs:Node ID: 380fbb0e-6fc1-41de-a4f6-3f22cd508df3\nSimilarity: 0.6087318771843091\nText: What I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u2014 CPU, disk drives, printer, card reader \u2014 sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\nI was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n\nWith microcomputers, everything changed. Conclusion & Next StepsIn this post, we showed you how to build a fully open source retriever using Nomic Embed and LlamaIndex. If you want to dive deeper, you can find the source code for Nomic Embed here. You can also use Nomic Atlas to visualize your retrieval database, and LlamaIndex to connect it to a generative model for full RAG.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//building-a-fully-open-source-retriever-with-nomic-embed-and-llamaindex-fc3d7f36d3e4"
        },
        {
            "date": "Jan 16, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201301\u201316",
            "tags": [
                "llm",
                "newsletter",
                "ai",
                "rag",
                "hackathon"
            ],
            "content": "Hello LlamaIndex Enthusiasts \ud83e\udd99,Get ready for an exciting week at LlamaIndex, teeming with dynamic community contributions and insightful learning resources. Dive into our range of new features, tutorials, guides, and events, all designed to enhance your LlamaIndex journey.We\u2019re excited to announce our very first in-person hackathon, scheduled for February 2nd-4th. Join us to connect with fellow RAG enthusiasts and compete for prizes totaling over $4,000!If you\u2019ve been working on a fascinating project, penned an insightful article, or produced an engaging video, we\u2019re eager to see it! Share your contributions with us at news@llamaindex.ai. Don\u2019t forget to subscribe to our newsletter on our website to receive all the latest updates directly in your inbox.\ud83e\udd29 The highlights:Chain-of-Table: Step-by-step table reasoning and operations for enhanced LLM tabular data understanding. LlamaPack, Tweet.LLM Self-Consistency: Merges textual and symbolic reasoning with majority voting for precise answers. LlamaPack, Tweet.Semantic Text Splitting in RAG: Greg Kamradt\u2019s embedding similarity method for efficient document splitting. LlamaPack, Tweet.Parallel RAG Ingestion: Up to 15x faster document processing in LlamaIndex. Notebook, Tweet.TogetherAI\u2019s Embeddings Support: Guide to build retrieval-augmented apps with MistralAI\u2019s 8x7b model and TogetherAI Embeddings. Blogpost , Tweet.\u2728 Feature Releases and Enhancements:We launched Chain-of-Table Framework in LlamaPack for LLM Tabular Data Understanding. This approach enables step-by-step table reasoning and operations like adding columns, row selection, grouping, and sorting, mimicking a data scientist\u2019s method for concise data representation. LlamaPack, Tweet.We launched LLM Self-Consistency Mechanism for Tabular Data in LlamaPack. This method combines textual and symbolic reasoning, utilizing a novel mix self-consistency approach with majority voting to select the best answer. LlamaPack, Tweet.We have Introduced Semantic Text Splitting in RAG with LlamaPack. Check Greg Kamradt\u2019s method of splitting documents based on embedding similarity between sentences. This auto-tuned threshold approach enhances RAG pipelines, soon to be available in LlamaPack using LlamaIndex abstractions. LlamaPack, Tweet.We launched Parallel RAG Ingestion in LlamaIndex for up to 15x Faster Document Processing. Notebook, Tweet.We have launched Support for TogetherAI\u2019s Embeddings Endpoint. Check the blog for a step-by-step guide on creating a retrieval-augmented generation app with MistralAI\u2019s 8x7b model and TogetherAI Embeddings. Blogpost , Tweet.We integrated AgentSearch-v1 as a data loader and Retriever in LlamaHub, offering a robust alternative for internet content search/retrieval without relying on Bing/Google APIs. LlamaPack, Tweet.Raduaschl introduced Ensembling and Fusion in Advanced RAG with LlamaPack. Learn to build an ensembling + fusion pipeline in about 30 lines of code using QueryPipeline syntax, featuring full async support. LlamaPack, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to Building Full-Stack RAG Applications with LlamaIndex and Azure Cosmos DB.Guide showing to combine auto-retrieval for semi-structured retrieval with metadata with MMR to enforce diversity in results.Guide by MountainMicky to understanding the Importance of Reranking in Advanced RAG Pipelines.\u270d\ufe0f Tutorials:Andrej Baranovskij tutorial on Transforming Invoice Data into JSON with LlamaIndex and Pydantic.NVIDIA tutorial on Building AI apps with local LLMs running on Windows with LlamaIndex and NVIDIAHarshad Suryawanshi tutorial on AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5.\ud83c\udfa5 Events:Ravi Theja gave talk on Building Multi-Tenancy RAG System with LlamaIndex and Qdrant at FOSS United, Bangalore, India.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-01-16-752195bed96d"
        },
        {
            "date": "Aug 17, 2023",
            "author": "Jerry Liu",
            "title": "Easily Finetune Llama 2 for Your Text-to-SQL Applications",
            "tags": [
                "llamaindex",
                "fine-tuning",
                "large-language-models",
                "ai",
                "nlp"
            ],
            "content": "Llama 2 is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the Hugging Face Open LLM Leaderboard. Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.Context: Llama-2\u20137B is Not Good at Text-to-SQLA downside of the smallest Llama 2 model (7B parameters), however, is that it\u2019s not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response:Here we plugged in a sample entry from the sql-create-context dataset.input: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)Meanwhile, here is the generated output vs. correct output:Generated output: SELECT * FROM `table_name_8` WHERE '1980' = YEAR AND TEAM = \"Boston Celtics\" ORDER BY OVERALL_PICK DESC LIMIT 1;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year = 1981 AND overall_pick = \"148\"This is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.This is exactly where fine-tuning comes in \u2014 given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g. how LoRA works).Once the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on \u201cin-context learning\u201d and \u201cretrieval-augmentation\u201d use cases \u2014 freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.Tutorial OverviewIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of LlamaIndex.Here is the stack that we use:b-mc2/sql-create-context from Hugging Face datasets as the training datasetOpenLLaMa open_llama_7b_v2 as the base modelPEFT for efficient finetuningModal for handling all cloud compute/orchestration for finetuning. And also for the excellent reference doppel-bot repo.LlamaIndex for text-to-SQL inference against any SQL database.Special mention to the awesome Llama 2 tutorial from Anyscale that helped to inspire this project.All of our materials can be found in our Github repo: https://github.com/run-llama/modal_finetune_sql (again emphasizing that this is adapted from doppel-bot). Also, the full tutorial can be found in our Jupyter notebook guide. Make sure to check it out!As mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don\u2019t cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.There are also certainly higher-level APIs that we could\u2019ve used (e.g. OpenAI, Lamini) in order to achieve this task. There\u2019s plenty of room for followup tutorials to cover these topics!Step 1: Loading Training Data for Finetuning LLaMaThe first step here is to open up the Jupyter notebook. The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.Our code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don\u2019t contain Python blocks of their own.First we use Modal to load in the b-mc2/sql-create-context dataset. This is a simple task that just loads in the dataset and formats it into a .jsonl file.modal run src.load_data_sql --data-dir \"data_sql\"As we can see, under the hood the task is quite straightforward:# Modal stubs allow our function to run remotely\n@stub.function(\n    retries=Retries(\n        max_retries=3,\n        initial_delay=5.0,\n        backoff_coefficient=2.0,\n    ),\n    timeout=60 * 60 * 2,\n    network_file_systems={VOL_MOUNT_PATH.as_posix(): output_vol},\n    cloud=\"gcp\",\n)\ndef load_data_sql(data_dir: str = \"data_sql\"):\n    from datasets import load_dataset\n\n    dataset = load_dataset(\"b-mc2/sql-create-context\")\n\n    dataset_splits = {\"train\": dataset[\"train\"]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n\n    for key, ds in dataset_splits.items():\n        with open(out_path, \"w\") as f:\n            for item in ds:\n                newitem = {\n                    \"input\": item[\"question\"],\n                    \"context\": item[\"context\"],\n                    \"output\": item[\"answer\"],\n                }\n                f.write(json.dumps(newitem) + \"\\n\")Step 2: Run Finetuning ScriptThe next step is to run our finetuning script on the parsed dataset.modal run src.finetune_sql --data-dir \"data_sql\" --model-dir \"model_sql\"The finetuning script performs the following steps.Splits the dataset into training and validation splitstrain_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)Formats each split into tuples of (input prompt, label): The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt \u2014 this allows the model to train on next-token prediction.def generate_and_tokenize_prompt(data_point):\n  full_prompt = generate_prompt_sql(\n      data_point[\"input\"],\n      data_point[\"context\"],\n      data_point[\"output\"],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n  if not train_on_inputs:\n      raise NotImplementedError(\"not implemented yet\")\n  return tokenized_full_promptThe input prompt is the exact same as what was given at the top of this blog.When the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not specified).Step 3: EvaluationThe model has been finetuned and can be served from the cloud. We can run some basic evaluations using sample data from sql-create-context to compare the performance of the finetuned model vs. the baseline Llama 2 model.modal run src.eval_sql::mainThe results demonstrate a massive improvement for the finetuned model:Input 1: {'input': 'Which region (year) has Abigail at number 7, Sophia at number 1 and Aaliyah at number 5?', 'context': 'CREATE TABLE table_name_12 (region__year_ VARCHAR, no_5 VARCHAR, no_7 VARCHAR, no_1 VARCHAR)', 'output': 'SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"sophia\" AND\nno_5 = \"aaliyah\"'}\nOutput 1 (finetuned model): SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"aaliyah\" AND no_5 = \"sophia\"\nOutput 1 (base model): SELECT * FROM table_name_12 WHERE region__year = '2018' AND no_5 = 'Abigail' AND no_7 = 'Sophia' AND no_1 = 'Aaliyah';\n\n\nInput 2: {'input': 'Name the result/games for 54741', 'context': 'CREATE TABLE table_21436373_11 (result_games VARCHAR, attendance VARCHAR)', 'output': 'SELECT result_games FROM table_21436373_11 WHERE attendance = 54741'}\nOutput 2 (finetuned model): SELECT result_games FROM table_21436373_11 WHERE attendance = \"54741\"\nOutput 2 (base model): SELECT * FROM table_21436373_11 WHERE result_games = 'name' AND attendance &gt; 0;Whereas the base model produces wrongly formatted outputs, or incorrect SQL statements,the finetuned model is able to produce outputs that are much closer to that of the expected output.Step 4: Integrating the Finetuned Model with LlamaIndexWe can now use this model in LlamaIndex for text-to-SQL over any database.We first define a test SQL database that we can then use to test the inference capabilities of the model.We create a toy city_stats table that contains city name, population, and country information, and populate it with a few sample cities.db_file = \"cities.db\"\nengine = create_engine(f\"sqlite:///{db_file}\")\nmetadata_obj = MetaData()\n# create city SQL table\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all(engine)This is stored in a cities.db file.We can then use Modal to load both the finetuned model and this database file into the NLSQLTableQueryEngine in LlamaIndex - this query engine allows users easily start performing text-to-SQL over a given database.modal run src.inference_sql_llamaindex::main --query \"Which city has the highest population?\" --sqlite-file-path \"nbs/cities.db\" --model-dir \"model_sql\" --use-finetuned-model TrueWe get a response like the following:SQL Query: SELECT MAX(population) FROM city_stats WHERE country = \"United States\"\nResponse: [(2679000,)]ConclusionAnd that\u2019s basically it! This tutorial provides a very high-level way for you to get started finetuning a Llama 2 model on generating SQL statements, and showcases end-to-end how you can plug it into your text-to-SQL workflows with LlamaIndex.ResourcesFor the sake of completeness we\u2019re linking all of our resources again here.Tutorial repo: https://github.com/run-llama/modal_finetune_sql (adapted from doppel-bot).Jupyter notebook guide.Stack:[b-mc2/sql-create-context from Hugging Face datasets](https://huggingface.co/datasets/b-mc2/sql-create-context)OpenLLaMaPEFTModal (+ doppel-bot repo).LlamaIndexSpecial mention: Llama 2 tutorial from Anyscale.",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"
        },
        {
            "date": "Oct 22, 2023",
            "author": "Ravi Theja",
            "title": "NVIDIA Research: RAG with Long Context LLMs",
            "tags": [
                "llamaindex",
                "llm",
                "nvidia",
                "research-paper",
                "llama-2"
            ],
            "content": "IntroductionWhy Long Context Matters and How Retrieval Augmentation Steps In:In the dynamic landscape of LLMs, two methods have gained traction and seem to be taking center stage: expanding the context window of Large Language Models (LLMs) and enhancing these models with retrieval capabilities. The continued evolution of GPU technology, coupled with breakthroughs in attention mechanisms, has given rise to long-context LLMs. Simultaneously, the concept of retrieval \u2014 where LLMs pick up only the most relevant context from a standalone retriever \u2014 promises a revolution in efficiency and speed.In the midst of these evolving narratives, some interesting questions emerge:Retrieval-augmentation versus long context window, which one is better for downstream tasks?Can both methods be combined to get the best of both worlds?To dissect these questions, in this blog post we turn to NVIDIA\u2019s recent study, which harnesses the power of two powerful LLMs: the proprietary GPT \u2014 43B and LLaMA2\u201370B, the research strives to provide actionable insights for AI practitioners.Prior Research and the NVIDIA Divergence:Interestingly, while NVIDIA\u2019s findings are interesting in many respects, Another recent work by Bai et al. (2023) also ventured into similar territory, although with differing outcomes.Their work explored the impact of retrieval on long context LLMs, evaluating models like GPT-3.5-Turbo-16k and Llama2\u20137B-chat-4k. However, their findings diverge from NVIDIA\u2019s in crucial ways. Bai et al. discerned that retrieval was beneficial only for the Llama2\u20137B-chat-4k with a 4K context window, but not for extended context models like GPT-3.5-Turbo-16k. One hypothesis for this difference centers on the challenges tied to experiments using black-box APIs and the smaller white-box LLMs they employed, which potentially had limited capability to integrate context through retrieval.NVIDIA\u2019s work distinguishes itself by tapping into much larger LLMs, yielding results that not only match top-tier models like ChatGPT-3.5 but even indicate further enhancements when incorporating retrieval methods.Models, Datasets, and Evaluation MetricsLarge Language Models (LLMs) Explored:The researchers delved deep into the potential of large language models for tasks like generative QA and summarization. Specifically, two models were the primary focus:Nemo GPT-43B: A proprietary 43 billion parameter model trained on 1.1T tokens, 70% of which were in English. This model was fed a rich diet of web archives, Wikipedia, Reddit, books, and more. It contains 48 layers and is trained using RoPE embeddings.LLaMA2\u201370B: A publicly available 70B parameter model trained on 2T tokens, primarily in English. It\u2019s structured with 80 layers and also utilizes RoPE embeddings.Context Window Extension:To enhance the models\u2019 capability to process longer contexts, their initial 4K context window length was augmented. The GPT-43B was modified to handle 16K, while the LLaMA2\u201370B was expanded to both 16K and 32K, employing the position interpolation method.Instruction Tuning:To optimize the LLMs for the tasks at hand, instruction tuning was implemented. A diverse dataset blend, comprising sources like Soda, ELI5, FLAN, and others, was created. A consistent format template was adopted for multi-turn dialogue training, and the models were meticulously fine-tuned to accentuate the answer segment.Retrieval Models Tested:Three retrieval systems were put to the test:Dragon: A state-of-the-art dual encoder model for both supervised and zero-shot information retrieval.Contriever: Utilizes a basic contrastive learning framework and operates unsupervised.OpenAI embedding: The latest version was used, accepting a maximum input of 8,191 tokens.The retrieval approach entailed segmenting each document into 300-word sections, encoding both questions and these chunks, and then merging the most pertinent chunks for response generation.Datasets Used for Evaluation:The study employed seven diverse datasets, sourced from the Scroll benchmark and LongBench.A snapshot of these datasets includes:QMSum: A query-based summarization dataset, QMSum consists of transcripts from diverse meetings and their corresponding summaries, built upon contextual queries.Qasper: A question-answering dataset centered on NLP papers, Qasper offers a mix of abstractive, extractive, yes/no, and unanswerable questions from the Semantic Scholar Open Research Corpus.NarrativeQA: Aimed at question-answering over entire books and movie scripts, NarrativeQA provides question-answer pairs created from summaries of these extensive sources.QuALITY: A multiple-choice question answering set based on stories and articles, QuALITY emphasizes thorough reading, with half the questions designed to be challenging and require careful consideration.MuSiQue: Designed for multi-hop reasoning in question answering, MuSiQue creates multi-hop questions from single-hop ones, emphasizing connected reasoning and minimizing shortcuts.HotpotQA: Based on Wikipedia, HotpotQA requires reading multiple supporting documents for reasoning. It features diverse questions and provides sentence-level support for answers.MultiFieldQA-en: Curated to test long-context understanding across fields, MFQA uses sources like legal documents and academic papers, with annotations done by Ph.D. students.Evaluation Metrics:The research team used a wide range of metrics suited to each dataset. The geometric mean of ROUGE scores for QM, the exact matching (EM) score for QLTY, and F1 scores for others were the primary metrics.ResultsBaseline models without retrieval, having a 4K sequence length, performed poorly since valuable texts get truncated.With retrieval, performance for 4K models like LLaMA2\u201370B-4K and GPT-43B-4K significantly improved.HotpotQA, a multi-hop dataset, particularly benefits from longer sequence models.Models with longer contexts (16K, 32K) outperform their 4K counterparts even when fed the same evidence chunks.There exists a unique \u201cU-shaped\u201d performance curve for LLMs due to the lost in the middle phenomenon, making them better at utilizing information at the beginning or end of the input.The study presents a contrasting perspective to LongBench\u2019s findings, emphasizing that retrieval is beneficial for models regardless of their context window size.Comparing to OpenAI Models:The LLaMA2\u201370B-32k model with retrieval surpasses the performance of GPT-3.5-turbo variants and is competitive with Davinci-003, underscoring its robustness in handling long context tasks.Comparison of Different Retrievers:Retrieval consistently enhances the performance across different retrievers.Public retrievers outperformed proprietary ones like OpenAI embeddings.Comparing with the number of retrieved chunks:The best performance is achieved by retrieving the top 5 or 10 chunks. Retrieving more, up to 20 chunks, doesn\u2019t offer additional benefits and can even degrade performance.The deterioration in performance when adding more chunks could be due to the lost-in-the-middle phenomenon or the model being sidetracked by non-relevant information.ConclusionAs we delved deep into understanding how retrieval augmentation and long-context extension interact when applied to leading language models fine-tuned for long-context question-answering and summarization tasks. Here are some things to be noted:Boost in Performance with Retrieval: Implementing retrieval techniques significantly enhances the performance of both shorter 4K context language models and their longer 16K/32K context counterparts.Efficiency of 4K Models with Retrieval: 4K context language models, when combined with retrieval augmentation, can achieve performance levels similar to 16K long context models. Plus, they have the added advantage of being faster during the inference process.Best Model Performance: After enhancing with both context window extension and retrieval augmentation, the standout model, LLaMA2\u201370B-32k-ret (LLaMA2\u201370B-32k with retrieval), surpasses well-known models like GPT-3.5-turbo-16k and davinci-003.References:Retrieval meets long context, large language models.Longbench: A bilingual, multitask benchmark for long context understanding.We trust that this blog post on the review of the paper on retrieval augmentation with long-context LLMs has furnished you with meaningful insights. We\u2019re keen to hear if your experiments align with our findings or present new perspectives \u2014 divergent results always make for interesting discussions and further exploration.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//nvidia-research-rag-with-long-context-llms-7d94d40090c4"
        },
        {
            "date": "Dec 8, 2023",
            "author": "LlamaIndex",
            "title": "Bridging the Language Gap in Programming: Introducing AutoTranslateDoc",
            "tags": [
                "llm",
                "ai",
                "translation",
                "i18n",
                "llamaindex"
            ],
            "content": "Author: Pierre-Loic DoulcetAs programmers, we often find ourselves limited by language barriers. Documentation for various programming frameworks and tools is predominantly available in English, and increasingly in languages like Chinese, creating challenges for non-native speakers. I faced similar obstacles in my early programming days, and it was only through community efforts like traduc.org\u2019s translation of man pages that I could surmount them.Today, we are excited to unveil a solution to this pervasive issue: AutoTranslateDoc, a command-line tool designed to democratize access to technical documentation by breaking down language barriers.How AutoTranslateDoc WorksCollect the Documentation: The tool connects to GitHub, identifying and downloading .md and .mdx files from any repository.Chunk and Prepare: The documentation is then chunked or split for translation.Translate Efficiently: Utilizing the power of LLMs like GPT-3.5 and GPT-4, each chunk of documentation is translated accurately.Verify and Enhance: The translation is automatically verified, with retranslation if needed, ensuring the highest quality.Consolidate: Finally, the chunks are amalgamated back into a cohesive document.Our initial tests on translating the llamaIndexTS documentation have been highly promising. You can now read our docs in over a dozen languages including Chinese, French, and Spanish!Getting StartedInstall AutoTranslateDoc easily via npm, or clone the repo (https://github.com/run-llama/automatic-doc-translate) :npm install -g autotranslatedocTry it out with run-lama/LlamaIndexTS or your favorite repo! You will need a GitHub Personal Access Token and an OpenAI API Key (the tool will prompt you to set these):# Translate\nautotranslatedoc translate run-llama LlamaIndexTS -d apps/docs -l fr#build\nautotranslatedoc build run-llama LlamaIndexTS -d apps/docs -l frThis translates the directory apps/docs in the GitHub repo run-llama/LlamaIndexTS.Improving Accuracy and ConsistencyOur commitment to improving translation accuracy led us to innovate in both the translation process and verification methods.Strategic Document Splitting:We approach translation by dividing each page of the documentation into sections. To provide enhanced context and coherence, each section\u2019s title hierarchy is appended to its respective chunk during translation. This technique ensures that the translated content maintains the original structure and thematic relevance.Rigorous Translation Verification:Our verification process is designed to rigorously assess the accuracy of translations. We employ several checks on the translated documentation:Translation Length Check: We compare the length of the translated text with the original to ensure consistency.Title Hierarchy Analysis: We verify that no new sections are inadvertently added in the translation.Link Count Validation: The number of hyperlinks is matched against the original to ensure none are missed or added unnecessarily.Code Block Accuracy: The presence and correctness of code blocks in the translation are checked against the original document.These checks address common issues with LLMs, such as hallucination or omission, and prompt retranslation when necessary. This rigorous process significantly enhances the accuracy of our translations. Moreover, we incorporate a unique self-critique feature, where the LLM evaluates its own translation output, further refining the quality.This dual approach of meticulous chunking and thorough verification ensures that our translations are not only accurate but also contextually relevant, maintaining the integrity and utility of the original documentation.Managing Documentation Updates: Keeping Translations CurrentDocumentation, by its nature, is a dynamic entity that evolves over time. Recognizing this, we\u2019ve integrated a robust system into AutoDocTranslate to manage documentation updates efficiently.Historical Tracking through JSON:When translating a repository using our tool, a .json file is generated, chronicling the history of translations. This file is crucial for tracking changes and versions in the documentation. It serves as a foundation for differential translation, a process that identifies and translates only the newly added or modified content. This feature can be accessed through the autotranslatedoc update command, streamlining the maintenance of up-to-date translations.Future Enhancements:We are actively working on enhancing this system with the following features:Manual Change Integration: Recognizing that translations might undergo manual edits post-generation, we are developing functionality to account for these manual changes during updates. This will ensure that any human revisions are retained and only new or altered sections from the source documentation are translated in subsequent updates.GUI for Translation Management: To further simplify the process of translation editing, tracking, and verification, we\u2019re in the early stages of developing a graphical user interface (GUI). This interface will allow users to interact more intuitively with the translations. An experimental version of this feature can be accessed through the autotranslatedoc serve command. This GUI will enable users to visually navigate through the translations, make edits, and verify the accuracy of the content more efficiently.By continually updating and refining these features, AutoDocTranslate aims to stay at the forefront of making technical documentation universally accessible and easy to maintain in multiple languages.The Future of Technical DocumentationAutoDocTranslate is more than a tool; it\u2019s a step towards an inclusive, barrier-free tech world where language is no longer an impediment to learning and growth. We\u2019re excited to see how it empowers programmers across the globe.Join us in this journey and contribute to a more accessible programming community!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//bridging-the-language-gap-in-programming-introducing-autotranslatedoc-ccc93fbcd3a8"
        },
        {
            "date": "Jun 8, 2023",
            "author": "Logan Markewich",
            "title": "LlamaIndex and Transformers Agents",
            "tags": [
                "llamaindex",
                "llm",
                "hugging-face",
                "generative-ai",
                "ai"
            ],
            "content": "SummaryAgents are a popular use-case for Large Language Models (LLMs), typically provide a structure that enables LLMs to make decisions, use tools, and accomplish tasks. These agents can take many forms, like the fully-autonomous versions seen with Auto-GPT, to more controlled implementations like Langchain Agents. With the recent release of Transformers Agents, we showcase how LlamaIndex continues to be a useful tool for agents, by augmenting their existing image-generator tool. Using an vector index created from 10K DiffusionDB prompts, the Text2Image Prompt Assistant tool we created can re-write prompts to generate more beautiful images. Full source code is available in the Hugging Face Space for the tool, and a colab notebook is available as a usage walkthrough.Creating the ToolTransformers Agents come with a variety of per-configured tools that leverage the vast amounts of open-source models hosted on Hugging Face-Hub. Furthermore, additional tools can be created and shared by simply publishing a new Hugging Face Space with the proper tool setup.To create a tool, your code simply needs a tool_config.json file that describes the tool, as well as a file containing the implementation of your tool. While the documentation was a little fuzzy for this part, we eventually were able to use the implementation of existing custom tools as the framework for our own.To enable LlamaIndex to write text-to-image prompts, we need a way to show the LLM what examples of good prompts look like. To do this, we indexed 10K random text-to-image prompts from DiffusionDB.from datasets import load_dataset\nfrom llama_index import VectorStoreIndex, Document\n\n# downloads a LOT of data\ndataset = load_dataset('poloclub/diffusiondb', '2m_random_10k')\n\ndocuments = []\nfor sample in dataset['train']:\n    documents.append(Document(sample['prompt']))\n\n# create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# store index\nindex.storage_context.persist(persist_dir=\"./storage\")To get LlamaIndex to write prompts using examples, we need to customize the prompt templates a bit. You can see the final prompt templates and how to use them below:text_qa_template = Prompt(\n    \"Examples of text-to-image prompts are below: \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the existing examples of text-to-image prompts, \"\n    \"write a new text-to-image prompt in the style of the examples, \"\n    \"by re-wording the following prompt to match the style of the above examples: {query_str}\\n\"\n)\n\n\nrefine_template = Prompt(\n    \"The initial prompt is as follows: {query_str}\\n\"\n    \"We have provided an existing text-to-image prompt based on this query: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing prompt \"\n    \"(only if needed) with some more relevant examples of text-to-image prompts below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new examples of text-to-image prompts, refine the existing text-to-image prompt to better \"\n    \"statisfy the required style. \"\n    \"If the context isn't useful, or the existing prompt is good enough, return the existing prompt.\"\n)\n\nquery_engine = index.as_query_engine(\n    text_qa_template=text_qa_template, \n    refine_template=refine_template\n)\n\nresponse = query_engine.query(\"Draw me a picture of a happy dog\")Snag #1One main drawback of Transformers Agents currently is that they will only pick one tool to solve each prompt. So if we want to augment the image-generator tool, we need to replace it! In our tool implementation, we actually load the original image-generator tool and call it after running LlamaIndex to generate a new text-to-image prompt.Snag #2The next bump in our journey is how Hugging Face downloads tools from the space. Initially, it only downloading the tool_config.json file and the source code for the tool. But we also need to download the prompts we spent time indexing!To get around this, during the setup() of the tool, we call hf_hub_download() to download the files we need to load the index.Back on TrackWith the index created and the general processes figured out, the actual tool implementation is fairly straightforward.class Text2ImagePromptAssistant(Tool):\n    \n    inputs = ['text']\n    outputs = ['image']\n    description = PROMPT_ASSISTANT_DESCRIPTION\n    \n    def __init__(self, *args, openai_api_key='', model_name='text-davinci-003', temperature=0.3, verbose=False, **hub_kwargs):\n        super().__init__()\n        os.environ['OPENAI_API_KEY'] = openai_api_key\n        if model_name == 'text-davinci-003':\n            llm = OpenAI(model_name=model_name, temperature=temperature)\n        elif model_name in ('gpt-3.5-turbo', 'gpt-4'):\n            llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n        else:\n            raise ValueError(\n                f\"{model_name} is not supported, please choose one \"\n                \"of 'text-davinci-003', 'gpt-3.5-turbo', or 'gpt-4'.\"\n            )\n        service_context = ServiceContext.from_defaults(llm_predictor=LLMPredictor(llm=llm))\n        set_global_service_context(service_context)\n        \n        self.storage_path = os.path.dirname(__file__)\n        self.verbose = verbose\n        self.hub_kwargs = hub_kwargs\n\n    def setup(self):\n        hf_hub_download(repo_id=\"llamaindex/text2image_prompt_assistant\", filename=\"storage/vector_store.json\", repo_type=\"space\", local_dir=self.storage_path)\n        hf_hub_download(repo_id=\"llamaindex/text2image_prompt_assistant\", filename=\"storage/index_store.json\", repo_type=\"space\", local_dir=self.storage_path)\n        hf_hub_download(repo_id=\"llamaindex/text2image_prompt_assistant\", filename=\"storage/docstore.json\", repo_type=\"space\", local_dir=self.storage_path)\n        \n        self.index = load_index_from_storage(StorageContext.from_defaults(persist_dir=os.path.join(self.storage_path, \"storage\")))\n        self.query_engine = self.index.as_query_engine(similarity_top_k=5, text_qa_template=text_qa_template, refine_template=refine_template)\n        \n        # setup the text-to-image tool too\n        self.text2image = load_tool('huggingface-tools/text-to-image')\n        self.text2image.setup()\n\n        self.initialized = True\n\n    def __call__(self, prompt):\n        if not self.is_initialized:\n            self.setup()\n\n        better_prompt = str(self.query_engine.query(prompt)).strip()\n        \n        if self.verbose:\n            print('==New prompt generated by LlamaIndex==', flush=True)\n            print(better_prompt, '\\n', flush=True)\n\n        return self.text2image(better_prompt)Running the ToolWith the tool setup, we can now test it with an actual agent! For testing, we used an OpenAIAgent with the text-davinci-003 model. When asked to draw a picture of a mountain, this is what we got:from transformers import OpenAiAgent\nagent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"your_api_key\")\n\nagent.run(\"Draw me a picture a mountain.\")The initial picture of mountains that the agent created.As you can see, the picture looks alright. But, text-to-image prompts are somewhat of an art.To use our new tool, we just need to replace the existing image-generator tool:from transformers import load_tool\nprompt_assistant = load_tool(\n    \"llamaindex/text2image_prompt_assistant\",\n    openai_api_key=\"your_api_key\",\n    model_name='text-davinci-003',\n    temperature=0.3,  # increase or decrease this to control variation\n    verbose=True\n)\n\nfrom transformers import OpenAiAgent\nagent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"your_api_key\")\n\n# replace the existing tool\nagent.toolbox['image_generator'] = prompt_assistant\n\nagent.run(\"Draw me a picture a mountain.\")Using Our new LlamaIndex Prompt Assistant tool, we get a much more stylized result. In the terminal, we see the prompt was re-written as \u201ca majestic mountain peak, surrounded by lush greenery, with a stunning sunset in the background,\u201d which resulted in the following image:Image generated by our Text2Image Prompt Assistant tool.Looks great! With the temperature variable, we can control how varied the generated prompts become. With a temperature above zero, each prompt generated by LlamaIndex with the same agent prompt will be brand new!ConclusionIn conclusion, we have demonstrated how LlamaIndex can be used to augment LLM agents, by implementing a Text2Image Prompt Assistant tool with a Transformers Agent. Using a vector database created from DiffusionDB, LlamaIndex can suggest better prompts when generating images.Custom tools in Transformers Agents are easily distributed and shared using Hugging Face Spaces, and we are excited to see what other people build and share!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-and-transformers-agents-67042ee1d8d6"
        },
        {
            "date": "Aug 29, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases",
            "tags": [
                "large-language-models",
                "llamaindex",
                "ai",
                "nlp",
                "software-development"
            ],
            "content": "Introduction:In the world of IT and Software Development, knowledge transfer (KT) stands out as a big challenge. Whether it\u2019s new hires trying to understand their roles, folks on their notice periods aiming for a smooth handover, or the daily tasks of developers and product specialists adapting to ever-changing projects \u2014 the KT process often leads to stress and worry.This gets more complicated with information spread out everywhere, the mix of new and old tech, and the fast pace of IT and Software Development projects. In this situation, broken bits of knowledge become the norm, causing delays, misunderstandings, and making learning harder.But amidst these challenges, might there be a beacon of optimism shining through?Vibhav and I have developed a system that seamlessly organizes KT sessions. By leveraging personal images, we generate video explanations that are paired with individual code snippets, making the code far more comprehensible. Our innovative approach was recognized when we secured the First Prize at the Google Cloud, Searce, and LifeSight hackathon. With the combined strengths of LlamaIndex and D-ID, our aim is not just to consolidate information but also to simplify tasks and elevate the KT process. In doing so, we\u2019re transforming a daunting industry challenge into a straightforward and manageable endeavor.Want to see how LlamaIndex plays a key role in this change?Let\u2019s dive in together!Solution:The solution has four stages:Code Parsing:Break down the code base into individual code snippets or blocks.Summary and Explanation Generation with LlamaIndex:Produce a comprehensive summary of the entire code base.Create detailed explanations for each individual code block using LlamaIndex.Video Creation with D-ID:Generate videos using text-to-speech capabilities provided by D-ID.Video-Code Integration:Seamlessly stitch together the individual code blocks with their corresponding generated videos.Let\u2019s dive into each stage in detail.\n\n\n\n1. Code Parsing: Breaking Down the Code\n\n\nCode Parser\n\n\n  Understanding a code base starts with a high-level summary, but the true depth\n  lies in individual snippets or blocks. However, using entire code bases for\n  explanations can overwhelm language models like LLMs, causing them to either\n  exceed token limits or miss key details.\n\n\n  Our approach is simple yet efficient: break the code into digestible sections\n  like import statements, classes, initializer functions, and methods without\n  losing the code\u2019s flow. This segmentation is done through a dependency graph\n  approach, utilizing Python\u2019s\n  ast library. By analyzing the code's structure, we can extract\n  classes, their docstrings, initializers, and other methods. This method not\n  only captures the essence of each segment but is also flexible, allowing for\n  further rules to extract additional code components.\n\n\n  The code_parser class embodies this strategy. It navigates the\n  code, distinguishing module-level functions from class-nested ones, and\n  arranges them systematically. The result? A granular yet comprehensive view of\n  the code, paving the way for precise and context-rich explanations.\n\n2. Summary and Explanation Generation with LlamaIndex\n\nProducing a Comprehensive Summary:\n\n\n  The initial step in understanding a code base is to grasp its overall essence.\n  This is achieved by generating a concise summary that gives a bird\u2019s-eye view\n  of the entire code. LlamaIndex\u2019s SummaryIndex\n  has been tailored for this exact task. In SummaryIndex, each block of code is\n  treated as a node. By inputting the structured blocks obtained from our code\n  parsing phase into SummaryIndex, we can produce a comprehensive snapshot that\n  serves as a summary of the entire code base.\n\n\n\n\n\nDetailed Explanations for Individual Code Blocks:\n\n\n  With a general understanding established, the next step is to delve into the\n  finer details. Starting from import statements, progressing to functions, and\n  eventually diving into classes and initializer functions, every block gets its\n  due attention. Here, LlamaIndex\u2019s\n  accumulateresponse mode is a valuable asset, providing in-depth\n  explanations for each block.\n\n\n  However, a challenge arises. While\n  accumulate provides in-depth insights into each block, it can\n  occasionally miss the broader context offered by preceding blocks. To address\n  this limitation, we\u2019ve adopted a two-pronged approach. As depicted in the\n  subsequent architecture, we employ two SummaryIndices for this endeavor.\n\n\n\n    We utilize the first SummaryIndex to generate a concise summary for each\n    block, treating each block as a Node in SummaryIndex.\n  \n\n    For the second SummaaryIndex in the stack, we feed the summarized context\n    from one node into the next. This ensures every node benefits from the\n    context of its predecessor. We then harness the\n    accumulate mode to provide detailed explanations, making\n    certain that every segment of the code is explained comprehensively,\n    preserving the broader perspective. The outcome? A deep, contextually rich\n    interpretation of each code section.\n  \n\n\n  Note: We utilized Google\u2019s PaLM API in conjunction with LlamaIndex to generate\n  summaries and explanations. Alternatively, models like GPT-3.5, GPT-4, or\n  other LLM\u2019s can be employed for this purpose.\n\n\n\n\n3. Video Creation with D-ID:\n\n  After carefully crafting summaries and detailed explanations for each code\n  block, it\u2019s essential to convey this information in a captivating and\n  accessible manner. Videos, given their dynamic appeal, have the power to make\n  intricate code explanations clearer and more engaging. This is where D-ID\n  comes into play.\n\n\n  With the prowess of D-ID\u2019s cutting-edge technology, we\u2019re able to create\n  realistic videos where avatars \u2014 whether they\u2019re of us or another chosen\n  figure \u2014 articulate each code block. Now, what brings these avatars to life?\n  The answer lies in Microsoft\u2019s text-to-speech synthesizer. This tool takes our\n  detailed textual explanations and transforms them into natural, fluent speech.\n  Thus, with D-ID, we\u2019re not just generating video but also integrating audio,\n  culminating in a comprehensive and fluid video explanation.\n\nTo see this in action, let\u2019s take a look at a sample output.\n\n\n\n4. Video-Code Integration:\n\n  After generating insightful videos with avatars elucidating the code and\n  having our individual code snippets ready, the next crucial step is to marry\n  these two elements. This fusion ensures that viewers receive an immersive\n  visual experience, where they can simultaneously watch the explanation and\n  observe the related code.\n\n\n  To achieve this, we employed the\n  carbon library, which transforms our code snippets into visually\n  appealing images. These images, when presented side-by-side with our\n  explanatory videos, offer a clearer understanding of the code in focus. The\n  final touch is added with the moviepy library, which seamlessly\n  stitches the video and code images together, ensuring a smooth and integrated\n  visual flow. Below, you'll find a sample illustrating this compelling\n  combination.\n\n\n\n\nFinal Automatic Knowledge Transfer (KT) Generated Video\n\n  Following our detailed process, we\u2019ve crafted a KT video where Jerry explains\n  the ChatEngine code base of LlamaIndex. Watch the video below to see it all\n  come together!\n\n\n\n  Code Repository:\n  https://github.com/ravi03071991/KT_Generator\n\nConclusion\n\n  Through this post, we\u2019ve showcased the transformative potential of LlamaIndex\n  in creating Knowledge Transfer (KT) Videos for code bases. It\u2019s genuinely\n  remarkable to envision the advancements we\u2019re making in this space. The\n  methodology we\u2019ve adopted is language-neutral, allowing flexibility in\n  adapting to various code bases. With some tweaks to the code parsing phase, we\n  believe it\u2019s feasible to scale this to cover expansive code repositories\n  within organizations. Imagine a platform akin to YouTube, perhaps\n  KodeTube(KT), where an organization\u2019s entire codebase is\n  cataloged through explanatory videos. The horizon is bright with the\n  opportunities LlamaIndex brings, and we\u2019re thrilled about the journey ahead.\n\n",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af"
        },
        {
            "date": "Feb 6, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201302\u201306",
            "tags": [
                "llamaindex",
                "newsletter",
                "llm",
                "rag",
                "replit"
            ],
            "content": "Hello, LlamaIndex Explorers \ud83e\udd99,Step into a week full of exciting updates at LlamaIndex! Our community\u2019s vibrant contributions and extensive educational resources are here to amplify your LlamaIndex exploration.Before diving into the updates, we have an exciting announcement: We\u2019ve launched a $2,000 bounty program with Replit. This initiative invites open-source contributors to create projects and templates focused on advanced RAG with LlamaIndex, from building RAG across thousands of documents to implementing cutting-edge RAG research and crafting advanced templates.We\u2019re inspired by your creativity! If you have a project, article, or video you\u2019re excited about, we\u2019re eager to see it. Send your amazing work to news@llamaindex.ai. If you haven\u2019t subscribed to our newsletter yet, don\u2019t miss out. Visit our website and subscribe today to get all the newest updates from LlamaIndex straight to your inbox.\ud83e\udd29 The highlights:Ollama Multimodal Integration Launch: Introduced day-1 integration with Ollama Multi-Modal for developing local multimodal applications, including image extraction, multimodal RAG, and captioning. Notebook, Tweetcreate-llama Enhanced RAG: Updated create-llama for improved website content crawling and the creation of comprehensive RAG applications. Tweet.Nomic Embedding: Guide to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex.\u2728 Feature Releases and Enhancements:We introduced day-1 integration with the Ollama Multi-Modal release enabling the creation of local multimodal applications on MacBook, including structured image extraction, multimodal RAG, and image captioning. Notebook, TweetWe have updated create-llama on crawling a website\u2019s content, and create a full-stack RAG application based on the data. Tweet.\ud83d\uddfa\ufe0f Guides:Guide to Building a Fully Open Source Retriever with Nomic Embed and LlamaIndex.\ud83c\udfa5 Demo:LlamaBot: Rohan developed an open-source Discord bot that listens to, remembers, and answers questions across servers, was inspired by a similar bot for Slack and developed using LlamaIndex, Gemini Pro, and Qdrant Engine. GitHub Repository, Tweet.\u270d\ufe0f Tutorials:Wenqi Glantz tutorial on Jump-start Your RAG Pipelines with Advanced Retrieval LlamaPacks and Benchmark with Lighthouz AI.Ravi Theja tutorial on Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG.Wenqi Glantz tutorial on 12 RAG Pain Points and Proposed Solutions.Cobus Reyling tutorial on Agentic RAG With LlamaIndex.ChristopherGS tutorial on Retrieval Augmented Generation (RAG) with Llama Index and Open-Source Models.Andrei workshop tutorial on Evaluation of Multimodal RAG Systems using the LlamaIndex.Tutorial on Building RAG application with Pinecone and LlamaIndex.Sudalai Rajkumar tutorial on RAG \u2014 Encoder and Reranker evaluation.Harshad Suryawanshi tutorial on RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex.Otmane Boughaba\u2019s tutorial on Building a Local RAG API with LlamaIndex, Qdrant, Ollama, and FastAPI.Iulia Brezeanu tutorial on How to Find the Best Multilingual Embedding Model for Your RAG.\ud83c\udfa5 EventsJerry Liu keynote on Beyond Naive Rag: Adding Agentic Layers.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-02-06-9a303130ad9f"
        },
        {
            "date": "Nov 8, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023-11\u201307",
            "tags": [
                "llm",
                "llamaindex",
                "ai",
                "newsletter"
            ],
            "content": "Hi again Llama Fans! \ud83e\udd99We hope you enjoyed our OpenAI Dev Day special edition yesterday! Here\u2019s our wrap-up of everything else that happened last week. As always, if you\u2019ve got a project, article, or video that\u2019s turning heads? We\u2019re all ears! Drop us a line at news@llamaindex.ai.And for all this goodness delivered directly to you, don\u2019t forget to subscribe to our newsletter via our website.\ud83e\udd29 First, the highlights:LlamaIndex Chat: We unveiled a customizable LLM chatbot template with system prompts and avatars, all within an open-source MIT-licensed framework using LlamaIndex for TypeScript. Explore the Demo or check the Tweet.Evaluator Fine-Tuning: We launched a method to enhance LLM output assessment by distilling GPT-4 into GPT-3.5, optimizing both cost and speed. See our Tweet.ParamTuner: We introduced a new hyperparameter tuning abstraction to refine RAG pipeline performance, featuring objective functions, grid search, and Ray Tune integration. Check out the Notebook and Tweet.CohereAI Embed v3 & Voyage AI Integration: We strengthened the LlamaIndex RAG pipeline with two powerful embedding model additions: the latest Embed v3 from CohereAI and the high-performing embedding model from Voyage AI. Tweet and tweet.\u2728 Feature Releases and Enhancements:We introduced LlamaIndex Chat, a new feature allowing users to create and share custom LLM chatbots tailored to their data, complete with personalized system prompts and avatars. Additionally, we\u2019re proud to share that it\u2019s a fully open-source template under the MIT license, crafted using LlamaIndexTS for a seamless start to LLM application development. Demo, Tweet.We introduced a method for fine-tuning an Evaluator to distill GPT-4 into GPT-3.5, enhancing LLM output assessment while reducing costs and improving speed. Tweet.We introduced ParamTuner, a hyperparameter tuning abstraction for LlamaIndex RAG, streamlining the process with objective functions and support for grid search, including integration with Ray Tune for enhanced optimization. Notebook, Tweet.\ud83c\udfa5 Demos:GPTDiscord is a versatile LLM-powered Discord bot with over 20 features, including multi-modal image understanding and advanced data analysis. It boasts an infinite conversational memory and the ability to interact with various file types and internet services. Tweet.\ud83d\uddfa\ufe0f Guides:We shared a guide for integrating Activeloop\u2019s Deep Memory with LlamaIndex, a module that enhances your embeddings at ingestion and can improve RAG metrics by 15%, all while seamlessly fitting into LlamaIndex\u2019s automated dataset and vector store features.We shared a guide inspired by Chengrun Yang and GoogleDeepMind\u2019s Optimization by Prompting paper, demonstrating how to automate prompt tuning in LlamaIndex RAG pipelines using meta-prompting, boosting evaluation performance while acknowledging the experimental nature of this technique.We shared a guide on how to implement Emotion Prompting in LlamaIndex, allowing you to enhance your RAG pipeline with various emotional stimuli and evaluate their impact on task performance.We showcased MongoDB starter kit, a comprehensive LlamaIndex RAG setup with Flask backend, Next frontend, and easy deployment to Render.\u270d\ufe0f Tutorials:Wenqi Glantz made a blog post on deploying the HuggingFace text-embeddings-inference server on an AWS EC2 GPU instance, enhancing LlamaIndex RAG pipeline's performance and results.Sophia Yang\u2019s tutorial on Zephyr-7b-beta showcases its leading capabilities in LLM technology, including how it\u2019s benchmarked with LlamaIndex for diverse AI tasks.Sudarshan Koirala gave a tutorial on how to build a multi-modal retrieval system with LlamaIndex, Qdrant, and bge/CLIP embeddings.Sophia Yang\u2019s gave another tutorial, this time on Small-to-Big Retrieval with LlamaIndex in building advanced RAG systems.Ravi Theja\u2019s tutorial on the Router Query Engine that helps you to set up multiple indices/ query engines for your dataset, allowing the LLM to choose the most suitable one for each specific question.\u2699\ufe0f Integrations & Collaborations:We integrated the Tavily AI research API into the LlamaIndex RAG pipeline, offering a robust tool for web research to enhance LLM agent automation. Notebook, Tweet.We integrated Noam Gat\u2019s LLM Enforcer into the LlamaIndex RAG pipeline to ensure structured outputs for various models. Docs, Tweet.We integrated the latest Embed v3 model from CohereAI, enhancing document retrieval quality within the LlamaIndex RAG pipeline. Notebook, Tweet.We integrated the new Voyage AI embedding model, a top-performing option for RAG pipelines. Notebook, Tweet.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-11-07-cf20b9a833aa"
        },
        {
            "date": "Oct 27, 2023",
            "author": "Tim Ho",
            "title": "NewsGPT(Neotice): Summarize news articles with LlamaIndex \u2014 Hackathon winning app",
            "tags": [],
            "content": "We\u2019re excited to share with you the thought process and solution design of NewsGPT (Neotice) \u2014 Clickbait Buster, a Streamlit LLM hackathon-winning app powered by LlamaIndex, Streamlit, and Qdrant. In this article, we\u2019ll define the problem we\u2019re trying to solve and discuss how we approached it. Lastly, we offer a workaround to enable LlamaIndex streaming on Streamlit Chat Bot and the all the code can be found here. We hope you\u2019ll find our insights helpful and informative.IntroductionProblem StatementIt\u2019s evident that people\u2019s habits of consuming information have changed over time. Previously, we read lengthy articles content and watched long videos, such as newspapers and YouTube videos. However, we currently prefer reading titles and consuming short-form content, such as TikTok and YouTube shorts. Although this shift has made it easier to get more information in less time, it has also led to clickbait headlines that often contain incorrect information.When we started developing NewsGPT, our primary focus was to solve the above-stated pain points and provide a solution that 1) provides accurate information and 2) saves time for users.NeoticeWe are excited to announce that the beta version of the Neotice app, which is the production version of NewsGPT, is now available for users to try out! We are grateful to the Streamlit Hackathon for showcasing our prototype and philosophy. With the help of this platform, we are confident that our app will revolutionize the way people consume news.Why NewsGPT Stands OutNewsGPT has four main components: Reliable News Sources, Tailored News Recommendations, Efficient Information Retrieval, and Time Saver.\u2705 Reliable News Sources:We\u2019ve established a dynamic data pipeline designed to ingest daily news, ensuring our information is up-to-date and relevant.Sophisticated Named-Entity Recognition, Text Embedding with OpenAI API, and asynchronous article embedding processes are incorporated. This data is systematically stored in the QdrantVector Database, promoting accuracy and efficiency.\u2705 Tailored News Recommendations:Our system does more than just present news; it learns from you. By analyzing your reading habits, we leverage article embeddings to curate a personal news feed tailored to your interests.A versatile search bar is always at your disposal, letting users explore any news topics that capture their interest.\u2705 Efficient Information Retrieval:With just a single click on an article of interest, NewsGPT gets to work. It collates similar news from multiple sources (3\u20135) and activates a Streamlit chatbot.Your engagement begins immediately: the first query is autonomously forwarded to our chatbot to fetch a concise news summary.For ease of user experience, we display predefined prompts as clickable buttons. This means users can receive information without the need for manual input.Curiosity welcomed: any questions users may have about the news article will be addressed as long as the answers are detailed within the source articles.\u2705 Time-Saving Reminder & Category Distribution Chart:To keep you informed, our sidebar displays the time saved using NewsGPT and visually represents news category distribution.Delving Deep into ArchitectureData PipelineWe start with a reliable and sustainable data pipeline to support the users to get fresh news with two powerful libraries, pygooglenews, and newspaper3k.pip install pygooglenews --upgrade\npip install newspaper3kBy utilizing Spark batch processing, we efficiently process data with NER(Named-Entity Recognition) and create embeddings via OpenAI API (Ada model). After the preprocessing of the data, we collect the metadata, including keywords, NER results, summary, body, title, author, and so on, in the payload and push the payload with embedding to the Qdrant Vector Database.We will skip the part on how to create embeddings with the OpenAI Ada model, as there are many existing tutorials available. To perform Named Entity Recognition, we utilize the dslim/bert-base-NER model from HuggingFace.from transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, batch_size=batch_size)\nner_results = nlp(texts)PersonalizationWhen accessing NewsGPT, we have two options: we can either create a new account or use a guest login to test the service. It should be noted, however, that using the guest login will not provide personalization. If we choose to sign up, we will be asked about our preferred news categories, which will help the service make initial recommendations during the cold start.Recommendation Pipeline for PersonalizationAfter users sign up and log in, we query the preferences and activities from Google Firebase, if any; otherwise, use the favorite categories for recommendation cold start. If the activities and preferences are available, we will call the recommendation API hosted on AWS Lambda and generate personalized articles for the users. The activities related to the user reading history. Users can indicate their preference for articles by clicking thumbs-up or down buttons on the article page. Some of the recommendation logic can be found in the code.Besides the personalized feeds, users can also choose different categories or search for specific topics using the search bar.Chat with Article Powered by LlamaIndexLlamaIndex PipelineUnlike other news aggregator apps, NewsGPT offers a unique service by providing users with a summary and discrepancy key takeaway of a topic from various news sources. This feature not only saves users time by eliminating the need to read multiple articles but also helps identify which information can be trusted by comparing discrepancies. What is under the hood is when the user clicks on the \u201cchat with article\u201d button, the system first uses a search API hosted on AWS Lambda to find related articles from various sources. Then, the system utilizes the LlamaIndex library to create a vector store and a query engine, which can be integrated with the Streamlit chat component to create an RAG application for information retrieval.Streaming Output with LlamaIndex and StreamlitThanks to the powerful library, Streamlit-Extras, which provides additional functionality not officially supported. To enhance the user experience and make it more like chatting with ChatGPT, we use the streaming_write functions from the Streamlit-Extras library. Additionally, we set streaming=True for the query_engine to ensure a smoother experience. Let\u2019s take a look at the code.pip install streamlit-extrasTo begin with, we set up the service_context.from llama_index import ServiceContext\n\nst.session_state[\"service_context\"] = ServiceContext.from_defaults(\n            llm=OpenAI(\n                model=\"gpt-3.5-turbo\",\n                temperature=0.2,\n                chunk_size=1024,\n                chunk_overlap=100,\n                system_prompt=\"As an expert current affairs commentator and analyst,\\\n                               your task is to summarize the articles and answer the questions from the user related to the news articles\",\n            ),\n            chunk_size=256, \n            chunk_overlap=20\n        )Next, we create a text splitter using TokenTextSplitter and a node parser using SimpleNodeParser to parse multiple articles.from llama_index.text_splitter import TokenTextSplitter\nfrom llama_index.node_parser import SimpleNodeParser\n\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=256, chunk_overlap=20)\n#create node parser to parse nodes from document\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\nnodes = node_parser.get_nodes_from_documents(documents)In the third step, we create an index using VectorStoreIndex. To enable streaming capability, ensure to set streaming=True while setting up the query_engine.from llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex(\n        nodes=nodes,\n        service_context=st.session_state[\"service_context\"]\n    )\nst.session_state[\"chat_engine\"] = index.as_query_engine(streaming=True)To add streaming capability to Streamlit chat components, we took inspiration from this code. Instead of using st.write() from the regular chat implementation, we replaced it with the write function from streaming_write.response = st.session_state[\"chat_engine\"].query(prompt)\ndef stream_example():\n    for word in response.response_gen:\n        yield word\nwrite(stream_example)Streaming DemoPredefined PromptsThree different prompts are predefined in the article chat page, allowing users to select from a drop-down menu to ask questions without typing. The prompts are 5W1H, Similar Viewpoints, and Discrepancy Viewpoints.prompt_content = {\n    \"5W1H\": 'Summarize the content details in the \"5W1H\" approach (Who, What, When, Where, Why, and How) in bullet points',\n    \"Similar Viewpoints\": \"Compare between the articles and provide the similar viewpoints in bullet points\",\n    \"Discrepency Viewpoints\": \"Compare between the articles and provide the discrepency viewpoints in bullet points\"\n}What\u2019s NextHuge thanks to LlamaIndex and Streamlit for generously providing a massive platform that allows more people to gain awareness of the organic news digest and save valuable time through NewsGPT. If you enjoyed reading the article and agree with our concept, please do not hesitate to leave a clap for the article and join Neotice, the production app of NewsGPT, to support us. We are confident in our mission and look forward to having you on board with us. Thank you!You can also connect us on LinkedIn: Kang-Chi Ho, Jian-An Wang\ud83c\udf89 Click Here to Join Neotice \ud83d\udc49 Neotice",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//newsgpt-neotice-summarize-news-articles-with-llamaindex-hackathon-winning-app-9d7c8bcf9f11"
        },
        {
            "date": "Nov 27, 2023",
            "author": "Harshad Suryawanshi",
            "title": "Multimodal RAG: Building \u2018AInimal Go!\u2019, a Pok\u00e9mon Go-Inspired App with ResNet, Cohere and Llamaindex",
            "tags": [
                "llamaindex",
                "llm",
                "cohere",
                "resnet",
                "streamlit"
            ],
            "content": "In the current landscape where GPT-4 Vision (GPT-4V) use cases are everywhere, I wanted to explore an alternative approach: pairing deep learning vision models with large language models (LLMs). My latest project, \u2018AInimal Go!\u2019, is an attempt to showcase how a specialized vision model like ResNet18 can seamlessly integrate with an LLM, using LlamaIndex as the orchestration layer and Wikipedia articles as the knowledge base.Project Overview\u2018AInimal Go!\u2019 is an interactive app that allows users to either capture or upload images of animals. Upon uploading an image, the ResNet18 model swiftly classifies the animal. Following this, the Cohere LLM API, adeptly orchestrated by LlamaIndex, takes over. It roleplays as the identified animal, enabling users to engage in unique conversations about and with the animal. The dialogue is informed and enriched by a knowledge base of nearly 200 Wikipedia articles, providing accurate and relevant responses to user queries.Why Not GPT-4V?Amidst the surge in GPT-4 Vision use cases, I wanted to explore an efficient yet powerful alternative. It is important to choose the right tool for the job \u2014 using GPT-4V for every multimodal task can be overkill, like using a sledgehammer to crack a nut. My approach was to harness the agility and precision of ResNet18 for animal identification. This method not only curtails costs but also underscores the adaptability of specialized models in multi-modal realms.Tools and TechResNet for Animal Detection: A blazing-fast implementation to identify animals in images, utilizing the ImageNet classification scheme.Cohere LLM: For generating engaging, informative conversations based on the identified animal.LlamaIndex: Seamlessly orchestrates the workflow, managing the retrieval of information from pre-indexed Wikipedia articles about animals.Streamlit for UIGif showing the demo in actionDeep Dive into app.pyThe heart of \u2018AInimal Go!\u2019 lies in the app.py script, where ResNet, Cohere LLM, and LlamaIndex seamlessly come together. Now, let\u2019s delve into the key aspects of the code:1. Image Capture/UploadIn \u2018AInimal Go!\u2019, the flow begins with the user uploading an image or capturing one using their device\u2019s camera. This is a crucial step as it sets the stage for the subsequent interaction with the identified animal.The code snippet below illustrates how Streamlit is used to create a UI for image upload and capture. It offers two options: a file uploader for selecting an image file and a camera input for real-time capture. Once an image is provided through either method, it\u2019s converted into a byte stream (BytesIO) for processing. This streamlining ensures a seamless user experience, whether the image is uploaded from a gallery or captured on the spot.# Image upload section.\n    image_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"uploaded_image\", on_change=on_image_upload)\n    \n    col1, col2, col3 = st.columns([1, 2, 1])\n    with col2:  # Camera input will be in the middle column\n        camera_image = st.camera_input(\"Take a picture\", on_change=on_image_upload)\n        \n    \n    # Determine the source of the image (upload or camera)\n    if image_file is not None:\n        image_data = BytesIO(image_file.getvalue())\n    elif camera_image is not None:\n        image_data = BytesIO(camera_image.getvalue())\n    else:\n        image_data = None\n    \n    if image_data:\n        # Display the uploaded image at a standard width.\n        st.session_state['assistant_avatar'] = image_data\n        st.image(image_data, caption='Uploaded Image.', width=200)2. Initializing ResNet for Image ClassificationOnce the user uploads or captures an image, the next critical step is identifying the animal within it. This is where ResNet18, a robust deep learning model for image classification, comes into play.The function load_model_and_labels performs two key tasks:Loading Animal Labels: It starts by loading a subset of ImageNet labels specific to animals. These labels are stored in a dictionary, mapping class IDs to their corresponding animal names. This mapping is essential for interpreting the output of the ResNet model.Initializing ResNet18: The function then initializes the feature extractor and the ResNet18 model. The feature extractor preprocesses the images to the format required by ResNet18, while the model itself is responsible for the actual classification task.def load_model_and_labels():\n    # Load animal labels as a dictionary\n    animal_labels_dict = {}\n    with open('imagenet_animal_labels_subset.txt', 'r') as file:\n        for line in file:\n            parts = line.strip().split(':')\n            class_id = int(parts[0].strip())\n            label_name = parts[1].strip().strip(\"'\")\n            animal_labels_dict[class_id] = label_name\n\n    # Initialize feature extractor and model\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-18\")\n    model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n\n    return feature_extractor, model, animal_labels_dict\n\nfeature_extractor, model, animal_labels_dict = load_model_and_labels()By integrating ResNet18 in this manner, \u2018AInimal Go!\u2019 leverages its speed and accuracy for the crucial task of identifying the animal in the user\u2019s image. This sets the foundation for the engaging and informative conversations that follow.3. Animal Detection with ResNet18After initializing ResNet18, the next step is to use it for detecting the animal in the uploaded image. The function get_image_caption handles this task.Image Preprocessing: The uploaded image is first opened and then preprocessed using the feature extractor initialized earlier. This preprocessing adapts the image to the format required by ResNet18.Animal Detection: The preprocessed image is then fed into ResNet18, which predicts the class of the image. The logits (the model\u2019s raw output) are processed to find the class with the highest probability, which corresponds to the predicted animal.Retrieving the Animal Name: The predicted class ID is mapped to the corresponding animal name using the label dictionary created earlier. This name is then displayed to the user.def get_image_caption(image_data):\n    image = Image.open(image_data)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label_id = logits.argmax(-1).item()\n    predicted_label_name = model.config.id2label[predicted_label_id]\n    st.write(predicted_label_name)\n    # Return the predicted animal name\n    return predicted_label_name, predicted_label_id4. Validating Animal Presence in ImagesTo ensure that the conversation in \u2018AInimal Go!\u2019 is relevant and engaging, it\u2019s crucial to verify that the uploaded image indeed depicts an animal. This verification is handled by the is_animal function.def is_animal(predicted_label_id):\n    # Check if the predicted label ID is within the animal classes range\n    return 0 &lt;= predicted_label_id &lt;= 398The function checks if the predicted label ID from ResNet18 falls within the range of animal classes (0 to 398 in ImageNet\u2019s classification). This simple yet effective check is essential for maintaining the app\u2019s focus on animal interactions.Further in the script, this function is utilized to validate the detected object:if not (is_animal(label_id)):\n    st.error(\"Please upload image of an animal!\")\n    st.stop()If the uploaded image does not depict an animal, the app prompts the user to upload an appropriate image, ensuring that the conversation remains on track.5. Initializing LLMThe init_llm function initializes the Cohere LLM along with the necessary contexts for storage and service (specify llm and embed_model). It also loads the pre-indexed Wikipedia articles for about 200 animals. The function sets up the environment in which the LLM operates, preparing it for generating responses.def init_llm(api_key):\n    llm = Cohere(model=\"command\", api_key=st.secrets['COHERE_API_TOKEN'])\n\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\n    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n    index = load_index_from_storage(storage_context, index_id=\"index\", service_context=service_context)\n    \n    return llm, service_context, storage_context, indexThis function is critical for setting up the LLM, ensuring that all necessary components are in place for the chat functionality.6. Creating the Chat EngineThe create_chat_engine function takes the animal description and utilizes it to create a query engine. This engine is responsible for handling user queries and generating responses based on the identified animal.def create_chat_engine(img_desc, api_key):\n    doc = Document(text=img_desc)\n    \n    query_engine = CitationQueryEngine.from_args(\n        index,\n        similarity_top_k=3,\n        citation_chunk_size=512,\n        verbose=True\n    )\n    \n    return query_enginesystem_prompt=f\"\"\"\n              You are a chatbot, able to have normal interactions. Do not make up information.\n              You always answer in great detail and are polite. Your job is to roleplay as an {img_desc}. \n              Remember to make {img_desc} sounds while talking but dont overdo it.\n              \"\"\"\n                    \nresponse = chat_engine.query(f\"{system_prompt}. {user_input}\")By creating a query engine specific to the identified animal, this function ensures that the conversations in the app are relevant, informative, and engaging. I have used the CitationQueryEngine to provide the future possibility of showing the sources as well, making the conversations not only engaging but also informative with credible references.7. Bringing It All TogetherWith all the technical components in place, \u2018AInimal Go!\u2019 combines everything into a user-friendly chat interface. Here, users can interact directly with the AI, asking questions and receiving responses about the identified animal. This final interaction loop, skillfully managed by Streamlit, perfectly showcases the seamless integration of vision and language models in the app.Wrapping Up\u2018AInimal Go!\u2019 represents an exciting fusion of vision models, language models, and Wikipedia, with LlamaIndex serving as the orchestrator that seamlessly integrates ResNet for animal identification and Cohere\u2019s LLM for engaging conversations. This app is a stepping stone to even more innovative visual-language applications. The possibilities are boundless, and your insights can shape its future. I encourage you to explore the demo, experiment with the code, and join me in pushing the boundaries of what AI can achieve in the realm of multimodal interactions.GitHub RepoConnect with Me on LinkedInLinkedIn PostLive Demo",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//multimodal-rag-building-ainimal-go-fecf8404ed97"
        },
        {
            "date": "Aug 12, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex: Harnessing the Power of Text2SQL and RAG to Analyze Product Reviews",
            "tags": [
                "llamaindex",
                "ai",
                "llm",
                "sql",
                "information-retrieval"
            ],
            "content": "IntroductionE-commerce platforms, such as Amazon and Walmart, are teeming with products that attract a multitude of reviews every single day. These reviews are crucial touchpoints that reflect consumer sentiments about products. But how can businesses sift through vast databases to derive meaningful insights from these reviews?The answer lies in combining SQL with RAG (Retrieval Augmented Generation) through LlamaIndex.Let\u2019s deep dive into this!Sample Dataset of Product ReviewsFor the purpose of this demonstration, we\u2019ve generated a sample dataset using GPT-4 that comprises reviews for three products: iPhone13, SamsungTV, and an Ergonomic Chair. Here\u2019s a sneak peek:iPhone13: \u201cAmazing battery life and camera quality. Best iPhone yet.\u201dSamsungTV: \u201cImpressive picture clarity and vibrant colors. A top-notch TV.\u201dErgonomic Chair: \u201cFeels really comfortable even after long hours.\u201dHere is a sample dataset.rows = [\n    # iPhone13 Reviews\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The iPhone13 is a stellar leap forward. From its sleek design to the crystal-clear display, it screams luxury and functionality. Coupled with the enhanced battery life and an A15 chip, it's clear Apple has once again raised the bar in the smartphone industry.\"},\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"This model brings the brilliance of the ProMotion display, changing the dynamics of screen interaction. The rich colors, smooth transitions, and lag-free experience make daily tasks and gaming absolutely delightful.\"},\n    {\"category\": \"Phone\", \"product_name\": \"Iphone13\", \"review\": \"The 5G capabilities are the true game-changer. Streaming, downloading, or even regular browsing feels like a breeze. It's remarkable how seamless the integration feels, and it's obvious that Apple has invested a lot in refining the experience.\"},\n\n    # SamsungTV Reviews\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"Samsung's display technology has always been at the forefront, but with this TV, they've outdone themselves. Every visual is crisp, the colors are vibrant, and the depth of the blacks is simply mesmerizing. The smart features only add to the luxurious viewing experience.\"},\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"This isn't just a TV; it's a centerpiece for the living room. The ultra-slim bezels and the sleek design make it a visual treat even when it's turned off. And when it's on, the 4K resolution delivers a cinematic experience right at home.\"},\n    {\"category\": \"TV\", \"product_name\": \"SamsungTV\", \"review\": \"The sound quality, often an oversight in many TVs, matches the visual prowess. It creates an enveloping atmosphere that's hard to get without an external sound system. Combined with its user-friendly interface, it's the TV I've always dreamt of.\"},\n\n    # Ergonomic Chair Reviews\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"Shifting to this ergonomic chair was a decision I wish I'd made earlier. Not only does it look sophisticated in its design, but the level of comfort is unparalleled. Long hours at the desk now feel less daunting, and my back is definitely grateful.\"},\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"The meticulous craftsmanship of this chair is evident. Every component, from the armrests to the wheels, feels premium. The adjustability features mean I can tailor it to my needs, ensuring optimal posture and comfort throughout the day.\"},\n    {\"category\": \"Furniture\", \"product_name\": \"Ergonomic Chair\", \"review\": \"I was initially drawn to its aesthetic appeal, but the functional benefits have been profound. The breathable material ensures no discomfort even after prolonged use, and the robust build gives me confidence that it's a chair built to last.\"},\n]Setting up an In-Memory DatabaseTo process our data, we\u2019re using an in-memory SQLite database. SQLAlchemy provides an efficient way to model, create, and interact with this database. Here\u2019s how our product_reviews table structure looks:id (Integer, Primary Key)category (String)product_name (String)review (String, Not Null)Once we\u2019ve defined our table structure, we populate it with our sample dataset.engine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n\n# create product reviews SQL table\ntable_name = \"product_reviews\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"id\", Integer(), primary_key=True),\n    Column(\"category\", String(16), primary_key=True),\n    Column(\"product_name\", Integer),\n    Column(\"review\", String(16), nullable=False)\n)\nmetadata_obj.create_all(engine)\n\nsql_database = SQLDatabase(engine, include_tables=[\"product_reviews\"])\n\nfor row in rows:\n    stmt = insert(city_stats_table).values(**row)\n    with engine.connect() as connection:\n        cursor = connection.execute(stmt)\n        connection.commit()Analysing Product Reviews \u2014 Text2SQL + RAGDeriving insights from data often requires intricate questioning.SQL + RAG in LlamaIndex simplifies this by breaking it into a three-step process:Decomposition of the Question:Primary Query Formation: Frame the main question in natural language to extract preliminary data from the SQL table.Secondary Query Formation: Construct an auxiliary question to refine or interpret the results of the primary query.2. Data Retrieval: Run the primary query using the Text2SQL LlamaIndex module to obtain the initial set of results.3. Final Answer Generation: Use List Index to further refine the results based on the secondary question, leading to the conclusive answer.Let\u2019s start doing it step by step.Decomposing User Query into Two PhasesWhen working with a relational database, it\u2019s often helpful to break down user queries into more manageable parts. This makes it easier to retrieve accurate data from our database and subsequently process or interpret this data to meet the user\u2019s needs. We\u2019ve designed an approach to decompose queries into two distinct questions by giving an example to gpt-3.5-turbo model to generate two distinct questions.Let\u2019s apply this to the query \u201cGet the summary of reviews of Iphone13\u201d and our system would generate:Database Query: \u201cRetrieve reviews related to iPhone13 from the table.\u201dInterpretation Query: \u201cSummarize the retrieved reviews.\u201dThis approach ensures that we cater to both the data retrieval and data interpretation needs, resulting in more accurate and tailored responses to user queries.def generate_questions(user_query: str) -&gt; List[str]:\n  system_message = '''\n  You are given with Postgres table with the following columns.\n\n  city_name, population, country, reviews.\n\n  Your task is to decompose the given question into the following two questions.\n\n  1. Question in natural language that needs to be asked to retrieve results from the table.\n  2. Question that needs to be asked on the top of the result from the first question to provide the final answer.\n\n  Example:\n\n  Input:\n  How is the culture of countries whose population is more than 5000000\n\n  Output:\n  1. Get the reviews of countries whose population is more than 5000000\n  2. Provide the culture of countries\n  '''\n\n  messages = [\n      ChatMessage(role=\"system\", content=system_message),\n      ChatMessage(role=\"user\", content=user_query),\n  ]\n  generated_questions = llm.chat(messages).message.content.split('\\n')\n\n  return generated_questions\n\nuser_query = \"Get the summary of reviews of Iphone13\"\n\ntext_to_sql_query, rag_query = generate_questions(user_query)Data Retrieval \u2014 Executing the Primary QueryWhen we decompose a user\u2019s question into its constituent parts, the first step is to convert the \u201cDatabase Query in Natural Language\u201d into an actual SQL query that can be run against our database. In this section, we\u2019ll use the LlamaIndex\u2019s NLSQLTableQueryEngine to handle the conversion and execution of this SQL query.Setting up the NLSQLTableQueryEngine:The NLSQLTableQueryEngine is a powerful tool that takes natural language queries and converts them into SQL queries. We initiate this by providing the necessary details:sql_database: This represents our SQL database connection details.tables: We specify which table(s) our query will be run against. In this scenario, we're targeting the product_reviews table.synthesize_response: When set to False, this ensures we receive raw SQL responses without additional synthesis.service_context: This is an optional parameter, which could be used to provide service-specific settings or plugins.sql_query_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"product_reviews\"],\n    synthesize_response=False,\n    service_context=service_context\n)Executing the natural language Query:After setting up the engine, the next step is executing our natural language query against it. The engine\u2019s query() method is used for this purpose.sql_response = sql_query_engine.query(text_to_sql_query)Processing the SQL Response:The result of our SQL query is usually a list of rows (with each row represented as a list of reviews). To make it more readable and usable for the third step of processing summarizing reviews, we convert this result into a single string.sql_response_list = ast.literal_eval(sql_response.response)\ntext = [' '.join(t) for t in sql_response_list]\ntext = ' '.join(text)You can check the generated SQL query in sql_response.metadata[\"sql_query\"].By following this process, we\u2019re able to seamlessly integrate natural language processing with SQL query execution. Let\u2019s go with the last step in this process for getting a summary of the reviews.Refining and Interpreting the reviews with ListIndex:After obtaining the primary set of results from the SQL query, there are often situations where further refinement or interpretation is required. This is where ListIndex from LlamaIndex plays a crucial role. It allows us to execute the secondary question on our obtained text data to get a refined answer.listindex = ListIndex([Document(text=text)])\nlist_query_engine = listindex.as_query_engine()\n\nresponse = list_query_engine.query(rag_query)\n\nprint(response.response)Now let\u2019s wrap everything under a function and try out a few interesting examples:\"\"\"Function to perform SQL+RAG\"\"\"\n\ndef sql_rag(user_query: str) -&gt; str:\n  text_to_sql_query, rag_query = generate_questions(user_query)\n\n  sql_response = sql_query_engine.query(text_to_sql_query)\n\n  sql_response_list = ast.literal_eval(sql_response.response)\n\n  text = [' '.join(t) for t in sql_response_list]\n  text = ' '.join(text)\n\n  listindex = ListIndex([Document(text=text)])\n  list_query_engine = listindex.as_query_engine()\n\n  summary = list_query_engine.query(rag_query)\n\n  return summary.responseExamples:sql_rag(\"How is the sentiment of SamsungTV product?\")The sentiment of the reviews for the Samsung TV product is generally positive. Users express satisfaction with the picture clarity, vibrant colors, and stunning picture quality. They appreciate the smart features, user-friendly interface, and easy connectivity options. The sleek design and wall-mounting capability are also praised. The ambient mode, gaming mode, and HDR content are mentioned as standout features. Users find the remote control with voice command convenient and appreciate the regular software updates. However, some users mention that the sound quality could be better and suggest using an external audio system. Overall, the reviews indicate that the Samsung TV is considered a solid investment for quality viewing.sql_rag(\"Are people happy with Ergonomic Chair?\")The overall satisfaction of people with the Ergonomic Chair is high.You can play around with the approach and dataset in the Google Colab Notebook \u2014 here.ConclusionIn the era of e-commerce, where user reviews dictate the success or failure of products, the ability to rapidly analyze and interpret vast swaths of textual data is paramount. LlamaIndex, through its ingenious integration of SQL and RAG, offers businesses a powerful tool to glean actionable insights from such datasets. By seamlessly blending structured SQL queries with the abstraction of natural language processing, we\u2019ve showcased a streamlined approach to transform vague user queries into precise, informative answers.With this approach, businesses can now efficiently sift through mountains of reviews, extract the essence of user sentiments, and make informed decisions. Whether it\u2019s about gauging the overall sentiment for a product, understanding specific feature feedback, or even tracking the evolution of reviews over time, the Text2SQL+RAG methodology in LlamaIndex is the harbinger of a new age of data analytics.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-harnessing-the-power-of-text2sql-and-rag-to-analyze-product-reviews-204feabdf25b"
        },
        {
            "date": "Nov 14, 2023",
            "author": "LlamaIndex",
            "title": "create-llama, a command line tool to generate LlamaIndex apps",
            "tags": [
                "llamaindex",
                "llm",
                "ai",
                "javascript",
                "typescript"
            ],
            "content": "Introducing create-llama, the easiest way to get started with LlamaIndex!Update 2023\u201311\u201320: we now have a guide to deploying your create-llama apps!Want to use the power of LlamaIndex to load, index and chat with your data using LLMs like GPT-4? It just got a lot easier! We\u2019ve created a simple to use command-line tool that will generate a full-stack app just for you \u2014 just bring your own data! To get started, run:npx create-llamaThe app will then ask you a series of questions about what kind of app you want. You\u2019ll need to supply your own OpenAI API key (or you can customize it to use a different LLM), and make a few decisions.How does it get my data?The generated app has a data folder where you can put as many files as you want; the app will automatically index them at build time and after that you can quickly chat with them. If you\u2019re using LlamaIndex.TS as the back-end (see below), you\u2019ll be able to ingest PDF, text, CSV, Markdown, Word and HTML files. If you\u2019re using the Python backend, you can read even more types, including audio and video files!Technical detailsThe front-end it generates is a Next.js application, with your choice of shadcn/ui or vanilla HTML and CSS for styling.For the back-end, you have 3 options:Next.js: if you select this option, you\u2019ll have a full stack Next.js application that you can deploy to a host like Vercel in just a few clicks. This uses LlamaIndex.TS, our TypeScript library.Express: if you want a more traditional Node.js application you can generate an Express backend. This also uses LlamaIndex.TS.Python FastAPI: if you select this option you\u2019ll get a backend powered by the llama-index python package, which you can deploy to a service like Render or fly.io.There are a couple of other questions you\u2019ll be asked:Streaming or non-streaming: if you\u2019re not sure, you\u2019ll probably want a streaming backend.SimpleChatEngine or ContextChatEngine : the ContextChatEngine is the one that uses your data. If you just want to chat with GPT, you can use the SimpleChatEngine.Go forth and customize!Once you\u2019ve got your app up and running, you can customize it to your heart\u2019s content! By default, for cost reasons, the app will use GPT-3.5-Turbo. If you\u2019d like to use GPT-4 you can configure that by modifying the file app/api/chat/llamaindex-stream.ts (in the Next.js backend) or you can configure it to use a different LLM entirely! LlamaIndex has integrations with dozens of LLMs, both APIs and local.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//create-llama-a-command-line-tool-to-generate-llamaindex-apps-8f7683021191"
        },
        {
            "date": "Oct 31, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201310\u201331",
            "tags": [
                "llm",
                "ai",
                "newsletter",
                "llamaindex"
            ],
            "content": "Greetings Llama Enthusiasts \ud83e\udd99!Another week has zoomed past, and here we are with our latest roundup of updates, features, tutorials, and so much more. Have a noteworthy project, article, or video to share? We\u2019d love to feature it! Reach out to us at news@llamaindex.ai.Want these updates straight to your inbox? Simply subscribe to our newsletter on our homepage.\ud83e\udd29 First, the highlights:Revamped Documentation: Overhauled docs for smoother LLM/RAG app development.Contribution Board: Our new board welcomes community-driven LlamaIndex enhancements.Zephyr-7b-beta Insights: Tested and verified for unmatched ReAct agent task efficiency on LlamaIndex.Image Captioning Boost For RAG: LLaVa\u2019s outputs are now supercharged with knowledge-based augmentation. Notebook, Tweet\u2728 Feature Releases and Enhancements:We introduced Retrieval-Augmented Image Captioning, enhancing LLaVa multi-modal model outputs with knowledge base insights. Notebook, Tweet.We introduced the ability to view and set prompts for LlamaIndex modules in just two lines of code. Docs, Tweet.We introduced the integration of our OpenAILike class, allowing users to tap into various open-source LLM projects with OpenAI-compatible APIs, irrespective of the model provider. Tweet.We introduced Prompt Compression for RAG: with LongLLMLingua, which helps to cut token usage and latency by up to 20x. Notebook, Tweet.We introduced a method to refine open-source LLMs like llama2 for structured data outputs. Using LlamaIndex, transform llama2\u20137b to produce Pydantic objects without PyTorch. Our guide covers synthetic dataset creation, fine-tuning, and RAG pipeline integration. Notebook, Tweet.\ud83c\udfa5 Demos:Harshad Suryawanshi did a demo on equity research report generator using LlamaIndex and Streamlit.Bharat Ramanathan built Wandbot, a live RAG app enabling chat over Weights & Biases documentation, integrated with Discord and SlackHQ. Key features include periodic data ingestion, custom document and code parsing, model fallback, and logging with Weights and biases.\ud83d\uddfa\ufe0f Guides:We introduced a revamped documentation structure tailored to guide users from prototyping to production of LLM/RAG apps using LlamaIndex. Dive into our 200+ guides to enhance your app. Docs, Tweet.We unveil our new Request For Contribution Github board here. It\u2019s your guide to contribute to LlamaIndex, streamlining community suggestions.We released the guide on using the Jina 8k open-source text embedding model with LlamaIndex.We introduce our comprehensive survey of llama2-chat models across varying capacities in LlamaIndex. The major insight: While reasoning is enhanced with more parameters, structured outputs remain a challenge. Tweet.We share a guide to test the newly released HuggingFace Zephyr-7b-beta model on LlamaIndex RAG/agent tasks, it stood out as the only 7B LLM capable of handling ReAct agent tasks over data.We share a new guide on Advanced Prompt Engineering for RAG. Learn about understanding, customizing, and extending RAG prompts, from QA templates to few-shot examples and context/query transformations. Tweet.\u270d\ufe0f Tutorials:Kiran made a blog post on Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser.Wenqi Glantz gave us an excellent blog post on Optimizing Text Embeddings with HuggingFace\u2019s text-embeddings-inference Server and LlamaIndex.Ravi Theja\u2019s blog post delves into NVIDIA Research on RAG vs Long Context LLMs, questioning the necessity of RAG in the presence of long-context LLMs.Sudarshan Koirala has a tutorial on Extracting Tables + Texts from .htm pages for RAG Using LlamaIndex.Wenqi Glantz also made a second blog post on Multimodal Retrieval with Text Embedding and CLIP Image Embedding for Backyard Birds.\u2699\ufe0f Integrations & Collaborations:We introduced our new cookbooks in partnership with Gradient AI, enabling effortless fine-tuning of open-source LLMs like Llama 2 and integration into your LlamaIndex RAG pipeline. Docs, Tweet.We introduced integration with HuggingFace Inference API which gives access to over 150,000 models. Now you can plugin any conversational, text_generation, feature_extraction endpoints into your LlamaIndex app. Docs, Tweet.\ud83c\udfa5 Webinars:Mayo Oshin and Jerry Liu gave a webinar on Unlocking ChatGPT for Business.\ud83d\udcdaWorkshops:Jerry Liu and Simon conducted a Multipart LlamaIndex workshop in collaboration with Anyscale.Ravi Theja conducted a day-long workshop on Retrieval Augmented Generation with LlamaIndex.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-10-31-36244e2b3f0c"
        },
        {
            "date": "Jan 25, 2024",
            "author": "LlamaIndex",
            "title": "Building a Slack bot that learns with LlamaIndex, Qdrant and Render",
            "tags": [
                "llm",
                "qdrant",
                "render",
                "ai",
                "slack"
            ],
            "content": "In this post we\u2019re going to walk you through the process of building and deploying a Slackbot that listens to your conversations, learns from them, and uses that knowledge to answer questions about what\u2019s going on in your Slack workspace. We\u2019ll also deploy it to production on Render!Things you\u2019ll need to startRudimentary understanding of LlamaIndex. If you haven\u2019t got that, the starter tutorial in our documentation will give you as much as you need to understand this tutorial and takes only a few minutes.A working knowledge of Python, and Python 3.11 or higher installedA Slack workspace you can install apps to (so you\u2019ll need to be an admin)A clone of our Slackbot repo on your local machine. We\u2019ll be referring to files in this repo throughout the post.Step 1: Create a Slack app, and install it to your workspaceThis is the most complicated step, because Slack is very picky about permissions.The very first version of your Slackbot is going to be only about 20 lines of code. All it does is provide a \u201cchallenge\u201d endpoint that Slack needs to verify your app is available. You can see this code as the file 1_flask.py in the repo. Let's walk through it.First we bring in your dependencies. You\u2019ll need to install these with pip or poetry if you don\u2019t have them already.from flask import Flask, request, jsonifyNow we\u2019ll create your flask app and set it up so it can run in development.flask_app = Flask(__name__)\n\nif __name__ == \"__main__\":\n    flask_app.run(port=3000)Between those lines we\u2019ll add our basic route: if a POST request is received that contains a JSON object with a challenge key, we'll return the value of that key. Otherwise we'll do nothing.@flask_app.route(\"/\", methods=[\"POST\"])\ndef slack_challenge():\n    if request.json and \"challenge\" in request.json:\n        print(\"Received challenge\")\n        return jsonify({\"challenge\": request.json[\"challenge\"]})\n    else:\n        print(\"Got unknown request incoming\")\n        print(request.json)\n    returnMake your app available to SlackTo configure a Slack app, it needs to be running somewhere Slack can see it. So let\u2019s run our Slack app:python 1_flask.pyAnd we\u2019ll set it up so the world can see it using ngrok. You\u2019ll need to download and install ngrok for this step. Once you have it installed, run the following command so it can find our app running on port 3000:ngrok http 3000ngrok will give you an HTTPS url like https://1bf6-64-38-189-168.ngrok-free.app. Make a note of it, because we need to give that to Slack. Also keep in mind that if you stop ngrok and start it again, this URL will change and you'll need to tell Slack about that. You'll only need this during development.Register your app with SlackGo to the Slack API site and click \u201cCreate New App\u201d. You\u2019ll see a screen like this, you\u2019ll want to pick \u201cfrom scratch\u201d:Pick a nice friendly name and the workspace you want to install it to. You\u2019ll see a screen like this:Next you\u2019ll want to set up what permissions your app needs. Click the \u201cPermissions\u201d link in the bottom right:This will bring you to the \u201cscopes\u201d screen where you\u2019ll need to add all the scopes you see in this picture, namely:channels:read \u2014 the lets your app see what channels are avaialblechannels:join \u2014 this lets your app join channelschannels:history \u2014 this lets your app see previous messages in channelschat:write \u2014 this lets your app send messagesusers:read \u2014 this lets your app see people\u2019s namesOnce you\u2019ve saved those scopes, scroll up to \u201cInstall to workspace\u201d to install your app.You now need to tell Slack where your app is so you can receive messages from it. Click the \u201cEvent Subscriptions\u201d link in the left nav and fill it out so it looks something like this, specifically:Set your Request URL to that URL that ngrok gave you earlierSubscribe to the message.channels eventIf your app is running and ngrok is correctly tunneling, your Request URL should be Verified.Phew! That was a lot. Your Slack app is now registered and Slack will send it messages. But to get those messages, you have to tell it to join a channel.Step 2: Join a channel, and reply to messagesTo do this we\u2019ll need to extend our app. You can see the final result of this step in 2_join_and_reply.py. Let's walk through what we've added:import dotenv, os\ndotenv.load_dotenv()We need some environment variables, so you\u2019ll need to add these lines and install python-dotenv. You'll also need to create a .env file in the root of your project with three values:OPENAI_API_KEY: your OpenAI API key. You don't need this quite yet but you may as well get it now.SLACK_BOT_TOKEN: you can find this in the \"OAuth and Permissions\" section of your Slack app.SLACK_SIGNING_SECRET: you can find this in the \"Basic Information\" section of your Slack app.We\u2019re going to use Slack\u2019s handy Python SDK to build our app, so pip install slack-bolt and then update all our imports:from slack_bolt import App\nfrom flask import Flask, request, jsonify\nfrom slack_bolt.adapter.flask import SlackRequestHandlerNow initialize a Slack Bolt app using those secrets we set just now:app = App(\n    token=os.environ.get(\"SLACK_BOT_TOKEN\"),\n    signing_secret=os.environ.get(\"SLACK_SIGNING_SECRET\")\n)\nhandler = SlackRequestHandler(app)To listen to messages, the bot has to be in a channel. You can get it to join any and all public channels, but for the purposes of testing I\u2019ve created a channel called #bot-testing and that's the one it's joining here:channel_list = app.client.conversations_list().data\nchannel = next((channel for channel in channel_list.get('channels') if channel.get(\"name\") == \"bot-testing\"), None)\nchannel_id = channel.get('id')\napp.client.conversations_join(channel=channel_id)app.client is the Bolt framework's Slack WebClient, so you can do anything a WebClient can do directly from within the framework. The final addition here is a very simple message listener:@app.message()\ndef reply(message, say):\n    print(message)\n    say(\"Yes?\")In the Bolt framework, the @app.message decorator tells the framework to trigger this method when it receives a message event. The say parameter is a function that will send a message back to the channel the message came from. So this code will send a message back to the channel saying \"Yes?\" every time it receives a message.Let\u2019s try it out! Stop running 1_flask.py and run python 2_join_and_reply.py instead. You don't need to restart ngrok, it will continue to send messages to port 3000 as before. Here's me trying it out:Success! We have a very annoying bot that replies to every single thing anybody says. We can do better!Step 3: reply only to messages that mention the botThis is a pretty simple change on the surface, but Slack\u2019s incoming message format is a little complicated so we have to add a fair bit of code. You can see the final results in 3_reply_to_mentions.py.First, to tell when our bot is being mentioned, we need our bot\u2019s User ID. Under the hood, Slack doesn\u2019t use user names or even @-handles, but a globally unique ID across all Slack installations. We have to get that:auth_response = app.client.auth_test()\nbot_user_id = auth_response[\"user_id\"]Now we add an annoyingly complicated chunk of code that parses through Slack\u2019s message object to see what user is mentioned in an incoming message. If it\u2019s the bot, the bot replies, otherwise it just ignores the message. As we go further, we\u2019ll treat messages to the bot as \u201cqueries\u201d and any other message as a \u201cfact\u201d for it to store, but we won\u2019t be storing it just yet.@app.message()\ndef reply(message, say):\n    if message.get('blocks'):\n        for block in message.get('blocks'):\n            if block.get('type') == 'rich_text':\n                for rich_text_section in block.get('elements'):\n                    for element in rich_text_section.get('elements'):\n                        if element.get('type') == 'user' and element.get('user_id') == bot_user_id:\n                            for element in rich_text_section.get('elements'):\n                                if element.get('type') == 'text':\n                                    query = element.get('text')\n                                    print(f\"Somebody asked the bot: {query}\")\n                                    say(\"Yes?\")\n                                    return\n    # otherwise do something else with it\n    print(\"Saw a fact: \", message.get('text'))Oof. That took a while to get right! But now our bot only replies when it\u2019s mentioned:Step 4: use LlamaIndex to store facts and answer questionsWe\u2019re all the way at step 4 and we still haven\u2019t done anything with LlamaIndex! But now\u2019s the time. In 4_incremental_rag.py you'll see a demonstration of a simple command-line Python script that uses LlamaIndex to store facts and answer questions. I won't walk you through every line (the script has helpful comments for that), but let's look at the important ones. Remember to pip install llama-index!First we create a new VectorStoreIndex, an in-memory vector store where we'll be storing our facts. It's empty to start with.index = VectorStoreIndex([])Next we create 3 Document objects and insert them each into our index. Real documents can be huge blocks of text, whole PDFs, even images, but these are just some simple, Slack-message-sized facts.doc1 = Document(text=\"Molly is a cat\")\ndoc2 = Document(text=\"Doug is a dog\")\ndoc3 = Document(text=\"Carl is a rat\")\n\nindex.insert(doc1)\nindex.insert(doc2)\nindex.insert(doc3)And finally we create a query engine from our index and ask it a question:# run a query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Molly?\")\nprint(response)The result is \u201cMolly is a cat\u201d plus a whole lot of debugging info because we turned on noisy debugging in 4_incremental_rag.py. You can see the prompt we sent to the LLM, the context it retrieved from the index, and the response it generated and sent back to us.Step 5: use LlamaIndex to store facts and answer questions in SlackIn 5_rag_in_slack.py we are combining the two things we had before: script 3, where we reply to queries, and script 4, where we store facts and answer questions. Once again we won't walk through every line, but here are the important changes:First pip install llama-index if you didn't already, and bring in your deps. Initialize your index while you're at it:from llama_index import VectorStoreIndex, Document\n\nindex = VectorStoreIndex([])Where previously we were just replying with \u201cYes?\u201d (line 73) let\u2019s instead send a query to the query engine and reply with the response:query = element.get('text')\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\nsay(str(response))And where previously we were just noting that we\u2019d seen a fact (line 82), let\u2019s store it in the index:index.insert(Document(text=message.get('text')))The result is a Slackbot that can answer questions about what it\u2019s been told:Amazing! You can easily imagine a bot that listens to everybody\u2019s conversations and is able to answer questions about things people said weeks or months ago, saving everybody time and effort searching through old messages.Step 6: persist our memoryOur bot has a critical flaw though: the index is stored only in memory. If we restart the bot, it forgets everything:In 6_qdrant.py we bring in Qdrant, an open-source, local vector database that stores these facts on disk instead. That way if we restart our bot it remembers what was said before. pip install qdrant-client and bring in some new deps:import qdrant_client\nfrom llama_index.vector_stores.qdrant import QdrantVectorStoreNow we\u2019ll initialize the Qdrant client, attach it to a storage context, and give that storage context to our index when we initialize it:client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"slack_messages\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex([],storage_context=storage_context)That\u2019s it for this step! Your bot now survives reboots, and remembers that I typoed \u201cDoug\u201d as \u201cDough\u201d and was too lazy to fix it for the screenshot:Step 7: make recent messages more importantWe now have a pretty capable bot! But it has a subtle problem: people can say conflicting things, and it doesn\u2019t have a way to decide who was \u201cright\u201d, such as when I change my mind about what the dog\u2019s name should be:In real Slack conversations, as a situation evolves people might move from saying a project is \u201cin planning\u201d to \u201cunderway\u201d to \u201claunched\u201d. So we need a way to tell the bot that more recent messages are more important than older ones.To make this happen we have to do quite a bit of refactoring, the final results of which you can see in 7_recency.py. First we need a bunch of new deps:import datetime, uuid\nfrom llama_index.schema import TextNode\nfrom llama_index.prompts import PromptTemplate\nfrom llama_index.postprocessor import FixedRecencyPostprocessor\nfrom llama_index import set_global_handlerTo make recent messages more important, we have to know when a message was sent. To do that we are going to stop inserting Documents into the index and instead insert Nodes, to which we're going to attach the timestamp as metadata (under the hood, our Documents were always being converted into Nodes anyway so this doesn't change much):dt_object = datetime.datetime.fromtimestamp(float(message.get('ts')))\nformatted_time = dt_object.strftime('%Y-%m-%d %H:%M:%S')\n\n# get the message text\ntext = message.get('text')\n# create a node with metadata\nnode = TextNode(\n    text=text,\n    id_=str(uuid.uuid4()),\n    metadata={\n        \"when\": formatted_time\n    }\n)\nindex.insert_nodes([node])I\u2019ve also factored out the reply logic from message handling into its own function, answer_question, just to make things a little easier to read. The first thing we're going to change is the prompt that we give to our LLM: we have to tell it that more recent messages are important. To do this we create a prompt template:template = (\n    \"Your context is a series of chat messages. Each one is tagged with 'who:' \\n\"\n    \"indicating who was speaking and 'when:' indicating when they said it, \\n\"\n    \"followed by a line break and then what they said. There can be up to 20 chat messages.\\n\"\n    \"The messages are sorted by recency, so the most recent one is first in the list.\\n\"\n    \"The most recent messages should take precedence over older ones.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"You are a helpful AI assistant who has been listening to everything everyone has been saying. \\n\"\n    \"Given the most relevant chat messages above, please answer this question: {query_str}\\n\"\n)\nqa_template = PromptTemplate(template)The fun thing about working with LLMs is how often you end up just describing what you\u2019re doing in English and that being what you send to the LLM. A prompt template will automatically get the context_str and query_str from the query engine. But we have to set this template on our query engine, like so:query_engine.update_prompts(\n    {\"response_synthesizer:text_qa_template\": qa_template}\n)Now there\u2019s two more things we\u2019re going to change. We\u2019re going to take the results we get from the vector store and sort them by recency, something LlamaIndex has a built-in class for. It\u2019s called the FixedRecencyPostprocessor. We tell it the key that holds the timestamp (which we defined earlier on the nodes, above) and how many results it should return:postprocessor = FixedRecencyPostprocessor(\n    top_k=20, \n    date_key=\"when\", # the key in the metadata to find the date\n    service_context=ServiceContext.from_defaults()\n)Then we need to create our query engine with the postprocessor attached:query_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor])While we were at it we did our final thing, which was pass similarity_top_k=20, which means the vector store will give us 20 Slack messages as context (the default is just 2, because usually the chunks of text in a Node are a lot bigger).Tada! Now the bot knows to take more recent statements as the truth.Step 8: draw the rest of the owlThis bot is working pretty well now, but I was having such fun when building it I got carried away and added two more features:I attached metadata about who was speaking, not just when, so the bot can answer questions like \u201cWhat did Logan say about the project?\u201dMy colleagues interacting with the bot tried to ask follow-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it\u2019s in a thread, and treat replies in a thread as follow-up questions, even if the user doesn\u2019t mention the bot directly:The code to make both of those happen is in 8_rest_of_the_owl.py but I'm not going to be stepping through it line by line. We have to deploy this thing!Step 9: deploy to RenderUntil now we\u2019ve been working with local scripts running through the ngrok tunnel, but even the most dedicated coder turns their laptop off sometimes. Let\u2019s put this thing on a real server.Login to RenderWe\u2019ll be deploying to Render, a Python-friendly hosting service that\u2019s free for small projects. Sign up for an account (I recommend logging in with GitHub).Create a new GitHub repositoryRender deploys things from GitHub repositories, so you\u2019ll need to create a new one and copy 2 files from our existing repo into it:pyproject.toml8_rest_of_the_owl.py which we're going to rename to \"app.py\" for simplicity.Commit those and push them up to GitHub.Create a new Render web serviceIn Render, create a new web service. Connect it to the repo on GitHub you just created:Render will probably automatically detect that this is a Python app but you should make sure the following settings are correct:Name: any name you chooseRegion: any region is fineBranch: mainRoot directory: (blank, meaning root)Runtime: Python 3Build command: poetry installStart command: gunicorn app:flask_app (this will definitely need to be set)You\u2019ll also need to scroll down and set some environment variables:PYTHON_VERSION: 3.11.6 (or whatever version you\u2019re using)OPENAI_API_KEY: your OpenAI API keySLACK_BOT_TOKEN: your Slack bot tokenSLACK_SIGNING_SECRET: your Slack signing secret from beforeThen click deploy and away you go!You now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations!What next?There\u2019s a whole bunch of features you could add to this bot, roughly in increasing order of difficulty:Join every channel instead of just one, clearly!Add a way to tell the bot to forget things (delete nodes)Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendarGive the bot \u201ctags\u201d so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain wayAdd multi-modal abilities, so the bot can read images and even reply with generated imagesAnd tons more!This bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"
        },
        {
            "date": "Mar 12, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-03-12",
            "tags": [
                "llm",
                "newsletter",
                "rag"
            ],
            "content": "Salutations, LlamaIndex fans! \ud83e\udd99It's been another thrilling week in LlamaLand! With the release of Anthropic\u2019s new models Claude-3 Opus, Sonnet, and Haiku, we have numerous tutorials, cookbooks, and updates to share with you.A quick reminder: we are running our first ever user survey. It takes only 3 minutes and it helps us out a lot! Now let\u2019s dive in.\ud83e\udd29\u00a0The highlights:LlamaParse JSON Mode: A new feature that transforms PDF content into structured data, simplifying RAG pipeline development for complex documents containing images, text and tables. Notebook, Tweet.Hierarchical Code Splitting: Enhance code understanding with a novel technique that organizes large codebases into a hierarchical structure for improved navigation and task-solving. LlamaPack, Tweet.Anthropic Cookbook Series: Learn to build various LLM applications with Claude 3, ranging from simple to complex, through detailed guides and tutorials. Cookbooks, Tweet.\u2728 Feature Releases and Enhancements:We launched LlamaParse JSON Mode, a new feature that structures text and images from PDFs into a dict format. With the integration of multimodal models like claude-3 opus, it's now simpler to develop RAG pipelines for complex PDFs containing text, images, and tables. Notebook, Tweet.We launched a novel hierarchical code splitting technique to enhance RAG/agents for code comprehension, featuring CodeHierarchyNodeParser by ryanpeach. This method breaks down large code files into a hierarchical structure, enabling a knowledge graph-like approach for efficient code navigation and task-solving. LlamaPack, Tweet.We integrated with Videodb to run RAG over video streams using LlamaIndex. This tool allows you to upload, search, and stream videos based on spoken words or visual scenes, now available as a built-in retriever in LlamaIndex. Tweet.\ud83c\udfa5\u00a0Demos:Build an AI Browser Copilot: a project by Daniel Huynh that demonstrates how to create a browser agent using RAG, local embeddings, and Mixtral to execute browser tasks from a Colab notebook, showcased with a video on navigating HuggingFace datasets.RAG over your code: a project by Akshay on creating a local code assistant using LlamaIndex, MistralAI, and Streamlit to index and query GitHub repositories, offering a foundational guide for advanced code QA.https://nething.xyz/: a project by Raymond Weitekamp on generating production-ready 3D CAD models from text prompts. It uses LLM code generation to create commands to generate printable 3D objects from text prompts using LlamaIndex. Blog, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to the Anthropic Cookbook Series: Create context-augmented LLM apps using Claude 3, from basic RAG to advanced agents, through six notebooks and four videos.Video guide exploring diverse applications of Claude-3 with LlamaIndex. tooling, covering Vanilla RAG, routing, sub-question query planning, structured data extraction, text-to-SQL, and agents\u2014a perfect starter kit for Claude enthusiasts.\u270d\ufe0f Tutorials:Tutorial by Ankush k Singal on building local LLM agents with Llama.cpp for step-wise execution and incorporating human feedback during execution.\u201cRAG over Complex PDFs V2\": a comprehensive tutorial by AI Makerspace on crafting advanced RAG pipelines for handling messy PDFs with LlamaParse and LlamaIndex, where naive RAG falls short.Tutorial on building a real-time RAG chatbot using Google Drive and Sharepoint by\u00a0Anup Surendran\u00a0and\u00a0Berke Can Rizai.Step-by-step tutorial to improve the quality of your RAG application using JinaAI reranker, LlamaIndex, and MistralAI.\ud83c\udfa5\u00a0Webinars:Webinar with Parth Sarthi, lead author of RAPTOR - Tree-Structured Indexing and Retrieval.\ud83d\udcc5\u00a0Events:We are hosting a RAG meetup in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-03-12"
        },
        {
            "date": "Dec 17, 2023",
            "author": "Harshad Suryawanshi",
            "title": "Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, GPT3.5, and Streamlit",
            "tags": [
                "natural-language-to-sql",
                "llamaindex",
                "llm",
                "gpt35",
                "openai"
            ],
            "content": "In the dynamic world of AI and data analytics, the ability to bridge the gap between complex data queries and non-technical users is a game-changer. My latest project, Na2SQL, showcases this exciting advancement. Leveraging the power of LlamaIndex and OpenAI\u2019s GPT-3.5, this app allows users, regardless of their SQL knowledge, to derive valuable insights from a database using simple natural language.FeaturesIntuitive Natural Language Queries: The core of this application is its ability to understand and process natural language queries. Users can ask questions in plain English and receive SQL queries and insights in return, all without any prior SQL experience.Advanced Data Processing: The app doesn\u2019t just stop at generating SQL queries; it executes these queries and analyzes the results to provide meaningful insights, making it a powerful tool for data analysis.User-Friendly Interface with Streamlit: I chose Streamlit for its simplicity and effectiveness in creating interactive web applications. The app\u2019s interface is straightforward, ensuring a smooth user experience.Database Viewer: An interactive database viewer in the sidebar on the left allows users to explore the database structure, enhancing their understanding and interaction with the data.The Tech StackThis project harmoniously integrates several advanced technologies:OpenAI\u2019s GPT-3.5: At the heart of the application is GPT-3.5, enabling the app to understand natural natural language queries and transform them into valid SQL queries. Furthermore, it also generates the final analysis considering both the user\u2019s query and the SQL output, thereby providing a comprehensive and relevant response.LlamaIndex: A pivotal component of the app is LlamaIndex\u2019s SQLTableQueryEngine. This powerful tool translates natural language queries into SQL, handles the execution of these queries, and plays a significant role in the subsequent analysis using GPT 3.5. Its integration ensures a smooth transition from user inputs to database insights, culminating in a meaningful final analysis that encapsulates the entire natural language-to-SQL-to-execution process.LlamaIndex\u2019s Streamlit LlamaPack: Using LlamaIndex\u2019s Streamlit LlamaPack, we quickly assemble and highly functional Streamlit UI. This framework significantly simplifies the UI development process, allowing for rapid deployment and an enhanced user experience.SQLite Database: The app interacts with an dummy SQLite ecommerce database, showcasing its ability to work with real-world data.Deep Dive into the CodeIn the heart of the application lies app.py, a script that brings to life the seamless interaction between natural language processing and SQL query generation.This code is an evolution of the Streamlit chatbot LlamaPack available on Llama Hub, further tailored to meet the specific needs of ecommerce data analytics. Let's dive into some key portions of the app.py script:1. Initial Imports and SetupThe script begins by importing necessary modules such as Streamlit, SQLAlchemy for database interaction, LlamaIndex for language model services, and other essential libraries.import streamlit as st\nfrom sqlalchemy import create_engine, inspect\nfrom typing import Dict, Any\n\nfrom llama_index import (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\nfrom llama_index.llama_pack.base import BaseLlamaPack\nfrom llama_index.llms import OpenAI\nimport openai\nimport os\nimport pandas as pd2. StreamlitChatPack ClassThe StreamlitChatPack class extends the base LlamaPack, setting up the page and modules necessary for the app's functionality.class StreamlitChatPack(BaseLlamaPack):\n\n    def __init__(\n        self,\n        page: str = \"Natural Language to SQL Query\",\n        run_from_main: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Init params.\"\"\"\n        self.page = page\n\n    # ... other methods ...3. The run MethodThis method is where the magic happens. It sets up the Streamlit page configuration and initializes the chat functionality.def run(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Run the pipeline.\"\"\"\n    import streamlit as st\n\n    st.set_page_config(\n        page_title=f\"{self.page}\",\n        layout=\"centered\",\n        initial_sidebar_state=\"auto\",\n        menu_items=None,\n    )\n    \n    # ... rest of the run method ...4. Database Schema Viewer in the SidebarA helpful feature is the Database Schema Viewer, conveniently located in the sidebar. This viewer serves as a reference tool, allowing users to see the structure and content of the database tables, enhancing their understanding of the data they\u2019re querying.# Sidebar for database schema viewer\nst.sidebar.markdown(\"## Database Schema Viewer\")\n\n# Create an inspector object\ninspector = inspect(engine)\n\n# Get list of tables in the database\ntable_names = inspector.get_table_names()\n\n# Sidebar selection for tables\nselected_table = st.sidebar.selectbox(\"Select a Table\", table_names)\n\ndb_file = 'ecommerce_platform1.db'\nconn = sqlite3.connect(db_file)\n\n# Display the selected table\nif selected_table:\n    df = get_table_data(selected_table, conn)\n    st.sidebar.text(f\"Data for table '{selected_table}':\")\n    st.sidebar.dataframe(df)\n\n# Close the connection\nconn.close()5. Database Interaction and LLM IntegrationThis part of the code loads the database from disk and initializes the LLM and the service context for use with Llamaindex. I\u2019ve used GPT3.5 here, but you can easily swap it out with any other LLM of your choice.# Function to load database and LLM\ndef load_db_llm():\n    engine = create_engine(\"sqlite:///ecommerce_platform1.db\")\n    sql_database = SQLDatabase(engine)  # Include all tables\n    llm2 = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-1106\")\n    service_context = ServiceContext.from_defaults(llm=llm2)\n    return sql_database, service_context, engine\n\nsql_database, service_context, engine = load_db_llm()6. Initializing the NLSQLTableQueryEngineOne of the most critical aspects of the application is the initialization of the NLSQLTableQueryEngine. This is where the app sets up the engine responsible for converting natural language queries into SQL queries, executing them and generating the final response, all with the help of GPT 3.5.# Initializing the query engine\nif \"query_engine\" not in st.session_state:\n    st.session_state[\"query_engine\"] = NLSQLTableQueryEngine(\n        sql_database=sql_database,\n        synthesize_response=True,\n        service_context=service_context\n    )7. User Interaction and Displaying ResultsThe script provides an interactive interface for users to input natural language queries, which are then translated into SQL queries and executed.The app concludes by displaying the SQL queries and responses, offering an informative and engaging user experienceif prompt := st.chat_input(\n    \"Enter your natural language query about the database\"\n):  # Prompt for user input and save to chat history\n    with st.chat_message(\"user\"):\n        st.write(prompt)\n    add_to_message_history(\"user\", prompt)\n\n# If last message is not from assistant, generate a new response\nif st.session_state[\"messages\"][-1][\"role\"] != \"assistant\":\n    with st.spinner():\n        with st.chat_message(\"assistant\"):\n            response = st.session_state[\"query_engine\"].query(\"User Question:\"+prompt+\". \")\n            sql_query = f\"```sql\\n{response.metadata['sql_query']}\\n```\\n**Response:**\\n{response.response}\\n\"\n            response_container = st.empty()\n            response_container.write(sql_query)\n            add_to_message_history(\"assistant\", sql_query)Wrapping UpThis app is more than just a tool; it\u2019s a step towards making data analytics accessible to a broader audience. It embodies the potential of AI in simplifying complex data interactions. I invite you to explore this application, witness its capabilities, and join me in this journey towards a more inclusive data-driven future.Link to Github RepoConnect with Me on LinkedInLinkedin Post:Harshad S. on LinkedIn: #ai #llamaindex #streamlit #largelanguagemodels\u2026AI Prototype 6: Transforming Natural Language to SQL and Insights for E-commerce with LlamaIndex, OpenAI GPT3.5, and\u2026www.linkedin.com",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//transforming-natural-language-to-sql-and-insights-for-e-commerce-with-llamaindex-gpt3-5-e08edefa21f9"
        },
        {
            "date": "Nov 28, 2023",
            "author": "Ravi Theja",
            "title": "OpenAI Cookbook: Evaluating RAG systems",
            "tags": [
                "llamaindex",
                "openai",
                "llm",
                "retrieval-augmented"
            ],
            "content": "We\u2019re excited to unveil our OpenAI Cookbook, a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you\u2019ll find it useful in enhancing the effectiveness of your RAG systems, and we\u2019re thrilled to share it with you.The OpenAI Cookbook has three sections:Understanding Retrieval-Augmented Generation (RAG): provides a detailed overview of RAG systems, including the various stages involved in building the RAG system.Building RAG with LlamaIndex: Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham\u2019s essay, utilizing the VectorStoreIndex.Evaluating RAG with LlamaIndex: The final section focuses on assessing the RAG system\u2019s performance in two critical areas: the Retrieval System and Response Generation.We use our unique synthetic dataset generation method, generate_question_context_pairs to conduct thorough evaluations in these areas.Our goal with this cookbook is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex.Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex.Keep building with LlamaIndex!\ud83e\udd99",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//openai-cookbook-evaluating-rag-systems-fe393c61fb93"
        },
        {
            "date": "Nov 17, 2023",
            "author": "Ravi Theja",
            "title": "GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques.",
            "tags": [
                "llamaindex",
                "gpt-4",
                "llm",
                "vision",
                "openai"
            ],
            "content": "IntroductionGPT-4V has amazed us with its ability to analyze images and even generate website code from visuals.This blog post investigates GPT-4V\u2019s proficiency in interpreting bar charts, scatter plots, and tables. We aim to assess whether specific questioning and chain of thought prompting can yield better responses compared to broader inquiries. Our demonstration seeks to determine if GPT-4V can exceed these known limitations with precise questioning and systematic reasoning techniques.We observed in these experiments that asking specific questions, rather than general ones, yields better answers. Let\u2019s delve into these experiments.You can also follow along with this blog post in our Google Colab Notebook.NOTE: This blog post aims to inform the community about GPT-4V\u2019s performance, though the results might not be universally applicable. We strongly advise conducting tests with similar questions on your own dataset before drawing conclusions.Experiment:What we\u2019ll do is take an image of each of a bar chart, scatter plot, and table and analyze them by asking three types of questions.General Question: Simply ask, \u201cAnalyse the image.\u201dSpecific Question: Specific Question the performance of a certain category by providing more details.Chain of Thought Prompting: Use a step-by-step reasoning method to walk through the analysis.These guidelines aim to test how different questioning techniques might improve the precision of the information we gather from the images.Data for experiments:To test out we selected the following images from Llama2 and MistralAI papers.Violation percentage of safety with different LLMs across categories. (Llama2 paper)Llama2 vs Mistral model performances across various NLP tasks.(Mistral paper)Performances of different LLMs across various NLP tasks. (Llama2 paper)Let\u2019s go through each of these data points one by one.Image 1 \u2014 Violation percentage of safety with different LLMs across categories.General Question:Question: Analyse the Image.Answer:The image you\u2019ve provided is a bar chart displaying data grouped into three categories of online violations: hateful and harmful, illicit and criminal activity, and unqualified advice. Each of these categories has a number in parentheses, indicating the sample size of reported incidents for each type of violation (395, 728, and 311, respectively). The x-axis of the chart specifies various social media platforms or services, such as \u201cVideo sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums & boards\u201d, \u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and \u201cOther\u201d. It appears to measure how often these categories of violations occur on each type of platform. The y-axis measures the percentage of violations reported, ranging from 0% to 60%. Each platform/service has three bars corresponding to the three violation categories, showing their respective percentages. The bars also have error bars, which typically represent the variability of the data, such as standard deviation, standard error, or confidence interval, indicating the precision of the estimates. The chart has a legend indicating the color corresponding to each of the three categories of violations. This visual representation helps to compare the prevalence of different types of violations across the different types of online services. Without specific numbers, it\u2019s not possible to provide exact percentages, but we can observe trends, such as: \u2014 \u201cForums & boards\u201d and \u201cSocial networking\u201d platforms have notably higher percentages across all three types of violations compared to other platforms. \u2014 \u201cCommercial Websites\u201d seem to have lower percentages of reported hateful and harmful activities and unqualified advice but higher percentages of illicit and criminal activities when compared to other platforms like \u201cWiki\u201d or \u201cP2P/File sharing\u201d. \u2014 \u201cGaming\u201d appears to have a moderate percentage of hateful and harmful violations, lower levels of illicit and criminal activity, and relatively higher levels of unqualified advice. Overall, this chart is a visual tool that likely aims to inform about the prevalence of certain types of online violations across different digital platforms.Observation:As you can see though the categories hateful and harmful, illicit and criminal activity, and unqualified advice but it hallucinated with x-axis values with \u2014 \u201cVideo sharing\u201d, \u201cSocial networking\u201d, \u201cGaming\u201d, \u201cDating\u201d, \u201cForums & boards\u201d, \u201cCommercial Websites\u201d, \u201cMedia sharing\u201d, \u201cP2P/File sharing\u201d, \u201cWiki\u201d, and \u201cOther\u201d.Specific Questions:Question: Which model among llama2 and vicuna models does better in terms of violation percentages in Hateful and harmful category.Answer:From the provided bar chart, we can analyze the violation percentage for both the Llama2 and Vicuna models in the \u201cHateful and harmful\u201d category. To determine which model does better (i.e., has a lower violation percentage), you would look at the height of the bars corresponding to each model within that category. In the \u201cHateful and harmful\u201d category (which is the first group of bars on the left), you can compare the blue bar (representing Llama2) to the light blue bar (representing Vicuna). The model with the shorter bar in this category will have a lower violation percentage, and hence, better performance with respect to minimizing hateful and harmful content. Please note that I cannot physically see the chart, so I\u2019m unable to specify which model has the lower violation percentage. If you provide the actual percentages or describe the relative bar lengths for Llama2 and Vicuna in the \u201cHateful and harmful\u201d category, I could then tell you which model performs better in that respect.Observation:It failed to accurately capture the information, mistakenly identifying the light blue bar as representing Vicuna when, in fact, it is the light blue bar that represents Llama2.Let us give more information on the question and see if it can improve the answer.Question: In the image provided to you depicts about the violation rate performance of various AI models across Hateful and harmful, Illicit and criminal activity, Unqualified advice categories. Hateful and harmful category is in first column. Bars with light blue are with Llama2 model and dark blue are with Vicuna models. With this information, Can you compare about Llama2 and Vicuna models in Hateful and harmful category.Answer:In the image you provided, which shows a bar graph for the violation rate performance of various AI models in the Hateful and harmful category, we can analyze the light blue bars that represent the Llama2 model and the dark blue bars that represent the Vicuna model. Based on the visual information given, I can compare the two models within this category by observing the heights of the light blue bars (Llama2) versus the heights of the dark blue bars (Vicuna) for each subsection within the category. A lower bar indicates a lower violation rate, which could be interpreted as better performance in minimizing violations for the given criteria. Since I can\u2019t give you the exact numbers or percentages, I would describe their relative performances. It appears that for some subsections, the Llama2 bars are shorter than the Vicuna bars, suggesting that the Llama2 model could have a lower violation rate in those areas. Conversely, in other subsections, the Vicuna bars might be shorter than the Llama2 bars, indicating a lower violation rate for the Vicuna model in those areas. The exact subsections where one model outperforms the other would depend on their relative bar heights, which should be compared individually for the given information.Observation:It did answer the question correctly.Chain Of Thought Prompting:Question: Based on the image provided. Follow the steps and answer the query \u2014 which model among llama2 and vicuna does better in terms of violation percentages in \u2018Hateful and harmful\u2019.Examine the Image: Look at the mentioned category in the query in the Image.Identify Relevant Data: Note the violation percentages.Evaluate: Compare if there is any comparison required as per the query.Draw a Conclusion: Now draw the conclusion based on the whole data.Answer:Examine the Image: The image is a bar chart depicting violation percentages in three categories: \u201cHateful and harmful,\u201d \u201cIllicit and criminal activity,\u201d and \u201cUnqualified advice.\u201d Each category has two bars next to each other representing two models named \u201cllama2\u201d and \u201cvicuna.\u201d Identify Relevant Data: We are specifically interested in the \u201cHateful and harmful\u201d category, which is the first set of bars on the left. The two bars indicate the violation percentages for \u201cllama2\u201d and \u201cvicuna.\u201d Evaluate: By visually inspecting the bar corresponding to \u201cHateful and harmful,\u201d we see that the blue bar (indicating \u201cllama2\u201d) has a lower percentage than the red bar (indicating \u201cvicuna\u201d). The exact percentages are not clear, but we can discern the relative performance between the two. Draw a Conclusion: From the data visible, \u201cllama2\u201d has a lower violation percentage than \u201cvicuna\u201d in the \u201cHateful and harmful\u201d category, thereby doing better in that respect according to the given image.Observation:With chain of thought prompting it did hallicunate with bar colours but answered correctly saying Llama2 has lower violation compared to vicuna in Hateful and harmful though for a section Llama2 has higher violation compared to vicuna.Image2 \u2014 Llama2 vs Mistral model performances across various NLP tasks.General Question:Question: Analyze the Image.Answer:The image appears to be a set of four line graphs that compare the performance of different natural language processing (NLP) models across four different metrics: Natural Language Understanding (NLU), Reasoning, Knowledge, and Commonsense. The models compared are LLaMA 2, LLaMA 13B, Mistral, and GPT-3. Each graph plots the performance metric (on the y-axis) against the model size measured in billions of effective parameters (on the x-axis). In general, these plots demonstrate that as the model size increases, the performance on each metric improves. Here are specific observations for each graph: 1. Natural Language Understanding (Top left): All models show an increase in NLU performance as the model size increases. Mistral is depicted as the highest-performing model at each size benchmark. 2. Reasoning (Top right): Again, larger models perform better on reasoning tasks, with Mistral consistently outperforming the other models mentioned. 3. Knowledge (Bottom left): Similar to the NLU and reasoning tasks, the trend is for better performance with larger models, with Mistral leading across the different sizes. 4. Commonsense (Bottom right): This graph follows the same trend with the Mistral model performing better at each size level compared to the other models. Based on these graphs, Mistral appears to be the top-performing model in all four metrics, suggesting that it might be more efficient or effective than LLaMA 2, LLaMA 13B, and GPT-3, at least within the scope of the parameters and tasks represented here. On a technical note, it\u2019s important to mention that the y-axis shows different scales for the metrics (ranging from 45\u201375% for NLU, Knowledge, and Commonsense, and from 50\u201372% for Reasoning), which suggests that these tasks might have different levels of difficulty or different benchmarks for success.Observation:It did answer the query but hallucinated with the NLU task which is the MMLU task and assumed Mistral is available across all different model parameters.Specific Questions:Question: How well does mistral model compared to llama2 model?Answer:The image you\u2019ve provided contains four graphs, each plotting different performance metrics for evaluating language models. It compares two models: Mistral and LLaMA2 across four different aspects \u2014 MMLU (Multi-tasking multiple-choice), Reasoning, Knowledge, and Commonsense. In each graph, the performance metric is plotted on the y-axis while the effective model size in billion parameters is plotted on the x-axis. Here\u2019s a summary based on the trends observed in the graphs: 1. **MMLU (Top Left Graph)**: LLaMA2 shows a steeper performance curve as the model size increases, starting at around 61% for the smallest size and reaching upwards of 66% for the largest model. Mistral also shows performance gains with model size but plateaus earlier, reaching a level just shy of 62%. 2. **Reasoning (Top Right Graph)**: In the reasoning task, LLaMA2 again has a steeper improvement curve, starting from about 61% and surpassing 72% for the largest model. Mistral, while improving, seems to plateau near 70% for the largest model. 3. **Knowledge (Bottom Left Graph)**: This graph reflects a similar trend to the previous ones, with LLaMA2 beginning at a lower performance around 46% and eclipsing 52%. Mistral starts higher at around 48% and appears to plateau near 52%. 4. **Commonsense (Bottom Right Graph)**: Here, LLaMA2 starts its performance at approximately 62% and reaches just above 66%. Mistral seems to start at a slightly higher point than LLaMA2 but ends at a similar level to LLaMA2\u2019s largest model. Overall, the LLaMA2 model appears to show a greater degree of improvement in performance as the model size increases compared to Mistral across these metrics. Meanwhile, Mistral starts at a higher performance for some metrics but tends to plateau earlier, suggesting that LLaMA2 may scale better with size in terms of performance gains.Observation:Incorrect answers and percentages are not accurate enough and again assumed mistral is available across all parameter models.Question: Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?Answer:This image appears to be a set of four graphs, each comparing the performance of three different language models on various tasks: Mistral, LLaMA-2, and a hypothetical \u201cSize 7B\u201d model. The graphs are labeled with task-specific performance metrics: \u201cMLM U\u201d, \u201cReasoning\u201d, \u201cKnowledge\u201d, and \u201cCommonsense\u201d. The x-axes on the graphs represent model size in terms of the number of parameters, with three points that likely correspond to the sizes of the models being compared. The y-axes represent performance as a percentage, which could mean accuracy, precision, recall, or another relevant performance metric depending on the specific task. The graphs appear to show that Mistral performs better than LLaMA-2 and the Size 7B model across all metrics. This indicates that within the context of these measurements and tasks, Mistral is a stronger model. The exact nature of the tasks or what \u201cMLM U\u201d, \u201cReasoning\u201d, \u201cKnowledge\u201d, and \u201cCommonsense\u201d specifically refer to are not detailed in the image, but they likely correspond to standard NLP tasks designed to test understanding of language, ability to reason, knowledge recall, and commonsense reasoning, respectively. The performance improvements are depicted as increasingly significant with larger model sizes.Observation:Now by giving the detail that mistral is available in 7B series, it is able to answer correctly.Chain of thought prompting:Question: Based on the image provided. Follow the steps and answer the query \u2014 Assuming mistral is available in 7B series. How well does mistral model compared to llama2 model?.Examine the Image: Look at the mentioned category in the query in the Image.Identify Relevant Data: Note the respective percentages.Evaluate: Compare if there is any comparision required as per the query.Draw a Conclusion: Now draw the conclusion based on the whole data.Answer:Examine the Image: The image contains four graphs, each graph compares the performance of three different models \u2014 Llama 2, Mistral, and an unnamed third model \u2014 across different metrics: mAP@L (mean Average Precision at L), Reason@L (presumably a reasoning score at L), Knowledge@L, and Comprehension@L. Each graph shows performance as a function of model size (in terms of billion parameters). Identify Relevant Data: We need to focus on the Mistral and Llama 2 models across all four graphs to extract the relevant data. For mAP@L: \u2014 Llama 2 reaches above 65% when reaching 70 billion parameters. \u2014 Mistral starts at around 5 billion parameters with about 48% and reaches almost 65% by the time it gets to 34 billion parameters. For Reason@L: \u2014 Llama 2 reaches about 68% when reaching 70 billion parameters. \u2014 Mistral starts at around 5 billion parameters with about 52% and reaches almost 68% by the time it gets to 34 billion parameters. For Knowledge@L: \u2014 Llama 2 starts at about 45% with 1.3 billion parameters and reaches just slightly under 60% by 70 billion parameters. \u2014 Mistral is not plotted in this graph, so no direct comparison can be made. For Comprehension@L: \u2014 Llama 2 starts just above 40% with 1.3 billion parameters and reaches just under 60% by 70 billion parameters. \u2014 Mistral starts around 13 billion parameters with about 52% and continues above 62% by 34 billion parameters. Evaluate: From the graphs, we can see that Mistral and Llama 2 are very comparable in performance in the metrics where both are shown. However, Mistral is not displayed on the Knowledge@L graph, so no comparison can be made for that particular metric. Draw a Conclusion: In the graphs where both models can be compared (mAP@L, Reason@L, and Comprehension@L), Mistral appears to perform comparably to Llama 2, reaching similar levels of performance at smaller parameter sizes relative to the plotted sizes for Llama 2. This suggests that Mistral could be more parameter-efficient. However, since there is no data for Mistral in the Knowledge@L graph, we cannot draw a complete comparisonObservation:There is a hallucination with a number of model parameters and percentage points though the final conclusion is partially correct.Image 3 \u2014 Performances of different LLMs across various NLP tasks.General Question:Question: Analyze the Image.Answer:The image appears to be a table containing numerical data, which seems to be a comparative analysis of various models across multiple parameters or tests. Each row represents a different model or configuration as indicated by names such as \u201cMPT\u201d and \u201cFalcon\u201d followed by a size specification like \u201c70B,\u201d \u201c7B,\u201d or some other parameter. The columns are labeled with test names or evaluation metrics, such as \u201cSize,\u201d \u201cAvg,\u201d \u201cAQuA-RAT,\u201d \u201cLogiQA,\u201d \u201cL-SAT-AR,\u201d \u201cL-SAT-IC,\u201d \u201cL-SAT-RC,\u201d \u201cSAT-en (w/o Psg.),\u201d and \u201cSAT-math.\u201d The data is likely related to performance scores of these models on these tests, where higher numbers probably indicate better performance. Without additional context, it is difficult to provide a comprehensive analysis of this data, but it seems clear that it is intended to provide a performance comparison between different models on various tasks, possibly in the field of machine learning or artificial intelligence evaluations, where such models are typically assessed on reasoning, comprehension, or problem-solving capabilities. The \u201c70B\u201d and \u201c7B\u201d demarcations could refer to the size of the model in terms of the number of parameters, commonly used in assessing language models. A deeper evaluation would require further insights into the specific nature of these tests and models, along with the intended use-case for which they were being compared.Observation:It did not analyze the image specifically but understood the overall data present in the image to some extent.Specific Questions:Question: which model has higher performance in SAT-en?Answer:In the table you\u2019ve provided, the models are compared based on their performance on several benchmarks, including SAT-en (SAT Analogies, or \u201cSAT\u201d in the table). To determine which model has higher performance specifically on the SAT-en benchmark, you\u2019ll need to look at the corresponding column. In the SAT-en column (second from the right), the two models with the highest scores are: \u2014 LLaMA1 65B: with a score of 57.9 \u2014 LLaMA2 70B: with a score of 63.4 Between these two, the LLaMA2 model with 70 billion parameters shows the higher performance on the SAT-en benchmark with a score of 63.4.Observation:It did answer correctly but the numbers are being hallucinated.Question: which model has higher performance in SAT-en in 7B series models?Answer:The table you\u2019ve provided shows performance benchmark scores for different model sizes across various AI models such as MPT, Falcon, and LLaMA on different tasks, such as Aqua-RAT, LogiQA, LastAR, SAT-en, and SAT-math. For the SAT-en task specifically, you asked which model in the 7B series has the highest performance. To find out, we need to look at the column labeled \u201cSAT-en (w/o Ps_8).\u201d In the 7B series of models, here are the scores: \u2014 MPT 7B: 37.1 \u2014 Falcon 7B: 37.3 \u2014 LLaMA 7B: 63.9 \u2014 Model2 7B: 37.4 The LLaMA 7B model outperforms the other 7B models on the SAT-en (w/o Ps_8) task with a score of 63.9.Observation:Incorrect answer and recognized Llama series of models and values incorrectly.Chain of thought prompting:Question: Based on the image provided. Follow the steps and answer the query \u2014 which model has higher performance in SAT-en in 7B series models?Examine the Image: Look at the mentioned category in the query in the Image.Identify Relevant Data: Note the respective percentages.Evaluate: Compare if there is any comparison required as per the query.Draw a Conclusion: Now draw the conclusion based on the whole data.Answer:To answer which model has higher performance in SAT-en in the 7B series models, I will follow the provided steps: Examine the Image: The categories in the image include model names and sizes, and performance scores for various tasks, including the SAT-en category. Identify Relevant Data: The relevant data for the SAT-en category are the percentages listed under it for each 7B series model. Evaluate: I will compare the SAT-en percentages of each 7B series model. Draw a Conclusion: The SAT-en scores for the 7B series models are as follows: \u2014 MPT 7B: 63.1% \u2014 Falcon 7B: 73.4% \u2014 LLama 1 7B: No data present for this category. \u2014 LLama 2 7B: 76.6% Based on the data, the LLama 2 7B model has the highest SAT-en performance among the 7B series models with a score of 76.6%.Observation:With chain of thought prompting we are able to get the right conclusion though it should be noted that it picked up wrong values.Final Observations:Observations made based on experiments on Hallucination and correctness. (Please note that these observations are specific to the images used and cannot be generalized, as they vary depending on the images.)SummaryIn this blog post, we have showcased experiments ranging from general inquiries to systematic questions and chain of thought prompting techniques and observed Hallucination and correctness metrics.However, it should be noted that the outputs from GPT-4V can be somewhat inconsistent, and the levels of hallucination are slightly elevated. Therefore, repeating the same experiment could result in different answers, particularly with generalized questions.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//gpt4-v-experiments-with-general-specific-questions-and-chain-of-thought-prompting-cot-techniques-49d82e6ddcc9"
        },
        {
            "date": "Oct 5, 2023",
            "author": "Ravi Theja",
            "title": "Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex",
            "tags": [
                "llamaindex",
                "ai",
                "llm",
                "openai",
                "retrieval"
            ],
            "content": "IntroductionRetrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system\u2019s efficiency and performance is the chunk_size. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex Response Evaluation comes in handy. In this blog post, we'll guide you through the steps to determine the best chunk size using LlamaIndex\u2019s Response Evaluation module. If you're unfamiliar with the Response Evaluation module, we recommend reviewing its documentation before proceeding.Why Chunk Size MattersChoosing the right chunk_size is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:Relevance and Granularity: A small chunk_size, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the similarity_top_k setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of \u2018hallucinations\u2019 and the \u2018relevancy\u2019 of responses based on the query and the retrieved contexts respectively.Response Generation Time: As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.In essence, determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.For a practical evaluation in choosing the right chunk_size, you can access and run the following setup on this Google Colab Notebook.SetupBefore embarking on the experiment, we need to ensure all requisite modules are imported:import nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.evaluation import (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\nfrom llama_index.llms import OpenAI\n\nimport openai\nimport time\nopenai.api_key = 'OPENAI-API-KEY'Download DataWe\u2019ll be using the Uber 10K SEC Filings for 2021 for this experiment.!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'Load DataLet\u2019s load our document.documents = SimpleDirectoryReader(\"./data/10k/\").load_data()Question GenerationTo select the right chunk_size, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various chunk_sizes. The DatasetGenerator will help us generate questions from the documents.data_generator = DatasetGenerator.from_documents(documents)\neval_questions = data_generator.generate_questions_from_nodes()Setting Up EvaluatorsWe are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, FaithfulnessEvaluator and RelevancyEvaluator, are initialised with the service_context .Faithfulness Evaluator \u2014 It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.Relevancy Evaluator \u2014 It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query.# We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)Response Evaluation For A Chunk SizeWe evaluate each chunk_size based on 3 metrics.Average Response Time.Average Faithfulness.Average Relevancy.Here\u2019s a function, evaluate_response_time_and_accuracy, that does just that which has:VectorIndex Creation.Building the Query Engine.Metrics Calculation.# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\ndef evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n    \"\"\"\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n    \n    Parameters:\n    chunk_size (int): The size of data chunks being processed.\n    \n    Returns:\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n    \"\"\"\n\n    total_response_time = 0\n    total_faithfulness = 0\n    total_relevancy = 0\n\n    # create vector index\n    llm = OpenAI(model=\"gpt-3.5-turbo\")\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n    # build query engine\n    query_engine = vector_index.as_query_engine()\n    num_questions = len(eval_questions)\n\n    # Iterate over each question in eval_questions to compute metrics.\n    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n    # we're using a loop here to specifically measure response time for different chunk sizes.\n    for question in eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    return average_response_time, average_faithfulness, average_relevancyTesting Across Different Chunk SizesWe\u2019ll evaluate a range of chunk sizes to identify which offers the most promising metrics.chunk_sizes = [128, 256, 512, 1024, 2048]\n\nfor chunk_size in chunk_sizes:\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\n  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")Bringing It All TogetherLet\u2019s compile the processes:import nest_asyncio\n\nnest_asyncio.apply()\n\nfrom llama_index import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.evaluation import (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\nfrom llama_index.llms import OpenAI\n\nimport openai\nimport time\n\nopenai.api_key = 'OPENAI-API-KEY'\n\n# Download Data\n!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n\n# Load Data\nreader = SimpleDirectoryReader(\"./data/10k/\")\ndocuments = reader.load_data()\n\n# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\neval_documents = documents[:20]\ndata_generator = DatasetGenerator.from_documents()\neval_questions = data_generator.generate_questions_from_nodes(num = 20)\n\n# We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n\n# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\ndef evaluate_response_time_and_accuracy(chunk_size):\n    total_response_time = 0\n    total_faithfulness = 0\n    total_relevancy = 0\n\n    # create vector index\n    llm = OpenAI(model=\"gpt-3.5-turbo\")\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n\n    query_engine = vector_index.as_query_engine()\n    num_questions = len(eval_questions)\n\n    for question in eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    return average_response_time, average_faithfulness, average_relevancy\n\n# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\nfor chunk_size in [128, 256, 512, 1024, 2048]\n  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n  print(f\"Chunk size {chunk_size} - Average Response time: {avg_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")ResultThe above table illustrates that as the chunk size increases, there is a minor uptick in the Average Response Time. Interestingly, the Average Faithfulness seems to reach its zenith at chunk_sizeof 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy.ConclusionIdentifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. With LlamaIndex\u2019s Response Evaluation module, you can experiment with various sizes and base your decisions on concrete data. When building a RAG system, always remember that chunk_size is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"
        },
        {
            "date": "May 12, 2023",
            "author": "Jerry Liu",
            "title": "Testing Anthropic Claude\u2019s 100k-token window on SEC 10-K Filings",
            "tags": [
                "large-language-models",
                "artificial-intelligence",
                "machine-learning",
                "financial"
            ],
            "content": "Anthropic\u2019s 100K Context Window expansion, just released yesterday, has taken the AI community by storm. A 100k token limit is approximately 75k words (~3x GPT4\u201332k\u2019s context window, ~25x that of GPT-3.5/ChatGPT); this means that you can now fit 300 pages of text in a single inference call.One of the core use cases highlighted in the Anthropic blog is analyzing an SEC 10-K filing; the model is capable of ingesting the entire report, and producing answers to different questions.Coincidentally, we published a tutorial a few months ago showing how LlamaIndex + Unstructured + GPT3 could help you perform different queries over UBER SEC 10-k filings. LlamaIndex provides a comprehensive toolkit to help manage external data on top of any LLM with limited context windows, and we show that we can execute a diverse range of queries, from questions over a single document to comparing sections across documents.How well does Anthropic\u2019s 100k model do over UBER SEC 10-k filings? Moreover, how well does it do without the help of any of LlamaIndex\u2019s more advanced data structures? In this blog we show the performance of Anthropic\u2019s model on different queries, using the simplest data structure available: the list index.High-Level FindingsWhere Anthropic\u2019s 100k model does well:Holistic understanding of the data (kind of, after some prompt tuning): Anthropic\u2019s model does demonstrate an impressive capability to synthesize insights across the entire context window to answer the question at hand (assuming we set response_mode=\"tree_summarize\" , see below). It can miss details though; see below!Latency: This one was surprising to us. Anthropic\u2019s model is able to crunch an entire UBER 10-k filing in ~60\u201390 seconds, which seems long but is much faster than repeated API calls to GPT-3 (which when added up can take minutes).Where Anthropic\u2019s 100k model doesn\u2019t do well:Cost: This one is obvious. Every query we ran processed hundreds of thousands of tokens. At $11 per million tokens for Claude-v1, this equates to $1 per query, which can quickly add up.Reasoning over more complicated prompts: Anthropic\u2019s model demonstrated a surprising lack of ability to understand our refine prompt for \u201ccreate-and-refine\u201d response synthesis, returning incorrect/irrelevant results. We ended up switching to \u201ctree summarization\u201d instead. See below for results.Overview of MethodologyWe want to test the capabilities of Anthropic\u2019s 100K model on top of UBER 10-k filings from 2019\u20132022. We also want to do this while using as little retrieval/synthesis constructs as possible. This means no embeddings, and no fancy retrieval mechanisms.Ideally, we can directly insert an entire 10-k filing (or even all four 10-k filings) into the prompt. However, we found that a single UBER 10-k filing actually consists of ~160k tokens, which is greater than the 100k context window. This means that we still have to chunk up each filing!We end up using our list index data structure \u2014 we split each text up into massive ~100k token chunks, and use our response synthesis strategies to synthesize an answer across multiple chunks.We run some queries over each filing as well as over multiple filings, similar to our original blog post. We report the results below.Tutorial SetupOur data ingestion is the same as the LlamaIndex + Unstructured blog post. We use Unstructured\u2019s HTML parser to parse the HTML DOM into nicely formatted text. We then create a Document object for each SEC filing.You can access Unstructured data loaders on LlamaHub.from llama_index import download_loader\nfrom pathlib import Path\n\nUnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nyears = [2022, 2021, 2020, 2019]\nfor year in years:\n    year_doc = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)[0]\n    # insert year metadata into each year\n    year_doc.extra_info = {\"year\": year}\n    doc_set[year] = year_doc\n    all_docs.append(year_doc)Next, we want to setup the Anthropic LLM. We\u2019re using claude-v1 by default. We also want to manually define the new 100k-token input size within our PromptHelper object; this will help us figure out how to \u201ccompact\u201d context into the input prompt space during response synthesis.We set the max_input_size to 100k and the output length to 2048. We also set the context chunk size to a high value (95k, leaving some buffer room for rest of the prompt). Context will only be chunked if the number of tokens exceeds this limit.from llama_index import PromptHelper, LLMPredictor, ServiceContext\nfrom langchain.llms import Anthropic\n\n# define prompt helper\n# set maximum input size\nmax_input_size = 100000\n# set number of output tokens\nnum_output = 2048\n# set maximum chunk overlap\nmax_chunk_overlap = 20\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n\nllm_predictor = LLMPredictor(llm=Anthropic(model=\"claude-v1.3-100k\", temperature=0, max_tokens_to_sample=num_output))\nservice_context = ServiceContext.from_defaults(\n    llm_predictor=llm_predictor, prompt_helper=prompt_helper,\n    chunk_size_limit=95000\n)Analyzing a Single DocumentLet\u2019s first analyze queries over a single document. We build a list index over the 2019 UBER 10-K:list_index = GPTListIndex.from_documents([doc_set[2019]], service_context=service_context)\nprint(len(list_index.index_struct.nodes))As mentioned, the 10-K exceeds the 100k token limit, and so there are two nodes within the list index.We then ask a query: \u201cWhat were some of the biggest risk factors in 2019?\u201dRecall that there are two approaches within LlamaIndex for response synthesis across multiple nodes (where the total context exceeds the context window): a \u201ccreate-and-refine\u201d strategy, and a \u201ctree summarize\u201d strategy.Create-and-Refine: sequentially go through each retrieved Node. Use a Question-Answer Prompt for the first Node, and use a Refine Prompt for subsequent Nodes. Make a separate LLM call per Node.Tree Summarize: Given a set of Node objects and the query, recursively construct a tree using the Question Answer Prompt and return the root node as the response. Good for summarization purposes.Claude-v1 100K does not do well with the \u201ccreate-and-refine\u201d strategy (which is the default).# NOTE: the default create/refine approach does not give good answers\nquery = \"What were some of the biggest risk factors in 2019?\"\nquery_engine = list_index.as_query_engine(service_context=service_context)\nresponse = query_engine.query(query)Here is the retrieved answer:Original answer:\n\nSome of the biggest risk factors for Uber in 2019 included:\n\n\u2022 Regulatory challenges and uncertainty. Uber faced regulatory challenges and uncertainty in many markets, including restrictions on its products and services, caps on pricing, and licensing requirements. For example, California's AB5 law and other similar laws increased the risk of Drivers being classified as employees. Uber also faced regulatory scrutiny and bans in London, Barcelona, and other markets.\n\n\u2022 Competition. The markets in which Uber operates are highly competitive, and Uber faced significant competition from well-established and low-cost alternatives in 2019. Competitors also aggressively competed for Drivers and consumers by offering significant incentives and discounts. \n\n\u2022 Safety and security. There were risks related to the safety and security of Uber's platform, including risks from vehicle or scooter accidents, assaults, and other incidents. Uber released a safety report in 2019 detailing reports of sexual assaults on its platform which brought additional scrutiny.\n\n\u2022 Financial performance and profitability. Uber continued to experience significant losses in 2019 as it invested in growth. There was no guarantee that Uber would achieve or sustain profitability in the future.  \n\n\u2022 Reliance on third parties. Uber relied on third parties for key elements of its business like cloud computing services, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could adversely impact Uber's business.\n\n\u2022 Macroeconomic conditions. Uber's business was sensitive to economic conditions and changes that impacted discretionary consumer spending. A decline in the economy could reduce demand for Uber's products and services.  \n\n\u2022 Reliance on Drivers and Restaurants. Uber's success depended on attracting and retaining enough Drivers and Restaurants to its platform. If Uber was unable to attract or retain enough Drivers and Restaurants, it could negatively impact its operations.\n\n\u2022 Intellectual property. If Uber was unable to protect its intellectual property, or if third parties claimed Uber was infringing on their intellectual property, it could harm Uber's business. Uber relied on a combination of copyright, trademark, patent, and trade secret laws to establish its intellectual property rights.\n\nRefined answer:\n\nSome of the biggest risk factors for Uber in 2019 included:\n\n\u2022 Regulatory challenges and uncertainty. Uber faced significant regulatory challenges in 2019, including AB5 in California which increased the risk of Drivers being classified as employees and regulatory scrutiny and temporary bans in London and other markets. These regulatory issues created uncertainty and posed risks to Uber's business model and financial results.\n\n\u2022 Competition. The ridesharing and meal delivery markets are highly competitive, and competitors offered significant incentives and discounts to take market share from Uber in 2019. This competition could negatively impact Uber's growth and profitability.\n\n\u2022 Safety and security. There were risks related to the safety and security of Uber's platform, including risks from vehicle or scooter accidents, assaults, and other incidents. Uber's safety report detailing sexual assault reports brought additional scrutiny and risks. Any failure to ensure safety could significantly damage Uber's reputation and business.\n\n\u2022 Financial performance and profitability. Uber continued to experience substantial losses in 2019 due to investments in growth, and there was no guarantee of future profitability. Uber's path to profitability was uncertain, and failure to become profitable could threaten its business model and access to capital.  \n\n\u2022 Reliance on third parties. Uber relied on third parties for services like cloud computing, payment processing, and background checks. If these third parties failed to provide services or increased costs, it could negatively impact Uber's offerings, growth, and profitability.  \n\n\u2022 Macroeconomic conditions. Uber's business was sensitive to the economy and consumer discretionary spending. An economic downturn could reduce demand for Uber's offerings and threaten its growth and financial performance.\n\n\u2022 Reliance on Drivers and Restaurants. Uber's success depended on attracting and retaining enough Drivers and Restaurants to support growth and meet consumer demand. Failure to attract or retain Drivers and Restaurants could significantly impact Uber's business.  \n\n\u2022 Intellectual property. Failure to protect intellectual property or infringement of others' intellectual property could damage Uber's competitive position and business. Uber relied on laws and policies to protect intellectual property, but monitoring infringement and unauthorized use was difficult.As shown, the answer contains the prompt within it; the point of the refine prompt was to produce just the refined answer, not produce both the original and refined answers.For some reason, using the Tree Summarize strategy works much better.# NOTE: tree_summarize gives better answers\nquery = \"What were some of the biggest risk factors in 2019?\"\nquery_engine = list_index.as_query_engine(service_context=service_context, response_mode=\"tree_summarize\")\nresponse = query_engine.query(query)Returned response:\u2022 Regulatory challenges and uncertainty: Uber faced significant regulatory challenges and uncertainty in 2019, including AB5 in California which codified a new test for determining whether workers should be classified as employees or independent contractors. Uber also faced regulatory scrutiny and bans in other markets like London, UK. These regulatory issues created uncertainty and risk around Uber's business model and operations.\n\n\u2022 Safety and security: Uber received negative publicity around safety incidents on its platform which could damage its brand and reputation. Uber released a safety report in 2019 on sexual assaults and other incidents which led to additional scrutiny. Safety and security risks remain an ongoing issue for Uber's business.\n\n\u2022 Competition: The markets in which Uber competes are intensely competitive, and Uber faces competition from new and existing companies in the various segments it operates in like ridesharing, food delivery, and logistics. Increased competition could make it difficult for Uber to attract and retain platform users.\n\n\u2022 Financial losses: Uber continued to experience significant losses from operations in 2019 as it invested in growth. Uber may not achieve or sustain profitability in the future which could negatively impact its business and stock price.\n\n\u2022 Reliance on third-parties: Uber relies on third-party data centers, cloud computing services, and software which exposes it to risks if these third-parties experience any disruptions or interference. Any disruption of third-party services could negatively impact Uber's business.\n\n\u2022 Macroeconomic conditions: Uber's business is sensitive to macroeconomic conditions, and any downturn could reduce discretionary consumer spending which would harm Uber's business. Issues like a recession, increased unemployment, or reduced consumer confidence could all potentially impact Uber's business.\n\n\u2022 Reliance on Drivers and Restaurants: Uber's business depends on attracting and retaining Drivers and Restaurants to its platform. If Uber is unable to attract or maintain a critical mass of Drivers and Restaurants, its platform will become less appealing to platform users and its financial results would be adversely impacted.The answer is much better. It goes into details about the US-China trade war, slowing economic growth, Brexit, and more (keep in mind 2019 is pre-COVID).Token Usage and LatencyThe document contains around ~170K tokens. For some reason, this number is not reflected on the Anthropic usage logs (the \u201cPrompt Tokens\u201d section seems capped at 10240). But the Prompt Length (in characters) is logged, as well as the model latency.Given the pricing, ~170K tokens would be equivalent to $1.5\u20132 USD.A query through one Uber SEC-10K takes around 150 seconds, including all LLM calls. This is actually a bit faster than repeated calls to ChatGPT/davinci. Each ChatGPT/davinci call (with the 4K token window maximized), empirically can take 6\u201310 seconds to complete \u2192 125\u2013250 seconds (or more).Analyzing Multiple DocumentsA popular example in our previous blog post was showcasing that you could compare/contrast different documents with LlamaIndex graph structures.We test whether we can do that here as well, by feeding in multiple SEC reports into Claude-v1 100k.Caveat: Considering that one UBER SEC-10K filing doesn\u2019t even fit in the context window, we\u2019ll of course also need to implement response synthesis strategies in order to handle ingesting multiple 10K filings.We build a list index over all 4 10K filings: 2019, 2020, 2021, and 2022.list_index = GPTListIndex.from_documents(all_docs, service_context=service_context)\nprint(len(list_index.index_struct.nodes))We then ask our question using our Tree Summarize response mode.query = \"How are the risk factors changing across years? Compare/contrast the risk factors across the SEC filings.\"\nquery_engine = list_index.as_query_engine(response_mode=\"tree_summarize\")\nresponse = query_engine.query(query)The full answer is given below:The risk factors disclosed in Uber's SEC filings have evolved over time based on Uber's business and industry changes. Some of the key differences in risk factors across the filings are:\n\n2017 10-K:\n- Focused heavily on risks related to negative publicity, competition, dependence on independent contractors, and regulatory challenges as Uber was still facing backlash from various PR crises and regulatory pushback. \n- Also highlighted risks from intellectual property litigation given various IP disputes at the time.\n\n2018 10-K:\n- Added more risks related to autonomous vehicles as Uber ramped up its self-driving car efforts. Specifically called out risks from accidents, technical challenges, and competition in the AV space.\n- Removed some risks related to negative publicity and PR crises as those issues had subsided. But added risks related to corporate culture and workplace environment given the Fowler scandal.\n\n2019 10-K: \n- Further expanded AV risks to include risks from partnerships and third party relationships as Uber relied more on partnerships with other companies like Toyota and Volvo. \n- Added risks related to environmental regulations and climate change as those issues received more mainstream attention. \n- Removed risks related to initial scaling challenges as Uber had matured as a company. But added risks related to growth into new offerings and markets as Uber expanded into freight shipping, scooters, etc.\n\nIn summary, Uber's risk factors have evolved to reflect the key issues and challenges facing the company at any point in time. The risks highlight both external factors like regulations as well as internal factors related to Uber's business, technology, and operations. The changes over time show how an innovative company's risks can shift quite quickly.\n\nSome of the key trends in how the risk factors are changing include:\n\n1. Risks related to negative publicity and PR crises decreased over time as those issues subsided, while risks related to other issues like AV technology, workplace culture, and new initiatives increased. This shows how Uber's risks evolved as the company matured.\n\n2. Risks tend to increase in areas where Uber is actively investing or expanding into. For example, risks related to AVs, partnerships, and new mobility offerings increased as Uber ramped up efforts in those areas. This highlights how risk profiles change with a company's strategic priorities.\n\n3. External risks like regulations and climate change were added as those issues gained more mainstream attention and importance. This shows how companies have to adapt their risk factors to account for changes in the overall business environment. \n\n4. Certain foundational risks around competition, growth, and reliance on independent contractors persisted over time. But the specifics and details provided for those risks evolved based on Uber's current challenges and priorities. So while the themes remained, the risks were actively updated.\n\nIn summary, Uber's risk factors changed over time to provide more details on the company's priorities, challenges, partnerships, and external environment at any given point. But certain core risks inherent to Uber's business model have also remained consistently highlighted, demonstrating how those foundational risks are long-term in nature. The changes in risks over time provide a glimpse into how an innovative company's risk profile is constantly evolving.This response only contains risk refactor analysis over the 2019 10-K (which in turn contains risk refactors for 2017 and 2018). It does not contain the years from 2020 onwards. Part of this is potentially due to our tree summarize response synthesis strategy. Nevertheless, it shows that trying to naively \u201cstuff\u201d documents into big 100K token chunks with simple response synthesis strategies still does not produce the optimal answers.Token Usage and LatencyAs expected, feeding all four documents into Anthropic necessitates many more chained LLM calls, which consumes way more tokens and takes a lot longer (on the order of 9\u201310 minutes).ConclusionIn general, the new 100K context window is incredibly exciting and offers developers a new mode of feeding in data into the LLM for different tasks/queries. It offers coherent analysis with a marginal token cost that is much cheaper than that of GPT-4.That said, trying to maximize this context window with each inference call does come with tradeoffs in terms of latency and cost.We look forward to doing more experiments/comparisons/thought pieces on top of Claude! Let us know your feedback.ResourcesYou can check out our full Colab notebook here.",
            "related_posts": [
                "Querying a network of knowledge with llama-index-networks",
                "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS",
                "Agentic RAG With LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//testing-anthropic-claudes-100k-token-window-on-sec-10-k-filings-473310c20dba"
        },
        {
            "date": "May 7, 2023",
            "author": "Ravi Theja",
            "title": "Building and Evaluating a QA System with LlamaIndex",
            "tags": [
                "large-language-models",
                "nlp",
                "question-answering",
                "openai",
                "gpt-4"
            ],
            "content": "IntroductionLlamaIndex (GPT Index) offers an interface to connect your Large Language Models (LLMs) with external data. LlamaIndex provides various data structures to index your data, such as the list index, vector index, keyword index, and tree index. It offers both a high-level API and low-level API \u2014 the high-level API allows you to build a Question-Answering (QA) system in just five lines of code, whereas the lower-level API allows you to customize various aspects of retrieval and synthesis.However, taking these systems into production requires careful evaluation of the performance of the overall system \u2014 the quality of the outputs given the inputs. Evaluation of retrieval-augmented generation can be challenging because the user would need to come up with a dataset of relevant questions for a given context. To overcome these obstacles, LlamaIndex provides Question Generation and label-free Evaluation modules.In this blog, we will discuss the three-step evaluation process using Question Generation and Evaluation modules:Question Generation from the documentGenerate answers/source nodes for questions using LlamaIndex QueryEngine abstractions, which manage the interaction between the LLM and data indices.Evaluate if the question (query), answer, and source nodes are matching/inline1. Question GenerationIt should be noted that this approach does not require ground-truth labels. The purpose of question generation is to generate an initial dataset of inputs over context that can be used to evaluate the question-answering system.LlamaIndex offers the DataGenerator class, which generates questions from a given document using ListIndex. By default, it uses OpenAI ChatGPT (get-3.5-turbo) for question generation.from llama_index.evaluation import DatasetGenerator\nfrom llama_index import SimpleDirectoryReader\n\n# Load documents\nreader = SimpleDirectoryReader(\"./data\")\ndocuments = reader.load_data()\n\n# Generate Question\ndata_generator = DatasetGenerator.from_documents(documents)\nquestion = data_generator.generate_questions_from_nodes()2. Generate Answers/Source Nodes (Context)Using List Index, we generate answers and source nodes for the generated questions in the response object.\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext\n\n# load documents\ndocuments = SimpleDirectoryReader('./data').load_data()\n\n# Create Index\nindex = GPTVectorStoreIndex.from_documents(documents)\n\n# save index to disk\nindex.set_index_id(\"vector_index\")\nindex.storage_context.persist('storage')\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir='storage')\n# load index\nindex = load_index_from_storage(storage_context, index_id=\"vector_index\")\n\n# Query the index\nquery_engine = index.as_query_engine(similarity_top_k=3)\nresponse = query_engine.query(&lt;Query&gt;)\n\n# Response object has both response and source nodes.\n3. EvaluationThe evaluation module can be used to answer the following three questions:Are the response generated and source nodes (context) matching? \u2014 Response + Source Nodes (Context)Are response generated, source nodes (context), and query matching? \u2014 Query + Response + Source Nodes (Context)Which source nodes of the retrieved source nodes are used to generate a response? \u2014 Query + Response + Individual Source Nodes (Context)Evaluation can be done with some combination of the query, context, and response, combining these with LLM calls.Response + Source Nodes (Context)This function answers the question: Are the response generated and source nodes (context) matching?The response object for a given query returns both the response and source nodes (context) with which it generated the response. We can now evaluate the response against the retrieved sources \u2014 without taking into account the query! This allows you to measure hallucination \u2014 if the response does not match the retrieved sources, this means that the model may be \u201challucinating\u201d an answer since it is not rooting the answer in the context provided to it in the prompt.The result is a binary response \u2014 either \u201cYES/NO\u201d.YES \u2014 Response and Source Nodes (Context) are matching.NO \u2014 Response and Source Nodes (Context) are not matching.from llama_index.evaluation import ResponseEvaluator\n\n# build service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n# Build index and get response object\n...\n\n# define evaluator\nevaluator = ResponseEvaluator(service_context=service_context)\n\n# evaluate using the response object\neval_result = evaluator.evaluate(response)Query + Response + Source Nodes (Context)This function answers the question: Are response generated, source nodes (context), and query matching?Often with the \u201cResponse + Source Nodes (Context)\u201d approach, the response generated is in line with the source nodes but may not be the answer to the query. Therefore, considering the query along with the response and source nodes is a good approach for a more accurate analysis.The goal is to determine if the response + source context answers the query. The result is a binary response \u2014 either \u201cYES/NO\u201d.YES \u2014 Query, Response, and Source Nodes (Context) are matching.NO \u2014 Query, Response, and Source Nodes (Context) are not matching.from llama_index.evaluation import QueryResponseEvaluator\n\n# build service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n# Build index and get response object\n...\n\n# define evaluator\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n# evaluate using the response object\neval_result = evaluator.evaluate(query, response)Query + Response + Individual Source Nodes (Context)This function answers the question: Which source nodes of the retrieved source nodes are used to generate a response?Often in the real world, the source nodes can be nodes from different documents. In these cases, it\u2019s important to understand which source nodes are relevant and show those documents to the users. This mode of evaluation will look at each source node and see if each source node contains an answer to the query.from llama_index.evaluation import QueryResponseEvaluator\n\n# build service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n# build index and get response object \n...\n\n# define evaluator\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n# evaluate using the response object\neval_result = evaluator.evaluate_source_nodes(response)Google Colab notebook for Evaluating QA systems using LlamaIndex \u2014Google ColaboratoryEvaluating QA systems using LlamaIndexConclusionLlamaIndex provides a comprehensive solution for building and evaluating QA systems without the need for ground-truth labels. By using the Question Generation and Evaluation modules, you can ensure that your system is accurate and reliable, making it suitable for production environments.",
            "related_posts": [
                "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB",
                "RAGArch: Building a No-Code RAG Pipeline Configuration & One-Click RAG Code Generation Tool Powered by LlamaIndex",
                "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG",
                "Agentic RAG With LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//building-and-evaluating-a-qa-system-with-llamaindex-3f02e9d87ce1"
        },
        {
            "date": "Jul 17, 2024",
            "author": "LlamaIndex",
            "title": "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud",
            "tags": [
                "case-study"
            ],
            "content": "The Challenge: Streamlining AI DevelopmentScaleport AI specializes in transforming emerging AI technology into tangible business results. They possess deep expertise in deploying AI across key industries such as Legal, eCommerce, Real Estate, and Finance, providing tailored generative AI solutions for production applications.Before adopting LlamaCloud and LlamaIndex, Scaleport AI faced several challenges:Long development timelines for creating technical prototypesDifficulty in demonstrating tangible value to clients during the sales processComplex setup requirements for ingestion pipelines and data processingSuboptimal OCR performance, as existing solutions were not meeting the required accuracy and efficiency standardsThe Solution: LlamaCloud's Comprehensive AI Development PlatformScaleport AI turned to LlamaCloud to address these challenges. LlamaCloud offered:Centralized Knowledge Interface: Simplified data management and reduced time spent on data wrangling.LlamaParse: Outperformed existing OCR solutions, offering superior accuracy and efficiency.Advanced Indexing and Retrieval: Enabled flexible integration with various data sources, enhancing data management and accessibility.Rapid Prototyping and Easy Production Deployments: LlamaCloud provides an intuitive UI for rapid prototyping and a seamless transition from UI to code for full-scale development.The Results: Accelerated Development and Enhanced Client EngagementLlamaCloud delivered remarkable improvements for Scaleport.ai:Accelerated Development Timelines: The team could build technical prototypes during the scoping phase, demonstrating tangible value instantly. This improved client engagement and sales outcomes.Enhanced OCR Performance: LlamaParse outperformed GPT-4 vision on several OCR tasks, providing superior accuracy and efficiency.Flexible Data Handling: LlamaCloud's integration with data sources and advanced indexing and retrieval capabilities allowed for quick delivery of high-quality results.Teemu Lahdenper\u00e4, CTO of Scaleport AI, shared his experience:\"LlamaCloud has really sped up our development timelines - whether it's prototyping or production deployments. Before LlamaCloud, building even a simple application took forever because we needed to write our own abstractions for everything. When building an app for a client, a LOT of the work is building the ingestion pipelines. Doing that stuff with LlamaCloud and LlamaParse is remarkably simpler.This in turn has really helped our sales outcomes since we can show tangible value instantly. We've also seen great results with LlamaParse!Specifically, we spent about 50-60% less development hours for one of our clients than we did for an equivalent application prior to LlamaCloud. The main time savings were around Llamaparse; not having to build a custom ingestion pipeline and having the indexing sorted. This helps our margins as well\u201dConclusion: A Game-Changer for AI DevelopmentLlamaCloud has proven to be a game-changer for Scaleport.ai, enabling them to develop apps faster and enhance their overall AI application performance. This has accelerated their sales process! By leveraging LlamaCloud's comprehensive suite of tools, Scaleport.ai has positioned itself at the forefront of AI solution providers, ready to meet the evolving needs of their clients with speed, flexibility, and cutting-edge technology.",
            "related_posts": [
                "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud",
                "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//case-study-how-scaleport-ai-accelerated-development-and-improved-sales-with-llamacloud"
        },
        {
            "date": "Oct 10, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex update 2023\u201310\u201310",
            "tags": [
                "newsletter",
                "llm",
                "llamaindex",
                "ai"
            ],
            "content": "Here\u2019s our weekly look at developments across the LLM space and RAG (Retrieval Augmented Generation) in particular, as well as the latest news and features from your favorite open source library. If you\u2019ve got a project (or a blog post, or a video) that you think people should hear about, we\u2019re happy to feature it in here! Drop us a line at news@llamaindex.ai.This update is now available in handy email form! Just head to our home page and enter your email to sign up.\ud83e\udd29 First, the highlights:Full observability with Arize AI Phoenix: we launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local. Notebook, Tweet.RetrieverEvaluator: new in the library, \u201cRetrieverEvaluator\u201d allows enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments. Docs, Tweet.HuggingFace Embeddings: we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format. Docs, Tweet.Multi-Document Agents: we\u2019ve introduced v0 experimental support for multi-document agents for advanced QA, beyond typical top-k RAG. It supports diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency. Docs, Tweet.\ud83c\udfc6 Congratulations to our Streamlit Hackathon Winners!We love seeing people build amazing things with LlamaIndex!NewsGPT by Kang-Chi Ho: https://buff.ly/46jkutxFinSight by Vishwas Gowda: https://buff.ly/3PzOnyC\u2728 Feature Releases and Enhancements:Multi-Document Agents: we introduced multi-document agents (V0) for advanced QA, beyond typical top-k RAG. They support diverse queries from single to multiple docs. This foundational version sets the stage for future enhancements like parallel query planning and reduced latency. Docs, Tweet.Ensemble Retriever: we\u2019re addressing the RAG challenge of determining chunk size by experimenting with diverse document chunking and ensembling for retrieval. Docs, Tweet.HuggingFace Embeddings: we added native support for three more Hugging Face embedding models, including the base embeddings wrapper, instructor embeddings, and optimum embeddings in ONNX format. Docs, Tweet.OpenAI Function Calling fine-tuning: we\u2019re using OpenAI\u2019s latest function calling fine-tuning which enhanced structured data extraction, optimizing gpt-3.5-turbo for improved extraction in RAG. Docs, Tweet.Metadata Extraction: we\u2019re making metadata extraction efficient by extracting a complete Pydantic object from a document with just one LLM call. Docs, Tweet.Structured RAG Outputs: we now efficiently structure RAG pipeline outputs with native Pydantic outputs from all queries without the need for an additional LLM parsing call. Docs, Tweet.Streamlined secinsights.ai deployment: Our open-sourced secinsights.ai offers a RAG app template, now enhanced with GitHub Codespaces and Docker for swift cloud deployment without setup hassles. Tweet.LongContextReorder: We introduced LongContextReorder****,**** Zeneto\u2019s approach to reposition vital context in RAG systems, addressing the challenge of over-retrieving which can obscure essential details. Docs, Tweet.RA-DIT: We drew inspiration from the RA-DIT paper, which introduced LLM fine-tuning for retrieval-augmented input prompts to improve RAG systems. This method fosters enhanced utilization of context and more effective answer synthesis, even in the presence of suboptimal context. Docs, Tweet.Blockchain: LlamaIndex data agents can be now used to analyze any blockchain subgraph using natural language queries. Tweet.\ud83d\udd0e RAG Evaluation Enhancements:RetrieverEvaluator: We introduced \u201cRetrieverEvaluator\u201d for enhanced retrieval evaluations, complementing LLM generation tests. The module supports benchmarking, standard ranking metrics, and synthetic dataset creation for comprehensive retrieval assessments. Docs, Tweet.SemanticSimilarityEvaluator: We introduced a new semantic similarity evaluator \u2014 SemanticSimilarityEvaluator for LLM/RAG outputs, comparing embedding similarity between reference and generated answers. Docs, Tweet.\ud83d\udcda Tutorials:Guide on building RAG from scratch with open-source modules.Dstack tutorial on implementing RAG with OSS LLMs using LlamaIndex and Weaviate.Wenqi Glantz turorial on Exploring ReAct Agent for Better Prompting in RAG Pipeline.Javier Torres tutorial on building a multi-document chatbot.Erika Cardenas tutorial on RAG techniques in LlamaIndex covering SQL Router Query Engine, Sub Question Query Engine, Recursive Retriever Query Engine, Self-Correcting Query Engine.Wenqi Glantz tutorial on 7 Query Strategies for Navigating Knowledge Graphs With LlamaIndex.Ravi Theja tutorial on Evaluating the Ideal Chunk Size for RAG using LlamaIndex.\u2699\ufe0f Integrations & Collaborations:Arize AI Phoenix: We launched a one-code-line integration with Arize AI for comprehensive tracing and observability in all RAG/agent pipelines. Enjoy local data storage, track LLM input/output prompts, monitor token usage, timing, retrieval visualizations, and agent loops. Additionally, export traces for evaluations and data analysis. All while ensuring your data stays local. Notebook, Tweet.Neo4j: We introduced an API spec for LLM-agent interaction with Neo4j, offering beyond just \u201ctext-to-cypher\u201d with full agent reasoning. Docs, Tweet.TimescaleDB: We integrated with TimescaleDB for enhanced time-based retrieval in RAG systems, offering time filters and cost-effective storage solutions. Blogpost, Tweet.BraintrustData: We integrated with BraintrustData, enabling seamless RAG pipeline construction, evaluations, and easy public URL sharing for results. Notebook, Tweet.LocalAI: We integrated LocalAI_API LLM support for on-prem runs or as an alternative to OpenAI LLM. Tweet.HoneyHiveAI: We integrated with HoneyHiveAI for enhanced multi-step RAG/agent pipeline monitoring. Log traces, gather user feedback, and utilize it for precise fine-tuning and evaluations. Docs, Tweet.UnstructuredIO: We integrated with UnstructuredIO to tackle the RAG challenge of querying embedded tables in 10-K filings. Now, seamlessly query any tabular data or text within a 10-K document. Notebook, Tweet.Clarifai: We integrated with Clarifai, offering access to 40+ LLMs and various embedding models. Tweet.\ud83c\udfa5 Webinars:Webinar by SingleStoreDB on How to Build a GenAI App with LlamaIndex.Webinar on projects built during the SuperAGI Autonomous Agents Hackathon featuring evo.ninja, RicAI, Atlas and MunichAI.\ud83c\udf88 Events:Jerry Liu and Simon conducted a workshop on RAG + Evaluation at RaySummit.Yi Ding spoke on \u2018LLM Quirks Mode\u2019 at MLOps community event.Jerry Liu spoke on Evals/ Benchmarking and Advanced RAG techniques at AIConf 2023.Ravi Theja conducted a workshop on Mastering RAG with LlamaIndex at PyCon India, 2023.Ravi Theja presented a poster on Automatic Knowledge Transfer(KT) Video generation on code bases using LlamaIndex at PyCon India, 2023.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-update-2023-10-10-3718a3d19fb9"
        },
        {
            "date": "Jul 2, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-07-02",
            "tags": [],
            "content": "Hello, Llama enthusiasts! \ud83e\udd99Welcome to this week\u2019s edition of the LlamaIndex newsletter! In this issue, we\u2019re excited to bring you exciting updates about llama-agents, live demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.Before moving into our newsletter, we have an exciting update on our enterprise offerings. We are thrilled to announce the waitlist release of LlamaCloud, our fully-managed ingestion service. Sign up now if you\u2019re eager to collaborate and build LLM applications with LlamaCloud.\ud83e\udd29\u00a0The highlights:Launched Llama-Agents Framework: Our new alpha-release, llama-agents, enables multi-agent AI systems for production with a distributed architecture, HTTP API communication, and agentic orchestration. It\u2019s designed for easy deployment, scalability, and observability. Blogpost, Tweet.create-llama Integrated with LlamaCloud: Streamline your LLM application data pipelines with create-llama, now integrated with LlamaCloud for faster setup and efficient system maintenance. Tweet.\u2728 Feature Releases and Enhancements:We have launched llama-agents - new alpha-release framework that enables multi-agent AI systems to go into production. It features a distributed, service-oriented architecture, communication through standard HTTP APIs, agentic orchestration of flows, and is designed for easy deployment, scalability, and observability. Blogpost, Tweet.create-llama is now integrated with LlamaCloud to streamline the setup and management of data pipelines for LLM applications, providing a fast and efficient way to deploy and maintain these systems. Tweet.We have integrated with DSPy for Optimized RAG by utilizing DSPy\u2019s optimization capabilities with LlamaIndex\u2019s data tools to enhance your query pipelines, optimize prompts, or repurpose DSPy predictors. Cookbook, Tweet.\ud83d\udca1\u00a0Demos:Automating Code Reviews, project by Composio with LlamaIndex automates code reviews using an AI agent in under 100 lines of code that monitors GitHub PRs, reviews them immediately upon creation, and posts feedback directly to your Slack channel. Codebase.\ud83d\uddfa\ufe0f Guides:Guide to Building an Agentic RAG Service with our comprehensive notebook that walks you through creating vector indexes, transforming them into query engines, turning each engine into a tool, providing these tools to agents, and launching the agents as services.Guide to AI Agents with LlamaIndex: Andrei\u2019s comprehensive workshop from Gen AI Philippines, showcasing LLM applications through LlamaIndex. This beginner-friendly session covers topics from RAG to multi-hop agents. Video, Notebook.\u270d\ufe0f Tutorials:Kingzzm\u2019s tutorial on crafting a custom hybrid retriever using LlamaIndex\u2019s flexible abstractions. This tutorial teaches you how to integrate full text and dense search capabilities from Elastic, and how to write your own reciprocal rank fusion function for optimal retrieval strategy.Jeff\u2019s tutorial on which outlines the essential tools needed to construct a report generator using a ReAct agent. Learn how to integrate a RAG tool over guideline documents, a web search tool, and a report generation tool that converts markdown text into PDFs.1littlecoder\u2019s tutorial on llama-agents provides a detailed introduction to transforming multi-agent systems into microservices for production, including setup examples and a walkthrough of the architecture involving the control plane, message queue, and agent services using LlamaIndex abstractions.Mervin Praison\u2019s tutorial on the llama-agents framework provides a concise guide to setting up agent services, from notebook synchronization to server-client interactions, complete with over 10 practical examples.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-07-02"
        },
        {
            "date": "Mar 20, 2024",
            "author": "Andrei",
            "title": "Retrieving Privacy-Safe Documents Over A Network",
            "tags": [
                "privacy",
                "llama-index-networks"
            ],
            "content": "In a recent blog post, we introduced our llama-index-networks library extension that makes it possible to build a network of RAG systems, which users can query. The benefits of such a network are clear: connecting to a diverse set of knowledge stores\u2014that one may not otherwise have access to\u2014means more accurate responses to an even wider breadth of queries.A main caveat to these networks though is that the data being shared across the network ought to be privacy safe. In this blog post, we demonstrate how to turn private, sensitive data into privacy-safe versions that can be subsequently and safely shared across a network. To do so, we\u2019ll be relying on some recent developments in the area of Privacy-Enhancing Techniques.The story of Alex, Bob and Beth continuesTo illustrate all of this, we will again make use of our three made-up characters Alex, Bob and Beth. As a quick reminder, Alex is a data consumer who wants to access the data sources that Bob and Beth possess and are willing to supply.We showed then how such data a collaboration could be permitted through llama-index-networks by taking the following steps:Bob and Beth both build their respective QueryEngine\u2019s (RAG in llama-index lingo)Bob and Beth both expose their QueryEngine behind a ContributorServiceAlex builds a NetworkQueryEngine that connects to Bob and Beth\u2019s ContributorService\u2019sIn part two of this story, we add the wrinkle that Bob and Beth possess private, sensitive data that must be carefully protected before to sharing to Alex. Or, put in another way, we need to add a step 0. to the above steps which applies protective measures to the private datasets.Measures for protecting data (or more specifically the data subjects) depends on the use-case factors such as what the data involves and how its intended to be shared and ultimately processed. De-anonymizing techniques such as wiping PII (i.e., personal identifiable indicators) are often applied. However, in this blog post we highlight another privacy-enhancing technique called Differential Privacy.Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can\u2019t unless protective measures are applied before sharing across the network.Part 2: of Alex, Bob and Beth. This time Bob and Beth have sensitive data that they want to share, but can\u2019t unless protective measures are applied before sharing across the network.Sidebar: differential privacy primerIn short, differential privacy is a method that provides mathematical guarantees (up to a certain level of chance) that an adversary would not be able to learn that a specific individual belonged to a private dataset after only seeing the output of running this private dataset through a protected data processing step. In other words, an individual\u2019s inclusion in the private dataset cannot be learned from the output of a differentially-private algorithm.By protecting against the threat of dataset inclusion, we mitigate the risk that an adversary is able to link the private data with their external sources to learn more about the data subject and potentially cause more privacy harms (such as distortion).A light introduction to differential privacy.A light introduction to differential privacy.Coming back to the story of Alex, Bob and Beth, in order to protect Bob and Beth\u2019s data, we will make use of an algorithm that uses a pre-trained LLM to create synthetic copies of private data that satisfies the differential private mathematical guarantees. This algorithm was introduced in the paper entitled \u201cPrivacy-preserving in-context learning with differentially private few-shot generation\u201d by Xinyu Tang et al., which appeared in ICLR 2024. It is the synthetic copies that we can use to share across the network!There we have it, the added privacy wrinkle and our differentially privacy approach means that we have to take the following steps to facilitate this data collaboration.Bob and Beth create privacy-safe synthetic copies of their private datasetsBob and Beth both build their respective QueryEngine\u2019s over their synthetic datasetsBob and Beth both expose their QueryEngine behind a ContributorServiceAlex builds a NetworkQueryEngine that connects to Bob and Beth\u2019s ContributorService\u2019sCreating differentially private synthetic copies of a private datasetFortunately, for step 0., we can make use of the DiffPrivateSimpleDataset pack.from llama_index.core.llama_datasets.simple import LabelledSimpleDataset\nfrom llama_index.packs.diff_private_simple_dataset.base import PromptBundle\nfrom llama_index.packs.diff_private_simple_dataset import DiffPrivateSimpleDatasetPack\nfrom llama_index.llms.openai import OpenAI\nimport tiktoken\n\n# Beth uses `DiffPrivateSimpleDatasetPack` to generate synthetic copies\n\nllm = OpenAI(\n    model=\"gpt-3.5-turbo-instruct\",\n    max_tokens=1,\n    logprobs=True,\n    top_logprobs=5,  # OpenAI only allows for top 5 next token\n)                    # as opposed to entire vocabulary\ntokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n\nbeth_private_dataset: LabelledSimpleDataset = ... # a dataset that contains\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# examples with two attributes\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# `text` and `reference_label`\n\nbeth_synthetic_generator = DiffPrivateSimpleDatasetPack(\n    llm=llm,\n    tokenizer=tokenizer,\n    prompt_bundle=prompt_bundle,    # params for preparing required prompts\n    simple_dataset=simple_dataset,  # to generate the synthetic examples \n)\n\nbeth_synthetic_dataset = await beth_synthetic_generator.arun(\n\t\tsize=3,  # number of synthetic observations to create\n\t\tsigma=0.5  # param that determines the level of privacy\n)With the synthetic dataset in hand, Bob and Beth can apply the steps introduced in our previous post to build their privacy-safe QueryEngine. It\u2019s worthwhile to mention here that as mentioned by the authors of the paper, the synthetic copies can be used as many times as one would like in a downstream task and it would incur no additional privacy cost! (This is due to the post-processing property of differential privacy.)Example: Symptom2DiseaseIn this section of the blog post, we go over an actual example application of the privacy-safe networks over the Symptom2Disease dataset. This dataset consists of 1,200 examples each containing a \u201csymptoms\u201d description as well as the associated \u201cdisease\u201d label \u2014 the dataset contains observations for 24 distinct disease labels. We split the dataset into two disjoint subsets, one for training and the other for testing. Moreover, we consider this original dataset to be private, requiring protective measures before being shared across a network.Generate privacy-safe synthetic observations of Symptom2DiseaseWe use the training subset and apply the DiffPrivateSimpleDatasetPack on it in order to generate privacy-safe, synthetic observations. But in order to do so, we first need to turn the raw Symptom2Disease dataset into a LabelledSimpleDataset object.import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom llama_index.core.llama_dataset.simple import (\n    LabelledSimpleDataExample,\n    LabelledSimpleDataset,\n)\nfrom llama_index.core.llama_dataset.base import CreatedBy, CreatedByType\n\n# load the Symptom2Disease.csv file\ndf = pd.read_csv(\"Symptom2Disease.csv\")\ntrain, test = train_test_split(df, test_size=0.2)\n\n# create a LabelledSimpleDataset (which is what the pack works with)\nexamples = []\nfor index, row in df.iterrows():\n    example = LabelledSimpleDataExample(\n        reference_label=row[\"label\"],\n        text=row[\"text\"],\n        text_by=CreatedBy(type=CreatedByType.HUMAN),\n    )\n    examples.append(example)\n\nsimple_dataset = LabelledSimpleDataset(examples=examples)Now we can use the llama-pack to create our synthetic observations.import llama_index.core.instrumentation as instrument\nfrom llama_index.core.llama_dataset.simple import LabelledSimpleDataset\nfrom llama_index.packs.diff_private_simple_dataset.base import PromptBundle\nfrom llama_index.packs.diff_private_simple_dataset import DiffPrivateSimpleDatasetPack\nfrom llama_index.llms.openai import OpenAI\nimport tiktoken\nfrom .event_handler import DiffPrivacyEventHandler\nimport asyncio\nimport os\n\nNUM_SPLITS = 3\nT_MAX = 150\n\nllm = OpenAI(\n    model=\"gpt-3.5-turbo-instruct\",\n    max_tokens=1,\n    logprobs=True,\n    top_logprobs=5,\n)\ntokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo-instruct\")\n\nprompt_bundle = PromptBundle(\n    instruction=(\n        \"You are a patient experiencing symptoms of a specific disease. \"\n        \"Given a label of disease type, generate the chosen type of symptoms accordingly.\\n\"\n        \"Start your answer directly after 'Symptoms: '. Begin your answer with [RESULT].\\n\"\n    ),\n    label_heading=\"Disease\",\n    text_heading=\"Symptoms\",\n)\n\ndp_simple_dataset_pack = DiffPrivateSimpleDatasetPack(\n    llm=llm,\n    tokenizer=tokenizer,\n    prompt_bundle=prompt_bundle,\n    simple_dataset=simple_dataset,\n)\n\nsynthetic_dataset = await dp_simple_dataset_pack.arun(\n    sizes=3,\n    t_max=T_MAX,\n    sigma=1.5,\n    num_splits=NUM_SPLITS,\n    num_samples_per_split=8,  # number of private observations to create a\n)                             # synthetic obsevation\nsynthetic_dataset.save_json(\"synthetic_dataset.json\")Create a network with two contributorsNext, we imagine that there are two contributors that each have their own set of Symptom2Disease datasets. In particular, we split the 24 categories of diseases into two disjoint sets and consider each Contributor to possess only one of the two sets. Note that we created the synthetic observations on the full training set, though we could have easily done this on the split datasets as well.Now that we have the synthetic observations, we can follow a slightly modified version of steps 1. through 3. defined in the story of Alex, Bob and Beth. The modification here is that we\u2019re using Retrievers instead of QueryEngine (the choice of Retriever or QueryEngine is completely up to the user).Step 1: Contributor\u2019s build their Retriever over their synthetic datasets.import os\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.llama_dataset.simple import LabelledSimpleDataset\nfrom llama_index.core.schema import TextNode\n\n\n# load the synthetic dataset\nsynthetic_dataset = LabelledSimpleDataset.from_json(\n    \"./data/contributor1_synthetic_dataset.json\"\n)\n\n\nnodes = [\n    TextNode(text=el.text, metadata={\"reference_label\": el.reference_label})\n    for el in synthetic_dataset[:]\n]\n\nindex = VectorStoreIndex(nodes=nodes)\nsimilarity_top_k = int(os.environ.get(\"SIMILARITY_TOP_K\"))\nretriever = index.as_retriever(similarity_top_k=similarity_top_k)Step 2: Contributor\u2019s expose their Retrievers behind a ContributorRetrieverServicefrom llama_index.networks.contributor.retriever.service import (\n    ContributorRetrieverService,\n    ContributorRetrieverServiceSettings,\n)\n\nsettings = ContributorRetrieverServiceSettings() # loads from .env file\nservice = ContributorRetrieverService(config=settings, retriever=retriever)\napp = service.appStep 3: Define the NetworkRetriever that connects to the ContributorRetrieverServicesfrom llama_index.networks.network.retriever import NetworkRetriever\nfrom llama_index.networks.contributor.retriever import ContributorRetrieverClient\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\n\n# ContributorRetrieverClient's connect to the ContributorRetrieverService\ncontributors = [\n    ContributorRetrieverClient.from_config_file(\n        env_file=f\"./client-env-files/.env.contributor_{ix}.client\"\n    )\n    for ix in range(1, 3)\n]\nreranker = CohereRerank(top_n=5)\nnetwork_retriever = NetworkRetriever(\n    contributors=contributors, node_postprocessors=[reranker]\n)With the NetworkRetriever established, we can retrieve synthetic observations from the two contributors data against a query.related_records = network_retriever.aretrieve(\"Vomitting and nausea\")\nprint(related_records) # contain symptoms/disease records that are similar to\n\t\t\t\t\t\t\t\t\t\t\t # to the queried symptoms.Evaluating the NetworkRetrieverTo evaluate the efficacy of the NetworkRetriever we make use of our test set in order to compute two traditional retrieval metrics, namely: hit rate and mean reciprocal rank.hit rate: a hit occurs if any of the retrieved nodes share the same disease label as the test query (symptoms). The hit rate then is the total number of hits divided by the size of the test set.mean reciprocal rank: similar to hit rate, but now we take into account the position of the first retrieved node that shares the same disease label as the test query. If there is no such retrieved node, then the reciprocal rank of the test is equal to 0. The mean reciprocal rank is then merely the average of all reciprocal ranks across the test set.In addition to evaluating the NetworkRetriever we consider the two baselines that represent Retrieving only over the individual Contributor\u2019s synthetic datasets.Retriever evaluations, with sigma equal to 1.5.In the image above, we observe that the NetworkRetriever outperforms both the individual contributor Retriever\u2019s in the test set. This shouldn\u2019t be hard to grasp however since the network retriever has access to more data since it has access to both the Contributor\u2019s synthetic observations\u2014this is the point after all of a network!Another important observation can be made upon inspection of these results. That is, the privacy-safe synthetic observations do indeed do the job of protecting privacy while still maintaining utility in the original dataset. This is often the concern when applying privacy measures such as differential privacy, where noise is incorporated to protect the data. Too much noise will provide high levels of privacy, but at the same time, may render the data useless in downstream tasks. From the table above, we see that at least for this example (though it does corroborate the results of the paper) that the synthetic observations still do match well with the test set, which are indeed real observations (i.e. not synthetically generated).Finally, this level of privacy can be controlled via the noise parameter sigma. In the example above we used a sigma of 1.5, which for this dataset amounts to an epsilon (i.e., privacy-loss measure) value of 1.3. (Privacy loss levels between 0 and 1 are generally considered to be quite private.) Below, we share the evaluations that result from using a sigma of 0.5, which amounts to an epsilon of 15.9\u2014higher values of epsilon or privacy-loss means less privacy.# use the `DiffPrivacySimpleDatasetPack` to get the value of epsilon\nepsilon = dp_simple_dataset_pack.sigma_to_eps(\n\t\tsigma=0.5,\n\t\tmechanism=\"gaussian\",\n\t\tsize=3*24,\n\t\tmax_token_cnt=150  # number of max tokens to generate per synthetic example\n)Retriever evaluations with less noise and thus less privacy i.e., sigma equal to 0.5.So we see after comparing the evaluation metrics with different levels of privacy that when we use the synthetic observations that have higher levels of privacy, we take a bit of a hit in the performance as seen in the decrease in both the hit rate and mean reciprocal rank. This indeed is an illustration of the privacy tradeoff. If we take a look at some of the examples from the synthetic datasets, we can perhaps gain insight as to why this may be happening.# synthetic example epsilon = 1.3\n{\n    \"reference_label\": \"Psoriasis\",\n    \"text\": \"[RESULTS] red, scalloped patches on skin; itching and burning sensation; thick, pitted nails on fingers and toes; joint discomfort; swollen and stiff joints; cracked and painful skin on palms and feet\",\n    \"text_by\": {\n        \"model_name\": \"gpt-3.5-turbo-instruct\",\n        \"type\": \"ai\"\n    }\n},\n\n# synthetic example epsilon = 15.9\n{\n  \"reference_label\": \"Migraine\",\n  \"text\": \"Intense headache, sensitivity to light and sound, nausea, vomiting, vision changes, and fatigue.\",\n  \"text_by\": {\n    \"model_name\": \"gpt-3.5-turbo-instruct\",\n    \"type\": \"ai\"\n  }\n},We can see that synthetic datasets with higher level of privacy are not as clean in terms of punctuation symbols in the text when compared to those with lower levels of privacy. This makes sense because the differential privacy algorithm adds noise to the mechanics of next-token generation. Thus, perturbing this process greatly has affect on the instruction-following capabilities of the LLM.In summaryWe used differential privacy to create privacy-safe, synthetic observations in order to permit the data collaboration of private data that may not be otherwise possible.We demonstrated the benefits of the NetworkRetriever that has access to more data than what the individual Contributor Retriever may have access to.We demonstrated the affects of varying degrees of privacy on the synthetic observations, and by extension, the NetworkRetriever.Learn more!To delve deeper into the materials of this blog post, we share a few links below:Source code for the privacy-safe networks retriever demo. With this, you can try the above all out yourself! (link)Demo notebooks for the DiffPrivateSimpleDataset (link)The source code for creating the synthetic Symptom2Disease observations using the DiffPrivateSimpleDataset (link)",
            "related_posts": [
                "Secure RAG with LlamaIndex and LLM Guard by Protect AI",
                "Retrieving Privacy-Safe Documents Over A Network",
                "PII Detector: hacking privacy in RAG"
            ],
            "link": "https://www.llamaindex.ai/blog//retrieving-privacy-safe-documents-over-a-network"
        },
        {
            "date": "Nov 21, 2023",
            "author": "Jerry Liu",
            "title": "Introducing RAGs: Your Personalized ChatGPT Experience Over Your Data",
            "tags": [
                "llamaindex",
                "streamlit",
                "llm",
                "nlp",
                "ai"
            ],
            "content": "Today we introduce RAGs, a Streamlit app that allows you to create and customize your own RAG pipeline and then use it over your own data \u2014 all with natural language! This means you can now setup a \u201cChatGPT over your data\u201d without needing to code.Setup and query a RAG pipeline in three simple steps:Easy Task Description: Simply describe your task (like \u201cload this web page\u201d) and define the parameters for your RAG systems (like retrieving a certain number of documents).Configurable Settings: Dive into the configuration view to see and alter the automatically generated parameters, such as top-k retrieval, summarization options, and more.Interactive RAG Agent: Once set up, you can interact with your RAG agent, asking questions and getting responses based on your data.The app is designed for both less-technical and technical users: if you\u2019re less-technical, you still need to clone the repo and pip install it, but you don\u2019t need to worry about what\u2019s going on under the hood. On the other hand, if you are technical, you can inspect and customize specific parameter settings (e.g. top-k, data).Home page for RAGsDetailed OverviewThe app contains the following sections, corresponding to the steps listed above.[1] \ud83c\udfe0 Home PageThis is the section where you build a RAG pipeline by instructing the \u201cbuilder agent\u201d. Typically to setup a RAG pipeline you need the following components:Describe the dataset: Currently we support either a single local file or a web page. We\u2019re open to suggestions here!Define the Task: Your description here initializes the \u201csystem prompt\u201d of the LLM powering the RAG pipeline.Set RAG Parameters: Configure typical RAG setup parameters, such as top-k retrieval, chunk size, and summarization options. See below for the full list of parameters.[2] \u2699\ufe0f RAG Config: Tailoring Your ExperienceAfter setting up the basics, you move to the RAG Config section. This part of the app provides an intuitive UI where you can:View Generated Parameters: The builder agent suggests parameters based on your initial setup.Edit and Customize: You have complete freedom to tweak these settings, ensuring the RAG agent behaves exactly as you need.Update the Agent: Any changes you make can be instantly applied by hitting the \u201cUpdate Agent\u201d button.This is the current set of parameters:System PromptInclude Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.)Top-KChunk SizeEmbed ModelLLM[3] \ud83e\udd16 Generated RAG Agent: Interacting with Your DataThe final piece of the RAGs experience is the Generated RAG Agent section. Here\u2019s what you can expect:Interactive Chatbot Interface: Just like ChatGPT, engage in conversations with your RAG agent.Data-Driven Responses: The agent utilizes top-k vector search and optional summarization tools to answer your queries based on the underlying data.Seamless Integration: The agent dynamically picks the right tools to fulfill your queries, ensuring a smooth and intelligent interaction with your dataset.ArchitectureWe\u2019ll cover the architecture in more detail in followups. At a high-level:We have a builder agent equipped with builder tools \u2014 tools necessary to construct a RAG pipeline.The builder agent will use these tools to set the configuration state. At the end of the initial conversational flow these parameters are then used to initialize the RAG agent.Let\u2019s Walk through an Example!Installation and SetupGetting RAGs up and running is straightforward:Clone the RAGs project and navigate to the rags project folder: https://github.com/run-llama/ragsInstall the required packages:pip install -r requirements.txt3. Launch the app:streamlit run 1_\ud83c\udfe0_Home.pyBuild the RAG AgentIn the below diagram we show a sequence of commands to \u201cbuild\u201d a RAG pipeline.Say that you want to build a chatbotDefine the dataset (here it\u2019s a web page, can also be a local file)Define the taskDefine params (chunk size 512, top-k = 3)Screenshot of the home page \u2014 build a RAG agentFollowup questions to set parameters and build the RAG agentView the ConfigurationWe can see the generated configuration in the below page, and view/edit them as necessary!E.g. we can set include_summarization to True.Test It OutNow we can ask questions! We can ask both specific questions as well as summarization questions.This uses both the vector search and summarization tools to answer the requisite questions.Question about a specific detail (performs vector search)Summarization question over the entire documentConclusionIn general RAGs is an initial take towards a world where LLM applications are built by and powered by natural language. Let us know your thoughts and feedback!ResourcesRAGs repo: https://github.com/run-llama/ragsContributions and SupportWe\u2019re committed to improving and expanding RAGs. If you encounter any issues or have suggestions, feel free to file a Github issue or join our Discord community.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-rags-your-personalized-chatgpt-experience-over-your-data-2b9d140769b1"
        },
        {
            "date": "Jun 18, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-06-18",
            "tags": [],
            "content": "Hey Llama Followers\ud83e\udd99Welcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re bringing you an exciting set of updates and valuable resources from Mixture-of-Agents (MoA) paper as LlamaPack to how AtomicWork\u2019s Atom AI assistant leverages LlamaIndex to boost productivity and manage data effectively. Be sure to check out our in-depth guides, educational tutorials, and webinars for deeper insights into our tools.\ud83e\udd29\u00a0The highlights:Mixture-of-Agents (MoA) LlamaPack: We have integrated the Mixture-of-Agents (MoA) demonstrating that open-source LLMs can boost task capabilities. MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks. LlamaPack, Tweet.TiDB Integration with LlamaIndex: PingCap has now integrated their TiDB database with our LlamaIndex\u2019s knowledge graph functionality, making it available as an open-source project. Docs, Tweet.RAG and Agents Cookbook: We have released a detailed cookbook on building RAG and Agents. This guide features enhanced observability through our LlamaIndex instrumentation module and ArizeAI. Notebook, Tweet.AtomicWork\u2019s Enterprise AI Assistant: AtomicWork\u2019s enterprise AI assistant, Atom, leverages LlamaIndex to handle diverse data formats, boosting productivity and improving the employee experience. Check out the details in their detailed blog.Guide to RAG Over Excel Files: Guide to use LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering.\u2728 Feature Releases and Enhancements:We have integrated Mixture-of-Agents (MoA) paper from TogetherAI as LlamaPack from demonstrating that open-source large language models (LLMs) can enhance task capabilities. The paper shows that MoA outperforms GPT-4 Omni in the AlpacaEval 2.0 benchmarks. LlamaPack, Tweet.PingCap has integrated their TiDB database with our LlamaIndex\u2019s knowledge graph functionality, now accessible as an open source project. Docs, Tweet.We have released a detailed cookbook on building RAG and Agents, featuring supercharged observability throughout the call stack, enabled by our LlamaIndex instrumentation module and ArizeAI. Notebook, Tweet.We have released the workshop slides and notebooks from our presentation on \u201cBuilding an Advanced Research Agent on Databricks\u201d at the Data AI Summit. This workshop focused on enhancing research assistants beyond the standard RAG setups. Slide deck, Notebook1, Notebook2, Tweet.\ud83d\udca1\u00a0Real-World Use cases:AtomicWork\u2019s enterprise AI assistant, Atom, utilizes LlamaIndex to handle various data formats, ensuring accurate and secure data retrieval. Atom enhances decision-making and manages unstructured data effectively, boosting productivity and improving the employee experience. Check out the details in their detailed blog.\ud83d\uddfa\ufe0f Guides:Guide to RAG Over Excel Files using LlamaParse to accurately represent Excel files in a spatial grid format, enhancing data interpretation and reducing errors in question-answering.Guide to Building a Multimodal RAG Pipeline by Pavan Belagatti using Claude-3 and SingleStoreDB.Guide to building fully local RAG application using MistralAI, Ollama and LlamaIndex.\u270d\ufe0f Tutorials:Tomaz Bratanic\u2019s tutorial on constructing a knowledge graph, perform entity deduplication, design a custom graph retriever, and implement a question-answering flow.Mervin Praison\u2019s tutorial on creating the core components of an agent defining tools, integrating them into an agent reasoning loop, and wrapping everything with a user interface. using local models and chainlit.Arkiti\u2019s tutorial on building a dynamic text-to-SQL solution using Llama 3 and GroqInc, highlighting the scalable and fast capabilities of SingleStoreDB Helios for multi-cloud deployments.Kingzzm\u2019s tutorial on Advanced RAG Patterns detailing effective strategies for handling documents with embedded tables, utilizing tools like LlamaParse and Nougat for enhanced QA performance.\ud83d\udcf9\u00a0Webinar:Webinar on The Future of Web Agents with MultiOn. Div Garg provided a full demo walkthrough and discuss the agentification of the internet.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-06-18"
        },
        {
            "date": "Dec 21, 2023",
            "author": "LlamaIndex",
            "title": "Running Mixtral 8x7 locally with LlamaIndex and Ollama",
            "tags": [
                "llm",
                "mistral-ai",
                "ollama",
                "llamaindex"
            ],
            "content": "You may have heard the fuss about the latest release from European AI powerhouse Mistral AI: it\u2019s called Mixtral 8x7b, a \u201cmixture of experts\u201d model \u2014 eight of them, each trained with 7 billion parameters, hence the name. Released originally as a mic-drop tweet they followed up a few days later with a blog post that showed it matching or exceeding GPT-3.5 as well as the much larger Llama2 70b on a number of benchmarks.Here at LlamaIndex we\u2019re naturally fans of open source software, so open models with permissive licenses like Mixtral are right up our alley. We\u2019ve had a few questions about how to get Mixtral working with LlamaIndex, so this post is here to get you up and running with a totally local model.Step 1: Install OllamaPreviously getting a local model installed and working was a huge pain, but with the release of Ollama, it\u2019s suddenly a snap! Available for MacOS and Linux (and soon on Windows, though you can use it today on Windows via Windows Subsystem For Linux), it is itself open source and a free download.Once downloaded, you can get Mixtral with a single command:ollama run mixtralThe first time you run this command it will have to download the model, which can take a long time, so go get a snack. Also note that it requires a hefty 48GB of RAM to run smoothly! If that\u2019s too much for your machine, consider using its smaller but still very capable cousin Mistral 7b, which you install and run the same way:ollama run mistralWe\u2019ll assume you\u2019re using Mixtral for the rest of this tutorial, but Mistral will also work.Once the model is running Ollama will automatically let you chat with it. That\u2019s fun, but what\u2019s the point of having a model if it can\u2019t work with your data? That\u2019s where LlamaIndex comes in. The next few steps will take you through the code line by line, but if you\u2019d prefer to save all the copying and pasting, all of this code is available in an open-source repo that you can clone to follow along there.Step 2: Install your dependenciesYou\u2019re going to need LlamaIndex installed, obviously! We\u2019ll also get you going with a handful of other dependencies that are about to come in handy:pip install llama-index qdrant_client torch transformersStep 3: Smoke testIf you\u2019ve got Ollama running and LlamaIndex properly installed, the following quick script will make sure everything is in order by asking it a quick \u201csmoke test\u201d question in a script all by itself:# Just runs .complete to make sure the LLM is listening\nfrom llama_index.llms import Ollama\n\nllm = Ollama(model=\"mixtral\")\nresponse = llm.complete(\"Who is Laurie Voss?\")\nprint(response)Step 4: Load some data and index itNow you\u2019re ready to load in some real data! You can use any data you want; in this case I\u2019m using a small collection of my own tweets which you can download, or use your own! We\u2019re going to be storing our data in the nifty, open source Qdrant vector database (which is why we got you to install it earlier). Create a new python file, and load in all our dependencies:from pathlib import Path\nimport qdrant_client\nfrom llama_index import (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\nfrom llama_index.llms import Ollama\nfrom llama_index.storage.storage_context import StorageContext\nfrom llama_index.vector_stores.qdrant import QdrantVectorStoreThen load our tweets out of our JSON file using a nifty JSONReader from LlamaHub, our convenient collection of open source data connectors. This will give you a pile of documents ready to be embedded and indexed:JSONReader = download_loader(\"JSONReader\")\nloader = JSONReader()\ndocuments = loader.load_data(Path('./data/tinytweets.json'))Get Qdrant ready for action by initializing it and passing it into a Storage Context we\u2019ll be using later:client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)Now set up our Service Context. We\u2019ll be passing it Mixtral as the LLM so we can test that things are working once we\u2019ve finished indexing; indexing itself doesn\u2019t need Mixtral. By passing embed_model=\"local\"we\u2019re specifying that LlamaIndex will embed your data locally, which is why you needed torch and transformers.llm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")Now bring it all together: build the index from the documents you loaded using the service and storage contexts you already set up, and give it a query:index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What does the author think about Star Trek? Give details.\")\nprint(response)Ollama will need to fire up Mixtral to answer the query, which can take a little while, so be patient! You should get output something like this (but with more details):Based on the provided context information, the author has a mixed opinion about Star Trek.Verify our indexNow to prove it\u2019s not all smoke and mirrors, let\u2019s use our pre-built index. Start a new python file and load in dependencies again:import qdrant_client\nfrom llama_index import (\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.llms import Ollama\nfrom llama_index.vector_stores.qdrant import QdrantVectorStoreThis time we won\u2019t need to load the data, that\u2019s already done! We will need the Qdrant client and of course Mixtral again:client = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")This time instead of creating our index from documents we load it directly from the vector store using from_vector_store. We\u2019re also passing similarity_top_k=20 to the query engine; this will mean it will fetch 20 tweets at a time (the default is 2) to get more context and better answer the question.index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\nquery_engine = index.as_query_engine(similarity_top_k=20)\nresponse = query_engine.query(\"Does the author like SQL? Give details.\")\nprint(response)Build a little web serviceIt\u2019s no good having an index that just runs as a script! Let\u2019s make an API out of this thing. We\u2019ll need two new dependencies:pip install flask flask-corsLoad in our dependencies as before into a new file:from flask import Flask, request, jsonify\nfrom flask_cors import CORS, cross_origin\nimport qdrant_client\nfrom llama_index.llms import Ollama\nfrom llama_index import (\n    VectorStoreIndex,\n    ServiceContext,\n)\nfrom llama_index.vector_stores.qdrant import QdrantVectorStoreGet the vector store, the LLM and the index loaded:# re-initialize the vector store\nclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\n# get the LLM again\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n# load the index from the vector store\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)Set up a really basic Flask server:app = Flask(__name__)\ncors = CORS(app)\napp.config['CORS_HEADERS'] = 'Content-Type'\n\n# This is just so you can easily tell the app is running\n@app.route('/')\ndef hello_world():\n    return 'Hello, World!'And add a route that accepts a query (as form data), queries the LLM and returns the response:@app.route('/process_form', methods=['POST'])\n@cross_origin()\ndef process_form():\n    query = request.form.get('query')\n    if query is not None:\n        query_engine = index.as_query_engine(similarity_top_k=20)\n        response = query_engine.query(query)\n        return jsonify({\"response\": str(response)})\n    else:\n        return jsonify({\"error\": \"query field is missing\"}), 400\n\nif __name__ == '__main__':\n    app.run()Note those last two lines, they\u2019re important! flask run is incompatible with the way LlamaIndex loads dependencies, so you will need to run this API directly like so (assuming your file is called app.py)python app.pyWith your API up and running, you can use cURL to send a request and verify it:curl --location '&lt;http://127.0.0.1:5000/process_form&gt;' \\\\\n--form 'query=\"What does the author think about Star Trek?\"'You\u2019re done!We covered a few things here:Getting Ollama to run Mixtral locallyUsing LlamaIndex to query Mixtral 8x7bBuilding and querying an index over your data using Qdrant vector storeWrapping your index into a very simple web APIAll open-source, free, and running locally!I hope this was a fun, quick introduction to running local models with LlamaIndex!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"
        },
        {
            "date": "Jan 5, 2024",
            "author": "Andrei",
            "title": "A Cheat Sheet and Some Recipes For Building Advanced RAG",
            "tags": [
                "retrieval-augmented",
                "llm",
                "retrieval-generation",
                "ai"
            ],
            "content": "It\u2019s the start of a new year and perhaps you\u2019re looking to break into the RAG scene by building your very first RAG system. Or, maybe you\u2019ve built Basic RAG systems and are now looking to enhance them to something more advanced in order to better handle your user\u2019s queries and data structures.In either case, knowing where or how to begin may be a challenge in and of itself! If that\u2019s true, then hopefully this blog post points you in the right direction for your next steps, and moreover, provides for you a mental model for you to anchor your decisions when building advanced RAG systems.The RAG cheat sheet shared above was greatly inspired by a recent RAG survey paper (\u201cRetrieval-Augmented Generation for Large Language Models: A Survey\u201d Gao, Yunfan, et al. 2023).Basic RAGMainstream RAG as defined today involves retrieving documents from an external knowledge database and passing these along with the user\u2019s query to an LLM for response generation. In other words, RAG involves a Retrieval component, an External Knowledge database and a Generation component.LlamaIndex Basic RAG Recipe:from llama_index import SimpleDirectoryReader, VectorStoreIndex\n\n# load data\ndocuments = SimpleDirectoryReader(input_dir=\"...\").load_data()\n\n# build VectorStoreIndex that takes care of chunking documents\n# and encoding chunks to embeddings for future retrieval\nindex = VectorStoreIndex.from_documents(documents=documents)\n\n# The QueryEngine class is equipped with the generator\n# and facilitates the retrieval and generation steps\nquery_engine = index.as_query_engine()\n\n# Use your Default RAG\nresponse = query_engine.query(\"A user's query\")Success Requirements for RAGIn order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements:Retrieval must be able to find the most relevant documents to a user query.Generation must be able to make good use of the retrieved documents to sufficiently answer the user query.Advanced RAGWith the success requirements defined, we can then say that building advanced RAG is really about the application of more sophisticated techniques and strategies (to the Retrieval or Generation components) to ensure that they are ultimately met. Furthermore, we can categorize a sophisticated technique as either one that addresses one of the two high-level success requirements independent (more or less) of the other, or one that addresses both of these requirements simultaneously.Advanced techniques for Retrieval must be able to find the most relevant documents to a user queryBelow we briefly describe a couple of the more sophisticated techniques to help achieve the first success requirement.Chunk-Size Optimization: Since LLMs are restricted by context length, it is necessary to chunk documents when building the External Knowledge database. Chunks that are too big or too small can pose problems for the Generation component leading to in accurate responses.LlamaIndex Chunk Size Optimization Recipe (notebook guide):from llama_index import ServiceContext\nfrom llama_index.param_tuner.base import ParamTuner, RunResult\nfrom llama_index.evaluation import SemanticSimilarityEvaluator, BatchEvalRunner\n\n### Recipe\n### Perform hyperparameter tuning as in traditional ML via grid-search\n### 1. Define an objective function that ranks different parameter combos\n### 2. Build ParamTuner object\n### 3. Execute hyperparameter tuning with ParamTuner.tune()\n\n# 1. Define objective function\ndef objective_function(params_dict):\n    chunk_size = params_dict[\"chunk_size\"]\n    docs = params_dict[\"docs\"]\n    top_k = params_dict[\"top_k\"]\n    eval_qs = params_dict[\"eval_qs\"]\n    ref_response_strs = params_dict[\"ref_response_strs\"]\n\n    # build RAG pipeline\n    index = _build_index(chunk_size, docs)  # helper function not shown here\n    query_engine = index.as_query_engine(similarity_top_k=top_k)\n  \n    # perform inference with RAG pipeline on a provided questions `eval_qs`\n    pred_response_objs = get_responses(\n        eval_qs, query_engine, show_progress=True\n    )\n\n    # perform evaluations of predictions by comparing them to reference\n    # responses `ref_response_strs`\n    evaluator = SemanticSimilarityEvaluator(...)\n    eval_batch_runner = BatchEvalRunner(\n        {\"semantic_similarity\": evaluator}, workers=2, show_progress=True\n    )\n    eval_results = eval_batch_runner.evaluate_responses(\n        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n    )\n\n    # get semantic similarity metric\n    mean_score = np.array(\n        [r.score for r in eval_results[\"semantic_similarity\"]]\n    ).mean()\n\n    return RunResult(score=mean_score, params=params_dict)\n\n# 2. Build ParamTuner object\nparam_dict = {\"chunk_size\": [256, 512, 1024]} # params/values to search over\nfixed_param_dict = { # fixed hyperparams\n  \"top_k\": 2,\n    \"docs\": docs,\n    \"eval_qs\": eval_qs[:10],\n    \"ref_response_strs\": ref_response_strs[:10],\n}\nparam_tuner = ParamTuner(\n    param_fn=objective_function,\n    param_dict=param_dict,\n    fixed_param_dict=fixed_param_dict,\n    show_progress=True,\n)\n\n# 3. Execute hyperparameter search\nresults = param_tuner.tune()\nbest_result = results.best_run_result\nbest_chunk_size = results.best_run_result.params[\"chunk_size\"]2. Structured External Knowledge: In complex scenarios, it may be necessary to build your external knowledge with a bit more structure than the basic vector index so as to permit recursive retrievals or routed retrieval when dealing with sensibly separated external knowledge sources.LlamaIndex Recursive Retrieval Recipe (notebook guide):from llama_index import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.node_parser import SentenceSplitter\nfrom llama_index.schema import IndexNode\n\n### Recipe\n### Build a recursive retriever that retrieves using small chunks\n### but passes associated larger chunks to the generation stage\n\n# load data\ndocuments = SimpleDirectoryReader(\n  input_file=\"some_data_path/llama2.pdf\"\n).load_data()\n\n# build parent chunks via NodeParser\nnode_parser = SentenceSplitter(chunk_size=1024)\nbase_nodes = node_parser.get_nodes_from_documents(documents)\n\n# define smaller child chunks\nsub_chunk_sizes = [256, 512]\nsub_node_parsers = [\n    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n]\nall_nodes = []\nfor base_node in base_nodes:\n    for n in sub_node_parsers:\n        sub_nodes = n.get_nodes_from_documents([base_node])\n        sub_inodes = [\n            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n        ]\n        all_nodes.extend(sub_inodes)\n    # also add original node to node\n    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n    all_nodes.append(original_node)\n\n# define a VectorStoreIndex with all of the nodes\nvector_index_chunk = VectorStoreIndex(\n    all_nodes, service_context=service_context\n)\nvector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)\n\n# build RecursiveRetriever\nall_nodes_dict = {n.node_id: n for n in all_nodes}\nretriever_chunk = RecursiveRetriever(\n    \"vector\",\n    retriever_dict={\"vector\": vector_retriever_chunk},\n    node_dict=all_nodes_dict,\n    verbose=True,\n)\n\n# build RetrieverQueryEngine using recursive_retriever\nquery_engine_chunk = RetrieverQueryEngine.from_args(\n    retriever_chunk, service_context=service_context\n)\n\n# perform inference with advanced RAG (i.e. query engine)\nresponse = query_engine_chunk.query(\n    \"Can you tell me about the key concepts for safety finetuning\"\n)Other useful linksWe have several of guides demonstrating the application of other advanced techniques to help ensure accurate retrieval in complex cases. Here are links to a select few of them:Building External Knowledge using Knowledge GraphsPerforming Mixed Retrieval with Auto RetrieversBuilding Fusion RetrieversFine-tuning Embedding Models used in RetrievalTransforming Query Embeddings (HyDE)Advanced techniques for Generation must be able to make good use of the retrieved documentsSimilar to previous section, we provide a couple of examples of the sophisticated techniques under this category, which can be described as ensuring that the retrieved documents are well aligned to the LLM of the Generator.Information Compression: Not only are LLMs are restricted by context length, but there can be response degradation if the retrieved documents carry too much noise (i.e. irrelevant information).LlamaIndex Information Compression Recipe (notebook guide):from llama_index import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.postprocessor import LongLLMLinguaPostprocessor\n\n### Recipe\n### Define a Postprocessor object, here LongLLMLinguaPostprocessor\n### Build QueryEngine that uses this Postprocessor on retrieved docs\n\n# Define Postprocessor\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=\"Given the context, please answer the final question\",\n    target_token=300,\n    rank_method=\"longllmlingua\",\n    additional_compress_kwargs={\n        \"condition_compare\": True,\n        \"condition_in_question\": \"after\",\n        \"context_budget\": \"+100\",\n        \"reorder_context\": \"sort\",  # enable document reorder\n    },\n)\n\n# Define VectorStoreIndex\ndocuments = SimpleDirectoryReader(input_dir=\"...\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n# Define QueryEngine\nretriever = index.as_retriever(similarity_top_k=2)\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=[node_postprocessor]\n)\n\n# Used your advanced RAG\nresponse = retriever_query_engine.query(\"A user query\")2. Result Re-Rank: LLMs suffer from the so-called \u201cLost in the Middle\u201d phenomena which stipulates that LLMs focus on the extreme ends of the prompts. In light of this, it is beneficial to re-rank retrieved documents before passing them off to the Generation component.LlamaIndex Re-Ranking For Better Generation Recipe (notebook guide):import os\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.postprocessor import LongLLMLinguaPostprocessor\n\n### Recipe\n### Define a Postprocessor object, here CohereRerank\n### Build QueryEngine that uses this Postprocessor on retrieved docs\n\n# Build CohereRerank post retrieval processor\napi_key = os.environ[\"COHERE_API_KEY\"]\ncohere_rerank = CohereRerank(api_key=api_key, top_n=2)\n\n# Build QueryEngine (RAG) using the post processor\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\nindex = VectorStoreIndex.from_documents(documents=documents)\nquery_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=[cohere_rerank],\n)\n\n# Use your advanced RAG\nresponse = query_engine.query(\n    \"What did Sam Altman do in this essay?\"\n)Advanced techniques for simultaneously addressing Retrieval and Generation success requirementsIn this sub section, we consider sophisticated methods that use the synergy of retrieval and generation in order to achieve both better retrieval as well as more accurate generated responses to user queries).Generator-Enhanced Retrieval: These techniques make use of the LLM\u2019s inherent reasoning abilities to refine the user query before retrieval is performed so as to better indicate what exactly it requires to provide a useful response.LlamaIndex Generator-Enhanced Retrieval Recipe (notebook guide):from llama_index.llms import OpenAI\nfrom llama_index.query_engine import FLAREInstructQueryEngine\nfrom llama_index import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n)\n### Recipe\n### Build a FLAREInstructQueryEngine which has the generator LLM play\n### a more active role in retrieval by prompting it to elicit retrieval\n### instructions on what it needs to answer the user query.\n\n# Build FLAREInstructQueryEngine\ndocuments = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nindex_query_engine = index.as_query_engine(similarity_top_k=2)\nservice_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-4\"))\nflare_query_engine = FLAREInstructQueryEngine(\n    query_engine=index_query_engine,\n    service_context=service_context,\n    max_iterations=7,\n    verbose=True,\n)\n\n# Use your advanced RAG\nresponse = flare_query_engine.query(\n    \"Can you tell me about the author's trajectory in the startup world?\"\n)2. Iterative Retrieval-Generator RAG: For some complex cases, multi-step reasoning may be required to provide a useful and relevant answer to the user query.LlamaIndex Iterative Retrieval-Generator Recipe (notebook guide):from llama_index.query_engine import RetryQueryEngine\nfrom llama_index.evaluation import RelevancyEvaluator\n\n### Recipe\n### Build a RetryQueryEngine which performs retrieval-generation cycles\n### until it either achieves a passing evaluation or a max number of\n### cycles has been reached\n\n# Build RetryQueryEngine\ndocuments = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nbase_query_engine = index.as_query_engine()\nquery_response_evaluator = RelevancyEvaluator() # evaluator to critique \n                                                # retrieval-generation cycles\nretry_query_engine = RetryQueryEngine(\n    base_query_engine, query_response_evaluator\n)\n\n# Use your advanced rag\nretry_response = retry_query_engine.query(\"A user query\")Measurement Aspects of RAGEvaluating RAG systems are, of course, of utmost importance. In their survey paper, Gao, Yunfan et al. indicate 7 measurement aspects as seen in the top-right portion of the attached RAG cheat sheet. The llama-index library consists of several evaluation abstractions as well as integrations to RAGAs in order to help builders gain an understanding of the level to which their RAG system achieves the success requirements through the lens of these measurement aspects. Below, we list a select few of the evaluation notebook guides.Answer Relevancy and Context RelevancyFaithfulnessRetrieval EvaluationBatch Evaluations with BatchEvalRunnerYou\u2019re Now Equipped To Do Advanced RAGAfter reading this blog post, we hope that you feel more equipped and confident to apply some of these sophisticated techniques for building Advanced RAG systems!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"
        },
        {
            "date": "Nov 8, 2023",
            "author": "Harshad Suryawanshi",
            "title": "Building My Own ChatGPT Vision with PaLM, KOSMOS-2 and LlamaIndex",
            "tags": [
                "llamaindex",
                "chatgpt",
                "google-palm",
                "kosmos",
                "streamlit"
            ],
            "content": "In the ever-evolving landscape of AI, OpenAI\u2019s ChatGPT with vision capabilities has opened a new chapter. It\u2019s an exciting time for developers and creators as we explore the fusion of visual understanding with conversational AI. Inspired by this innovation, I set out to build my own multi-modal prototype, not just as a replica but as a launchpad for more advanced and tailored visual-language applications.The tools at our disposal are nothing short of extraordinary. KOSMOS-2 a true powerhouse in painting vivid narratives from mere pixels, making image captioning seem almost magical. Then there\u2019s the Google PaLM API, bringing a level of conversational depth that truly understands and responds with relevance. And of course, there\u2019s LlamaIndex - the brains of the operation, orchestrating these elements with such finesse that the interaction flows as naturally as a conversation between old friends.Features OverviewThe outcome of my curiosity and coding is a Streamlit app \u2014 a prototype that stands as an homage and alternative to ChatGPT\u2019s vision capabilities. Here\u2019s what it brings to the table:Real-Time Image Interaction: Upload your images and instantly dive into a dialogue about them.Automatic Captioning with KOSMOS-2: Microsoft\u2019s AI model offers a descriptive base for the conversation.Conversational Depth with PaLM: Google\u2019s language model ensures the chat is as rich and nuanced as the images themselves.User-Friendly Interface: Streamlit powers an intuitive and clean UI, making it easy for anyone to navigate and interact.Deep Dive into the Tech StackThe project is a symphony of technologies, each playing a crucial role:Microsoft AI KOSMOS-2 via Replicate breathes life into images by providing them a narrative.Google PaLM API adds the layer of linguistic intelligence, making the conversation about the images insightful and engaging.LlamaIndex acts as the maestro, coordinating the models to work in harmony.Unveiling app.py: The Core of the ApplicationThe app.py script is the heart of the app, where we bring together KOSMOS-2 and PaLM with Llamaindex to create a seamless multimodal experience. Let\u2019s walk through it, from start to finish.1. Initial SetupWe start by importing the necessary libraries and setting up our Streamlit page. Here, we lay the groundwork for image processing and conversation management.import streamlit as st\nimport extra_streamlit_components as stx\nimport requests\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom io import BytesIO\nimport replicate\nfrom llama_index.llms.palm import PaLM\nfrom llama_index import ServiceContext, VectorStoreIndex, Document\nfrom llama_index.memory import ChatMemoryBuffer\nimport os\nimport datetime\n\nst.set_page_config(layout=\"wide\")\nst.write(\"My version of ChatGPT vision. You can upload an image and start chatting with the LLM about the image\")2. User InterfaceNext, we craft the sidebar and the main area, ensuring that the user knows who created the app and has access to other projects, enhancing credibility and engagement.# Sidebar\nst.sidebar.markdown('## Created By')\nst.sidebar.markdown(\"[Harshad Suryawanshi](https://www.linkedin.com/in/harshadsuryawanshi/)\")\nst.sidebar.markdown('## Other Projects')\n# ...sidebar content continues3. Image Upload and ProcessingUpon uploading an image, the app not only displays it but also invokes the get_image_caption function to generate a relevant caption. This function, decorated with @st.cachefor caching, uses the KOSMOS-2 model through Replicate to provide a brief description of the uploaded image. The description is then used as the basis for the initial conversation with the user.@st.cache\ndef get_image_caption(image_data):\n    input_data = {\n        \"image\": image_data,\n        \"description_type\": \"Brief\"\n    }\n    output = replicate.run(\n        \"lucataco/kosmos-2:3e7b211c29c092f4bcc8853922cc986baa52efe255876b80cac2c2fbb4aff805\",\n        input=input_data\n    )\n    # Split the output string on the newline character and take the first item\n    text_description = output.split('\\n\\n')[0]\n    return text_description4. Conversational Flow with PaLM and LlamaindexWith the image caption in hand, the create_chat_engine function is called to set up the chat engine. This function is crucial as it establishes the context for the conversation and initializes the PaLM API for interaction.@st.cache_resource\ndef create_chat_engine(img_desc, api_key):\n    llm = PaLM(api_key=api_key)\n    service_context = ServiceContext.from_defaults(llm=llm)\n    doc = Document(text=img_desc)\n    index = VectorStoreIndex.from_documents([doc], service_context=service_context)\n    chatmemory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n    \n    chat_engine = index.as_chat_engine(\n        chat_mode=\"context\",\n        system_prompt=(\n            f\"You are a chatbot, able to have normal interactions, as well as talk. \"\n            \"You always answer in great detail and are polite. Your responses always descriptive. \"\n            \"Your job is to talk about an image the user has uploaded. Image description: {img_desc}.\"\n        ),\n        verbose=True,\n        memory=chatmemory\n    )\n    return chat_engineThe create_chat_engine function builds the infrastructure for our app's conversation capabilities. It starts by instantiating a PaLM object with the provided API key, setting up the service context, and creating a document with the image description. This document is then indexed to prepare it for Llamaindex\u2019s context chat engine. Finally, the chat engine is configured with a prompt that instructs the AI on how to engage in the conversation, referencing the image description and defining the chatbot's behavior.5. User Interaction and Message HandlingThe application ensures an engaging and controlled user experience by limiting the number of messages to 20 per session in the demo version. If this limit is reached, it gracefully notifies the user and disables further input to manage resources effectively.if message_count &gt;= 20:\n    st.error(\"Notice: The maximum message limit for this demo version has been reached.\")\n    # Disabling the uploader and input by not displaying them\n    image_uploader_placeholder = st.empty()  # Placeholder for the uploader\n    chat_input_placeholder = st.empty()      # Placeholder for the chat inputHowever, when the message count is within the limit, the application provides a clear chat option and handles the image upload process. Upon uploading, it immediately processes the image to get a caption, sets up the chat engine, and updates the user interface to reflect the successful upload.else:\n    # Add a clear chat button\n    if st.button(\"Clear Chat\"):\n        clear_chat()\n\n    # Image upload section\n    image_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"uploaded_image\", on_change=on_image_upload)\n    # ...code for image upload and displayFor each user input, the message is added to the chat history, and the chat engine is queried for a response. The app ensures that each message \u2014 whether from the user or the assistant \u2014 is displayed in the chat interface, maintaining a coherent conversation flow.# ...code for handling user input and displaying chat history\n\n# Call the chat engine to get the response if an image has been uploaded\nif image_file and user_input:\n    try:\n        with st.spinner('Waiting for the chat engine to respond...'):\n            # Get the response from your chat engine\n            response = chat_engine.chat(user_input)\n        # ...code for appending and displaying the assistant's response\n    except Exception as e:\n        st.error(f'An error occurred.')\n        # ...exception handling codeWrapping UpThis app is the foundation, a springboard for more complex visual-language applications. The potential is limitless, and your insights can shape its future. I invite you to dive into the demo, tinker with the code, and join me in pushing the envelope of what AI can do.Link to GitHub RepoConnect with Me on LinkedInLinkedIn Post:",
            "related_posts": [
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse",
                "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud"
            ],
            "link": "https://www.llamaindex.ai/blog//building-my-own-chatgpt-vision-with-palm-kosmos-2-and-llamaindex-9f9fdd13e566"
        },
        {
            "date": "Jan 25, 2024",
            "author": "Jiang Chen",
            "title": "Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines",
            "tags": [
                "retrieval-augmented",
                "llamaindex",
                "milvus",
                "llm",
                "ai"
            ],
            "content": "IntroductionWe are seeing a huge wave of developers building Retrieval Augmented Generation (RAG) applications. The RAG tech stack generally contains a retrieval pipeline, LLM and prompt, among which LLM is accessible and developers are comfortable with prompt customization. However, developers new to search and index often need extensive help to build an effective retrieval pipeline. A production-ready retrieval pipeline typically consists of the following components:Document loader that parses and splits the long textEmbedding model serving as core indexing componentA vector database that stores the vector embeddingsAdvanced components to future optimize retrieval quality, such as re-ranker model to judge semantic similarity betterIt\u2019s challenging to operate this complex tech stack. It involves managing software package dependencies, hosting services in Kubernetes clusters, and monitoring the performance of ML models. The high DevOps cost distracts developers from the most critical part of the user experience of RAG applications: prompt engineering, answer generation, and user interface.While experienced search infrastructure engineers may still manage a complicated tech stack for its flexibility, Zilliz believes that most RAG developers could benefit from a retrieval API service that is user-friendly and allows for lighter customization.Integrating Zilliz Cloud Pipelines and LlamaIndex brings a new approach to solving this problem. Zilliz Cloud Pipelines is a fully managed, scalable retrieval service. LlamaIndex is a flexible RAG framework that provides libraries and tools for organizing business logics such as retrieval and prompt engineering. The API service of Zilliz Cloud Pipelines is abstracted as a ManagedIndex in LlamaIndex. RAG developers using ZillizCloudPipelineIndex can easily scale the app from one user to millions of users without the hassle of setting up and maintaining the complex retrieval tech stack. It hides the technical complexity behind a few function calls, so that developers can focus on the core user experience of their RAG apps.In this blog, we show how to use ZillizCloudPipelineIndex to build a high quality RAG chatbot. The chatbot is scalable and supports multi-tenancy through metadata filtering.Set upSince Zilliz Cloud Pipelines is an API service, first you need to set up a Zilliz Cloud account and create a free serverless cluster.Now you can construct ZillizCloudPipelineIndex and get the handler to index docs and query later.from llama_index.indices import ZillizCloudPipelineIndex\n\nzcp_index = ZillizCloudPipelineIndex(\n    project_id=\"&lt;YOUR_ZILLIZ_PROJECT_ID&gt;\",\n    cluster_id=\"&lt;YOUR_ZILLIZ_CLUSTER_ID&gt;\",\n    token=\"&lt;YOUR_ZILLIZ_API_KEY&gt;\",\n)\nzcp_index.create_pipelines(metadata_schema={\"user_id\": \"VarChar\", \"version\": \"VarChar\"})You can copy the Project ID, Cluster ID and API Key from your Zilliz account as follows:Ingest DocumentsSuppose your application has multiple users, and you would like to tag each user\u2019s document to provide isolation. Your application logic can be implemented as follows. For simplicity, here we demo ingesting public documents. Currently, Zilliz Cloud Pipelines supports documents stored and managed in AWS S3 and Google Cloud Storage. Local document upload will also be supported soon.# user1 ingests a document, it is technical documentation for v2.3 version. \nzcp_index.insert_doc_url(\n    url=\"https://publicdataset.zillizcloud.com/milvus_doc.md\",\n    metadata={\"user_id\": \"user1\", \"version\": \"2.3\"},\n)\n# user2 ingests a document, it is technical documentation for v2.2 version. \nzcp_index.insert_doc_url(\n    url=\"https://publicdataset.zillizcloud.com/milvus_doc_22.md\",\n    metadata={\"user_id\": \"user2\", \"version\": \"2.2\"},\n)QueryTo conduct semantic search with ZillizCloudPipelineIndex, you can use it as_query_engine() by specifying a few parameters:search_top_k: How many text nodes/chunks to retrieve. Optional, defaults to DEFAULT_SIMILARITY_TOP_K (2).filters: Metadata filters. Optional, defaults to None. In this example, we set the filter to only retrieve docs of a specific user, to provide user-level data isolation.output_metadata: What metadata fields to return with the retrieved text node. Optional, defaults to [].# Query the documents in ZillizCloudPipelineIndex\nfrom llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n\nquery_engine_for_user1 = zcp_index.as_query_engine(\n    search_top_k=3,\n    filters=MetadataFilters(\n        filters=[\n            ExactMatchFilter(key=\"user_id\", value=\"user1\")\n        ]  # The query would only search from documents of user1.\n    ),\n    output_metadata=[\"user_id\", \"version\"], # output these tags together with document text\n)\n\nquestion = \"Can users delete entities by complex boolean expressions?\"\n# The chatbot will only answer with the retrieved information from user1's documents\nanswer = query_engine_for_user1.query(question)Thanks to the abstraction of LlamaIndex and Zilliz Cloud Pipelines, with just 30 lines of code, we can demo a RAG service that supports multi-tenancy. Most importantly, this simple RAG app could easily scale to serving millions of users without changing any code.What to do next?You can check out the official LlamaIndex documentation to learn about advanced customization of ZillizCloudPipelineIndex. Please ask questions at Zilliz user group or LlamaIndex discord if you have any questions. Zilliz Cloud Pipelines will soon support local file upload and more choices of embedding and re-ranker models. Get a free Zilliz Cloud account and stay tuned for more updates!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//building-scalable-rag-applications-with-llamaindex-and-zilliz-cloud-pipelines-4879e9768baf"
        },
        {
            "date": "Sep 27, 2023",
            "author": "Jerry Liu",
            "title": "Timescale Vector x LlamaIndex: Making PostgreSQL a Better Vector Database for AI Applications",
            "tags": [
                "llamaindex",
                "nlp",
                "llm",
                "ai",
                "timescaledb"
            ],
            "content": "Authors: Avthar Sewrathan, Matvey Arye, Jerry Liu, Yi DingIntroducing the Timescale Vector integration for LlamaIndex. Timescale Vector enables LlamaIndex developers to build better AI applications with PostgreSQL as their vector database: with faster vector similarity search, efficient time-based search filtering, and the operational simplicity of a single, easy-to-use cloud PostgreSQL database for not only vector embeddings but an AI application\u2019s relational and time-series data too.PostgreSQL is the world\u2019s most loved database, according to the Stack Overflow 2023 Developer Survey. And for a good reason: it\u2019s been battle-hardened by production use for over three decades, it\u2019s robust and reliable, and it has a rich ecosystem of tools, drivers, and connectors.And while pgvector, the open-source extension for vector data on PostgreSQL, is a wonderful extension (and all its features are offered as part of Timescale Vector), it is just one piece of the puzzle in providing a production-grade experience for AI application developers on PostgreSQL. After speaking with numerous developers at nimble startups and established industry giants, we saw the need to enhance pgvector to cater to the performance and operational needs of developers building AI applications.Here\u2019s the TL;DR on how Timescale Vector helps you build better AI applications with LlamaIndex:Faster similarity search on millions of vectors: Thanks to the introduction of a new search index inspired by the DiskANN algorithm, Timescale Vector achieves 3X faster search speed at ~99% recall than a specialized database and outperforms all existing PostgreSQL search indexes by between 39.39% and 1,590.33% on a dataset of one million OpenAI embeddings. Plus, enabling product quantization yields a 10x index space savings compared to pgvector. Timescale Vector also offers pgvector\u2019s Hierarchical Navigable Small Worlds (HNSW) and Inverted File Flat (IVFFlat) indexing algorithms.Efficient similarity search with time-based filtering: Timescale Vector optimizes time-based vector search queries, leveraging the automatic time-based partitioning and indexing of Timescale\u2019s hypertables to efficiently find recent embeddings, constrain vector search by a time range or document age, and store and retrieve large language model (LLM) response and chat history with ease. Time-based semantic search also enables you to use Retrieval Augmented Generation (RAG) with time-based context retrieval to give users more useful LLM responses.Simplified AI infra stack: By combining vector embeddings, relational data, and time-series data in one PostgreSQL database, Timescale Vector eliminates the operational complexity that comes with managing multiple database systems at scale.Simplified metadata handling and multi-attribute filtering: Developers can leverage all PostgreSQL data types to store and filter metadata and JOIN vector search results with relational data for more contextually relevant responses. In future releases, Timescale Vector will further optimize rich multi-attribute filtering, enabling even faster similarity searches when filtering on metadata.On top of these innovations for vector workloads, Timescale Vector provides a robust, production-ready cloud PostgreSQL platform with flexible pricing, enterprise-grade security, and free expert support.In the rest of this post, we\u2019ll dive deeper (with code!) into the unique capabilities Timescale Vector enables for developers wanting to use PostgreSQL as their vector database with LlamaIndex:Faster similarity search with DiskANN, HNSW and IVFFlat index types.Efficient similarity search when filtering vectors by time.Retrieval Augmented Generation (RAG) with time-based context retrieval.(If you want to jump straight to the code, explore this tutorial).\ud83c\udf89 LlamaIndex Users Get 3 Months of Timescale Vector for FreeWe\u2019re giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won\u2019t be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.Faster Vector Similarity Search in PostgreSQLTimescale Vector speeds up Approximate Nearest Neighbor (ANN) search on large-scale vector datasets, enhancing pgvector with a state-of-the-art ANN index inspired by the DiskANN algorithm. Timescale Vector also offers pgvector\u2019s HNSW and IVFFlat indexing algorithms, giving developers the flexibility to choose the right index for their use case.Our performance benchmarks using the ANN benchmarks suite show that Timescale Vector achieves between 39.43% and 1,590.33% faster search speed at ~99% recall than all existing PostgreSQL search indexes, and 3X faster search speed at ~99% recall than specialized vector databases on a dataset of one million OpenAI embeddings. You can read more about the performance benchmark methodology, the databases compared and results here.Timescale Vector\u2019s new DiskANN-inspired index outperforms all existing PostgreSQL index types when performing approximate nearest neighbor searches at 99 % recall on 1 million OpenAI embeddings.Using Timescale Vector\u2019s DiskANN, HNSW, or IVFFLAT indexes in LlamaIndex is incredibly straightforward.Simply create a Timescale Vector vector store and add the data nodes you want to query as shown below:from llama_index.vector_stores import TimescaleVectorStore\n\n# Create a timescale vector store with specified params\nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name=\"your_table_name\",\n   time_partition_interval= timedelta(days=7),\n)\nts_vector_store.add(nodes)Then run:# Create a timescale vector index (DiskANN)\nts_vector_store.create_index()This will create a timescale-vector index with the default parameters.We should point out that the term \u201cindex\u201d is a bit overloaded. For many VectorStores, an index is the thing that stores your data (in relational databases this is often called a table), but in the PostgreSQL world an index is something that speeds up search, and we are using the latter meaning here.We can also specify the exact parameters for index creation in the create_index command as follows:# create new timescale vector index (DiskANN) with specified parameters\nts_vector_store.create_index(\"tsv\", max_alpha=1.0, num_neighbors=50)Advantages to this Timescale Vector\u2019s new DiskANN-inspired vector search index include the following:Faster vector search at 99% accuracy in PostgreSQL.Optimized for running on disks, not only in memory use.Quantization optimization compatible with PostgreSQL, reducing the vector size and consequently shrinking the index size (by 10x in some cases!) and expediting searches.Efficient hybrid search or filtering additional dimensions.For more on how Timescale Vector\u2019s new index works, see this blog post.Pgvector is packaged as part of Timescale Vector, so you can also access pgvector\u2019s HNSW and IVFFLAT indexing algorithms in your LlamaIndex applications. The ability to conveniently create ANN search indexes from your LlamaIndex application code makes it easy to create different indexes and compare their performance:# Create an HNSW index\n# Note: You don't need to specify m and ef_construction parameters as we set smart defaults.\nts_vector_store.create_index(\"hnsw\", m=16, ef_construction=64)\n\n# Create an IVFFLAT index\n# Note: You don't need to specify num_lists and num_records parameters as we set smart defaults.\nts_vector_store.create_index(\"ivfflat\", num_lists=20, num_records=1000)Add Efficient Time-Based Search Functionality to Your LlamaIndex AI ApplicationTimescale Vector optimizes time-based vector search, leveraging the automatic time-based partitioning and indexing of Timescale\u2019s hypertables to efficiently search vectors by time and similarity.Time is often an important metadata component for vector embeddings. Sources of embeddings, like documents, images, and web pages, often have a timestamp associated with them, for example, their creation date, publishing date, or the date they were last updated, to name but a few.We can take advantage of this time metadata in our collections of vector embeddings to enrich the quality and applicability of search results by retrieving vectors that are not just semantically similar but also pertinent to a specific time frame.Here are some examples where time-based retrieval of vectors can improve your LlamaIndex applications:Finding recent embeddings: Finding the most recent embeddings that are semantically similar to a query vector. For example, finding the most recent news, documents, or social media posts related to elections.Search within a time-range: Constraining similarity search to only vectors within a relevant time range. For example, asking time-based questions about a knowledge base (\u201cWhat new features were added between January and March 2023?\u201d).Chat history: Storing and retrieving LLM response history. For example, chatbot chat history.Let\u2019s take a look at an example of performing time-based searches on a git log dataset. In a git log, each entry has a timestamp, an author, and some information about the commit.To illustrate how to use TimescaleVector\u2019s time-based vector search functionality, we\u2019ll ask questions about the git log history for TimescaleDB. Each git commit entry has a timestamp associated with it, as well as a message and other metadata (e.g., author).We\u2019ll illustrate how to create nodes with a time-based UUID and how to run similarity searches with time range filters using the Timescale Vector vector store..Create nodes from each commit in the gitlogFirst, we load the git log entries from the demo CSV file using Pandas:import pandas as pd\nfrom pathlib import Path\n\n\n# Read the CSV file into a DataFrame\nfile_path = Path(\"../data/csv/commit_history.csv\")\ndf = pd.read_csv(file_path)Next, we\u2019ll create nodes of type `TextNode` for each commit in our git log dataset, extracting the relevant information and assigning it to the node\u2019s text and metadata, respectively.from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n# Create a Node object from a single row of data\ndef create_node(row):\n   record = row.to_dict()\n   record_name = split_name(record[\"author\"])\n   record_content = str(record[\"date\"]) + \" \" + record_name + \" \" + str(record[\"change summary\"]) + \" \" + str(record[\"change details\"])\n   node = TextNode(\n       id_=create_uuid(record[\"date\"]),\n       text= record_content,\n       metadata={\n           'commit': record[\"commit\"],\n           'author': record_name,\n           'date': create_date(record[\"date\"]),\n       }\n   )\n   return node\n\nnodes = [create_node(row) for _, row in df.iterrows()]Note: The code above references two helper functions to get things in the right format (`split_name()` and `create_date()`), which we\u2019ve omitted for brevity. The full code is included in the tutorial linked in the Resources section at the end of this post.Create UUIDs for each node based on the date of each git commitWe will take a closer look at a helper function we use to create each node\u2019s id_. For time-based search in LlamaIndex, Timescale Vector uses the \u2018datetime\u2019 portion of a UUID v1 to place vectors in the correct time partition. Timescale Vector\u2019s Python client library provides a simple-to-use function named `uuid_from_time` to create a UUID v1 from a Python DateTime object, which we\u2019ll then use as our `ids` for the TextNodes.from timescale_vector import client\n# Function to take in a date string in the past and return a uuid v1\ndef create_uuid(date_string: str):\n   if date_string is None:\n       return None\n   time_format = '%a %b %d %H:%M:%S %Y %z'\n   datetime_obj = datetime.strptime(date_string, time_format)\n   uuid = client.uuid_from_time(datetime_obj)\n   return str(uuid)Since we are dealing with timestamps in the past, we take advantage of the `uuid_from_time` function to help generate the correct UUIDs for each node. If you want the current date and time associated with your Nodes (or Documents) for time-based search, you can skip this step. A UUID associated with the current date and time will be automatically generated as the nodes are added to the table in Timescale Vector by default.Let\u2019s take a look at the contents of a node:print(nodes[0].get_content(metadata_mode=\"all\"))commit: 44e41c12ab25e36c202f58e068ced262eadc8d16\nauthor: Lakshmi Narayanan Sreethar\ndate: 2023-09-5 21:03:21+0850\n\nTue Sep 5 21:03:21 2023 +0530 Lakshmi Narayanan Sreethar Fix segfault in set_integer_now_func When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037Create vector embeddings for the text of each nodeNext, we\u2019ll create vector embeddings of the content of each node so that we can perform similarity searches on the text associated with each node. We\u2019ll use the `OpenAIEmbedding` model to create the embeddings.# Create embeddings for nodes\nfrom llama_index.embeddings import OpenAIEmbedding\nembedding_model = OpenAIEmbedding()\n\nfor node in nodes:\n   node_embedding = embedding_model.get_text_embedding(\n       node.get_content(metadata_mode=\"all\")\n   )\n   node.embedding = node_embeddingLoad nodes into Timescale Vector vector storeNext, we\u2019ll create a `TimescaleVectorStore` instance and add the nodes we created to it.# Create a timescale vector store and add the newly created nodes to it\nts_vector_store = TimescaleVectorStore.from_params(\n   service_url=TIMESCALE_SERVICE_URL,\n   table_name=\"li_commit_history\",\n   time_partition_interval= timedelta(days=7),\n)\nts_vector_store.add(nodes)To take advantage of Timescale Vector\u2019s efficient time-based search, we need to specify the `time_partition_interval` argument when instantiating a Timescale Vector vector store. This argument represents the length of each interval for partitioning the data by time. Each partition will consist of data that falls within the specified length of time.In the example above, we use seven days for simplicity, but you can pick whatever value makes sense for the queries used by your application \u2014 for example, if you query recent vectors frequently, you might want to use a smaller time delta like one day, or if you query vectors over a decade-long time period, then you might want to use a larger time delta like six months or one year. As a rule of thumb, common queries should touch only a couple of partitions and at the same time your full dataset should fit within a 1000 partitions, but don\u2019t stress too much \u2014 the system is not very sensitive to this value.Similarity search with time filtersNow that we\u2019ve loaded our nodes that contain vector embeddings data and metadata into a Timescale Vector vector store, and enabled automatic time-based partitioning on the table our vectors and metadata are stored in, we can query our vector store with time-based filters as follows:# Query the vector database\nvector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=5)\n\n# Time filter variables for query\nstart_dt = datetime(2023, 8, 1, 22, 10, 35) # Start date = 1 August 2023, 22:10:35\nend_dt = datetime(2023, 8, 30, 22, 10, 35) # End date = 30 August 2023, 22:10:35\n\n# return most similar vectors to query between start date and end date date range\n# returns a VectorStoreQueryResult object\nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, end_date = end_dt)Let\u2019s take a look at the date and contents of the nodes returned by our query:# for each node in the query result, print the node metadata date\nfor node in query_result.nodes:\n   print(\"-\" * 80)\n   print(node.metadata[\"date\"])\n   print(node.get_content(metadata_mode=\"all\"))--------------------------------------------------------------------------------\n2023-08-3 14:30:23+0500\ncommit:  7aeed663b9c0f337b530fd6cad47704a51a9b2ec\nauthor: Dmitry Simonenko\ndate: 2023-08-3 14:30:23+0500\n\nThu Aug 3 14:30:23 2023 +0300 Dmitry Simonenko Feature flags for TimescaleDB features This PR adds..\n--------------------------------------------------------------------------------\n2023-08-29 18:13:24+0320\ncommit:  e4facda540286b0affba47ccc63959fefe2a7b26\nauthor: Sven Klemm\ndate: 2023-08-29 18:13:24+0320\n\nTue Aug 29 18:13:24 2023 +0200 Sven Klemm Add compatibility layer for _timescaledb_internal functions With timescaledb 2.12 all the functions present in _timescaledb_internal were\u2026\n--------------------------------------------------------------------------------\n2023-08-22 12:01:19+0320\ncommit:  cf04496e4b4237440274eb25e4e02472fc4e06fc\nauthor: Sven Klemm\ndate: 2023-08-22 12:01:19+0320\n\nTue Aug 22 12:01:19 2023 +0200 Sven Klemm Move utility functions to _timescaledb_functions schema To increase schema security we do not want to mix\u2026\n--------------------------------------------------------------------------------\n2023-08-29 10:49:47+0320\ncommit:  a9751ccd5eb030026d7b975d22753f5964972389\nauthor: Sven Klemm\ndate: 2023-08-29 10:49:47+0320\n\nTue Aug 29 10:49:47 2023 +0200 Sven Klemm Move partitioning functions to _timescaledb_functions schema To increase schema security\u2026\n--------------------------------------------------------------------------------\n2023-08-9 15:26:03+0500\ncommit:  44eab9cf9bef34274c88efd37a750eaa74cd8044\nauthor: Konstantina Skovola\ndate: 2023-08-9 15:26:03+0500\n\nWed Aug 9 15:26:03 2023 +0300 Konstantina Skovola Release 2.11.2 This release contains bug fixes since the 2.11.1 release\u2026Success! Notice how only vectors with timestamps within the specified start and end date ranges of 1 August, 2023, and 30 August, 2023, are included in the results.Here\u2019s some intuition for why Timescale Vector\u2019s time-based partitioning speeds up ANN queries with time-based filters.Timescale Vector partitions the data by time and creates ANN indexes on each partition individually. Then, during search, we perform a three-step process:Step 1: filter our partitions that don\u2019t match the time predicate.Step 2: perform the similarity search on all matching partitions.Step 3: combine all the results from each partition in step 2, rerank, and filter out results by time.Timescale Vector leverages TimescaleDB\u2019s hypertables, which automatically partition vectors and associated metadata by a timestamp. This enables efficient querying on vectors by both similarity to a query vector and time, as partitions not in the time window of the query are ignored, making the search a lot more efficient by filtering out whole swaths of data in one go.When performing a vector similarity search on `TimescaleVectorStore`, rather than specifying the start and end dates for our search, we can also specify a time filter with a provided start date and time delta later:# return most similar vectors to query from start date and a time delta later\nquery_result = ts_vector_store.query(vector_store_query, start_date = start_dt, time_delta = td)And we can specify a time filter within a provided end_date and time delta earlier. This syntax is very useful for filtering your search results to contain vectors before a certain date cutoff.# return most similar vectors to query from end date and a time delta earlier\nquery_result = ts_vector_store.query(vector_store_query, end_date = end_dt, time_delta = td)Powering Retrieval Augmented Generation With Time-Based Context Retrieval in LlamaIndex Applications With Timescale VectorLet\u2019s put everything together and look at how to use the TimescaleVectorStore to power RAG on the git log dataset we examined above.To do this, we can use the TimescaleVectorStore as a QueryEngine. When creating the query engine, we use TimescaleVector\u2019s time filters to constrain the search to a relevant time range by passing our time filter parameters as `vector_strore_kwargs`.from llama_index import VectorStoreIndex\nfrom llama_index.storage import StorageContext\n\nindex = VectorStoreIndex.from_vector_store(ts_vector_store)\nquery_engine = index.as_query_engine(vector_store_kwargs = ({\"start_date\": start_dt, \"end_date\":end_dt}))\n\nquery_str = \"What's new with TimescaleDB functions? When were these changes made and by whom?\"\nresponse = query_engine.query(query_str)\nprint(str(response))We asked the LLM a question about our gitlog, namely, \u201cWhat\u2019s new with TimescaleDB functions. When were these changes made and by whom?\u201dHere\u2019s the response we get, which synthesizes the nodes returned from semantic search with time-based filtering on the Timescale VectorStore:TimescaleDB functions have undergone changes recently. These changes include the addition of several GUCs (Global User Configuration) that allow for enabling or disabling major TimescaleDB features. Additionally, a compatibility layer has been added for the \"_timescaledb_internal\" functions, which were moved into the \"_timescaledb_functions\" schema to enhance schema security. These changes were made by Dmitry Simonenko and Sven Klemm. The specific dates of these changes are August 3, 2023, and August 29, 2023, respectively.This is a simple example of a powerful concept \u2014 using time-based context retrieval in your RAG applications can help provide more relevant answers to your users. This time-based context retrieval can be helpful to any dataset with a natural language and time component. Timescale Vector uniquely enables this thanks to its efficient time-based similarity search capabilities, and taking advantage of it in your LlamaIndex application is easy thanks to the Timescale Vector integration.Resources and next stepsNow that you\u2019ve learned how Timescale Vector can help you power better AI applications with PostgreSQL, it\u2019s your turn to dive in. Take the next step in your learning journey by following one of the tutorials or reading one of the blog posts in the resource set below:Up and Running Tutorial: learn how to use Timescale Vector in LlamaIndex using a real-world dataset. You\u2019ll learn how to use Timescale Vector as a Vectorstore, Retriever, and QueryEngine and perform time-based similarity search on vectors.Timescale Vector explainer: learn more about the internals of Timescale Vector.Timescale Vector website: learn more about Timescale Vector and Timescale\u2019s AI Launch Week.\ud83c\udf89 And a reminder: LlamaIndex Users get Timescale Vector free for 90 daysWe\u2019re giving LlamaIndex users an extended 90-day trial of Timescale Vector. This makes it easy to test and develop your applications on Timescale Vector, as you won\u2019t be charged for any cloud PostgreSQL databases you spin up during your trial period. Try Timescale Vector for free today.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications-924b0bd29f0"
        },
        {
            "date": "Mar 26, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-03-26",
            "tags": [
                "llm",
                "rag"
            ],
            "content": "Hi there, LlamaIndex followers! \ud83e\udd99Welcome to another thrilling weekly update from the LlamaUniverse. We're excited to bring you a fantastic array of updates, including Privacy-Preserving In-Context Learning with LlamaPacks and RAG Networks. Dive into our guides on MistralAI, explore Gemma LLMs, and enjoy a plethora of engaging tutorials using LlamaIndex, alongside upcoming webinars and events.\ud83e\udd29\u00a0The highlights:Privacy-Preserving In-Context Learning: Leveraging Xinyu Tang\u2019s paper, we've introduced LlamaPack for LLM/RAG apps, enabling the creation of few-shot demonstrations that maintain privacy and data integrity. LlamaPack, Tweet.Privacy-Preserving RAG Network: We present Privacy-Preserving RAG Network which facilitates the use of confidential datasets in healthcare and online platforms while safeguarding user privacy. Blogpost, Tweet.Advanced RAG and Agents with MistralAI: Guide on using MistralAI with LlamaIndex and LlamaParse, advancing RAG capabilities and agent development through custom pipelines and sophisticated parsing.\u2728 Feature Releases and Enhancements:We launched a LlamaPack based on Xinyu Tang\u2019s paper ****for secure in-context learning in LLM/RAG apps, focusing on generating few-shot demonstrations from private datasets with differential privacy, ensuring the synthetic examples reflect the data distribution without exposing sensitive details. LlamaPack, Tweet.We introduced a privacy-preserving RAG network by Andrei in LlamaIndex, enabling the use of sensitive datasets like healthcare and online user data without compromising individual privacy. This approach allows data providers to synthetically generate and share data for RAG queries securely. Blogpost, Tweet.We introduce a template by Sasha for agent-human interaction in RAG implementations, focusing on minimal human input. It triggers human intervention only for vague or malformed queries, enhancing clarity and precision in the response process. LlamaPack, Tweet.BAM Elevate integrated Databricks Vector Search into LlamaIndex, enabling vector search capabilities within the Databricks ecosystem. Blogpost, Tweet.We launched LlamaParse integration with LlamaIndex TypeScript, an industry-leading parser for PDFs and various document types accessible directly from JS/TS. Utilize the create-llama command-line tool or integrate LlamaParse directly into your app for enhanced document processing. Example, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to Advanced RAG and Agents with MistralAI using LlamaIndex and LlamaParse to construct sophisticated RAG and agents, including custom query pipelines, document parsing, and reference applications.Guide to Integrating Custom Models with LlamaIndex: Leonie Monigatti demonstrates the process of incorporating your custom model, like Gemma, into LlamaIndexGuide to combat prompt injection attacks, like the \"white text\" attack, by rigorously screening data during ingestion and retrieval, ensuring the integrity of LLM-powered systems against deceptive manipulations by Oleksandr Yaremchuk from Protect AI.\u270d\ufe0f Tutorials:Akriti Upadhyay\u2019s tutorial to prototype on patient data safely, featuring synthetic dataset generation, storage in Qdrant Vector DB, and querying with llama.cpp LLM using LlamaIndex.Frank Baele\u2019s tutorial on developing a production-grade RAG pipeline with LlamaParse, detailing document parsing, advanced ingestion techniques, Vector DB selection, and insights on evaluation, deployment, and budget management.Video tutorial by Ashish on creating an advanced PDF RAG agent, utilizing LlamaParse for text and tables extraction, defining retrievers and routers, and adding a sub-question layer, all integrated with LlamaIndex and MistralAI.UpTrain tutorial on Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations.Ravi Theja tutorial on showcasing RAG with LlamaIndex on 15 Indian languages using Navarasa-2.0 - a Gemma finetuned model on 15 Indian languages.\ud83c\udfa5\u00a0Webinars:Register for a webinar with Daniel Huynh featuring LaVague, an agent that can navigate the web in your Jupyter/Colab notebook.\ud83d\udcc5\u00a0Events:Join us for a Panel discussion on 'Why RAG Will Never Die - The Context Window Myth\u2019 with panelists from LlamaIndex, Vectara, Nvidia, and TogetherAI.We are hosting a RAG meetup in Paris on March 27th featuring talks on advanced RAG strategies, building a RAG CLI, and the significance of open-source RAG in business.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-03-26"
        },
        {
            "date": "Jan 23, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201301\u201323",
            "tags": [
                "llamaindex",
                "newsletter",
                "ai",
                "rag",
                "llm"
            ],
            "content": "Hello LlamaIndex Explorers \ud83e\udd99,Another exciting week at LlamaIndex, filled with vibrant community contributions and educational resources. Explore our array of new features, tutorials, guides, and demos, all tailored to enrich your experience with LlamaIndex.Before delving into the updates, we have two significant announcements:We\u2019re thrilled to host our first in-person hackathon, set for February 2nd-4th. This is a fantastic opportunity to meet fellow RAG enthusiasts, collaborate, and compete for prizes totaling over $8000!Don\u2019t miss our webinar featuring Sehoon Kim and Amir Gholami, scheduled for Thursday at 9 am PT. They will introduce LLMCompiler, an agent compiler for parallel multi-function planning and execution.We\u2019re always excited to see your projects, articles, or videos. If you\u2019ve created something you\u2019re proud of, share it with us at news@llamaindex.ai. Also, remember to subscribe to our newsletter on our website to get all the latest news straight to your inbox.\ud83e\udd29 The highlights:RankGPT: Introducing RankGPT leveraging GPT-3.5 and GPT-4 for top-tier document ranking and a novel sliding window technique for extensive context management. Notebook, Tweet.Composable Retrievers: An interface centralizing advanced retrieval and RAG techniques, enhancing RAG setups with IndexNodes for linking diverse retrievers and pipelines. Docs, Tweet.Advanced QA over Tabular Data Tutorial: A detailed guide to crafting query pipelines over tabular data, featuring Pandas, SQL, and Query Pipelines for an integrated few-shot, LLM, and custom function setup. Text-to-SQL, Text-to-Pandas.Long-Context Embedding Models: Explore models like M2-BERT-80M-32k-retrieval tackling the embedding chunking problem in RAG, with a focus on hybrid retrieval methods and hierarchical retrieval approaches. Guide.\u2728 Feature Releases and Enhancements:We have introduced RankGPT in our advanced module that utilizes GPT-3.5 and GPT-4 for efficient document ranking, featuring a unique sliding window strategy for handling large contexts. Notebook, Tweet.We have launched Composable Retrievers which centralizes various advanced retrieval and RAG techniques into a versatile interface. It simplifies creating complex RAG setups by allowing you to define IndexNodes to link different retrievers or RAG pipelines. Docs, Tweet.Anoop Sharma has introduced LlamaPack for Multi-Stock Ticker Analysis for analyzing various stock tickers with a single code line, enabling easy specification of tickers, time frames, and structured queries. LlamaPack, Tweet.LlamaIndex.TS (LITS) supports streaming on all endpoints. Tweet.We announced a new integration with Tonic Validate to allow simple access to LLM-powered evaluations. Blog post\ud83c\udfa5 Demo:RAG-Maestro for ArXiv Research: Developed by Aymen Kallala, this web app utilizes RAG to efficiently search scientific concepts in ArXiv papers. It extracts keywords using RAKE, queries ArXiv for relevant papers, and offers on-the-fly indexing with in-line citations \u2014 a valuable tool for ML researchers navigating through ArXiv\u2019s extensive library. Demo, GitHub Repo.\ud83d\uddfa\ufe0f Guides:Guide to Advanced QA over Tabular Data which provides a comprehensive tutorial on creating sophisticated query pipelines over tabular data using Pandas or SQL, constructing a query DAG using our Query Pipelines, integrating few-shot examples, linked prompts, LLMs, custom functions, retrievers, and more. Text-to-SQL, Text-to-Pandas.Guide to a Five-Part Series on Building a Full-Stack RAG Chatbot by Marco Bertelli, extensive tutorials covering every aspect of creating an RAG chatbot \u2014 from model selection and Flask backend setup to constructing the ChatEngine and optimizing the RAG pipeline.Guide to Long-Context Embedding Models: The models, like M2-BERT-80M-32k-retrieval, offer a solution to the embedding chunking issue in RAG by grounding retrieval in broader semantic contexts. Learn about hybrid retrieval, combining chunk and document-level similarity, and other approaches like hierarchical retrieval.\u270d\ufe0f Tutorials:Wenqi tutorial on Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference with LlamaIndex.Andrej tutorial on FastAPI and LlamaIndex RAG: Creating Efficient APIs.Lulia Brezeanu tutorial on Advanced Query Transformations to Improve RAG.Akash Mathur in-depth tutorial on Advanced RAG: Query Augmentation for Next-Level Search using LlamaIndex.Ryan Nguyen tutorial on Live Indexing for RAG: A Guide For Real-Time Indexing Using LlamaIndex and AWS.Nipuna (Paragon AI) tutorial on Building a Full-Stack Complex PDF AI chatbot with LlamaIndex.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-01-23-11ee2c211bab"
        },
        {
            "date": "Jul 9, 2024",
            "author": "LlamaIndex",
            "title": "LlamaCloud - Built for Enterprise LLM App Builders",
            "tags": [
                "llamacloud"
            ],
            "content": "RAG is only as Good as your DataBuilding production-ready LLM applications is hard. We've been chatting with hundreds of users, ranging from Fortune 500 enterprises to pre-seed startups and here's what they tell us they struggle with:Data Quality Issues: Most companies deal with large sets of complex, heterogeneous documents. Think PDFs with messy formatting, images, tables across multiple pages, different languages - the list goes on. Ensuring high-quality data input is crucial. \"Garbage in, garbage out\" holds especially true for LLM applications.Scalability Hurdles: Each new data source requires significant engineering hours for custom parsing and tuning. Keeping data sources in sync isn't easy either.Accuracy Concerns: Bad retrievals and hallucinations are common problems when LLMs interact with enterprise data, leading to unreliable outputs.Configuration Overload: Fine-tuning LLM applications involves numerous parameters and often requires deep technical expertise, making iterative improvement a daunting task.As developers shift from prototypes towards building production applications - complex orchestration is needed and they want to centralize their abstractions for managing their data. They want a unified interface for processing and retrieving over their diverse sources of data.To address these difficulties, we soft-launched LlamaCloud and made LlamaParse widely available a few months ago to bring production-grade context-augmentation to your LLM and RAG applications. LlamaParse can already support 50+ languages and 100+ document formats. The adoption has been incredible - we have grown to tens of thousands of active users for LlamaParse who have processed tens of million pages! Here\u2019s an example from Dean Barr, Applied AI Lead at Carlyle:As an AI Applied Data Scientist who was granted one of the first ML patents in the U.S., and who is building cutting-edge AI capabilities at one of the world's largest Private Equity Funds, I can confidently say that LlamaParse from LlamaIndex is currently the best technology I have seen for parsing complex document structures for Enterprise RAG pipelines. Its ability to preserve nested tables, extract challenging spatial layouts, and images is key to maintaining data integrity in advanced RAG and agentic model building.The Rise of Centralized Knowledge ManagementWe have designed LlamaCloud to cater to the need of\u00a0production-grade\u00a0context-augmentation\u00a0for your LLM and RAG applications. Let's take a tour of what LlamaCloud brings to the table:LlamaParse: Our state-of-the-art parser that turns complex documents with tables and charts into LLM-friendly formats. You can learn more about LlamaParse here.Managed Ingestion: Connect to enterprise data sources and your choice of data sinks with ease. We support multiple data sources and are adding more. LlamaCloud provides default parsing configurations for generating vector embeddings, while also allowing deep customization for specific applications.Advanced Retrieval: LlamaCloud allows basic semantic search retrieval as well as advanced techniques like hybrid search, reranking, and metadata filtering to improve the accuracy of the retrieval. This provides the necessary configurability to build end to end RAG over complex documents.LlamaCloud Playground: An interactive UI to test and refine your ingestion and retrieval strategies before deployment.Scalability and Security: Handle large volumes of production data. Compliance certifications as well as deployment options are available based on your security needs.This video gives a detailed walk through of LlamaCloud:PlayOur customers tell us that LlamaCloud enables developers to spend less time setting up and iterating on their data pipelines for LLM use cases, allowing them to iterate through the LLM application development lifecycle much more quickly. Here\u2019s what Teemu Lahdenpera, CTO at Scaleport.ai had to say:LlamaCloud has really sped up our development timelines. Getting to technical prototypes quickly allows us to show tangible value instantly, improving our sales outcomes. When needed, switching from the LlamaCloud UI to code has been really seamless. The configurable parsing and retrieval features have significantly improved our response accuracy.We've also seen great results with LlamaParse and found it outperforming GPT-4 vision on some OCR tasks!Try it yourselfWe\u2019ve opened up an official waitlist for LlamaCloud. Here's how you can get involved:Join the LlamaCloud Waitlist: Sign up here.Get in Touch: Have questions? Want to discuss unlimited commercial use? Contact us and let's chat! Note: we support private deployments for a select number of enterprisesStay Updated: Follow us on Twitter and join our Discord community to stay in the loop.In the meantime, anyone can create an account at https://cloud.llamaindex.ai/. While you\u2019re waiting for official LlamaCloud access, anyone can immediately start using our LlamaParse APIs.We\u2019re shipping a lot of features in the next few weeks. We look forward to seeing the context-augmented LLM applications that you can build on top of LlamaCloud! \ud83d\ude80\ud83e\udd99FAQHave you got some examples of how to use LlamaCloud?We sure do! One of the strengths of LlamaCloud is how easily the endpoints integrate into your existing code. Our llamacloud-demo repo has lots of examples from getting started to running evaluations.Is this competitive with vector databases?No. LlamaCloud is focused primarily on data parsing and ingestion, which is a complementary layer to any vector storage provider. The retrieval layer is orchestration on top of an existing storage system. LlamaIndex open-source integrates with 40+ of the most popular vector databases, and we are integrating LlamaCloud with storage providers based on customer requests.",
            "related_posts": [
                "LlamaCloud - Built for Enterprise LLM App Builders",
                "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud"
            ],
            "link": "https://www.llamaindex.ai/blog//llamacloud-built-for-enterprise-llm-app-builders"
        },
        {
            "date": "Oct 18, 2023",
            "author": "Kiran Neelakanda Panicker",
            "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser",
            "tags": [
                "nlp",
                "large-language-models",
                "llamaindex",
                "pdf",
                "rag"
            ],
            "content": "\n  Despite recent motivation to utilize NLP for wider range of real world\n  applications, most NLP papers, tasks and pipelines assume raw, clean texts.\n  However, many texts we encounter in the wild, including a vast majority of\n  legal documents (e.g., contracts and legal codes), are not so clean, with many\n  of them being visually structured documents (VSDs) such as PDFs. PDFs are\n  versatile, preserving the visual integrity of documents, but they often pose a\n  significant challenge when it comes to extracting and manipulating their\n  contents.\n\n\n  In this discussion, our focus will primarily be on text-only layered PDFs, a\n  category often regarded by many as a resolved issue.\n\nComplexity of Parsing PDFs\n\n\nLayout Complexity: PDFs can contain complex layouts, such\n    as multi-column text, tables, images, and intricate formatting. This layout\n    diversity complicates the extraction of structured data.\n  \n\nFont encoding issues: PDFs use a variety of font encoding\n    systems, and some of these systems do not map directly to Unicode. This can\n    make it difficult to extract the text accurately.\n  \n\nNon-linear text storage: PDFs do not store text in the\n    order it appears on the page. Instead, they store text in objects that can\n    be placed anywhere on the page. This means that the order of the text in the\n    underlying code may not match the order of the text as it appears visually.\n  \n\nInconsistent use of spaces: In some PDFs, spaces are not\n    used consistently or are not used at all between words. This can make it\n    difficult to even identify word boundaries.\n  \n\nDo we need an efficient parser?\n\n\n    In the Age of LLMs, is an Efficient Parser Still Essential When LLMs Can\n    Process Entire PDFs?\n  \n\n\n  This question gains relevance if the answer to this next question is \u201cYes\u201d.\n\n\nDo we need Retrieval-Augmented Generation (RAG)?\n\n\n  While LLMs are powerful, they have certain limitations in terms of the amount\n  of text they can process at once and the scope of information they can\n  reference. Further recent research have suggested LLM performance is often\n  highest when relevant information occurs at the beginning or end of the input\n  context, and significantly degrades when models must access relevant\n  information in the middle of long contexts. Techniques like RAG help overcome\n  these limitations, enabling more effective and efficient processing of large\n  documents and broader information retrieval.\n\n\nStill Skeptical? Let\u2019s ask an LLM for confirmation.\n\n\n\n\n\n  Now that we\u2019ve established the importance of an efficient parser, it becomes\n  instrumental in constructing an effective Retrieval-Augmented Generation (RAG)\n  pipeline to address the limitations of an LLM. Let\u2019s explore how we are\n  achieving this today. It\u2019s crucial to remember that the quality of the context\n  fed to an LLM is the cornerstone of an effective RAG, as the saying goes,\n  \u2018Garbage In \u2014 Garbage Out.\u2019\n\n\n  In the context of building LLM-related applications,\n  chunking is the process of breaking down large pieces of text\n  into smaller segments. It\u2019s an essential technique that helps optimize the\n  relevance of the content we get back from a database once we use the LLM to\n  embed content. Some of the strategies involved are\n\n\n\nFixed-size chunking. This is the most common and\n    straightforward approach to chunking: we simply decide the number of tokens\n    in our chunk and, optionally, whether there should be any overlap between\n    them. Easy to implement & most commonly used, but never makes it to a\n    production setting because the output is satisfactory in a Proof of Concept\n    (POC) setup, but its accuracy degrades as we conduct further testing.\n  \n\n\u201cContent-aware\u201d chunking. Set of methods for taking\n    advantage of the nature of the content we\u2019re chunking and applying more\n    sophisticated chunking to it. Challenging to implement due to the reasons\n    mentioned above, but if tackled correctly, it could be the most ideal\n    building block for a production-grade Information Retrieval (IR) engine.\n  \n\nWhere\u2019s This Article Headed, Anyway?\n\n  Certainly, let\u2019s put an end to the historical and background details, shall\n  we?\n\n\n  Introducing\n  LayoutPDFReader for \u201cContext-aware\u201d chunking.\n  LayoutPDFReader\n  can act as the most important tool in your RAG arsenal by parsing PDFs along\n  with hierarchical layout information such as:\n\n\n\n    Identifying sections and subsections, along with their respective hierarchy\n    levels.\n  \nMerging lines into coherent paragraphs.\nEstablishing connections between sections and paragraphs.\n\n    Recognizing tables and associating them with their corresponding sections.\n  \nHandling lists and nested list structures with precision.\n\n\n  The first step in using the\n  LayoutPDFReader\n  is to provide a URL or file path to it (assuming it\u2019s already been installed)\n  and get back a document object.\n\nfrom llmsherpa.readers import LayoutPDFReaderllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdfpdf_reader = LayoutPDFReader(llmsherpa_api_url)doc = pdf_reader.read_pdf(pdf_url)\nVector search and RAG with Smart Chunking\n\n  LayoutPDFReader employs intelligent chunking to maintain the cohesion of\n  related text:\n\n\n\n    It groups all list items together, along with the preceding paragraph.\n  \nItems within a table are chunked together.\n\n    It incorporates contextual information from section headers and nested\n    section headers.\n  \n\n\n  As a quick example, the following code snippet generates a\n  LlamaIndex\n  query engine from the document chunks produced by LayoutPDFReader.\n\nfrom llama_index.readers.schema.base import Documentfrom llama_index import VectorStoreIndexindex = VectorStoreIndex([])for chunk in doc.chunks():    index.insert(Document(text=chunk.to_context_text(), extra_info={}))query_engine = index.as_query_engine()# Let's run one queryresponse = query_engine.query(\"list all the tasks that work with bart\")print(response)\nWe get the following response:\nBART works well for text generation, comprehension tasks, abstractive dialogue, question answering, and summarization tasks.\nKey Considerations:\n\n\n    LLMSherpa leverages a cost-free and open API server. Your PDFs are not\n    retained beyond temporary storage during the parsing process.\n  \n\n    LayoutPDFReader has undergone extensive testing with a diverse range of\n    PDFs. However, achieving flawless parsing for every PDF remains a\n    challenging task.\n  \n\n    Please note that OCR (Optical Character Recognition) functionality is\n    presently unavailable. The tool exclusively supports PDFs equipped with a\n    text layer.\n  \n\n    For inquiries regarding private hosting options, OCR support, or tailored\n    assistance with particular PDF-related concerns, feel free to reach out to\n    contact@nlmatics.com\n    or to\n    me\n    directly.\n  \n\n\n  If you have any questions, please leave them in the comments section, and I\n  will try to respond ASAP.\n\n\nConnect?\n\n\n  If you want to get in touch, feel free to shoot me a message on\n  LinkedIn or via\n  email.\n\nReferences\n\nhttps://github.com/nlmatics/llmsherpa\n\n\nCapturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser\n\n\nLost in the Middle: How Language Models Use Long Contexts\n\n",
            "related_posts": [
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-12",
                "LlamaIndex Newsletter 2024\u201302\u201327"
            ],
            "link": "https://www.llamaindex.ai/blog//mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125"
        },
        {
            "date": "Nov 15, 2023",
            "author": "LlamaIndex",
            "title": "Announcing LlamaIndex 0.9",
            "tags": [
                "llm",
                "ai",
                "llamaindex",
                "genai",
                "python"
            ],
            "content": "Our hard-working team is delighted to announce our latest major release, LlamaIndex 0.9! You can get it right now:pip install --upgrade llama_indexIn LlamaIndex v0.9, we are taking the time to refine several key aspects of the user experience, including token counting, text splitting, and more!As part of this, there are some new features and minor changes to current usage that developers should be aware of:New IngestionPipline concept for ingesting and transforming dataData ingestion and transforms are now automatically cachedUpdated interface for node parsing/text splitting/metadata extraction modulesChanges to the default tokenizer, as well as customizing the tokenizerPackaging/Installation changes with PyPi (reduced bloat, new install options)More predictable and consistent import pathsPlus, in beta: MultiModal RAG Modules for handling text and images!Have questions or concerns? You can report an issue on GitHub or ask a question on our Discord!Read on for more details on our new features and changes.IngestionPipeline \u2014 New abstraction for purely ingesting dataSometimes, all you want is to ingest and embed nodes from data sources, for instance if your application allows users to upload new data. New in LlamaIndex V0.9 is the concept of an IngestionPipepline .An IngestionPipeline uses a new concept of Transformations that are applied to input data.What is a Transformation though? It could be a:text splitternode parsermetadata extractorembeddings modelHere\u2019s a quick example of the basic usage pattern:from llama_index import Document\nfrom llama_index.embeddings import OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor\nfrom llama_index.ingestion import IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\nnodes = pipeline.run(documents=[Document.example()])Transformation CachingEach time you run the same IngestionPipeline object, it caches a hash of the input nodes + transformations and the output of that transformation for each transformation in the pipeline.In subsequent runs, if there is a cache hit, that transformation will be skipped and the cached result will be used instead. The greatly speeds up duplicate runs, and can help improve iteration times when deciding which transformations to use.Here\u2019s an example with a saving and loading a local cache:from llama_index import Document\nfrom llama_index.embeddings import OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor\nfrom llama_index.ingestion import IngestionPipeline, IngestionCache\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ]\n)\n# will only execute full pipeline once\nnodes = pipeline.run(documents=[Document.example()])\nnodes = pipeline.run(documents=[Document.example()])\n# save and load\npipeline.cache.persist(\"./test_cache.json\")\nnew_cache = IngestionCache.from_persist_path(\"./test_cache.json\")\nnew_pipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n    ],\n    cache=new_cache,\n)\n# will run instantly due to the cache\nnodes = pipeline.run(documents=[Document.example()])And here\u2019s another example using Redis as a cache and Qdrant as a vector store. Running this will directly insert the nodes into your vector store and cache each transformation step in Redis.from llama_index import Document\nfrom llama_index.embeddings import OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor\nfrom llama_index.ingestion import IngestionPipeline, IngestionCache\nfrom llama_index.ingestion.cache import RedisCache\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nimport qdrant_client\nclient = qdrant_client.QdrantClient(location=\":memory:\")\nvector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    cache=IngestionCache(cache=RedisCache(), collection=\"test_cache\"),\n    vector_store=vector_store,\n)\n# Ingest directly into a vector db\npipeline.run(documents=[Document.example()])\n# Create your index\nfrom llama_index import VectorStoreIndex\nindex = VectorStoreIndex.from_vector_store(vector_store)Custom TransformationsImplementing custom transformations is easy! Let\u2019s add a transform to remove special characters from the text before calling embeddings.The only real requirement for transformations is that they must accept a list of nodes and return a list of nodes.import re\nfrom llama_index import Document\nfrom llama_index.embeddings import OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.ingestion import IngestionPipeline\nfrom llama_index.schema import TransformComponent\n\nclass TextCleaner(TransformComponent):\n  def __call__(self, nodes, **kwargs):\n    for node in nodes:\n      node.text = re.sub(r'[^0-9A-Za-z ]', \"\", node.text)\n    return nodes\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TextCleaner(),\n        OpenAIEmbedding(),\n    ],\n)\nnodes = pipeline.run(documents=[Document.example()])Node Parsing/Text Splitting \u2014 Flattened and Simplified InterfaceWe\u2019ve made our interface for parsing and splitting text a lot cleaner.Before:from llama_index.node_parser import SimpleNodeParser\nfrom llama_index.node_parser.extractors import (\n\tMetadataExtractor, TitleExtractor\n) \nfrom llama_index.text_splitter import SentenceSplitter\n\nnode_parser = SimpleNodeParser(\n  text_splitter=SentenceSplitter(chunk_size=512),\n  metadata_extractor=MetadataExtractor(\n  extractors=[TitleExtractor()]\n ),\n)\nnodes = node_parser.get_nodes_from_documents(documents)After:from llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor \n\nnode_parser = SentenceSplitter(chunk_size=512)\nextractor = TitleExtractor()\n\n# use transforms directly\nnodes = node_parser(documents)\nnodes = extractor(nodes)Previously, the NodeParser object in LlamaIndex had become extremely bloated, holding both text splitters and metadata extractors, which caused both pains for users when changing these components, and pains for us trying to maintain and develop them.In V0.9, we have flattened the entire interface into a single TransformComponent abstraction, so that these transformations are easier to setup, use, and customize.We\u2019ve done our best to minimize the impacts on users, but the main thing to note is that SimpleNodeParser has been removed, and other node parsers and text splitters have been elevated to have the same features, just with different parsing and splitting techniques.Any old imports of SimpleNodeParser will redirect to the most equivalent module, SentenceSplitter.Furthermore, the wrapper object MetadataExtractor has been removed, in favour of using extractors directly.Full documentation for all this can be found below:Node Parsers and Text SplittersMetadata ExtractorsTokenization and Token Counting \u2014 Improved defaults and CustomizationA big pain point in LlamaIndex previously was tokenization. Many components used a non-configurable gpt2 tokenizer for token counting, causing headaches for users using non-OpenAI models, or even some hacky fixes like this for OpenAI models too!In LlamaIndex V0.9, this global tokenizer is now configurable and defaults to the CL100K tokenizer to match our default GPT-3.5 LLM.The single requirement for a tokenizer is that it is a callable function, that takes a string, and returns a list.Some examples of configuring this are below:from llama_index import set_global_tokenizer\n\n# tiktoken\nimport tiktoken\nset_global_tokenizer(\n  tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n# huggingface\nfrom transformers import AutoTokenizer\nset_global_tokenizer(\n  AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").encode\n)Furthermore, the TokenCountingHandler has gotten an upgrade with better token counting, as well as using token counts from API responses directly when available.Packaging \u2014 Reduced BloatIn an effort to modernize the packaging of LlamaIndex, V0.9 also comes with changes to installation.The biggest change here is that LangChain is now an optional package, and will not be installed by default.To install LangChain as part of your llama-index installation you can follow the example below. There are also other installation options depending on your needs, and we are welcoming further contributions to the extras in the future.# installs langchain\npip install llama-index[langchain]\n \n# installs tools needed for running local models\npip install llama-index[local_models]\n\n# installs tools needed for postgres\npip install llama-index[postgres]\n\n# combinations!\npip isntall llama-index[local_models,postgres]If you were previously importing langchain modules in your code, please update your project packaging requirements appropriately.Import Paths \u2014 More Consistent and PredictableWe are making two changes to our import paths:We\u2019ve removed uncommonly used imports from the root level to make importing llama_index fasterWe now have a consistent policy for making \u201cuser-facing\u201d concepts import-able at level-1 modules.from llama_index.llms import OpenAI, ...\nfrom llama_index.embeddings import OpenAIEmbedding, ...\nfrom llama_index.prompts import PromptTemplate, ...\nfrom llama_index.readers import SimpleDirectoryReader, ...\nfrom llama_index.text_splitter import SentenceSplitter, ...\nfrom llama_index.extractors import TitleExtractor, ...\nfrom llama_index.vector_stores import SimpleVectorStore, ...We still expose some of the most commonly used modules at the root level.from llama_index import SimpleDirectoryReader, VectorStoreIndex, ...MultiModal RAGGiven the recent announcements of the GPT-4V API, multi-modal use cases are more accessible than ever before.To help users use these features, we\u2019ve started to introduce a number of new modules to help support use-cases for MultiModal RAG:MultiModal LLMs (GPT-4V, Llava, Fuyu, etc.)MultiModal Embeddings (i.e clip) for join image-text embedding/retrievalMultiModal RAG, combining indexes and query enginesOur documentation has a full guide to multi-modal retrieval.Thanks for all your support!As an open-source project we couldn\u2019t exist without our hundreds of contributors. We are so grateful for them and the support of the hundreds of thousands of LlamaIndex users around the world. See you on the Discord!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//announcing-llamaindex-0-9-719f03282945"
        },
        {
            "date": "Oct 17, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201310\u201317",
            "tags": [
                "llm",
                "programming",
                "ai",
                "llamaindex",
                "newsletter"
            ],
            "content": "Hello Llama Enthusiasts \ud83e\udd99!Another week has flown by, and we\u2019re back with a jam-packed newsletter filled with updates on hackathons, guides, integrations, features, webinars, tutorials, blogs, and demos. If you have a project, blog post, or video that deserves a spotlight, we\u2019d love to feature it! Just reach out to us at news@llamaindex.ai.Bonus: You can now get all these updates straight to your inbox! Simply visit our homepage and sign up for our email updates.\ud83e\udd29 First, the highlights:AI.Engineer Summit: At the AI.Engineer Summit, Jerry Liu discussed RAG applications, while Simon led a workshop on RAG app optimization (Jerry\u2019s slides, Simon\u2019s slides)Text to pgVector: we launched PGVectorSQLQueryEngine for combined SQL and vector queries on PostgreSQL. (Docs, Tweet)Hugging Face Integration: Integrated with HuggingFace\u2019s text-embeddings-inference server for high-speed, large-scale BERT model serving. (Docs, Tweet)Multi-Document Agents: New V1 agents support advanced multi-document retrieval and async query planning. (Docs, Tweet)Unstructured Parsing: Unveiled UnstructuredElementNodeParser, a hierarchical parser for embedded tables/text using UnstructuredIO. (Docs, Tweet)LLM Compatibility: We have charted LLM performances on various tasks and found that the Zephyr-7b-alpha model stands out as the top-performing 7B model in advanced RAG tasks. (Docs)\ud83c\udfc6 Congratulations to our AGI House Hackathon Winners!We love seeing people build amazing things with LlamaIndex!Build:DemostifyStick with Fit, SafeQuery, CherryBreak:Fuzzy AccessTest:X-Ray InsightHonorable Mentions:KindleGPTPenTest\ud83c\udfa4 LlamaIndex at AI.Engineer Summit:Jerry Liu gave a talk on Building production-ready RAG applications. Slides.Simon conducted a workshop on Building, Evaluating, and Optimizing your RAG App for Production with LlamaIndex. Slides, Code.\ud83d\uddfa\ufe0f Guides:LLM Compatibility Tracking: We\u2019ve charted LLM performances on various tasks, revealing zephyr-7b-alpha as the only current 7B model excelling in advanced RAG/ Agentic tasks. Docs.Evaluations: Adjusting chunk size is essential for RAG apps. Having more chunks isn\u2019t necessarily better, and re-ranking might be counterproductive. To fine-tune, experiment with different chunk sizes and top-k values. The Arize AI team has provided a guide to help you evaluate using Arize AI Phoenix and Llama Index. Slides, Notebook.\u270d\ufe0f Tutorials:Shahul\u2019s tutorial demonstrates how to choose the best embeddings for your data, emphasizing that retriever performance and embedding quality are crucial for a RAG system\u2019s efficacy using the LlamaIndex and RAGAS libraries.Wenqi Glantz\u2019s tutorial on Evaluation Driven Development for RAG Pipelines.Wenqi Glantz\u2019s tutorial on Masking PII Data in the RAG Pipeline.Ofer Mendelevitch\u2019s from Vectara has a tutorial on Retrieval Augmented Generation with LlamaIndex on comparing Vectara\u2019s new Boomerang model to OpenAI and Cohere.Patrick Loeber from AssemblyAI has a tutorial on Build LlamaIndex Audio Apps.Pradip Nichite made a tutorial on NL2SQL with LlamaIndex: Querying Databases Using Natural Language.Mayo Oshin has a tutorial on How to Compare Multiple Large PDF Files.Sudarshan Koirala made a tutorial on Chat With Documents with LlamaIndex and Pinecone.\ud83d\udca1 Demos:Siva Surendira built YC Bot to get instant startup advice from your favorite YC mentors.\u2728 Feature Releases and Enhancements:Text to pgVector: We introduced the PGVectorSQLQueryEngine, which allows you to query a PostgreSQL database using both full SQL and vector search simultaneously. Docs, Tweet.Multi-Document Agents: We introduce Multi-Document Agents (V1) that can now retrieve across multiple docs and plan queries asynchronously, offering a superior analysis compared to standard RAG. Docs, Tweet.UnstructuredIO: We\u2019ve partnered with UnstructuredIO to enhance LLM/RAG applications. By extracting tables from PDFs, we\u2019ve improved query methods beyond basic vector indexing, enabling hybrid queries and cross-document comparisons, especially for tabular questions. Docs, Tweet.UnstructuredElementNodeParser: Going beyond basic text-splitting, we introduce the UnstructuredElementNodeParser. It models embedded tables/text hierarchically in a data graph using UnstructuredIO. Docs, Tweet.Cross-Encoder Fine-Tuning: Cross-encoders enhance RAG by refining post-embedding search results. With LlamaIndex, you can now fine-tune cross-encoders on any document, boosting performance. Docs, Tweet.\u2699\ufe0f Integrations & Collaborations:Assembly AI: We introduced a new data reader for audio data integration with AssemblyAI. This integration allows effortless audio loading and facilitates building vector store indices and query engines for inquiries. Docs, Tweet.Nougat \u2014 MetaAI: We integrated Nougat, an exceptional OCR tool from Meta, that excels in interpreting scientific papers, notably mathematical notations, and LaTeX as a loader in LlamaHub, allowing streamlined processing of ArXiv papers within the RAG pipeline. Docs, Tweet.Hugging Face-Text Embeddings Inference: We integrated with the new text-embeddings-inference server from HuggingFace offering production-scale serving with distributed tracing for all BERT models at impressive speeds. Docs, Tweet.\ud83c\udfa5 Webinars And Podcast:Webinar with Timescale on Time-based retrieval for RAG.Webinar with Omar Khattab and Thomas Joshi on DSPy \u2014 a framework for LLMs that emphasizes programming over prompting.Jerry Liu\u2019s podcast with Latent Space on LlamaIndex\u2019s origin story, fine-tuning, and more.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-10-17-33514cbc04a2"
        },
        {
            "date": "Sep 6, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex Update \u2014 09/03/2023",
            "tags": [
                "large-language-models",
                "llamaindex",
                "openai",
                "ai",
                "nlp"
            ],
            "content": "Hello LlamaIndex Community!We\u2019re thrilled to bring you the latest edition of our LlamaIndex Update series. Whether you\u2019ve been a part of our journey from the start or have just recently joined us, your engagement and input are invaluable to us.In this update, we\u2019re excited to unveil some significant advancements. We\u2019ve got comprehensive updates on new features for both the Python and TypeScript versions of LlamaIndex. In addition, we\u2019re offering some expert insights on RAG tips that you won\u2019t want to miss. To keep you ahead of the curve, we\u2019ve also curated a selection of webinars, tutorials, events, and demos.So without further ado, let\u2019s delve into the latest developments.New Features:LlamaIndexLlamaIndex introduces the Sweep AI code splitter for RAG apps, addressing the challenges of traditional code splitting. This tool features recursive splitting combined with CSTs across 100+ languages, enhancing the LlamaIndex experience. BlogPost, Tweet.LlamaIndex now supports streaming data ETL, enhancing structured data extraction with the OpenAI Function API. By inputting a Pydantic object class in LlamaIndex, users can receive streamed data objects from OpenAI individually. Docs, Tweet.LlamaIndex has teamed up with Neo4j to amplify knowledge graph capabilities with LLM\u2019s. This integration not only allows for storing any knowledge graph created in LlamaIndex directly in Neo4j but also introduces a specialized text-to-cypher prompt for Neo4j users. Docs, Tweet.LlamaIndex, in collaboration with Mendable AI and Nomic AI, unveils a Nomic Atlas visual map detailing user questions from the Mendable AI bot. This innovative tool groups similar questions, providing insights for improved app deployment, prompt control, language support, and documentation. New users can find the helpful Mendable AI bot on LlamaIndex\u2019s documentation site. Tweet.LlamaIndex, in collaboration with Predibase, offers an optimal way to operationalize LLMs. Experience top-tier RAG by privately hosting open-source LLMs on managed infrastructure right within your VPC. Docs, Tweet.The LlamaIndex playground app enhances the RAG experience. Updates include new Temperature and Top P options, along with intuitive tooltips offering plain language explanations.LlamaIndex Tip\ud83d\udca1: Boost your RAG systems by adding structured data to raw text. This allows for easier metadata filtering and optimal embedding biases. Dive into our guide on harnessing the HuggingFace span marker for targeted entity extraction. Docs, Tweet.LlamaIndex now has the Semantic Scholar Loader. With it, users can swiftly set up citation-based Q&A systems. Docs, Tweet.LlamaIndex highlights the significance of text chunk size in LLM QA systems. To determine the best chunk size without human intervention, we suggest ensembling different sizes and using a reranker for context relevance during queries. This method involves simultaneous queries across retrievers of various sizes and consolidating results for reranking. Though experimental, this approach aims to discern the optimal chunk size strategy. Docs, Tweet.LlamaIndex\u2019s customer support bot seamlessly interfaces with Shopify\u2019s 50k-line GraphQL API Spec. Through smart tools and LlamaIndex features, it offers quick insights like refunded orders despite the vast spec size. Efficient indexing ensures precise user query responses. Docs, Tweet.LlamaIndex\u2019s integration with Xinference enables users to effortlessly expand models like llama 2, chatglm, and vicuna to incorporate RAG and agents. Docs, Tweet.LlamaIndex introduces One-click Observability. With just a single code line, integrate LlamaIndex with advanced observability tools from partners like Weights & Biases, ArizeAI, and TruEra, simplifying LLM app debugging for production. Docs, Tweet.LlamaIndex has updated the LLM default temperature value to 0.1. Tweet.LlamaIndex integration with Zep, enhancing the memory layer of LLM apps. It\u2019s not just about storage but also enriching data with summaries, metadata, and more. BlogPost, Tweet.LlamaIndex has revamped its defaults! Now, gpt-3.5-turbo is the go-to LLM, with enhanced prompts and a superior text splitter. Additionally, if OpenAI\u2019s key isn\u2019t set, it has backup options with llama.cpp. New embedding features have also been added. Tweet.LlamaIndex now seamlessly integrates with FastChat by lmsysorg. Elevate your LLM deployments like Vicuna and Llama 2, serving as an alternative to OpenAI. Tweet.LlamaIndex provides a seamless integration with Azure AI Services. Dive into a richer ecosystem of AI tools from Computer Vision, Translation, and speech enhancing your multi-modal AI interactions. Docs1, Docs2, Docs3, Tweet.LlamaIndex unveils Graph RAG \u2014 an approach to enhance LLMs with context from graph databases. Extract valuable subgraphs from any knowledge graph for superior question-answering capabilities. Docs, Tweet.LlamaIndex has expanded native async support, enhancing the scalability of full-stack LLM apps. We now offer async agents, tool execution, and callback support, and have introduced async methods in vector stores. Tweet.LlamaIndex enhances debugging with data agent trace observability. Additionally, system prompts can now be added to any query engine and we have begun the transition of LLM and embedding modules to Pydantic. Docs, Tweet.LlamaIndex\u2019s Recursive Document Agents enhance RAG by retrieving based on summaries and adjusting chunk retrieval per need. This boosts querying across varied documents, offering both question-answering and summarization within a document. Docs, Tweet.LlamaIndex integrates with Metaphor to supercharge data agents. This integration offers a specialized search engine tailored for LLMs, allowing dynamic data lookup beyond just RAG, and answering a broader range of questions. BlogPost, Tweet.LlamaIndex now supports integration with OpenAI\u2019s fine-tuned models via their new endpoint. Seamlessly integrate these models into your RAG pipeline. Docs, Tweet.LlamaIndex introduces the OpenAIFineTuningHandler to streamline data collection for fine-tuning gpt-3.5-turbo with GPT-4 outputs. Run RAG with GPT-4 and effortlessly generate a dataset to train a more cost-effective model. Notebook, Tweet.LlamaIndex presents the Principled Development Practices guide, detailing best practices for LLM app development Observability, Evaluation, and Monitoring. Docs, Tweet.LlamaIndex introduces a refined Prompt system. With just three core classes: PromptTemplate, ChatPromptTemplate, and SelectorPromptTemplate, users can effortlessly format as chat messages or text and tailor prompts based on model conditions. Docs, Tweet.LlamaIndex delves into chunk dreaming a concept inspired by Thomas H. Chapin IV. By auto-extracting metadata from a text chunk, it can identify potential questions and provide summaries over neighboring nodes. This enriched context boosts RAG\u2019s performance. Docs, Tweet.LlamaIndex is integrated with BagelDB, enabling developers to effortlessly tap into vector data stored on BagelDB. Tweet.LlamaIndex now lets the LLM choose between vector search for semantic queries or our BM25 retriever for keyword-specific ones. Docs, Tweet.LlamaIndex introduces the AutoMergingRetriever, crafted with insights from Jason and ChatGPT. This technique fetches precise context chunks and seamlessly merges them, optimizing LLM responses. Using the HierarchicalNodeParser, we ensure interconnected chunks for enhanced context clarity. Docs, Tweet.LlamaIndex introduces embedding finetuning for optimized retrieval performance. Beyond enhancing RAG, we\u2019ve simplified retrieval evaluations with automatic QA dataset generation from text, streamlining both finetuning and evaluation processes. Docs, Tweet.LlamaIndex now integrates directly with Airbyte sources including Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform, and Zendesk Support. Easily enhance your LlamaIndex application with these platforms implemented as data loaders. BlogPost, Tweet.LlamaIndex integrates with DeepEval, a comprehensive library to evaluate LLM and RAG apps. Assess on four key metrics: Relevance, Factual Consistency, Answer Similarity, and Bias/Toxicity. Docs, Tweet.LlamaIndex recommends evaluating LLM + RAG step-by-step, especially retrieval. Create synthetic retrieval datasets from text chunks using LLMs. This method not only evaluates retrieval but also fine-tunes embeddings. Docs, Tweet.LlamaIndex unveils a managed index abstraction simplifying RAG\u2019s ingestion and storage processes with Vectara. Docs, Tweet.LlamaIndex has significantly enhanced its callback handling support, encompassing features like tracebacks, LLM token counts, templates, and detailed agent tool information. These advancements pave the way for smoother integrations with evaluation and observability applications. Tweet.LlamaIndex has integrated with AskMarvinAI, enabling automated metadata extraction from text corpora. Just annotate a Pydantic model and effortlessly log metadata from all associated text chunks. Docs, Tweet.LlamaIndex is integrated with RunGPT by JinaAI, an outstanding framework for one-click deployment of various open-source models such as Llama, Vicuna, Pythia, and more. Coupled with LlamaIndex\u2019s innate chat/streaming capabilities, users can now deploy and utilize powerhouse models like Llama-7B seamlessly. Docs, Tweet.LlamaIndex.TSLITS has Full Azure OpenAI integration. Tweet.LITS Enhanced Llama2 support, new default temperature (0.1), and GPT chat integration. Tweet.LITS helps to use fromDocuments without repeat checks; auto SHA256 comparison. Tweet.LITS now supports OpenAI v4, Anthropic 0.6, & Replicate 0.16.1., CSV loader, Merged NodeWithEmbeddings & BaseNode. Tweet.LITS now supports PapaCSVLoader for math. Tweet.LITS is now integrated with LiteLLM. Tweet.LITS now has additional session options for proxy server support, Default timeout reset to 60 seconds for OpenAI. Tweet.LITS now has Pinecone integration. Tweet.LITS has Optimized ChatGPT prompts, fixed metadata rehydration issues, and OpenAI Node v4.1.0 with fine-tuned model support. Tweet.LITS has introduced enhanced text-splitting features, including a specialized tokenizer for Chinese, Japanese, and Korean, and refinements to the SentenceSplitter for handling decimal numbers. Tweet.LITS has a Markdown loader and metadata support in the response synthesizer. Tweet.LITS revamped usability: ListIndex is now SummaryIndex for clarity, and prompts have been made typed and customizable to enhance user control and experience. Tweet.LITS has Notion Reader. Now, users can effortlessly import their documents directly into their RAG or Data Agent application in LITS. Tweet.RAG Tips:LlamaIndex shares four tactics to boost your RAG pipeline:1\ufe0f\u20e3 Use summaries for retrieval, and a broader context for synthesis.2\ufe0f\u20e3 Use metadata for structured retrieval over large docs.3\ufe0f\u20e3 Deploy LLMs for dynamic retrieval based on tasks.4\ufe0f\u20e3 Fine-tune embeddings for better retrieval.Tutorials:Jason's tutorial on adding Image Responses to GPT knowledge retrieval apps.Wenqi Glantz tutorial on Building Production-Ready LLM Apps with LlamaIndex: Document Metadata for Higher Accuracy RetrievalStreamlit tutorial on Building a chatbot with custom data sources, powered by LlamaIndex.Wenqi Glantz tutorial on Building Production-Ready LLM Apps With LlamaIndex: Recursive Document Agents for Dynamic Retrieval.Erika Cardenas covers the usage of LlamaIndex in building an RAG app.Argilla blog post on Fine-tuning and evaluating GPT-3.5 with human feedback for RAG using LlamaIndex.KDNuggests blog post on Build Your Own PandasAI with LlamaIndex.From the LlamaIndex team:Jerry Liu\u2019s tutorial on fine-tuning Llama 2 for Text-to-SQL Applications.Jerry Liu's tutorial on Fine-Tuning Embeddings for RAG with Synthetic Data.Ravi Theja\u2019s tutorial on combining Text2SQL and RAG with LlamaIndex to analyze product reviews.Ravi Theja\u2019s tutorial on different Indicies, Storage Context, and Service Context of LlamaIndex.Ravi Theja\u2019s tutorial on Custom Retrievers and Hybrid Search in LlamaIndex.Adam's tutorial on Introduction to Data Agents for Developers.Ravi Theja\u2019s tutorial on creating Automatic Knowledge Transfer (KT) Generation for Code Bases using LlamaIndex.Webinars:Webinar with members from Docugami on Document Metadata and Local Models for Better, Faster Retrieval.Webinar with Shaun and Piaoyang on building Personalized AI Characters with RealChar.Webinar with Bob (Weaviet), Max (sid.ai), and Tuana (HayStack) on making RAG Production-Ready.Workshop by Wey Gu on Building RAG with Knowledge Graphs.Webinar with Jo Bergum and Shishir Patil on fine-tuning and RAG.Events:Jerry Liu spoke about LlamaIndex at the NYSE Floor Talk.Ravi Theja spoke about LlamaIndex at the Fifth Elephant conference in Bengaluru, India.Ravi Theja conducted a workshop on LlamaIndex in Bengaluru, India.Demos And Papers:The paper titled Performance of ChatGPT, human radiologists, and context-aware ChatGPT in identifying AO codes from radiology reports is an intriguing medical research. It leverages both LlamaIndex and ChatGPT to pinpoint AO codes within radiology reports, enhancing fracture classification. A fantastic fusion of tech and medicine!SEC Insights AI does SEC document analysis using LlamaIndex is on Product Hunt as the 5th product of the day.RentEarth: an agent to build your own startup with an amazing 3D interface and LlamaIndex.In wrapping up this edition of our LlamaIndex Update series, we\u2019re reminded of the power of collaboration and innovation. From new features to integrations and tutorials, our mission to revolutionize the AI realm marches forward. To every member of our community, thank you for your unwavering support and enthusiasm. Let\u2019s continue to elevate the world of AI together!",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-update-09-03-2023-4a7c21c0f60b"
        },
        {
            "date": "Apr 23, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-04-23",
            "tags": [],
            "content": "Hello LlamaIndex Community! \ud83e\udd99Welcome to another thrilling weekly update from LlamaWorld! We're excited to bring you a variety of outstanding updates, including Cookbooks, demos, guides, and tutorials.\ud83e\udd29\u00a0The highlights:MistralAI's 8x22b Model Cookbook: Released cookbook for MistralAI's 8x22b model with detailed guidance on RAG, query routing, and tool applications. Docs, Tweet.Llama 3 Model Cookbook: A comprehensive cookbook for Meta's Llama 3 model from simple prompt runs to complex RAG pipeline, agents and tools, accessible directly from Hugging Face. Docs, Tweet.create-llama Llama 3 template: create-llama template for Meta's Llama 3 to quickly start building full-stack LLM applications using the nextjs-llama3 template with a single CLI command. Tweet.\u2728 Feature Releases and Enhancements:We have released a cookbook for the latest MistralAI model, the powerful 8x22b, which sets a new standard for open models. The cookbook covers RAG, query routing, and tool use cases. Docs, Tweet.We have released a cookbook for latest Meta's new Llama 3 model, available directly from Hugging Face. This guide covers everything from running basic prompts to setting up a full RAG pipeline, agents and tools. Docs, Tweet.We have introduced a template for integrating Meta's Llama 3 in create-llama. Simply run npx create-llama and select the nextjs-llama3 template to build full-stack LLM application with Llama 3 in one CLI command. Tweet.\ud83c\udfa5\u00a0Demos:Open Source AI Diagram Generator by Rohan using LlamaIndex's Pydantic program with partial JSON parsing and Vercel AI SDK to generate and stream diagrams dynamically for an enhanced user experience.DREAM: A Distributed RAG Experimentation Framework by Aishwarya Prabhat, featuring a full-stack blueprint for optimizing RAG setups in a distributed environment. This setup includes Ray for computing, LlamaIndex for advanced techniques, Ragas for synthetic data, MinIO, MLflow, Project Jupyter, and ArgoCD.Firecrawl from Mendable is an API service that crawls a given URL and converts its content, including all accessible subpages, into clean markdown format. It utilizes LlamaParse from LlamaIndex for PDF parsing.\ud83d\uddfa\ufe0f Guides:Guide to integrating Qdrant Hybrid Cloud with LlamaIndex, featuring JinaAI embeddings, MistralAI's Mixtral 8x7b, and our LlamaParse document parser.Guide to building RAG using completely open and free components from Elastic, featuring Ollama and MistralAI, demonstrates how to assemble a RAG application with LlamaIndex using entirely free software.Guide to Building a Code-Writing Agent: TechWithTimm demonstrated how to create an agent that writes code by reading your documentation. Learn how to set up local LLMs with Ollama, parse documentation using LlamaParse, build an agent, and teach it to write code.Guide to Fine-tuning Embedding Models for RAG with LoRA by Mariboo demonstrates how to enhance Hugging Face models using LlamaIndex's finetuning techniques, including steps from quantization to fine-tuning with QLoRA.\u270d\ufe0f Tutorials:Khye Wei's tutorial from Microsoft demonstrates how to use LlamaIndex with Azure's AI Search to create powerful RAG applications, including Hybrid Search, Query Rewriting, and SubQuestionQuery Engine.Hanane Dupouy's tutorial on Building a Finance Agent with LlamaIndex to query public companies with tools for looking up stock prices, summarizing financial news, and plotting stock data, all streamlined through LlamaIndex's ReAct agent and API abstractions.Andy Singal's tutorial on Building a ColBERT-powered Retrieval Agent with Memory demonstrates how to enhance a RAG pipeline with \"state\" storage for a more personalized, conversational assistant using LlamaIndex's custom agent and query pipeline abstractions.Mariboo\u2019s tutorial on Fine-tuning Embedding Models for RAG with LoRA using LlamaIndex's finetuning abstractions.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-04-23"
        },
        {
            "date": "May 14, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-05-14",
            "tags": [],
            "content": "Hello LlamaIndex Family! \ud83e\udd99Welcome to another thrilling weekly update from LlamaIndex! We're excited to share a variety of outstanding updates, guides, and tutorials with you. But first, we have an exciting announcement to make.We are thrilled to announce a new course in collaboration with DeepLearningAI\u2014Building Agentic RAG. In this course, you\u2019ll learn how to build a research assistant that can reason over multiple documents and answer complex questions. You\u2019ll also learn how to step through the execution of the agent and steer it with human feedback. Check it out and take your RAG skills to the next level!\ud83e\udd29\u00a0The highlights:Day 0 Support for GPT-4o - TweetLlama3 Cookbook - TweetTypeScript Agent Guide.\u2728 Feature Releases and Enhancements:We have introduced day 0 support for GPT-4o in both Python and TypeScript. Additionally, we've created demo notebooks (demo1 and demo2) to help you easily experiment with GPT-4o using LlamaIndex. Tweet.We have launched Llama3 cookbooks showcasing interesting use cases for Llama 3, from basic chat functionalities to advanced agent development. Ideal for anyone building with local models, either on your laptop or through an API. Notebook, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to building agents in TypeScript: Dive into our comprehensive, open-source guide developed by Laurie that walks you through every step of agent development, from setting up with basic functions to integrating advanced features like local and remote LLMs, and data querying with vectorDB.Guide to using RAG for content moderation: CloudRaft shows how to set up a RAG pipeline to moderate user-generated images effectively, ensuring compliance with predefined rules through techniques like semantic search and efficient inferencing with small LLMs.\u270d\ufe0f Tutorials:Kxsystems advanced workshop on \"Building Advanced RAG over Complex PDFs with LlamaParse\" to demonstrate how LlamaParse can tackle the challenge of extracting diverse elements like text, tables, images, and graphs from complex research papers. Video Tutorial, BlogPost, Notebook.Arslan Shahid tutorial on Generating PowerPoints with Llama 3, using LlamaIndex to create a Llama3 RAG pipeline. The approach not only answers questions but also generates PowerPoint slide decks by utilizing the python-pptx library to write code programmatically for slide creation.Hanane Dupouy demonstrates Building a Financial Agent that can Perform Reflection. The approach helps in analyzing stock prices by implementing two types of reflection: CRITIC (tool use) and self-reflection (no tools).zhaozhiming\u2019s tutorial on evaluating RAG systems, utilizing evaluation libraries like TruLens, Ragas, UpTrain, and DeepEval to assess RAG systems using metrics such as faithfulness, relevance, and answer correctness.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-05-14"
        },
        {
            "date": "Apr 29, 2024",
            "author": "Apr 29, 2024",
            "title": "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB",
            "tags": [
                "mongodb",
                "fireworks",
                "llamaparse",
                "hackathons"
            ],
            "content": "This is a guest post from Team CLAB, the winners of \"Best Use of LlamaIndex\" at our recent hackathon with MongoDB.Imagine this: you\u2019re deep in a coding project, and a critical question pops up about a specific tool or library. You start the dreaded documentation shuffle\u200a\u2014\u200asearching through wikis, FAQs, maybe even firing up a separate chatbot for specific tools (like those from LlamaIndex, FireworksAI or anyone else). It\u2019s frustrating! \ud83e\udd2f We wanted to change that.That\u2019s why Team CLAB built LlamaWorksDB (try it out!), your friendly AI-powered doc wizard \u2728. No more scattered searches! It taps into the knowledge of multiple sponsors of our hackathon including LlamaIndex, Fireworks.ai, and MongoDB, all through a single chatbot interface. Need something explained from MongoDB\u2019s docs? Got it! Want a code example from Fireworks.ai? Easy!The foundation: LlamaIndex and data ingestionLlamaIndex was the heart and soul of LlamaWorksDB. It\u2019s like a super versatile toolbox for handling all kinds of documentation! We primarily used their open-source readers to grab info straight from websites. A cool hack we did was customize the SimpleWebPageReader. We taught it to ignore website navigation bars, saving us a ton of precious tokens. \ud83d\udcaa While this worked great for the documentation sites, we used LlamaIndex\u2019s GithubRepositoryReader to easily read through each repo.from llama_index.readers.web import SimpleWebPageReader\nimport re\n\nclass LlamaDocsPageReader(SimpleWebPageReader):\n   def load_data(self, urls):\n       documents = super().load_data(urls)\n       processed_documents = []\n       for doc in documents:\n           processed_doc = self.process_document(doc)\n           processed_documents.append(processed_doc)\n       return processed_documents\n\n   def process_document(self, document):\n       # Split the document text by \"Table of Contents\"\n       pattern = r'(?i)\\n\\n*table\\s*of\\s*contents\\n\\n*'\n       parts = re.split(pattern, document.text, maxsplit=1)\n       # If there is a part after \"Table of Contents\", use it as the document text\n       if len(parts) > 1:\n           document.text = \"Table of contents\".join(parts[1:])\n       return documentChoosing how to split up the docs was interesting. LlamaIndex has options ranging from the basic SentenceSplitter to their SemanticNodeParser, which uses AI to group similar ideas. We went with the latter for those perfectly sized, meaningful chunks.Finally, we embedded each \u2018node\u2019 and sent each as a document to MongoDB. Talk about streamlined! MongoDB stored the text, metadata, and our embeddings\u200a\u2014\u200aideal for the kind of search we wanted to build. We used Nomic\u2019s flexible embedding model via Fireworks, which let us fine-tune the dimensions for maximum efficiency.# FireworksEmbedding defaults to using model\nembed_model = FireworksEmbedding(api_key=os.getenv('FIREWORKS_API_KEY'),\n                                model=\"nomic-ai/nomic-embed-text-v1.5\",\n                                embed_batch_size=10,\n                                dimensions=768 # can range from 64 to 768\n                                )\n\n# the tried and true sentence splitter\ntext_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n# the semantic splitter uses our embedding model to group semantically related sentences together\nsemantic_parser = SemanticSplitterNodeParser(embed_model=embed_model)\n\n# we set up MongoDB as our document and vector database\nvector_store = MongoDBAtlasVectorSearch(\n   pymongo.MongoClient(os.getenv('MONGO_URI')),\n   db_name=\"fireParse\",\n   collection_name=\"llamaIndexDocs\",\n   index_name=\"llama_docs_index\"\n)\n\n#finally we use LlamaIndex's pipeline to string this all together\npipeline = IngestionPipeline(\n   transformations=[\n       semantic_parser, #can replace with text_splitter\n       embed_model,\n   ],\n   vector_store=vector_store,\n)Once we have everything set up, we can create documents from URLs in MongoDB! Below is an example of using three URLs but we used hundreds.example_urls = [\n   \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/llama3_cookbook\",\n   \"https://docs.llamaindex.ai/en/stable/examples/cookbooks/anthropic_haiku/\",\n  \"https://docs.llamaindex.ai/en/stable/examples/vector_stores/MongoDBAtlasVectorSearch/\"\n]\n\n# read in the documents and pass them through our pipeline\ndocuments = LlamaDocsPageReader(html_to_text=True).load_data(example_urls)\npipeline.run(documents=documents, show_progress=True)You can see in MongoDB how our documents have text, embedding (with 768 dimensions), and metadata.Example document in MongoDB Atlas that resulted from the pipelineMongoDB Atlas for vector searchMongoDB Atlas was our go-to for storing both the documentation text and the embeddings themselves. It\u2019s incredibly versatile! Setting up vector search within Atlas is a breeze, allowing us to quickly find the most relevant document chunks. Plus, LlamaIndex\u2019s metadata parsing played perfectly with Atlas\u200a\u2014\u200awe could easily filter results based on things like document source or topic.Setting up Vector Search: It was remarkably simple! We just specified these few things:Path to the embedding field within our documents.Embedding dimension size.Similarity metric (e.g., cosine similarity).That it\u2019s a vector index.Filtering Power (Optional): For even finer control, we could add paths to fields we wanted to filter our searches by (like the company\u2019s name).Whether you\u2019re building a complex web app or a quick Streamlit prototype, LlamaIndex ChatEngines have you covered. They effortlessly manage conversation history, let you perform lightning-fast vector searches, and unlock a whole suite of powerful tools.We built our ChatEngine directly from our trusty MongoDB index. This integration was surprisingly simple:def get_index():\n   logger.info(\"Connecting to index from MongoDB...\")\n   store = MongoDBAtlasVectorSearch(\n       db_name=os.environ[\"MONGODB_DATABASE\"],\n       collection_name=os.environ[\"MONGODB_VECTORS\"],\n       index_name=os.environ[\"MONGODB_VECTOR_INDEX\"],\n   )\n   index = VectorStoreIndex.from_vector_store(store)\n   logger.info(\"Finished connecting to index from MongoDB.\")\n   return index\n\nindex = get_index()\nindex.as_chat_engine(\n    llm = Fireworks(\n             api_key=env_vars['FIREWORKS_API_KEY'],\n             model=\"accounts/fireworks/models/mixtral-8x22b-instruct\" #Can be changed out for Llama3\n             )\n    chat_mode=\"best\", \n    context_prompt=(\n           \"\"\" You are a software developer bot that is an expert at reading over documentation to answer questions.\n           Use the relevant documents for context:\n           {context_str}\n           \\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\n           \"\"\"\n           ),\n    verbose=True\n    )create-llama: from idea to app in record timeWe were seriously impressed by Create-Llama. Usually, building a full-stack app takes time, but Create-Llama had us up and running in under 15 minutes! All we did was point it towards our vector database and give a few basic details. Honestly, it made development a joy! This blog post goes into more detail about how to use create-llama.The create-llama setup screenThe create-llama app, customized and ready to goDeployment: Render and VercelTo make LlamaWorksDB production-ready and easily accessible, we turned to Render and Vercel. Render was a perfect fit for our Python FastAPI backend, as it focuses on ease of deployment and scalability. Vercel seamlessly handled our Next.js frontend\u200a\u2014\u200awe loved its developer-centric approach and the effortless build process. Both platforms made deployment a breeze, letting us focus on coding rather than complex infrastructure setup.Future directionsOur hackathon success is just the beginning. We envision LlamaWorksDB evolving into a powerhouse for developers seeking answers within technical documentation. Here\u2019s how we see it growing:Enhanced Retrieval: We\u2019re excited to experiment with LlamaIndex\u2019s powerful capabilities like MultiVectorSearch to further refine our results. Integrating different LLMs will open up new possibilities for how LlamaWorksDB understands and interacts with technical content.A Focus on Documentation: We want to double down on making LlamaWorksDB the ultimate tool for navigating documentation. This means exploring specialized techniques and tools designed specifically for understanding complex technical information.LlamaWorksDB is an open-source project in Beta, and we believe in the power of collaboration! If you\u2019re passionate about AI-powered documentation tools, we invite you to:Try it out: Explore our GitHub repo and give LlamaWorksDB a spin!Contribute: Help us build new features, test integrations, and refine our search capabilities.Share your feedback: Let us know how we can make LlamaWorksDB even better.Together, let\u2019s revolutionize the way developers interact with documentation!\ud83d\udd17 Explore our project and join the innovation: https://github.com/clab2024/clab/ https://clab-ui.vercel.app/ (front-end) (leverages free credits responds late) https://clab.onrender.com/docs (back-end)Meet Team CLAB! \ud83c\udf89Chris Wood: Up-and-coming tech whiz, ready to graduate with valuable insights from his internship at Tutello.Leo Walker: Data Scientist with the discipline and precision of a Military Veteran.Andrew Townsend: A Computer Science graduate from SJSU, bringing fresh academic perspectives.Barath Subramaniam: The strategic mind behind Product Security AI and Data engineering at Adobe. Twitter: @baraths84Team CLAB (plus Laurie)",
            "related_posts": [
                "Streamlining knowledge work with LlamaIndex, Fireworks and MongoDB",
                "LlamaIndex Newsletter 2024-03-19",
                "Launching the first GenAI-native document parsing platform",
                "Special Feature: Berkeley Hackathon Projects (LlamaIndex Prize Winners)"
            ],
            "link": "https://www.llamaindex.ai/blog//streamlining-knowledge-work-with-llamaindex-fireworks-and-mongodb"
        },
        {
            "date": "Jun 26, 2024",
            "author": "LlamaIndex",
            "title": "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
            "tags": [
                "agents"
            ],
            "content": "We're excited to announce the alpha release of llama-agents, a new open-source framework designed to simplify the process of building, iterating, and deploying multi-agent AI systems and turn your agents into production microservices. Whether you're working on complex question-answering systems, collaborative AI assistants, or distributed AI workflows, llama-agents provides the tools and structure you need to bring your ideas to life.Key Features of llama-agentsDistributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an \u201cagentic orchestrator\u201d that decides which agents are relevant to the task.Ease of deployment: launch, scale and monitor each agent and your control plane independently.Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent serviceLet's dive into how you can start using llama-agents to build your own multi-agent systems.Getting Started with llama-agentsFirst, install the framework using pip:pip install llama-agents llama-index-agent-openaiBasic System SetupHere's a simple example of how to set up a basic multi-agent system using llama-agents. First we\u2019ll bring in our dependencies and set up our control plane, which contains our LLM-powered orchestratorimport dotenv\ndotenv.load_dotenv() # our .env file defines OPENAI_API_KEY\nfrom llama_agents import (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    AgentOrchestrator,\n)\nfrom llama_index.core.agent import FunctionCallingAgentWorker\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nimport logging\n\n# turn on logging so we can see the system working\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n\n# Set up the message queue and control plane\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)Next we create our tools using LlamaIndex\u2019s existing abstractions, provide those tools to an agent, and turn that agent into an independent microservice:# create a tool\ndef get_the_secret_fact() -> str:\n    \"\"\"Returns the secret fact.\"\"\"\n    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n# Define an agent\nworker = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nagent = worker.as_agent()\n\n# Create an agent service\nagent_service = AgentService(\n    agent=agent,\n    message_queue=message_queue,\n    description=\"General purpose assistant\",\n    service_name=\"assistant\",\n)Finally we launch the service and the control plane. Note that here we\u2019re using a helper function to run a single query through the system and then exit; next we\u2019ll show how to deploy this to production.# Set up the launcher for local testing\nfrom llama_agents import LocalLauncher\n\nlauncher = LocalLauncher(\n    [agent_service],\n    control_plane,\n    message_queue,\n)\n\n# Run a single query through the system\nresult = launcher.launch_single(\"What's the secret fact?\")\nprint(result)Deploying Your Multi-Agent SystemOnce you've tested your system locally, you can deploy it as a set of services for real production use. Here's how you might set that up. This is similar to the previous example, but we\u2019ve added a second agent service and we\u2019re using a different launcher. Let\u2019s bring in our dependencies and set up our control plane again:import dotenv\ndotenv.load_dotenv()\nfrom llama_agents import (\n    AgentService,\n    AgentOrchestrator,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n)\n\nfrom llama_index.core.agent import FunctionCallingAgentWorker\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.llms.openai import OpenAI\nimport logging\n\n# change logging level to enable or disable more verbose logging\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n\n# create our multi-agent framework components\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=OpenAI()),\n)Then as before we create a tool and an agent, though this time we\u2019ll add a second agent:# create a tool\ndef get_the_secret_fact() -> str:\n    \"\"\"Returns the secret fact.\"\"\"\n    return \"The secret fact is: A baby llama is called a 'Cria'.\"\n\ntool = FunctionTool.from_defaults(fn=get_the_secret_fact)\n\n# create our agents\nworker1 = FunctionCallingAgentWorker.from_tools([tool], llm=OpenAI())\nworker2 = FunctionCallingAgentWorker.from_tools([], llm=OpenAI())\nagent1 = worker1.as_agent()\nagent2 = worker2.as_agent()We turn those agents into services:agent_server_1 = AgentService(\n    agent=agent1,\n    message_queue=message_queue,\n    description=\"Useful for getting the secret fact.\",\n    service_name=\"secret_fact_agent\",\n    host=\"localhost\",\n    port=8003\n)\nagent_server_2 = AgentService(\n    agent=agent2,\n    message_queue=message_queue,\n    description=\"Useful for getting random dumb facts.\",\n    service_name=\"dumb_fact_agent\",\n    host=\"localhost\",\n    port=8004\n)And finally we launch each service as an independent agent. Here we\u2019re doing them all from a single script, but each of these could be a totally separate service, launched and scaled independently:from llama_agents import ServerLauncher, CallableMessageConsumer\n\n# Additional human consumer\ndef handle_result(message) -> None:\n    print(f\"Got result:\", message.data)\n\n\n# the final result is published to a \"human\" consumer\n# so we define one to handle it!\nhuman_consumer = CallableMessageConsumer(\n    handler=handle_result, message_type=\"human\"\n)\n\n# Define Launcher\nlauncher = ServerLauncher(\n    [agent_server_1, agent_server_2],\n    control_plane,\n    message_queue,\n    additional_consumers=[human_consumer]\n)\n\nlauncher.launch_servers()Real-time monitoringOne of the coolest debugging features of our multi-agent system is our agent monitor, which is built right in. You launch it like this:llama-agents monitor --control-plane-url http://127.0.0.1:8000Once launched, you get an intuitive, point-and-click terminal application. You can see both of the agents running, and at the bottom you can inject a task like the query \u201cWhat is the secret fact?\u201d You\u2019ll get a job ID which you can then click on to see your results:Building a Query Rewriting RAG SystemLet's look at a more complex example: a Query Rewriting RAG system. This system will rewrite user queries to improve retrieval, then use the rewritten query to perform RAG over a document.This example demonstrates how to create a more sophisticated system that combines query rewriting with RAG to improve question-answering capabilities. See this notebook for a fuller explanation of what\u2019s going on.import dotenv\ndotenv.load_dotenv() # our .env defines OPENAI_API_KEY\nfrom llama_index.core import VectorStoreIndex, Document\nfrom llama_index.core.agent import FnAgentWorker\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.query_pipeline import QueryPipeline\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_agents import (\n    AgentService,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    PipelineOrchestrator,\n    ServiceComponent,\n)\nfrom llama_agents.launchers import LocalLauncher\nfrom llama_index.llms.openai import OpenAI\nimport logging\n\n# change logging level to enable or disable more verbose logging\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n\n# Load and index your document\ndocs = [Document(text=\"The rabbit is a small mammal with long ears and a fluffy tail. His name is Peter.\")]\nindex = VectorStoreIndex.from_documents(docs)\n\n# Define a query rewrite agent\nHYDE_PROMPT_STR = (\n    \"Please rewrite the following query to include more detail:\\n{query_str}\\n\"\n)\nHYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\n\ndef run_hyde_fn(state):\n    prompt_tmpl, llm, input_str = (\n        state[\"prompt_tmpl\"],\n        state[\"llm\"],\n        state[\"__task__\"].input,\n    )\n    qp = QueryPipeline(chain=[prompt_tmpl, llm])\n    output = qp.run(query_str=input_str)\n    state[\"__output__\"] = str(output)\n    return state, True\n\nhyde_agent = FnAgentWorker(\n    fn=run_hyde_fn,\n    initial_state={\"prompt_tmpl\": HYDE_PROMPT_TMPL, \"llm\": OpenAI()}\n).as_agent()\n\n# Define a RAG agent\ndef run_rag_fn(state):\n    retriever, llm, input_str = (\n        state[\"retriever\"],\n        state[\"llm\"],\n        state[\"__task__\"].input,\n    )\n    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n    response = query_engine.query(input_str)\n    state[\"__output__\"] = str(response)\n    return state, True\n\nrag_agent = FnAgentWorker(\n    fn=run_rag_fn,\n    initial_state={\"retriever\": index.as_retriever(), \"llm\": OpenAI()}\n).as_agent()\n\n# Set up the multi-agent system\nmessage_queue = SimpleMessageQueue()\n\nquery_rewrite_service = AgentService(\n    agent=hyde_agent,\n    message_queue=message_queue,\n    description=\"Query rewriting service\",\n    service_name=\"query_rewrite\",\n)\n\nrag_service = AgentService(\n    agent=rag_agent,\n    message_queue=message_queue,\n    description=\"RAG service\",\n    service_name=\"rag\",\n)\n\n# Create the pipeline\npipeline = QueryPipeline(chain=[\n    ServiceComponent.from_service_definition(query_rewrite_service),\n    ServiceComponent.from_service_definition(rag_service),\n])\norchestrator = PipelineOrchestrator(pipeline)\n\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=orchestrator,\n)\n\n# Set up the launcher\nlauncher = LocalLauncher(\n    [query_rewrite_service, rag_service],\n    control_plane,\n    message_queue,\n)\n\n# Run a query\nresult = launcher.launch_single(\"Tell me about rabbits\")\nprint(result)Public roadmapThis is an alpha release, meaning that we\u2019d love your feedback on features to better help you build multi-agent systems in production! We\u2019ve created a public roadmap showing where we plan to go from here. We\u2019re actively seeking public feedback on what works for you and what doesn\u2019t.Dive in!llama-agents provides a powerful, flexible framework for building complex multi-agent AI systems. Whether you're prototyping a new idea or scaling to production, llama-agents offers the tools you need to bring your AI vision to life. Check out the repo to learn more, especially our library of examples.We're excited to see what the community builds with llama-agents. Happy coding!",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-llama-agents-a-powerful-framework-for-building-production-multi-agent-ai-systems"
        },
        {
            "date": "Jul 25, 2023",
            "author": "Simon Suo",
            "title": "Data Agents + Zapier NLA",
            "tags": [
                "agents",
                "llamaindex",
                "artificial-intelligence",
                "api",
                "zapier"
            ],
            "content": "Joint blog by LlamaIndex team & Zapier NLA teamWouldn\u2019t it be great to have a personal assistant that can access your data and perform tasks for you?Introducing LlamaIndex data agents, now more powerful with Zapier NLA. Within 5 lines of code, you can access the 5,000+ third party apps and over 30,000 actions on Zapier.from llama_hub.tools.zapier.base import ZapierToolSpec\nfrom llama_index.agent import OpenAIAgent\n\nzapier_spec = ZapierToolSpec(api_key=\"sk-ak-your-key\")\nagent = OpenAIAgent.from_tools(zapier_spec.to_tool_list(), verbose=True)\n\nagent.chat('Can you summarize the unread emails and send it to me on Slack?')",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "LlamaIndex Newsletter 2024\u201302\u201327"
            ],
            "link": "https://www.llamaindex.ai/blog//data-agents-zapier-nla-67146395ce1"
        },
        {
            "date": "Jan 26, 2024",
            "author": "LlamaIndex",
            "title": "Introducing the LlamaIndex retrieval-augmented generation command-line tool",
            "tags": [
                "llm",
                "cli",
                "llamaindex",
                "ai",
                "chromadb"
            ],
            "content": "Want to try out retrieval-augmented generation (RAG) without writing a line of code? We got you covered! Introducing the new llamaindex-cli tool, installed when you pip install llama-index ! It uses Chroma under the hood, so you\u2019ll need to pip install chromadb as well.How to use itSet the OPENAI_API_KEY environment variable: By default, this tool uses OpenAI\u2019s API. As such, you\u2019ll need to ensure the OpenAI API Key is set under the OPENAI_API_KEY environment variable whenever you use the tool.$ export OPENAI_API_KEY=&lt;api_key&gt;2. Ingest some files: Now, you need to point the tool at some local files that it can ingest into the local vector database. For this example, we\u2019ll ingest the LlamaIndex README.md file:$ llamaindex-cli rag --files \"./README.md\"You can only specify a file glob pattern such as$ llamaindex-cli rag --files \"./docs/**/*.rst\"3. Ask a Question: You can now start asking questions about any of the documents you\u2019d ingested in the prior step:$ llamaindex-cli rag --question \"What is LlamaIndex?\" \nLlamaIndex is a data framework that helps in ingesting, structuring, and accessing private or domain-specific data for LLM-based applications. It provides tools such as data connectors to ingest data from various sources, data indexes to structure the data, and engines for natural language access to the data. LlamaIndex follows a Retrieval-Augmented Generation (RAG) approach, where it retrieves information from data sources, adds it to the question as context, and then asks the LLM to generate an answer based on the enriched prompt. This approach overcomes the limitations of fine-tuning LLMs and provides a more cost-effective, up-to-date, and trustworthy solution for data augmentation. LlamaIndex is designed for both beginner and advanced users, with a high-level API for easy usage and lower-level APIs for customization and extension.4. Open a Chat REPL: You can even open a chat interface within your terminal! Just run llamaindex-cli rag --chat and start asking questions about the files you\u2019ve ingested.Customize it to your heart\u2019s content!You can customize llamaindex-cli to use any LLM model, even local models like Mixtral 8x7b through Ollama, and you can build more advanced query and retrieval techniques. Check the documentation for details on how to get started.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-the-llamaindex-retrieval-augmented-generation-command-line-tool-a973fa519a41"
        },
        {
            "date": "Jan 9, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201301\u201309",
            "tags": [
                "llamaindex",
                "rag",
                "ai",
                "newsletter",
                "llm"
            ],
            "content": "Hola, LlamaIndex Lovers \ud83e\udd99,Welcome to another thrilling week at LlamaIndex, filled with vibrant community contributions and enriching educational content. Immerse yourself in our engaging tutorials, guides, community demos, and webinars, all crafted to amplify your LlamaIndex experience. Before we jump into our latest updates, we\u2019re thrilled to share two major announcements:\ud83e\uddd1\u200d\ud83c\udfeb Join Our LlamaIndex Community Office Hours: Struggling with complex LLM/RAG queries or have feedback that our documentation doesn\u2019t cover? Register for our community office hours for a chance to have an enlightening conversation and get your questions answered!\ud83d\uddfa\ufe0f Explore Our Open-Source Roadmap for 2024: We\u2019re excited to unveil our ambitious roadmap for the LlamaIndex ecosystem. Over the next 3\u20136 months, we aim to enhance LlamaIndex\u2019s production readiness, accessibility, and its advanced features, including RAG, agents, and more. This living document is available on our GitHub discussions page \u2014 a must-visit to be part of our exciting journey!Additionally, if you\u2019ve been working on an interesting project, written an insightful article, or created a captivating video, we\u2019d love to hear about it! Please share your work with us at news@llamaindex.ai. And remember to subscribe to our newsletter through our website to get all these exciting updates straight to your inbox\ud83e\udd29 The highlights:Query Pipelines: Introducing a new declarative API for effortless orchestration of simple to complex RAG query workflows. Docs, Blogpost, Tweet.ETL Pipeline Launch: New repository for setting up production ETL pipelines in RAG/LLM apps, boasting a 4x speed boost and integrating Hugging Face, RabbitMQ, and AWS EKS. Github Repo, Blogpost, Tweet.Multimodal ReAct Agent: Launch of an agent capable of processing text and images, enhancing RAG pipeline and web search functionalities using GPT-4V. Notebook, Tweet.RAGatouille LlamaPack: Introduction of an easy-to-use pack for ColBERT retrieval, enabling one-line code integration in LlamaIndex RAG pipelines. Docs, Tweet.Advanced RAG Cheat Sheet: A comprehensive cheat sheet with techniques for RAG enhancement, perfect for both new and experienced LLM users.\u2728 Feature Releases and Enhancements:We have introduced Query Pipelines, a declarative API designed to simplify the creation and customization of advanced RAG workflows. This tool enables the orchestration of query workflows, ranging from basic sequential chains to complex DAGs, tailored to specific use cases. Docs, Blogpost, Tweet.We have launched a repository for easily setting up a production ETL pipeline for RAG/LLM apps, offering a 4x speed increase over laptop-based operations. This solution integrates Hugging Face, RabbitMQ, Llama Index, and AWS EKS, providing fast document indexing and efficient data handling, complete with an AWS Lambda API endpoint. Ideal for RAG apps transitioning to production, especially on AWS. Github Repo, Blogpost, Tweet.We have launched the Multimodal ReAct Agent, combining GPT-4V with the ability to process both text and images. This agent can perform tasks like querying a RAG pipeline or conducting web searches based on visual and textual inputs. Notebook, Tweet.RAGatouille LlamaPack: RAGatouille simplifies the use of ColBERT, a more advanced retrieval model compared to dense embedding-based retrieval techniques. This pack allows you to build an end-to-end LlamaIndex RAG pipeline with just one line of code by ingesting documents using any of our 150+ data loaders, combined with your preferred LLM for response synthesis. Docs, Tweet.We have integrated with Pathway\u2019s open data processing framework which enables us to handle dynamic data sources in production, automatically updating indexes based on real-time changes, ensuring up-to-date and accurate query responses. Docs, Tweet.Ian McCrystal has added the StripeDocsLoader to LlamaHub, enabling a quick setup of RAG over Stripe\u2019s documentation using Llama Index. Docs.Jeremy Dyer has integrated NVIDIA\u2019s Triton Inference Server which allows you to run optimized inference on any AI framework. It supports the TensorRT-LLM backend, enhancing LLM performance on Nvidia GPUs. Notebook, Tweet.\ud83d\udc40 Community Demos:Context-Augmented Agent for Food Delivery: A full-stack application guide by lucastonon for creating an RAG agent. This tool performs in-browser tasks like opening restaurant pages and adding food to carts, purely via voice commands, integrating with Llama Index, Pinecone, OpenAI\u2019s Whisper, LLMs, Function Calling, vue.js, and FastAPI. Github Repo, Tweet.GRDN.AI: A fascinating side project from Danielle Heymann, using a genetic algorithm and LLM to optimize plant placement based on compatibility. This project harnesses local models from HuggingFace, accessed through LlamaIndex for the LLM part, combining traditional mathematical strategies with LLMs. Blogpost, Tweet.Build an AI Shopping Assistant with RAG and Agents: This assistant can analyze a picture of an item and suggest weather-appropriate accessories. The work by D. Kiedanski and Lucas Micol from Tryolabs explains how to transform APIs into problem-solving tools for a LlamaIndex agent.\ud83d\uddfa\ufe0f Guides:Guide to Advanced RAG: Our comprehensive cheat sheet offers insights into improving RAG with techniques like optimized retrieval, effective document use in generation, and interleaving generation with retrieval. Ideal for both new and seasoned LLM users, it\u2019s a must-have resource, complete with LlamaIndex links.Guide to building advanced RAG CHATBOT with NVIDIA\u2019S TensorRT-LLM: This chatbot is designed to maintain contiguous document or code blocks, avoiding awkward chunking. It features a stack combining Llama Index\u2019s auto-merging retriever with NVIDIA\u2019s TensorRT-LLM and a custom postprocessor, optimized for RAG using open-source models.\u270d\ufe0f Tutorials:BentoML tutorial on Building An Intelligent Query-Response System with LlamaIndex and OpenLLM.Akash Mathur tutorial on Advanced RAG: Optimizing Retrieval with Additional Context & MetaData using LlamaIndex.\ud83c\udfa5 Webinars:Weights & Biases podcast with Jerry Liu on Revolutionizing AI Data Management.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-01-09-6209000da2e6"
        },
        {
            "date": "Jul 10, 2024",
            "author": "LlamaIndex",
            "title": "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex",
            "tags": [
                "case-study"
            ],
            "content": "What is Lyzr?Lyzr is a full-stack agent framework that specializes in building fully autonomous AI agents for enterprises. Their focus is on achieving Organizational General Intelligence (OGI) by harnessing agent data. Lyzr offers pre-built agents like Jazon, an AI sales development representative, and Skott, an AI content marketing agent, as well as a no-code builder for custom agent creation. Their platform enables organizations to build, deploy, and manage AI agents that can handle complex tasks and workflows autonomously.How does LlamaIndex help?LlamaIndex plays a crucial role in Lyzr's technology stack:Context augmentation: LlamaIndex components supply essential context to Lyzr's agents, creating Retrieval-Augmented Generation (RAG) systems that enable them to perform focused and effective work.Custom data access: LlamaIndex\u2019s data connectors are the preferred way for Lyzr agents to access customer-specific information. LlamaIndex\u2019s huge library of connectors means they can connect no matter where the customer stores their data.Flexible retrieval: Lyzr uses LlamaIndex's customizable retrieval methods to optimize performance for different use cases. Lyzr\u2019s AutoRAG system determines the optimal retrieval model, chunk size, and other parameters based on input data and use case and passes those to LlamaIndex.What have the results been like?The integration of LlamaIndex into Lyzr's framework has contributed to significant growth and improved performance:1. Rapid revenue growth: Lyzr's annual recurring revenue jumped from around $100,000 to about $1.5 million in less than 60 days.2. Enhanced agent accuracy: LlamaIndex's advanced capabilities and accurate RAG have led to highly accurate agents with very low error rates, providing a great alternative to OpenAI\u2019s Assistant API.3. Scalability: The flexibility provided by LlamaIndex has allowed Lyzr to sustain its growth and expand its agent offerings.What do customers think?Customer reception of Lyzr's LlamaIndex-powered agents has been overwhelmingly positive:1. High adoption rate: 75% of Lyzr's customers use two or more AI agents including custom workflow agents, indicating strong adoption of Lyzr Agent Framework.2. Customer Persona: Lyzr\u2019s \u2018fully autonomous\u2019 AI agents seem to have captured customer\u2019s imagination with SaaS CTOs being the primary adopter of Lyzr AI Agents to automate their backend workflows in a more reliable, secure and predictable manner.3. Positive testimonials: Customers like SurePeople love Lyzr:\"SurePeople is delighted to announce our partnership with Lyzr.ai, a key player in fortifying the scalability, security, and future-readiness of our AI infrastructure. Thanks to their versatile Agents, we're empowered to operate at the forefront of innovation, underpinned by a robust framework that bolsters our AI applications. In an ever-evolving landscape of artificial intelligence, Lyzr.ai's Agents ensure we remain at the cutting edge. Additionally, our collaboration has been enriched by their exceptionally skilled and cooperative team.\" \u2013 Niko Drakoulis, CEO of SurePeopleWhat's next for Lyzr?Lyzr has ambitious plans for the future, building on their success with LlamaIndex, including new agents such as Kathy, an AI competitor analyst, and Diane, an AI HR agent, with several others in the pipeline. They\u2019re also developing a framework called Lyzr AgentMesh to enable interaction between these different AI agents, creating a cohesive AI-driven workforce.\u201cWe are extremely thankful to Jerry and team for bringing LlamaIndex to the AI community. You guys have saved countless hours of tackling data retrieval challenges for us and many other builders in this space.\u201d - Siva Surendira, Founder, Lyzr AI.By continuing to use LlamaIndex as their RAG partner, Lyzr is well-positioned to expand its offerings and further establish itself as a leader in autonomous AI agent technology for enterprises.",
            "related_posts": [
                "Case Study: How Scaleport.ai Accelerated Development and Improved Sales with LlamaCloud",
                "Case study: Lyzr: Taking autonomous AI agents to $1M+ ARR with LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//case-study-lyzr-taking-autonomous-ai-agents-to-usd1m-arr-with-llamaindex"
        },
        {
            "date": "Aug 29, 2023",
            "author": "Jerry Liu",
            "title": "Introducing Airbyte sources within LlamaIndex",
            "tags": [
                "airbyte",
                "llamaindex",
                "ai",
                "llm",
                "etl"
            ],
            "content": "Authored by Joe Reuter, Software Engineer at Airbyte(cross-posted from the Airbyte blog; check it out here!)ContentIt\u2019s now possible to utilize the Airbyte sources for Gong, Hubspot, Salesforce, Shopify, Stripe, Typeform and Zendesk Support directly within your LlamaIndex-based application, implemented as data loaders.For example, to load the Stripe invoices for a user, you can use the AirbyteStripeLoader. Installing it is super simple, when you have LlamaIndex installed locally you only need to install the source you are interested in, and you are ready to go:pip install airbyte-source-stripe\npip install llama-hubAfter that, simply download the loader and pass in configuration and the stream you want to load:from llama_hub.airbyte_stripe.base import AirbyteStripeReader\n\nconfig = {\n  \"client_secret\": \"&lt;secret key&gt;\",\n  \"account_id\": \"&lt;account id&gt;\",\n  \"start_date\": \"&lt;date from which to start retrieving records from in ISO format, e.g. 2020\u201310\u201320T00:00:00Z&gt;\"\n}\nreader = AirbyteStripeReader(config=config)\ndocuments = reader.load_data(stream_name=\"invoices\")Why does this matter?This is the beginning of making Airbyte\u2019s 300+ sources available as data loaders in LlamaHub.Airbyte can move data from just about any source to your warehouse or vector database to power your LLM use case (check out this tutorial for setting up such a data pipeline!). This is normally done by using Airbyte Cloud or a local Airbyte instance, setting up a connection, and running it on a schedule (or via API trigger) to make sure your data stays fresh.But if you are just getting started and are running everything locally, using a full Airbyte instance (including the UI, scheduling service, scale-out capabilities, etc..) may be overkill.With this release, it\u2019s easier than ever to run any Python-based source in LlamaIndex directly within your Python runtime \u2014 no need to spin up an Airbyte instance or make API calls to Airbyte Cloud.Moving between hosted and embedded AirbyteSince the same code is running under the hood, every Airbyte-built loader is compatible with the respective source in the Airbyte service. This means it\u2019s trivial to lift your embedded loading pipeline into your self-hosted Airbyte installation or your Airbyte Cloud instance. The schema of the loader configuration object and that of the output records is 100% compatible.Running syncs on hosted Airbyte means:UI to keep track of running pipelinesEvent notifications including alerting on failing syncs or running post-sync operationsEasily running pipelines on a scheduleScale-out capabilitiesAPI to power programmatic use casesOut-of-the-box state management of your connectionsSupportAnd moreRunning syncs with LlamaIndex loaders means:No overhead for running yet another serviceFull control over timing and pipeline executionCombining Airbyte loaders with indices and query enginesAs Airbyte loaders are behaving like regular loaders, they can easily be combined with all LlamaIndex utilities to build powerful LLM-based applications:relevant_keys = [\"customer_name\", \"total\", \"currency\"]\nreader = AirbyteStripeReader(\n    config=strip_config,\n    record_handler=lambda record, id: Document(\n        doc_id=id,\n        text=record.data[\"description\"] or \"\",\n        extra_info={\n            key: record.data[key] for key in relevant_keys if key in record.data\n        },\n    ),\n)\n\nindex = ListIndex.from_documents(reader.load_data(stream_name=\"invoices\"))\nquery_engine = index.as_query_engine()\nquestion = input(\"What do you want to know about your customers?\")\nprint(query_engine.query(question))Incremental loadsSince your python application is basically acting as the Airbyte platform, you have full control over how the \u201csync\u201d is executed. For example you can still benefit from incremental syncs if your stream supports it by accessing the \u201clast_state\u201d property of the loader. This allows you to load only documents that changed since the last time you loaded, allowing you to update an existing vector database effectively:import airbyte_cdk.models.airbyte_protocol import AirbyteMessage\nwith open('stripe_sync_checkpoint.json', 'w') as file:\n  file.write(reader.last_state.json())\n\n# later\nwith open('stripe_sync_checkpoint.json', 'r') as file:\n  current_state = AirbyteStateMessage.parse_raw(file.read())\nnew_docs = reader.load_data(stream_name=\"invoices\", state=current_state)Mapping Airbyte records to LlamaIndex documentsBy default, each record gets mapped to a Document as part of the loader, with all the various fields in the record becoming a part of the `extra_info` property of the Document (the `extra_info` represents structured metadata for each document) . The text portion of the document is set to the JSON representation of the record. By default, any metadata defined on the Document will be concatenated with the text in downstream modules, so all the fields in the record will be used for embedding and synthesis purposes within a LlamaIndex app. You can pass in a record handler to customize this behavior to build the text part of a record depending on the data:def handle_record(record, id):\n  return Document(doc_id=id, text=record.data[\"title\"], extra_info=record.data)\nreader = AirbyteGongReader(config=gong_config, record_handler=handle_record)Custom sourcesFor now, the following Airbyte sources are available as pip packages (with more to come):Gong pip install airbyte-source-gongHubspot pip install airbyte-source-hubspotSalesforce pip install airbyte-source-salesforceShopify pip install airbyte-source-shopifyStripe pip install airbyte-source-stripeTypeform pip install airbyte-source-typeformZendesk Support pip install airbyte-source-zendesk-supportHowever, if you have implemented your own custom Airbyte sources, it\u2019s also possible to integrate them by using the AirbyteCDKReader base class that works with the Source interface of the Airbyte CDK:from llama_index import download_loader\nfrom my_source.source import MyCustomSource # plug in your own source here\nAirbyteCDKReader = download_loader(AirbyteCDKReader)\nconfig = {\n  # your custom configuration\n}\nreader = AirbyteCDKReader(source_class=MyCustomSource, config=config)\ndocuments = reader.load_data(stream_name=\"my-stream\")You can also install sources from the main Airbyte repository by installing directly via git \u2014 for example, to fetch the Github source, simply runpip install \"source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github\"After that, the source is available to be plucked into the AirbyteCDKLoader:from source_github.source import SourceGithub\nissues_loader = AirbyteCDKReader(source_class=SourceGithub, config=config)\ndocuments = reader.load_data(stream_name=\"issues\")Check out the connector development documentation for how to get started writing your own sources \u2014 it\u2019s easy to get started with them and will allow you to move from local embedded loaders to using a hosted Airbyte instance seamlessly depending on your needs.Any questions? We would love to hear from youIf you are interested in leveraging Airbyte to ship data to your LLM-based applications, please take a moment to fill out our survey so we can make sure to prioritize the most important features.If you have questions or are interested in other existing sources being exposed as loaders this way, do not hesitate to reach out on our community slack channel or in the integrations channel on the LlamaIndex discord server.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-airbyte-sources-within-llamaindex-42209071722f"
        },
        {
            "date": "Jun 23, 2023",
            "author": "Jerry Liu",
            "title": "Llama Index & Prem AI Join Forces",
            "tags": [
                "large-language-models",
                "nlp",
                "ai",
                "machine-learning"
            ],
            "content": "Co-authors: Simone Giacomelli (co-founder at Prem), Jerry Liu (co-founder/CEO at LlamaIndex)We\u2019re pleased to share the successful integration of Prem App and Llama Index, a union that brings a new level of privacy to AI development. Prem\u2019s self-hosting AI models and Llama\u2019s versatile data framework enhances the ability to build AI applications in a customizable and flexible manner.Integration DetailsBy combining Prem\u2019s self-hosting AI models with Llama Index\u2019s data framework, developers can now connect custom data sources to large language models easily. This simplifies the process of data ingestion, indexing, and querying, streamlining the overall AI development cycle.Getting StartedTo leverage this integration, simply download the Prem App and connect your data sources through the Llama Index platform. This allows you to self-host your AI models with Prem App and utilize Llama Index\u2019s capabilities to manage your data efficiently. This integration, therefore, significantly boosts AI application development, giving developers greater control and flexibility over their projects.Getting StartedInstall PremYou can run Prem in two different ways:MacOS: go to https://premai.io and download Prem App.Server: run the installer script: wget -q <https://get.prem.ninja/install.sh> -O install.sh; sudo bash ./install.shRun the services in the GUIWhen the UI is up and running, you can see all the services available. With just one click you can download the service you are interested in. In the background, the docker image associated with the service will be downloaded based on your hardware requirements.While waiting for the download to be completed, read more about the service, in the detail view. Just click on the card and you will be redirected to the service page. Each service page is packaged with some general info as well as complete documentation giving more details into the model exposed. When the download has been completed, just click on Open and the service will start. You can interact with the service from the playground or from APIs.You can check the port on which the service is running from the service detail view.Start Building Your AppIn this quick tutorial will show you how to build a simple Talk to your Data use case using Prem landing page content.In order to achieve that we will need to run three services:Redis: we will use Redis as a vector store to store the embeddings.Vicuna 7B Q4: we will use Vicuna in order to generate a proper response for the user based on the most similar document we get using Redis similarity searchAll MiniLM L6 V2: we will use sentence transformers in order to generate the embeddings out of our documents.If all the services necessary are running, you will see a similar interface as the one beyond.You can now start integrating the services using Llama Index library. In the following code snippets, we will show you how you can build a simple talk to your data use case using Prem and Llama Index.Import all necessary dependencies and assign a random string to OPENAI_API_KEY environment variable.import os\n\nfrom llama_index.vector_stores import RedisVectorStore\nfrom llama_index.storage.storage_context import StorageContext\nfrom llama_index import ListIndex, LLMPredictor, Document\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom llama_index import LangchainEmbedding, ServiceContext\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"2. Load the Data / Create some Documents. In this example, I am using Prem landing page content creating manually some documents.doc1 = Document(text=\"Prem is an easy to use open source AI platform. With Prem you can quickly build privacy preserving AI applications.\")\ndoc2 = Document(text=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(text=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")3. Instantiate the LLMs connecting to the running services.# Instantiate a llm predictor using Langchain pointing to vicuna-7b-q4 service\nllm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_base=\"http://localhost:8111/api/v1\", max_tokens=128))\n\n# Instantiate the embeddings object using Langchain pointing to all-MiniLM-L6-v2 service\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/api/v1\")\nembed_model = LangchainEmbedding(embeddings)\n\n# define a service context using the embeddings and llm defined above.\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)4. Configure the Vector Store# instantiate the vectorstore connecting to Redis service\nvector_store = RedisVectorStore(\n    index_name=\"prem_landing\",\n    index_prefix=\"llama\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)5. Index the documentsindex = ListIndex.from_documents([doc1, doc2, doc3], storage_context=storage_context)6. Perform an example queryquery_engine = index.as_query_engine(\n    retriever_mode=\"embedding\", \n    verbose=True, \n    service_context=service_context\n)\nresponse = query_engine.query(\"What are Prem benefits?\")\nprint(response)The benefits of using Prem include: Effortless Integration, Ready for the Real World, Rapid Iterations, Instant Results, Privacy Above All, Comprehensive Documentation, Preserve Your Anonymity, and an intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.And Done \ud83c\udf89 You are now using Prem with Llama Index.More InformationCheck out our documentation at: https://github.com/premai-io/prem-appCheck out a simple talk to your data notebook with Llama Index: https://github.com/premAI-io/prem-daemon/blob/main/resources/notebooks/llama_index.ipynbCheckout our YouTube tutorialsGetting Started with Prem: https://www.youtube.com/watch?v=XixH46Ysl5ADeploy Prem in your Paperspace instance: https://www.youtube.com/watch?v=aW8t6wouwx0Join UsOur partnership is based on a shared understanding that the future of AI is open, composable, and privacy-centric.Join us on this journey!",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llama-index-prem-ai-join-forces-51702fecedec"
        },
        {
            "date": "Dec 18, 2023",
            "author": "Tomaz Bratanic",
            "title": "Multimodal RAG pipeline with LlamaIndex and Neo4j",
            "tags": [
                "neo4j",
                "llm",
                "llamaindex",
                "chatgpt",
                "graph-database"
            ],
            "content": "The field of AI and large language models is evolving rapidly. One year ago, nobody ever used an LLM to enhance their productivity. Today, most of us can\u2019t imagine working without or not offloading at least some minor tasks to LLMs. Due to much research and interest, LLMs are getting better and wiser every day. Not only that, but their comprehension is starting to span across multiple modalities. With the introduction of GPT-4-Vision and other LLMs that followed it, it seems that LLMs today can tackle and comprehend images very well. Here\u2019s one example of ChatGPT describing what\u2019s in the image.Using ChatGPT to describe images.As you can observe, ChatGPT is quite good at comprehending and describing images. We can use its ability to understand images in an RAG application, where instead of relying only on text to generate an accurate and up-to-date answer, we can now combine information from text and pictures to generate more accurate answers than ever before. Using LlamaIndex, implementing multimodal RAG pipelines is as easy as it gets. Inspired by their multimodal cookbook example, I decided to test if I could implement a multimodal RAG application with Neo4j as the database.To implement a multimodal RAG pipeline with LlamaIndex, you simply instantiate two vector stores, one for images and one for text, and then query both of them in order to retrieve relevant information to generate the final answer.Workflow diagram for the blog post. Image by author.Articles are first split into images and text. These elements are then converted into vector representations and indexed separately. For text we will use ada-002 text embedding model, while for images we will be using dual encoder model CLIP, which can embed both text and images in the same embedding space. When a question is posed by an end user, two vector similarity search are performed; one to find relevant images and the other for documents. The results are fed into a multimodal LLM, which generates an answer for the user, demonstrating an integrated approach to processing and utilizing mixed media for information retrieval and response generation.The code is available on GitHub.Data preprocessingWe will use my Medium articles from 2022 and 2023 as the grounding dataset for an RAG application. The articles contain vast information about Neo4j Graph Data Science library and combining Neo4j with LLM frameworks. When you download your own articles from Medium, you get them in an HTML format. Therefore, we need to employ a bit of coding to extract text and images separately.def process_html_file(file_path):\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        soup = BeautifulSoup(file, \"html.parser\")\n\n    # Find the required section\n    content_section = soup.find(\"section\", {\"data-field\": \"body\", \"class\": \"e-content\"})\n\n    if not content_section:\n        return \"Section not found.\"\n\n    sections = []\n    current_section = {\"header\": \"\", \"content\": \"\", \"source\": file_path.split(\"/\")[-1]}\n    images = []\n    header_found = False\n\n    for element in content_section.find_all(recursive=True):\n        if element.name in [\"h1\", \"h2\", \"h3\", \"h4\"]:\n            if header_found and (current_section[\"content\"].strip()):\n                sections.append(current_section)\n            current_section = {\n                \"header\": element.get_text(),\n                \"content\": \"\",\n                \"source\": file_path.split(\"/\")[-1],\n            }\n            header_found = True\n        elif header_found:\n            if element.name == \"pre\":\n                current_section[\"content\"] += f\"```{element.get_text().strip()}```\\n\"\n            elif element.name == \"img\":\n                img_src = element.get(\"src\")\n                img_caption = element.find_next(\"figcaption\")\n                caption_text = img_caption.get_text().strip() if img_caption else \"\"\n                images.append(ImageDocument(image_url=img_src))\n            elif element.name in [\"p\", \"span\", \"a\"]:\n                current_section[\"content\"] += element.get_text().strip() + \"\\n\"\n\n    if current_section[\"content\"].strip():\n        sections.append(current_section)\n\n    return images, sectionsI won\u2019t go into details for the parsing code, but we split the text based on headers h1\u2013h4 and extract image links. Then, we simply run all the articles through this function to extract all relevant information.all_documents = []\nall_images = []\n\n# Directory to search in (current working directory)\ndirectory = os.getcwd()\n\n# Walking through the directory\nfor root, dirs, files in os.walk(directory):\n    for file in files:\n        if file.endswith(\".html\"):\n            # Update the file path to be relative to the current directory\n            images, documents = process_html_file(os.path.join(root, file))\n            all_documents.extend(documents)\n            all_images.extend(images)\n\ntext_docs = [Document(text=el.pop(\"content\"), metadata=el) for el in all_documents]\nprint(f\"Text document count: {len(text_docs)}\") # Text document count: 252\nprint(f\"Image document count: {len(all_images)}\") # Image document count: 328We get a total of 252 text chunks and 328 images. It\u2019s a bit surprising that I created so many photos, but I know that some are only images of table results. We could use a vision model to filter out irrelevant photos, but I skipped this step here.Indexing data vectorsAs mentioned, we have to instantiate two vector stores, one for images and the other for text. The CLIP embedding model has a dimension of 512, while the ada-002 has 1536 dimension.text_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=\"text_collection\",\n    node_label=\"Chunk\",\n    embedding_dimension=1536\n)\nimage_store = Neo4jVectorStore(\n    url=NEO4J_URI,\n    username=NEO4J_USERNAME,\n    password=NEO4J_PASSWORD,\n    index_name=\"image_collection\",\n    node_label=\"Image\",\n    embedding_dimension=512\n\n)\nstorage_context = StorageContext.from_defaults(vector_store=text_store)Now that the vector stores have been initiated, we use the MultiModalVectorStoreIndex to index both modalities of information we have.# Takes 10 min without GPU / 1 min with GPU on Google collab\nindex = MultiModalVectorStoreIndex.from_documents(\n    text_docs + all_images, storage_context=storage_context, image_vector_store=image_store\n)Under the hood, MultiModalVectorStoreIndex uses text and image embedding models to calculate the embeddings and store and index the results in Neo4j. Only the URLs are stored for images, not actual base64 or other representations of images.Multimodal RAG pipelineThis piece of code is copied directly from the LlamaIndex multimodal cookbook. We begin by defining a multimodal LLM and the prompt template and then combine everything as a query engine.openai_mm_llm = OpenAIMultiModal(\n    model=\"gpt-4-vision-preview\", max_new_tokens=1500\n)\n\nqa_tmpl_str = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nqa_tmpl = PromptTemplate(qa_tmpl_str)\n\nquery_engine = index.as_query_engine(\n    multi_modal_llm=openai_mm_llm, text_qa_template=qa_tmpl\n)Now we can go ahead and test how well it performs.query_str = \"How do vector RAG application work?\"\nresponse = query_engine.query(query_str)\nprint(response)ResponseGenerated response by an LLM.We can also visualize which images the retrieval fetched and were used to help inform the final answer.Image input to LLM.The LLM got two identical images as input, which just shows that I reuse some of my diagrams. However, I am pleasantly surprised by CLIP embeddings as they were able to retrieve he most relevant image out of the collection. In a more production setting, you might want to clean and deduplicate images, but that is beyond the scope of this article.ConclusionLLMs are evolving faster than what we are historically used to and are spanning across multiple modalities. I firmly believe that by the end of the next year, LLMs will be soon able to comprehend videos, and be therefore able to pick up non-verbal cues while talking to you. On the other hand, we can use images as input to RAG pipeline and enhance the variety of information passed to an LLM, making responses better and more accurate. The multimodal RAG pipelines implementation with LlamaIndex and Neo4j is as easy as it gets.The code is available on GitHub.",
            "related_posts": [
                "Customizing property graph index in LlamaIndex",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//multimodal-rag-pipeline-with-llamaindex-and-neo4j-a2c542eb0206"
        },
        {
            "date": "Aug 1, 2023",
            "author": "Ravi Theja",
            "title": "LlamaIndex Update \u2014 08/01/2023",
            "tags": [
                "large-language-models",
                "llamaindex",
                "ai",
                "nlp",
                "chat"
            ],
            "content": "Greetings once again, LlamaIndex community!Welcome to the third installment of our LlamaIndex Update series. Your active participation continues to drive our open-source community forward. We appreciate every contribution whether you\u2019re an experienced LlamaIndex contributor or a newcomer!In our latest edition, we\u2019ve prepared an assortment of updates for you. From advancements in Data Agents and LlamaIndex TS, benchmarking, and a host of inspiring events, webinars, blog posts, and demos, we\u2019ve got plenty in store.Without more ado, let\u2019s dive into these updates.New Features:We heard you! LlamaIndex has completely revamped our documentation. The update includes new clearer documents on high-level concepts, detailed module guides, comprehensive tutorials, and an all-inclusive API reference. Docs, TweetLlamaIndex launched Data Agents, an innovative feature that combines AI agents with data. This launch introduces components like an agent reasoning loop and tool abstractions. Accompanied by an extensive upgrade to LlamaHub, the new feature offers more than 15 tool specs for easy integration. Data Agents enhance query capabilities and are designed to handle varied data applications. Docs, Tweet, Blog PostLlamaIndex launched LlamaIndex.TS, a lean Typescript package for building robust Retrieval Augmented Generation (RAG) systems. It simplifies tasks like document parsing and tackling context window limitations. LlamaIndex.TS is ideal for quickly building apps like using frameworks like Next.JS to chat over your data. Docs, Tweet, BlogpostLlamaIndex teams up with Zapier Natural Language API (NLA), reducing the cognitive load on the data agent when handling APIs with multiple parameters. Zapier NLA translates complex third-party APIs into simpler interfaces using a single natural language parameter: instruction. This helps the data agent concentrate on tool selection and action orchestration. Tweet, BlogpostLlamaIndex\u2019s ContextChatEngineaddresses the issue of conversational agents hallucinating information by ensuring retrieval of context with every user interaction. This feature, compatible with all ReAct and OpenAI Function agent types, prepends retrieved-context as a system message. Docs, TweetThis month marked the launch of two new exciting LLMs. First off was Anthropic Claude 2.0. We launched with day 1 support of the new model. Docs, Tweet. The other one was Llama2, and LlamaIndex now offers best-in-class integration with the Llama2 model on Replicate. Docs, TweetLlamaIndex is day one compatible with Chroma v0.4.0, enhancing support for in-memory, persisted, and self-hosted databases. This upgrade simplifies the use of Chroma within LlamaIndex, making database handling easier and more efficient. TweetLlamaIndex\u2019s newly launched Data Agents can automatically interact with any API defined via an OpenAPI spec. It handles indexing/loading of large data from API specs and facilitates easy integration of the OpenAPI Tool, enhancing the ability to call web services. Docs, TweetLlamaIndex now utilizes the rebel-large model for high-speed relation extraction. Combined with CUDA, you can generate knowledge graphs from your text data. TweetLlamaIndex introduced a code interpreter tool. This feature equips any LLM with the ability to analyze data and generate visualizations, expanding their capabilities similar to those of ChatGPT. TweetLlamaIndex now integrates with Eduardo Reis\u2019s Llama 2 functions API at llama-api.com. TweetLlamaIndex TS now supports integration with OpenAI Whisper. Docs, TweetLlamaIndex now seamlessly integrates with K\u00f9zudb, allowing users to directly store extracted knowledge graphs/triples for advanced processing, querying, and visualization. Docs, TweetLlamaIndex combines data agents with text-to-image models enhancing user prompts with relevant context from a knowledge base. This integration allows for more advanced multimodal reasoning by merging LLM RAG systems with text-to-image tools. Docs, TweetBenchmarking:LlamaIndex now supports BEIR, an Information Retrieval benchmark. Users can define custom retrievers within LlamaIndex, apply the vector index, or implement reranking steps, and then easily evaluate their methods using any dataset from BEIR. TweetLlamaIndex\u2019s Llama2 agents have shown promising performance in our agent task benchmark. Especially notable is their capability to appropriately use tools within a ReACT loop. However, the tasks\u2019 difficulty varies, with both 13B and 70B models notably refraining from dialing a phone number, underlining the AI\u2019s limitations. TweetLlamaIndex now has integration with the HotpotQA benchmark! This enables rigorous testing of LLM\u2019s multi-hop reasoning capabilities by providing the full context to the models, helping you evaluate their performance more accurately. Perfect for stress-testing LLMs like ChatGPT, Claude 2, PaLM, and more. Plus, explore how context reordering can simplify tasks for your LLMs. TweetLlamaIndex now supports over 20 vector databases, each with unique features and capabilities. To help understand their differences, we have compiled a comprehensive comparison table, guiding the choice of the optimal database for the use case. TweetTutorials:We were excited to see so many people making tutorials for LlamaIndex this month!Adam Hofmann\u2019s blog post on Building Better Tools for LLM Agents.Weaviate\u2019s tutorial on using the Llama2 model with LlamaIndex and Weaviet on external data.Erika\u2019s tutorial on VectorStore Index, List Index, and Tree Index.James Maslek\u2019s tutorial on Breaking Barriers with OpenBB and LlamaIndex: Simplifying data access to 100+ trusted sources.Ayush Thakur\u2019s tutorial on Building Advanced Query Engine and Evaluation with LlamaIndex and W&B.Trulens\u2019s tutorial on using LlamaIndex Yelp agent to answer queries using Yelp data, and evaluate it for definitiveness and accuracy using custom feedback functions, compare its performance against a standalone LLM.Airbyte\u2019s tutorial on Chat with your data warehouse without writing SQL.Anil Chandra Naidu\u2019s tutorial on Retrievers and QueryEngines.Wenqi Glantz\u2019s tutorial on Exploring Snowflake and Streamlit With LlamaIndex Text-to-SQL.And from the LlamaIndex team:Logan\u2019s tutorial on a comprehensive understanding of embedding models, their benchmarking, and their implementation in LlamaIndex, with a focus on OpenAI and Instructor embeddings, enabling semantic search through numerical text representations.Logan\u2019s tutorial on the evaluation of query engines using LlamaIndex, learn to handle uncontrolled outputs and runtime costs while measuring performance with GPT-4.Ravi Theja\u2019s tutorial on Key Components to build QA Systems.Webinars:Webinar with Didier Lopes, CEO/Co-Founder at OpenBB on LLMs for Investment Research.Webinar on Building & Evaluating an Advanced Query Engine Over Your Data with Weights and Biases.Webinar with Jason Liu on From Prompt to Schema Engineering with Pydantic.Events:LlamaIndex and Arize workshop on LLM Search & Retrieval Systems with Arize and LlamaIndex: Powering LLMs on Your Proprietary Data.LlamaIndex and TruLens workshop on building an LLM App.TPF (The Product Folks) workshop session on Building QA Systems With LlamaIndex by Ravi Theja.Ravi Theja talk at the Speciale VC GenAI meetup in Chennai on Beyond the Basics: Leveraging LlamaIndex from Concept to Production.Data Agents session at TPF X Nexus VC Buildathon by Ravi Theja.Demos:Tali.AI at the Augment hackathon dove into the future of support roles by developing an Autonomous Support Bot using LlamaIndex. TweetSuperAGI integrated with LlamaIndex which enables AI agents to process a wide variety of data from both structured and unstructured sources including Docx, PDF, CSV files, videos, and images. Tweet",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-update-08-01-2023-185514d9b897"
        },
        {
            "date": "Apr 2, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-04-02",
            "tags": [
                "llm"
            ],
            "content": "Greetings, LlamaIndex community! \ud83e\udd99Welcome to another exciting weekly update from LlamaGalaxy! We're thrilled to share a range of fantastic updates with you, including the introduction of RAFT LlamaPack, enhanced memory and cost efficiency in RAG with Cohere's embeddings, and much more.\ud83e\udd29\u00a0The highlights:DeepLearningAI Course: JavaScript RAG Web Apps with\u00a0LlamaIndex collaborative course with DeepLearningAI. Course, Tweet.RAFTDatasetPack LlamaPack: Introduced RAFTDatasetPack for dataset generation using RAFT - Retrieval Augmented Fine Tuning for training models to differentiate between relevant 'oracle' documents and 'distractor' documents. LlamaPack, Tweet.Memory Efficiency with Cohere Embeddings: Utilize Cohere's Int8 and binary embeddings for cost-effective and low-memory RAG operations. Notebook, Tweet.Python Docs Makeover: Revamped Python documentation with accessible example notebooks, advanced search, and comprehensive API details. API Ref, Tweet, Docs\u2728 Feature Releases and Enhancements:We introduced RAFT - Retrieval Augmented Fine Tuning, a method from Tianjun Zhang\u00a0and Shishir Patil\u00a0to enhance domain-specific RAG performance in LLMs. By training models to differentiate between relevant 'oracle' documents and 'distractor' documents, RAFT improves context understanding. Try it out with our new RAFTDatasetPack LlamaPack for dataset generation. LlamaPack, Tweet.We collaborated with DeepLearningAI for a course that goes beyond teaching RAG techniques; it guides you on integrating RAG into a full-stack application. Learn to construct a backend API, develop an interactive React component, and tackle the unique challenges of deploying RAG on a server rather than just in a notebook. Course, Tweet.We integrated with Cohere's Int8 and Binary Embeddings for a memory-efficient solution for your RAG pipeline. This addresses the high memory usage and costs associated with large dataset operations in RAG. Notebook, TweetWe launched revamped Python docs with top-level example notebooks, improved search with previews, and overhauled API documentation. API Ref, Tweet, Docs\ud83c\udfa5\u00a0Demos:RestAI, a project by Pedro Dias is a nifty platform that offers RAG, advanced text-to-SQL, and multimodal inference as a service with a nifty UI.Ragdoll and Ragdoll Studio by bennyschmidt: Create AI Personas for characters, web assistants, or game NPCs using LlamaIndex TS, local LLMs, and image generation with Ollama and StabilityAI.\ud83d\uddfa\ufe0f Guides:Guide to Designing RAG Systems by Micha\u0142 Oleszak for an in-depth look at crucial design decisions in building efficient RAG systems, spanning five key areas: Indexing, Storing, Retrieval, Synthesis, and Evaluation.\u270d\ufe0f Tutorials:Sujit Patil tutorial on combining semantic chunking with hierarchical clustering and indexing for RAG content enrichment.Florian June's tutorial on crafting a dynamic RAG system with integrated reflection, a guide to building Self-RAG from scratch.Laurie's video tutorial on using LlamaParse's LLM-powered parsing turns complex insurance policies into clear yes-or-no statements, improving LLM responses on coverage queries.Akriti\u2019s tutorial on Building Real-Time Financial News RAG Chatbot with Gemini, and Qdrant.Marco Bertelli's tutorial on deploying a RAG server for real-time use, and covering efficient embedding serving, concurrent request handling, and failure resilience.Sudarshan Koirala\u2019s tutorial on building advanced PDF RAG with LlamaParse and purely local models for embedding, LLMs, and reranking.\ud83c\udfa5\u00a0Webinars:Register for a webinar with Tianjun Zhang\u00a0and Shishir Patil\u00a0on how to do retrieval-augmented fine-tuning (RAFT).Webinar with Daniel on CodeGPT - a platform for AI Copilots that help your coding workflows, with components built on top of LlamaIndex components.Vectara\u2019s Panel Discussion on 'Why RAG will Never Die?\u2019.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-04-02"
        },
        {
            "date": "Nov 21, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201311\u201321",
            "tags": [
                "llm",
                "llamaindex",
                "rag",
                "newsletter",
                "ai"
            ],
            "content": "Hello Llama Fam \ud83e\udd99What an amazing week we\u2019ve had! We\u2019re excited to share that, according to the Retool State of AI 2023 survey, 1 in 12 respondents are now using LlamaIndex. We\u2019re grateful for all your support.If you have a fascinating project or video you\u2019d like to share, we\u2019d love to see it! Feel free to send it to us at news@llamaindex.ai. And remember to subscribe to our newsletter on our website to stay in the loop. We can\u2019t wait to connect with you there!\ud83e\udd29 First, the highlights:LlamaIndex 0.9 Release: we introduced LlamaIndex version 0.9 featuring streamlined data handling with a new IngestionPipeline, automated caching, improved text processing interfaces, tokenizer updates, PyPi packaging enhancements, consistent import paths, and a beta version of MultiModal RAG Modules. Blog post, Tweet.Multi-Modal Evaluation Tools: we launched multi-modal evaluation with the introduction of MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, plus a guide for their application in multi-modal settings. Blog post, Tweet.create-llama CLI Tool: we unveiled create-llama, a versatile CLI tool for building full-stack LLM apps with options like FastAPI, ExpressJS, and Next.js for backends and a Next.js frontend with Vercel AI SDK components. Blog post, Tweet.Cohere Reranker Fine-Tuning: we enhanced RAG pipeline retrieval performance with the fine-tuning of the Cohere reranker. Blog post, Tweet.Coming up this week: we have a YouTube live event in partnership with AI Makerspace exploring the potential of LlamaIndex to handle complex PDFs with tables, charts and more. Register for free!\u2728 Feature Releases and Enhancements:We introduced the LlamaIndex 0.9 version with updates on streamlined data handling with new IngestionPipeline, automated caching, improved interfaces for text processing, tokenizer updates, enhanced PyPi packaging, consistent import paths, and a beta of MultiModal RAG Modules for text and image integration. Blog post, Tweet.We introduced multi-modal evaluation which includes MultiModalRelevancyEvaluator and MultiModalFaithfulnessEvaluator, and a guide on using them in multi-modal applications. Blog post, Tweet.We introduced create-llama, a CLI tool for easily building full-stack LLM apps, offering choices like FastAPI, ExpressJS, and Next.js backends with Llama Index, and a Next.js frontend with Vercel AI SDK components, enabling extensive customization for AI engineers. Blog post, Tweet.We introduced fine-tuning of the cohere reranker to improve retrieval performance in the RAG pipeline. Blog post, Tweet.Integrations:We integrated with Chroma\u2019s multi-modal collections which allows for indexing both text and images in a single collection, enhancing RAG pipelines by combining text and image information for use with multi-modal models like GPT-4V, LLaVa, and Fuyu. Docs, Tweet.\ud83d\uddfa\ufe0f Guides:Guide on Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles.Guide on LlamaIndex by Nanonets covering over 12 key areas such as data management, indexing/storage, querying with top-k RAG, structured outputs, chat functionalities with memory, and agent development incorporating tool use.Guide on using Ingestion pipeline focusing on showcasing experiments on chunk overlaps and the use of metadata extractors, including title, summary, and other elements.Guide on using Perplexity API with LlamaIndex by Vishhvak.Guide on using Fleet Context to download the embeddings for LlamaIndex\u2019s documentation and build a hybrid dense/sparse vector retrieval engine on top of it.Guide on building a full-stack financial analysis bot using create-llama and Llama Index's RAG, capable of querying text and tables across SEC filings.\u270d\ufe0f Tutorials:Wenqi Glantz made a tutorial on LLaVA vs. GPT-4V Amidst Snow Geese Migration.Glenn Parham\u2019s cookbook on LlamaIndex, hosted in the Department of Defense\u2019s official repository, showcases methods for applying RAG on unclassified DoD policy documents.Sudarshan Koirala made a tutorial on Using Perplexity API with LlamaIndex.Ravi Theja analysis on GPT4-V Experiments with General, Specific questions and Chain Of Thought prompting(COT) techniques\ud83c\udfa5 Webinars:Check out our CEO \u2014 Jerry Liu\u2019s talk on Building Production-Ready RAG Applications at AI.engineer Summit.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-11-21-aa3a71e339f8"
        },
        {
            "date": "Mar 18, 2024",
            "author": "Langfuse",
            "title": "One-click Open Source RAG Observability with Langfuse",
            "tags": [
                "llm",
                "observability"
            ],
            "content": "This is a guest post from the team at LangfuseThere are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once.Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your production RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point?Thus, we started working on Langfuse.com (GitHub) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends\u2019 problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either self-host Langfuse or use the cloud instance maintained by us.We are thrilled to announce our new integration with LlamaIndex today. This feature was highly requested by our community and aligns with our project's focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase!The challengeWe love LlamaIndex, since the clean and standardized interface abstracts a lot of complexity away. Let\u2019s take this simple example of a VectorStoreIndex and a ChatEngine.from llama_index.core import SimpleDirectoryReader\nfrom llama_index.core import VectorStoreIndex\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nchat_engine = index.as_chat_engine()\n\nprint(chat_engine.chat(\"What problems can I solve with RAG?\"))\nprint(chat_engine.chat(\"How do I optimize my RAG application?\"))In just 3 lines we loaded our local documents, added them to an index and initialized a ChatEngine with memory. Subsequently we had a stateful conversation with the chat_engine.This is awesome to get started, but we quickly run into questions like:\u201cWhat context is actually retrieved from the index to answer the questions?\u201d\u201cHow is chat memory managed?\u201d\u201cWhich steps add the most latency to the overall execution? How to optimize it?\u201dOne-click OSS observability to the rescuePlayWe integrated Langfuse to be a one-click integration with LlamaIndex using the global callback manager.PreparationInstall the community package (pip install llama-index-callbacks-langfuse)Copy/paste the environment variables from the Langfuse project settings to your Python project: 'LANGFUSE_SECRET_KEY', 'LANGFUSE_PUBLIC_KEY' and 'LANGFUSE_HOST'Now, you only need to set the global langfuse handler:from llama_index.core import set_global_handler\n\nset_global_handler(\"langfuse\")And voil\u00e1, with just two lines of code you get detailed traces for all aspects of your RAG application in Langfuse. They automatically include latency and usage/cost breakdowns.Group multiple chat threads into a sessionWorking with lots of teams building GenAI/LLM/RAG applications, we\u2019ve continuously added more features that are useful to debug and improve these applications. One example is session tracking for conversational applications to see the traces in context of a full message thread.To activate it, just add an id that identifies the session as a trace param before calling the chat_engine.from llama_index.core import global_handler\n\nglobal_handler.set_trace_params(\n  session_id=\"your-session-id\"\n)\n\nchat_engine.chat(\"What did he do growing up?\")\nchat_engine.chat(\"What did he do at USC?\")\nchat_engine.chat(\"How old is he?\")Thereby you can see all these chat invocations grouped into a session view in Langfuse Tracing:Next to sessions, you can also track individual users or add tags and metadata to your Langfuse traces.Trace more complex applications and use other Langfuse features for prompt management and evaluationThis integration makes it easy to get started with Tracing. If your application ends up growing into using custom logic or other frameworks/packages, all Langfuse integrations are fully interoperable.We have also built additional features to version control and collaborate on prompts (langfuse prompt management), track experiments, and evaluate production traces. For RAG specifically, we collaborated with the RAGAS team and it\u2019s easy to run their popular eval suite on traces captured with Langfuse (see cookbook).Get startedThe easiest way to get started is to follow the cookbook and check out the docs.Feedback? Ping usWe\u2019d love to hear any feedback. Come join us on our community discord or add your thoughts to this GitHub thread.",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//one-click-open-source-rag-observability-with-langfuse"
        },
        {
            "date": "Apr 30, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-04-30",
            "tags": [
                "newsletter"
            ],
            "content": "Greetings, LlamaIndex fans! \ud83e\udd99It\u2019s delightful springtime weather out here in San Francisco and we hope you\u2019re having a good day! Check out this week\u2019s summary of news, guides and tutorials.\ud83e\udd29\u00a0The highlights:Day 0 support for Microsoft\u2019s Phi-3 Mini! Tweetcreate-llama now supports Llama 3 and Phi-3 and has lots of new features! TweetSimon was on a security podcast! Tweet\u2728 Feature Releases and Enhancements:Jina AI released powerful new open-source rerankers and we have day 0 support as usual! TweetPhi-3 mini was released by Microsoft, a powerful new small model, and we put it through its paces (spoiler: it\u2019s good!) and released day-0 support via Ollama! TweetOur create-llama application generator was updated with many features including being able to show the sources it retrieved from, as well as Llama3 and Phi-3 support. Build an app from scratch in 30 seconds! TweetLanguage Agent Tree Search (LATS) is a powerful new technique that iteratively plans out an array of potential futures, interleaving tool use and reflection to solve problems. We released a Llama Pack implementation. Tweet\ud83c\udfa5\u00a0Demos:Memary is a reference implementation of using long-term memory in knowledge graph form for building agents. TweetOur hackathon winners wrote a blog post about their winning project, a knowledge-retrieval bot trained on documentation, including how they built it. Tweet\ud83d\uddfa\ufe0f Guides:Co-founder Jerry shared his latest deck, a guide to building a context-augmented research assistant that enables multi-hop Q&A, reflection and more. Slides, tweetCorrective RAG or CRAG adds a retrieval evaluation module that determines whether the retrieved context is \u201ccorrect\u201d and improves retrieval. Check out this guide on how to build it step-by-step! TweetJerry also went in-depth on the ingredients necessary for building a complex agent. TweetMichael of KX Systems demonstrated making retrieval a multi-hop process for better results. TweetA reference architecture for advanced RAG with LlamaIndex and AWS Bedrock. Tweet\u270d\ufe0f Tutorials:Build a best-in-class RAG application using Qdrant as a vector store, Jina AI embeddings, and Mixtral 8x7b as the LLM. TweetLearn 3+ patterns for building LLM apps on AWS with LlamaIndex. TweetA 9-part series on taking RAG from prototype to production. Tweet\ud83c\udfa5\u00a0Webinars:KX Systems are hosting a webinar on May 1 about getting the most out of LlamaParse! TweetCo-founder Simon appeared on the MLSecOps podcast talking about security in LLM applications. Tweet\ud83d\udc6f\u200d\u2640\ufe0f\u00a0Community:We launched a LlamaIndex user group in Korea!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-12"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-04-30"
        },
        {
            "date": "Jun 22, 2023",
            "author": "Jerry Liu",
            "title": "LlamaIndex and Weaviate",
            "tags": [
                "nlp",
                "ai",
                "vector-database",
                "question-answering",
                "machine-learning"
            ],
            "content": "Co-authors:Jerry Liu (co-founder/CEO of LlamaIndex)Erika Cardenas (Developer Advocate, Weaviate)While large language models (LLMs) like GPT-4 have impressive capabilities in generation and reasoning, they have limitations in terms of their ability to access and retrieve specific facts, figures, or contextually relevant information. A popular solution to this problem is setting up a retrieval-augmented generation (RAG) system: combine the language model with an external storage provider, and create an overall software system that can orchestrate the interactions with and between these components in order to create a \u201cchat with your data\u201d experience.The combination of Weaviate and LlamaIndex provide the critical components needed to easily setup a powerful and reliable RAG stack, so that you can easily deliver powerful LLM-enabled experiences over your data, such as search engines, chatbots, and more. First, we can use Weaviate as the vector database that acts as the external storage provider. Next, we can use a powerful data framework such as LlamaIndex to help with data management and orchestration around Weaviate when building the LLM app.In this blog post, we walk through an overview of LlamaIndex and some of the core data management and query modules. We then go through an initial demo notebook.We\u2019re kicking off a new series to guide you on how to use LlamaIndex and Weaviate for your LLM applications.An Introduction to LlamaIndexLlamaIndex is a data framework for building LLM applications. It provides a comprehensive toolkit for ingestion, management, and querying of your external data so that you can use it with your LLM app.Data IngestionOn data ingestion, LlamaIndex offers connectors to 100+ data sources, ranging from different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack, Discord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data connectors are primarily hosted on [LlamaHub](https://llamahub.ai/). This makes it easy for users to integrate data from their existing files and applications.Data IndexingOnce the data is loaded, LlamaIndex offers the ability to index this data with a wide variety of data structures and storage integration options (including Weaviate). LlamaIndex supports indexing unstructured, semi-structured, and structured data. A standard way to index unstructured data is to split the source documents into text \u201cchunks\u201d, embed each chunk, and store each chunk/embedding in a vector database.Data QueryingOnce your data is ingested/stored, LlamaIndex provides the tools to define an advanced retrieval / query \u201cengine\u201d over your data. Our retriever constructs allow you to retrieve data from your knowledge base given an input prompt. A query engine construct allows you to define an interface that can take in an input prompt, and output a knowledge-augmented response \u2014 it can use retrieval and synthesis (LLM) modules under the hood.Some examples of query engine \u201ctasks\u201d are given below, in rough order from easy to advanced:Semantic Search: Retrieve the top-k most similar items from the knowledge corpus by embedding similarity to the query, and synthesize a response over these contexts.Structured Analytics: Convert natural language to a SQL query that can be executedQuery Decomposition over Documents: Break down a query into sub-questions, each over a subset of underlying documents. Each sub-question can be executed against its own query engine.Demo Notebook WalkthroughLet\u2019s walk through a simple example of how LlamaIndex can be used with Weaviate to build a simple Question-Answering (QA) system over the Weaviate blogs!The full code can be found in the Weaviate recipes repo.The first step is to setup your Weaviate client. In this example, we connect to a local Weaviate instance through port http://localhost:8080:import weaviate\n# connect to your weaviate instance\nclient = weaviate.Client(\"http://localhost:8080\")The next step is to ingest the Weaviate documentation and parse the documents into chunks. You can choose to use one of our many web page readers to scrape any website yourself \u2014 but luckily, the downloaded files are already readily available in the recipes repo.from llama_index.node_parser import SimpleNodeParser\n# load the blogs in using the reader\nblogs = SimpleDirectoryReader('./data').load_data()\n# chunk up the blog posts into nodes\nparser = SimpleNodeParser()\nnodes = parser.get_nodes_from_documents(blogs)Here, we use the SimpleDirectoryReader to load in all documents from a given directory. We then use our SimpleNodeParser to chunk up the source documents into Node objects (text chunks).The next step is to 1) define a WeaviateVectorStore, and 2) build a vector index over this vector store using LlamaIndex.# construct vector store\nvector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n# setting up the storage for the embeddings\nstorage_context = StorageContext.from_defaults(vector_store = vector_store)\n# set up the index\nindex = VectorStoreIndex(nodes, storage_context = storage_context)Our WeaviateVectorStore abstraction creates a central interface between our data abstractions and the Weaviate service. Note that the VectorStoreIndex is initialized from both the nodes and the storage context object containing the Weaviate vector store. During the initialization phase, the nodes are loaded into the vector store.Finally, we can define a query engine on top of our index. This query engine will perform semantic search and response synthesis, and output an answer.\u200b\u200bquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is the intersection between LLMs and search?\")\nprint(response)You should get an answer like the following:The intersection between LLMs and search is the ability to use LLMs to improve search capabilities, such as retrieval-augmented generation, query understanding, index construction, LLMs in re-ranking, and search result compression. LLMs can also be used to manage document updates, rank search results, and compress search results. LLMs can be used to prompt the language model to extract or formulate a question based on the prompt and then send that question to the search engine, or to prompt the model with a description of the search engine tool and how to use it with a special `[SEARCH]` token. LLMs can also be used to prompt the language model to rank search results according to their relevance with the query, and to classify the most likely answer span given a question and text passage as input.Next Up in this SeriesThis blog post shared an initial overview of the LlamaIndex and Weaviate integration. We covered an introduction to the toolkits offered in LlamaIndex and a notebook on how to build a simple QA engine over Weaviate\u2019s blog posts. Now that we have a baseline understanding, we will build on this by sharing more advanced guides soon. Stay tuned!What\u2019s next\u200bCheck out Getting Started with Weaviate, and begin building amazing apps with Weaviate.You can reach out to us on Slack or Twitter, or join the community forum.Weaviate is open source, and you can follow the project on GitHub. Don\u2019t forget to give us a \u2b50\ufe0f while you are there!",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-and-weaviate-ba3ff1cbf5f4"
        },
        {
            "date": "Jan 14, 2024",
            "author": "Harshad Suryawanshi",
            "title": "AI Voice Assistant: Enhancing Accessibility in AI with LlamaIndex and GPT3.5 (Deployed in Prod on Vercel and Render)",
            "tags": [
                "llamaindex",
                "voice-assistant",
                "openai",
                "accessibility",
                "inclusive-ai"
            ],
            "content": "IntroductionThe C3 Voice Assistant is my latest project aimed at making Large Language Model (LLM) and Retrieval-Augmented Generation (RAG) applications more accessible. This voice-activated assistant caters to a broad audience, including those facing typing challenges or accessibility issues.FeaturesVoice Activation: Initiated by saying \u201cC3.\u201d Alternatively, users can click the blue ring to activate the listening mode of the app. The wake word \u201cC3\u201d is configurable and you can choose any other word.Universal Accessibility: Ideal for users preferring voice commands or facing typing challenges.LLM Integration: Capable of general queries and document-specific inquiries (e.g., Nvidia\u2019s FY 2023 10K report).User-Friendly Interface: The interface of the AI voice assistant is designed for simplicity and ease of use, focusing on voice chat interactions. It features a minimalistic and user-friendly React.js layout. Additionally, there is a convenient sidebar that displays the entire chat history in text format, allowing users to review and reflect on their interactions with the AI.The Tech StackThe app is built on a robust and flexible tech stack that ensures a smooth, reliable, and efficient user experience. Here\u2019s an overview:Frontend: The user interface is a custom application developed using React.js. It\u2019s designed to be minimalistic yet highly functional, prioritizing ease of use and accessibility.Backend: The server-side operations are powered by Python Flask. I\u2019ve utilized the innovative \u2018create-llama\u2019 feature from LlamaIndex, which significantly streamlines the development process.Hosting: For a seamless performance, the frontend of the C3 Voice Assistant is hosted on Vercel. The backend, on the other hand, is deployed on Render, ensuring efficient management and operation of server-side tasks.Building the FrontendThe frontend, built with React.js, focuses on user interaction and accessibility. The App.js script incorporates features like wake word recognition, speech-to-text conversion, state management, and dynamic UI elements like speech bubbles and spinners.1. Component and State InitializationThis section sets up the React component and initializes various states, such as appState to track the current state of the app (idle, listening, speaking), and transcript to store the text transcribed from user speech.import React, { useState, useRef, useEffect } from \"react\";\nimport \"./App.css\";\n\nconst App = () =&gt; {\n  const [appState, setAppState] = useState(\"idle\");\n  const [transcript, setTranscript] = useState(\"\");\n  // Additional state and ref declarations...\n};2. Speech Recognition SetupIn this useEffect hook, two speech recognition instances are initialized: one for detecting the wake word \u201cC3\u201d and another for the main speech recognition. This setup ensures that the app starts listening for commands when \u201cC3\u201d is mentioned.You can easily swap \u201cC3\u201d with any other wake word of your choice.  useEffect(() =&gt; {\n    // Wake word listener setup\n    const WakeWordSpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (WakeWordSpeechRecognition &amp;&amp; !wakeWordRecognitionRef.current) {\n      wakeWordRecognitionRef.current = new WakeWordSpeechRecognition();\n      wakeWordRecognitionRef.current.continuous = true;\n      wakeWordRecognitionRef.current.interimResults = false;\n\n      wakeWordRecognitionRef.current.onresult = (event) =&gt; {\n        const transcript = event.results[event.results.length - 1][0].transcript\n          .trim()\n          .toLowerCase();\n        if (transcript.includes(\"c3\")) {\n          toggleRecording(); // Start the main speech recognition process\n        }\n      };\n\n      wakeWordRecognitionRef.current.start();\n    }\n\n    // Main speech recognition setup\n    const SpeechRecognition =\n      window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition &amp;&amp; !recognitionRef.current) {\n      recognitionRef.current = new SpeechRecognition();\n      recognitionRef.current.continuous = false;\n      recognitionRef.current.interimResults = false;\n\n      recognitionRef.current.onresult = (event) =&gt; {\n        const lastResultIndex = event.results.length - 1;\n        const transcriptResult = event.results[lastResultIndex][0].transcript;\n        setTranscript(transcriptResult);\n        setAppState(\"playing\");\n        setShowSpeechBubble(true);\n        setTimeout(() =&gt; setShowSpeechBubble(false), speechBubbleTimeout);\n        fetchResponseFromLLM(transcriptResult);\n      };\n\n      recognitionRef.current.onend = () =&gt; {\n        setShowSpinner(true);\n      };\n    }\n  }, []);3. Handling User Speech and ResponsetoggleRecording controls the speech recognition process, while fetchResponseFromLLM sends the user's speech to the LLM backend and handles the response. This response is then spoken out via speech synthesis and also used to update the chat history displayed on the UI. const toggleRecording = () =&gt; {\n    try {\n      if (appState === \"idle\") {\n        recognitionRef.current.start();\n        setAppState(\"listening\");\n      } else if (appState === \"listening\") {\n        recognitionRef.current.stop();\n      }\n    } catch (error) {\n    }\n  };  const fetchResponseFromLLM = async (text) =&gt; {\n    try {\n      const response = await fetch(\n        `https://c3-python-nostream.onrender.com/api/chat`,\n        {\n          method: \"POST\",\n          headers: { \"Content-Type\": \"application/json\" },\n          body: JSON.stringify({\n            messages: [\n              {\n                role: \"user\",\n                content:\n                  \"You are an AI voice assistant called C3. You can provide any general information as well as answer basic questions about the Nvidia 10k report for year ended Jan 2023\" +\n                  text,\n              },\n            ],\n          }),\n        }\n      );\n      const data = await response.json();\n\n      setChatHistory((prevHistory) =&gt; [\n        ...prevHistory,\n        { query: text, response: data.result.content },\n      ]);\n      speak(data.result.content);\n    } catch (error) {\n      console.error(\"Error communicating with LLM:\", error);\n    }\n  };4. Speech SynthesisThe speak function takes the text response from the LLM and uses the SpeechSynthesis API to read it aloud, providing an interactive experience for the user.  const speak = (text) =&gt; {\n    if (synthRef.current &amp;&amp; text) {\n      const utterance = new SpeechSynthesisUtterance(text);\n\n      const voices = window.speechSynthesis.getVoices();\n      if (voices.length &gt; 0) {\n        utterance.voice = voices[3]; // You can change this to select different voices\n      }\n\n      utterance.onstart = () =&gt; {\n        console.log(\"TTS starts speaking\");\n        setShowSpinner(false);\n      };\n\n      utterance.onend = () =&gt; {\n        setAppState(\"idle\");\n        if (wakeWordRecognitionRef.current) {\n          wakeWordRecognitionRef.current.start(); // Restart wake word listener after speaking\n        }\n      };\n      synthRef.current.speak(utterance);\n    }5. UI RenderingThe return statement of the App function contains the JSX code for rendering the app's UI. This includes buttons for starting/stopping the voice interaction, a display area for the transcript, and a chat sidebar showing the history of interactions.By combining voice recognition, LLM integration, and speech synthesis, this frontend component provides a comprehensive and accessible interface for interacting with the C3 Voice Assistant.Backend Server SetupInitialize Create-Llama: Run npx create-llama@latest in your terminal.Follow the prompts to set up a Python FastAPI backend, which we can be integrated with our frontend.Use poetry install and poetry shell to prepare the environment.Create a .env file with OPENAI_API_KEY=<openai_api_key>.Generate Embeddings (optional): If a ./data directory exists, run python app/engine/generate.py.Execute python main.pyto start the server.Test the API: Use curl --location 'localhost:8000/api/chat' --header 'Content-Type: application/json' --data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"Hello\" }] }' to test.Modify API behavior in app/api/routers/chat.py. The server supports CORS for all origins, alterable with the ENVIRONMENT=prod setting.IntegrationOnce the backend server is set up, integrating it with the frontend is straightforward. Simply update the fetchResponseFromLLM function in your frontend's App.js to call the backend server URL. This change ensures that when the frontend makes a request, it communicates with your newly configured backend, thus effectively integrating the two components.Final ThoughtsWrapping up, the C3 Voice Assistant isn\u2019t just a tech showcase; it\u2019s a stride towards democratizing AI. It\u2019s about making powerful AI tools, like LLMs and RAG, accessible and user-friendly. This project is more than lines of code \u2014 it\u2019s a push to break down tech barriers and empower everyone.Your thoughts and feedback are invaluable \u2014 let\u2019s make AI more accessible together!Link to Github Repo: Frontend and BackendConnect with Me on LinkedInLinkedin Post",
            "related_posts": [
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse",
                "LlamaIndex Newsletter 2024\u201302\u201320: introducing LlamaCloud"
            ],
            "link": "https://www.llamaindex.ai/blog//ai-voice-assistant-enhancing-accessibility-in-ai-with-llamaindex-and-gpt3-5-f5509d296f4a"
        },
        {
            "date": "Jul 4, 2023",
            "author": "Jerry Liu",
            "title": "LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development",
            "tags": [
                "llamaindex",
                "ai",
                "machine-learning",
                "llm",
                "nlp"
            ],
            "content": "A few months ago, we launched LlamaIndex 0.6.0, which included a massive rewrite of our codebase to make our library more modular, customizable, and accessible to both beginner and advanced users:We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines).We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system.Today, we\u2019re excited to launch LlamaIndex 0.7.0. Our latest release continues the theme of improving modularity/customizability at the lower level to enable bottoms-up development of LLM applications over your data. You now have even more control over using key abstractions: the LLM, our response synthesizer, and our Document and Node objects.We\u2019ve created standalone LLM abstractions (OpenAI, HuggingFace, PaLM).We\u2019ve made our response synthesis module an independent module you can use completely independently of the rest of our abstractions \u2014 get rid of the prompt boilerplate of trying to figure out how to fit context within a context window.We\u2019ve added extensive metadata management capabilities to our Document/Node objects \u2014 now you have complete control over context you decide to inject into your documents.Below, we describe each section more in detail. We also outline a full list of breaking changes at the bottom.Standalone LLM AbstractionsWe\u2019ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM. These abstractions can be used on their own, or as part of an existing LlamaIndex system (query engines, retrievers).High-level MotivationWe did this for multiple reasons:Cleaner abstractions in the codebase. Before, our LLMPredictor class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize.Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use \u201ctext-davinci-003\u201d, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it\u2019s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain\u2019s LLMs if you wish.Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It\u2019s reflective of our bigger push in 0.7.0 to let users compose their own workflows.Using on their ownOur LLM abstractions support both complete and chat endpoints. The main difference is that complete is designed to take in a simple string input, and output a CompletionResponse (containing text output + additional fields). chat takes in a ChatMessage and outputs a ChatResponse (containing a chat message + additional fields).These LLM endpoints also natively support streaming via stream_complete and stream_chat.Here\u2019s on how you can use the LLM abstractions on their own:from llama_index.llms import OpenAI\n\n# using complete endpoint\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n# get raw object\nresp_raw = resp.raw\n# using chat endpoint\nfrom llama_index.llms import ChatMessage, OpenAI\nmessages = [\n    ChatMessage(role='system', content='You are a pirate with a colorful personality'),\n    ChatMessage(role='user', content='What is your name')\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n# get raw object\nresp_raw = resp.raw\n# using streaming endpoint\nfrom llama_index.llms import OpenAI\nllm = OpenAI()\nresp = llm.stream_complete('Paul Graham is ')\nfor delta in resp:\n    print(delta, end='')Here\u2019s how you can use the LLM abstractions as part of an overall LlamaIndex system.from llama_index.llms import OpenAI\nfrom llama_index.indices.service_context import ServiceContext\nfrom llama_index import VectorStoreIndex\n\nllm = OpenAI(model='gpt-3.5-turbo', temperature=0)\nservice_context = ServiceContext.from_defaults(llm=llm)\nindex = VectorStoreIndex.from_documents(docs, service_context=service_context)\nresponse = index.as_query_engine().query(\"&lt;question&gt;\")Note: Our top-level LLMPredictor still exists but is less user-facing (and we might deprecate in the future). Also, you can still use LangChain LLMs through our LangChainLLM class.ResourcesAll of our notebooks have by default been updated to use our native OpenAI LLM integration. Here\u2019s some resources to show both the LLM abstraction on its own as well as how it can be used in the overall system:OpenAI LLMUsing LLM in LLMPredictorChanging LLM within Index/Query EngineDefining a custom LLM ModelStandalone Response Synthesis ModulesContextIn any RAG system, there is retrieval and there is synthesis. The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM.Fundamentally, the synthesis module needs to synthesize a response over any context list, regardless of how long that context list is. This is essentially \u201cboilerplate\u201d that an LLM developer / \u201cAI engineer\u201d must write.We had this as an internal abstraction in LlamaIndex before (as a ResponseSynthesizer), but the external-facing UX was unfriendly to users. The actual piece that gathered responses (the ResponseBuilder ) was hard to customize, and the ResponseSynthesizer itself was adding an extra unnecessary layer.Now we have a set of standalone modules that you can easily import. Previously, when you set the response_mode in the query engine, these were being setup for you. Now they are more directly available and user-facing.Here\u2019s a list of all the new Response Synthesiszer modules available from llama_index.response_synthesizer:Refine - Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk.Accumulate - Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responsesCompact - The same as Refine, but puts as much text as possible into each LLM callCompactAndAccumulate - The same as Accumulate, but puts as much text as possibleTreeSummarize - Create a bottom-up summary from the provided text chunks, and return the root summarySimpleSummarize - Combine and truncate all text chunks, and summarize in a single LLM callUsageAs detailed above, you can directly set a response synthesizer in a query engine, or let the response_mode fetch the relevant response synthesizer.Furthermore though, you can directly call and use these synthesizers as low level modules. Here\u2019s a small example:from llama_index import ServiceContext\nfrom llama_index.response_synthesizers import CompactAndRefine\n\n# you can also configure the text_qa_template, refine_template, \n# and streaming toggle from here\nresponse_synthesizer = CompactAndRefine(\n  service_context=service_context.from_defaults()\n)\nresponse = response_synthesizer.get_response(\n \"What skills does Bob have?\",\n  text_chunks=[\" ...\"]  # here would be text, hopefully about Bob's skills\n)ResourcesHere are some additional notebooks showing how to use get_response_synthesizer :Low-level API Usage PatternCustom RetrieversMetadata Management CapabilitiesIf you want to have good performance in any LLM application over your data (including a RAG pipeline), you need to make sure that your documents actually contain relevant context for the query. One way to do this is to add proper metadata, both at the document-level and after the documents have been parsed into text chunks (into Nodes).We allow you to define metadata fields within a Document, customize the ID, and also customize the metadata text/format for LLM usage and embedding usage.Defining Metadata Fieldsdocument = Document(\n    text='text', \n    metadata={\n        'filename': '&lt;doc_file_name&gt;', \n        'category': '&lt;category&gt;'\n    }\n)Customizing the IDThe ID of each document can be set multiple waysWithin the constructor: document = Document(text=\"text\", doc_id_=\"id\")After constructing the object: document.doc_id = \"id\"Automatically using the SimpleDirectoryReader : SimpleDirectoryReader(filename_as_id=True).load_data()Customizing the Metadata Text for LLMs and EmbeddingsAs seen above, you can set metadata containing useful information. By default, all the metadata will be seen by the embedding model and the LLM. However, sometimes you may want to only include data to bias embeddings, or only include data as extra information for the LLM!With the new Document objects, you can configure what each metadata field is used for:document = Document(\n    text='text', \n    metadata={\n        'filename': '&lt;doc_file_name&gt;', \n        'category': '&lt;category&gt;'\n    },\n    excluded_llm_metadata_keys=['filename', 'category'],\n    excluded_embed_metadata_keys=['filename']\n)Customizing the Metadata Format TemplateWhen the metadata is inserted into the text, it follows a very specific format. This format is configurable at multiple levels:from llama_index.schema import MetadataMode\n\ndocument = Document(\n  text='text',\n  metadata={\"key\": \"val\"},\n  metadata_seperator=\"::\",\n    metadata_template=\"{key}=&gt;{value}\",\n    text_template=\"Metadata: {metadata_str}\\\\n-----\\\\nContent: {content}\"\n)\n# available modes are ALL, NONE, LLM, and EMBED\nprint(document.get_content(metadata_mode=MetadataMode.ALL))\n# output:\n# Metadata: key=&gt;val\n# -----\n# textPlease check out this guide for more details!Full List of Breaking ChangesResponse Synthesis + Node PostprocessorsThe ResponseSynthesizer object class has been removed, and replaced with get_response_synthesizer . In addition to this, node post processors are now handled by the query engine directly, and the old SentenceEmbeddingOptimizer has been switched to become a node post processor instance itself.Here is an example of the required migration to use all moved features.Oldfrom llama_index import (\n    VectorStoreIndex,\n    ResponseSynthesizer,\n)\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.optimizers import SentenceEmbeddingOptimizer\nfrom llama_index.query_engine import RetrieverQueryEngine\n\ndocuments = ...\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n# configure retriever\nretriever = index.as_retriever(\n   similarity_top_k=3\n)\n# configure response synthesizer\nresponse_synthesizer = ResponseSynthesizer.from_args(\n   response_mode=\"tree_summarize\",\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7),\n        SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n    ]\n)\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)Newfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.indices.postprocessor import (\n    SimilarityPostprocessor,\n    SentenceEmbeddingOptimizer\n)\n\ndocuments = ...\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n   response_mode=\"tree_summarize\",\n)\n# assemble query engine\nquery_engine = index.as_query_engine(\n  similarity_top_k=3,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7),\n        SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n    ]\n)LLM PredictorWhile introducing a new LLM abstraction, we cleaned up the LLM Predictor and removed several deprecated functionalities:Remove ChatGPTLLMPredictor and HuggingFaceLLMPredictor (use OpenAI and HuggingFaceLLM instead, see migration guide)Remove support for setting cache via LLMPredictor constructor.Removed llama_index.token_counter.token_counter module (see migration guide).Now, the LLM Predictor class is mostly a lightweight wrapper on top of the LLM abstraction that handles:conversion of prompts to the string or chat message input format expected by the LLMlogging of prompts and responses to a callback managerWe advice users to configure the llm argument in ServiceContext directly (instead of creating LLM Predictor).Chat EngineWe updated the BaseChatEngine interface to take in a List[ChatMessage]] for the chat_history instead of tuple of strings. This makes the data model consistent with the input/output of the LLM , also more flexibility to specify consecutive messages with the same role.Oldengine = SimpleChatEngine.from_defaults(\n\tchat_history=[(\"human message\", \"assistant message\")],\n)\nresponse = engine.chat(\"new human message\")Newengine = SimpleChatEngine.from_defaults(\n    service_context=mock_service_context,\n    chat_history=[\n        ChatMessage(role=MessageRole.USER, content=\"human message\"),\n        ChatMessage(role=MessageRole.ASSISTANT, content=\"assistant message\"),\n    ],\n)\nresponse = engine.chat(\"new human message\")We also exposed chat_history state as a property and supported overriding chat_history in chat and achat endpoints.Prompt HelperWe removed some previously deprecated arguments: max_input_size, embedding_limit, max_chunk_overlapConclusionAt a high-level, we hope that these changes continue to enable bottoms-up development of LLM applications over your data. We first encourage you to play around with our new modules on their own to get a sense what they do and where they can be used. Once you\u2019re ready to use them in more advanced workflows, then you can figure out how to use our outer components to setup a sophisticated RAG pipeline.As always, our repo is here and our docs are here. If you have thoughts/comments, don\u2019t hesitate to hop in our Discord!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"
        },
        {
            "date": "May 10, 2023",
            "author": "Jerry Liu",
            "title": "LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)",
            "tags": [
                "artificial-intelligence",
                "machine-learning",
                "large-language-models",
                "nlp"
            ],
            "content": "OverviewI had the pleasure of speaking with Sam Charrington on the TWIML AI podcast about LlamaIndex, and the episode was just released this past Monday (5/8/23).\n\nI thought it would be a fun experiment to distill some highlights from the podcast! And what better way to do this than using LlamaIndex itself (plus OpenAI Whisper for transcription)? \ud83d\ude09I did the following:Ran the podcast through whisper.cppDid some light manual cleaning of the text, and uploaded it as a Dropbox file.Get a high-level summary + ask some more targeted questions over the podcast transcript. Uses LlamaIndex \ud83e\udd99. Colab notebook here!We used the our list index to get an overall summary of the podcast, and also our vector index to ask more specific questions. We ask some basic questions ourselves, but also directly ask questions that Sam asked during the podcast in order to extract more concise versions of the responses that I gave (including removing a bunch of filler \u201cyou know\u201d words).All the distilled highlights using LlamaIndex are showcased below. Let us know what you think!Distilled HighlightsWhat are three key points described in this podcast? Give a followup of each key point with an explanation.LlamaIndex is a toolkit for connecting language models to data. It was initially designed as a fun tool to play around with and solve a specific problem, but has since evolved into a set of useful tools and instructions that people can use to build applications on top of. The technique people were playing around with at the time was the idea of prompting and context learning, which is providing the right conditioning variable for the input and using that to send some sort of input prompt to the language model and get back a response. Additionally, LlamaIndex offers more advanced primitives such as decision-making at the top level to route queries to the right sub-data structure, and the ability to synthesize an answer from multiple data sources. It also provides an outer abstraction layer that can automatically reason which structure to use given an outer query request, and can be used as a drop-in module on top of existing data systems without having to worry about complexity. Examples of applications that can be built on top of LlamaIndex include ingesting video and structured data to parse into an audio transcript, running image captioning models, and creating augmented chatbot experiences on top of web scrapers.LlamaIndex is also exploring the idea of automation and unifying everything under a single query interface, so that users don\u2019t have to specify a different parameter for every use case. This includes optimizing token usage, making queries faster, and reducing costs for the user. Additionally, LlamaIndex is looking into applying automation to the data system world, such as teaching Oracle databases how to spit out natural language prompt responses, and making the data stack more efficient. This includes simplifying the data stack over time, especially as language models take off, and leveraging capabilities of LLM\u2019s and various components of the data landscape to simplify the number of steps it takes from raw data to insight for the user. They are also exploring the idea of inferring the right schemas and writing structured data from unstructured data, as well as automatically building a natural language query interface with a view of the data within the data system.LlamaIndex is also exploring the idea of agents as a layer of automation for decision making over any sort of function that you want to run. This includes taking in some input and doing reasoning under the hood to decide, make a decision over some input, as well as some access to some context, for instance, over your data or over the set of tools that is able to have access to. Additionally, LlamaIndex is looking into ways to reduce cost and latency, such as using more fine-tuned distilled models that are a bit smaller, and making sure that the more decisions that are chained together, the less errors propagate over time. They are also exploring the idea of observability and evidence across a chain of relatively independent decisions that individual agents are making, as well as the interfaces that these agents might use, such as traditional software and agent worlds.What is the origin story of LlamaIndex?The origin story of LlamaIndex is that it was founded in November by Jerry, who was trying to build a sales bot. He was playing around with GPT-3 and wanted to use it on his internal company data. He wanted to use it to synthesize a to do list for him for the next customer meeting, as he had to spend 20\u201330 minutes reviewing notes from the previous call transcripts. This led to the idea of stuffing data from Notions, Slack, Salesforce, data lakes, vector databases, and structure databases into language models. This was the impetus for LlamaIndex, which is focused on connecting data to language models and tapping into the capabilities of language models to utilize them on top of private sources of data.What is LlamaIndex doing beyond top-k retrieval?LlamaIndex is offering more advanced primitives on top of basic top-k retrieval in order to provide responses to more complicated questions. These primitives include decision-making at the top level to route queries to the right sub-data structure, synthesizing information from multiple data systems, and providing trade-offs between different approaches.Additionally, LlamaIndex is working on building tooling to help users create customizable indexes and views of their data to allow them to execute different types of queries. This includes connecting to existing data systems, defining metadata on top of each unit of data, providing the building blocks to create different types of indexes, and abstracting away complexity with an outer agent layer that can automatically reason which structure to use given a query request. This allows users to get the best results for a query, while also providing an alternative to something like a langchain or using it as part of building a broader solution.[Sam] It sounds like we\u2019re starting to identify a higher level of abstraction that different use cases will fall under. Is it more the case that there\u2019s some manageable number of these primitives, like 10, 20, or is it that every use case is going to be a little bit different, and there are hundreds of thousands of kind of fundamental ways that people want to work with their documents, and so you need to just give them a very open capability?Jerry\u2019s response is that there are probably a few different use cases that people tend to want to get answers from over their data, and it is possible there is a giant long tail of different tasks. He believes that the complexity of the task scales with the number of steps it requires to execute, and that users need to be given customizable building blocks in order to get the results they want. He also believes that the next natural step is to automate the process and unify everything under a single query interface, so that users don\u2019t have to specify different parameters for every use case.He also believes that this paradigm is displacing more static paradigms like ETL, and that it is applicable to a wide range of applications. He sees this agent type environment becoming fundamental infrastructure that reimagines the entire existing enterprise data stack, and that it can be used to parse unstructured data into structured data, as well as to automatically reason how to best transform data from one place to another. He also believes that this will make the job of the data engineer and data scientist much more efficient, and that it will enable the creation of natural language query interfaces that have a view of the data within the data system.[Sam] When you think about the interface between LLM-based data processing system and the data sources of record, what does that interface evolve to look like? For example, does it evolve to look like the chat GPT plugin model, where we\u2019re going to teach our Oracle databases how to spit out natural language prompt responses, that kind of thing, or do you think that there\u2019s some more efficient way of doing that or is that more efficient? Like, what\u2019s your view of the way these things evolve?I think the way this interface will evolve is that it will become more automated and efficient. We will be able to use language models to understand raw text and extract the relevant information from it, without having to manually enter data into a structured format. We will also be able to use agents to automate decision making and provide a unified query interface, so that users don\u2019t have to specify different parameters for every use case.Additionally, we can use LlamaIndex to structure data in a way that allows us to make use of the limited prompt size of GPT-3, while still being able to achieve the task. We can also use this data stack to infer the right schemas and further write structured data from unstructured data, as well as automatically build a natural language query interface that has a view of the data within the data system. This will enable us to make the job of the data engineer and data scientist much more efficient by having automated reasoning agents over deciding, making decisions at every stage of the data infrastructure stack.Want to ask your own questions over the podcast?If you want to build your own LLM-powered chatbot over our TWIML podcast, check out the resources below!Colab notebookRaw TranscriptPodcast on Spotify",
            "related_posts": [
                "Querying a network of knowledge with llama-index-networks",
                "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS",
                "Agentic RAG With LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-on-twiml-ai-a-distilled-summary-using-llamaindex-de2a88551595"
        },
        {
            "date": "Jan 26, 2024",
            "author": "Ethan P",
            "title": "Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex",
            "tags": [
                "llm",
                "ai",
                "evaluation",
                "github",
                "ci-cd-pipeline"
            ],
            "content": "In this technical walkthrough, we\u2019ll highlight the functionality of Tonic Validate and its integration with LlamaIndex. Sign up for a free account here before you start.IntroductionAs enterprise adoption of generative AI technologies continues, companies are turning to Retrieval Augmented Generation (RAG) systems to extend the application of large-language models (LLMs) to their private data (e.g., a chatbot that can answer questions based on internal technical documentation). Traditionally in software engineering, companies have placed a high emphasis on implementing continuous integration tests to ensure systems remain performant when updates are made. More recently, these same principles have been applied to machine learning models in production.However, as a young technology, RAG currently lacks best practices for integration tests to ensure breaking changes aren\u2019t introduced to the production system. In this article, we will demonstrate how you can use Tonic Validate\u2019s RAG performance monitoring capabilities, LlamaIndex, and GitHub Actions to create novel integration tests that alert you to changes in RAG system performance. To make things easy, Tonic Validate is available natively within LlamaIndex\u2019s core library \u2014 you can read more about that here.What is Tonic Validate?Tonic Validate is a RAG benchmarking and evaluation platform that monitors performance of RAG systems in production. It provides comprehensive metrics for measuring the performance of each component in your RAG system, visualizations for comparing performance across time as the system changes, and workflows for creating benchmark question-answer sets and reviewing LLM responses. Tonic Validate shines a light on how your RAG system is truly performing, enabling continuous performance monitoring of your production RAG systems. You can learn more and sign up for a free account.Setting up LlamaIndexTo get started, let\u2019s create an example RAG system for us to test. In this case, LlamaIndex provides a tool called create-llama which can generate a full-stack RAG application for us. To install it, we need to make sure we have Node.JS installed and run the following command:npx create-llama@latestThis command will take you through a series of prompts. Here are the options to select for each prompt:What is your project named? \u00bb llama-validate-demo\nWhich template would you like to use? \u00bb Chat without streaming\nWhich framework would you like to use? \u00bb FastAPI (Python)\nWould you like to install dependencies automatically? \u00bb No\nWhich model would you like to use? \u00bb gpt-4\u20131106-preview\nWhich data source would you like to use? \u00bb Use an example PDF\nWould you like to use a vector database? \u00bb No, just store the data in the file systemOnce these options are selected, your project should be created in a folder called llama-validate-demo. For this demo, we are going to replace the example PDF create-llama provides with our own larger dataset. The dataset consists of a collection of essays from Paul Graham\u2019s blog. This should more closely replicate a real world scenario where a company runs RAG on a larger internal dataset. To add the essays, download them from our Github and unzip them inside the root folder of your created project. Make sure the unzipped folder is named data. Be sure to delete any existing files in the folder before copying the new dataset.After you have the essays in the right directory, you can set up your OpenAI API key by setting it as an environment variable called OPENAI_API_KEY. You can do this either via setting the environment variable system wide or by creating a .env file in the root folder of your create-llama project. Then you can run the following commands in the root folder for your create-llama project:poetry install\npoetry shell\npython app/engine/generate.pyThis will install the dependencies and generate the RAG embeddings for the Paul Graham essays. After this, you can run the chatbot with:python main.pyTo test the chatbot, you can send a request via curl:curl - location 'localhost:8000/api/chat' \\\n - header 'Content-Type: application/json' \\\n - data '{ \"messages\": [{ \"role\": \"user\", \"content\": \"In the early days, how were the Airbnb founders financing their startup?\" }] }'LlamaIndex will then return a response:{\n    \"result\": {\n        \"role\": \"assistant\",\n        \"content\": \"In the early days, the Airbnb founders financed their startup by creating and selling themed breakfast cereals. They created limited-edition cereal boxes, such as \\\"Obama O's\\\" and \\\"Cap'n McCain's,\\\" during the 2008 U.S. presidential election, which became a collectible and helped them raise funds for their company. This creative approach to funding allowed them to sustain the business in its initial phase before securing more traditional forms of investment.\"\n    }\n}Finally, in llama-validate-demo/app/api/routers/chat.py we want to replace the return _Result line at the end of the chat function with the following.return _Result(\n    result=_Message(\n        role=MessageRole.ASSISTANT,\n        content=response.response,\n        context=[x.text for x in response.source_nodes]\n    )\n)This allows the LlamaIndex API to return the RAG context that was used to answer the question asked. Now, we can move on to setting up Tonic Validate!Setting up Tonic ValidateTo set up Tonic Validate, first install it via poetry:poetry add tonic-validateNow, we can create our tests for Tonic Validate. To get started, create a file inside llama-validate-demo/tests called validate_test.py. We will also need to create a list of test questions and answers which you can find here. Alternatively, you can also use the Tonic Validate UI to create the test set and call it via the SDK \u2014 we\u2019ll be adding a feature to help generate these benchmarks using synthetic data to make this process even easier. Download the qa_pairs.json file from the link and paste it into llama-validate-demo/tests. Once we have both of these files, we can add the following code into validate_test.py.import json\nimport os\nfrom tonic_validate import ValidateApi\nfrom tonic_validate.metrics import AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric\nfrom llama_index.evaluation import TonicValidateEvaluator\nimport requests\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef get_llm_response(prompt):\n    url = \"http://localhost:8000/api/chat\"\n\n    payload = json.dumps({\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n    })\n    headers = { 'Content-Type': 'application/json' }\n    response = requests.request(\"POST\", url, headers=headers, data=payload).json()\n    result = response['result']\n    return result['content'], result['context']This code sets up the dependency imports and also specifies a get_llm_response function which sends a request to the LlamaIndex API server we set up earlier to get a response. Now, let\u2019s create a function that gets the list of questions to ask LlamaIndex for our testing.def get_q_and_a():\n    # Load qa_pairs.json\n    qa_pairs = json.load(open('./tests/qa_pairs.json'))\n    return ([x['question'] for x in qa_pairs], [x['answer'] for x in qa_pairs])This function gets the question-answer pairs from our json file. The questions are what we will ask the RAG system and the answers are the correct answers for those questions. For instance, if the question was \u201cWhat is the capital of France?\u201d then the answer would be \u201cParis\u201d.Next, we can add the code that queries LlamaIndex:def get_responses(questions):\n    llm_answers = []\n    context_lists = []\n    for item in questions:\n        llm_answer, llm_context_list = get_llm_response(item)\n        llm_answers.append(llm_answer)\n        context_lists.append(llm_context_list)\n    return (llm_answers, context_lists)This code iterates over the questions, queries LlamaIndex, and logs each response into an array. We have two arrays. One is the actual answer from LlamaIndex. The other is a list of the snippets of text (called the context list) that LlamaIndex provided to help the LLM answer the question.Now we have a list of LLM responses generated from a list of test questions. Let\u2019s score them:def score_run(questions, context_lists, reference_answers, llm_answers):\n    metrics = [\n        AnswerSimilarityMetric(),\n        RetrievalPrecisionMetric(),\n        AugmentationPrecisionMetric(),\n        AnswerConsistencyMetric()\n    ]\n    scorer = TonicValidateEvaluator(metrics, model_evaluator=\"gpt-4-1106-preview\")\n    run = scorer.evaluate_run(\n        questions, llm_answers, context_lists, reference_answers\n    )\n    return run, metricsWe first need to define the metrics in Tonic Validate that we want to use. You can find a list of available metrics and their definitions here. After we create the metrics, we can take advantage of Tonic Validate\u2019s integration with LlamaIndex. Since Tonic Validate is built into LlamaIndex\u2019s evaluation framework as an evaluator, all we need to do is create a TonicValidateEvaluator, which scores the LlamaIndex responses across the chosen metrics. Then we return the results along with the metrics.Finally, we can create our test function for pytest which evaluates LlamaIndex.def test_llama_index():\n    questions, reference_answers = get_q_and_a()\n    llm_answers, context_lists = get_responses(questions)\n    run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)\n    # Upload results to web ui\n    validate_api = ValidateApi()\n    # Get project id from env\n    project_id = os.getenv(\"PROJECT_ID\")\n    validate_api.upload_run(project_id, run)This runs all the code we\u2019ve written to get the scores and then sends them to Tonic Validate\u2019s API to visualize in the UI. In order to send the metrics for each run to the UI, you need to sign up for a free account, which you can do here. I highly recommend utilizing the UI to make visualizing and monitoring performance changes a breeze. Once you sign up, you will be taken through a short onboarding process where you create an API key and a project. The API key should be stored in an environment variable called TONIC_VALIDATE_API_KEY and the project ID in an environment variable called PROJECT_ID.Once you have set up your account and configured your environment variables, you can run the test via the following commands:poetry shell\npytestYou can also make the test fail if the metrics score too low. This would be a pertinent step to add in if you want to avoid introducing breaking changes to a production RAG system; for example, if you update the model version and the answer similarity score suddenly drop below a certain threshold, you could have the test fail and issue a warning to debug the issue.# Check none of the metrics scored too low    \nfor metric in metrics:\n    if metric.name == AnswerSimilarityMetric.name:\n        assert run.overall_scores[metric.name] &gt;= 3.5\n    else:\n        assert run.overall_scores[metric.name] &gt;= 0.7Setting up GitHub ActionsWith LlamaIndex and Tonic Validate configured, we have the ability to connect data to an LLM and measure the accuracy of LLM responses. You can push this setup into production and have a functional chatbot. As is common in modern software development practices, you will likely continue to fix bugs, make improvements, and add new data or features to your RAG system. Before pushing to production, QA testing is in place to catch any changes to your code that may introduce unintended effects. For example, adding a new dataset or updating an LLM to a new version could lead to changes in the quality of responses. One approach, the one that we recommend, for adding QA testing for your RAG system is to use GitHub Actions to establish an integration test using Tonic Validate that checks the LLM response quality of your RAG system, allowing you to catch and rectify any performance degradation before it is pushed into production.To set up Tonic Validate to run in GitHub Actions, we can create a folder llama-validate-demo/.github/workflows with a file called python-app.yml. In this file, we will include the following code configuration that defines the integration test workflow:# This workflow will install Python dependencies and run tests with LlamaIndex\n\nname: Python application\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\npermissions:\n  contents: read\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    environment: Actions\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.11\n      uses: actions/setup-python@v3\n      with:\n        python-version: \"3.11\"\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install poetry\n        poetry config virtualenvs.create false\n        poetry install --no-root --no-dev --no-directory\n    - name: Set PYTHONPATH\n      run: echo \"PYTHONPATH=$GITHUB_WORKSPACE\" &gt;&gt; $GITHUB_ENV\n    - name: Set up vector index\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n      run: |\n        python app/engine/generate.py\n    - name: Start up test server\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        MODEL: gpt-4-1106-preview\n      run: |\n        python main.py &amp;\n        sleep 10\n    - name: Test with pytest\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        TONIC_VALIDATE_API_KEY: ${{ secrets.TONIC_VALIDATE_API_KEY }}\n        PROJECT_ID: ${{ secrets.PROJECT_ID }}\n      run: |\n        pytestThis configures GitHub to run the tests defined with Tonic Validate upon every commit. The GitHub Actions configuration downloads the repo, sets up the dependencies, generates the embeddings, and then starts up the test server and runs the test.After this file is set up, we just need to set our secrets in GitHub. In GitHub, go to Settings > Secrets and variables > Actions for your repo and create a secret called OPENAI_API_KEY, TONIC_VALIDATE_API_KEY, and PROJECT_ID. These values will all be the same as the values you set earlier. Now your GitHub actions set up is complete and you can proactively monitor changes to your RAG system during development and before going into production.Try pushing some commits to it and watch it run! To view the results, go to Tonic Validate\u2019s web app and navigate to your project. You should see a view like this that shows recent metrics and their evolution over time:Now you and your team can track your RAG system\u2019s performance over time to make sure there aren\u2019t any dips in performance! Thank you for reading and make sure to check out Tonic Validate!For more information on Tonic Validate, visit our website and sign up for a free account today. You can also visit our GitHub page to view all of the code used in this post and the rest of our SDK. Our LlamaIndex integration is available here.",
            "related_posts": [
                "Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"
            ],
            "link": "https://www.llamaindex.ai/blog//tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"
        },
        {
            "date": "Jul 24, 2023",
            "author": "Yi Ding",
            "title": "Introducing LlamaIndex.TS",
            "tags": [
                "artificial-intelligence",
                "large-language-models",
                "chatgpt",
                "chatbots",
                "deep-learning"
            ],
            "content": "We are beyond excited to announce v0.0.1 of LlamaIndex.TS, a Typescript first library focused on helping you use your private data with large language models.What is LlamaIndex?Our core goal for LlamaIndex is to help developers easily integrate their data with Large Language Models (LLMs). LLMs, like ChatGPT, have been a revolution in the way we think about handling textual input and data, but all of them have the limitation in what data they have access to. In addition to the \u201cknowledge cutoff\u201d (we are nearing the 2 year anniversary for when ChatGPT\u2019s latest data was trained) LLMs can\u2019t access data from your companies, from your personal analyses, or from the data your users generate.With LlamaIndex.TS, we look to achieve that goal by meeting developers at their (my) language of choice, in this case Typescript. We are committed to making this library the easiest to use, most robust solution out there for using data with LLMs.BackstoryIt was at the Emergency ChatGPT Hackathon hosted by Pete Huang and Rachel Woods that I met Jerry. Having worked in the JS world for the last 8 years, my first question was \u201cwhy don\u2019t you build this in Javascript?\u201d After he demurred, he very patiently guided me through setting up the Python dev environment. (I think it took us 20 minutes before we figured it all out!) So, when Jerry offered to let me build LlamaIndex.TS I obviously couldn\u2019t turn him down. Can\u2019t wait to see what you build with it!DesignAt a high level, LlamaIndex.TS first takes the file inputs, loads them into a standardized format, and creates an Index (knowledge base).We then retrieve the relevant information from the index and use that in our query to the LLM to generate more a grounded response.Check out our docs for a more in depth explanation!PlaygroundWe are building an open source playground for LlamaIndex.TS. Please check it out at https://llama-playground.vercel.app/ PRs are welcome here! https://github.com/run-llama/ts-playgroundMain Differences from LlamaIndex PythonAll function names are \ud83d\udc2a camel cased.The prompt interface is much simpler and uses native javascript template literals.We do not ship non-async versions of functions. Please use await or .then callbacks.We use interfaces and POJOs in lieu of classes where it makes sense. For example, ChatEngine, a base class in Python is an interface in JS. ServiceContext, a class in Python is an interface/POJO in JS.RuntimesCurrently, we support NodeJS v18 and up. Lots of plans on this front though. Stay tuned!ContributingOnly the core features are built out so far, so there is a lot of work that needs to be done on the loader and integration side. If you\u2019re interested in contributing, please send us a message or even better a PR!https://github.com/run-llama/LlamaIndexTS",
            "related_posts": [
                "MultiModal RAG for Advanced Video Processing with LlamaIndex & LanceDB",
                "Agentic RAG With LlamaIndex",
                "Free Advanced RAG Certification course with Activeloop and LlamaIndex",
                "Multimodal RAG pipeline with LlamaIndex and Neo4j"
            ],
            "link": "https://www.llamaindex.ai/blog//introducing-llamaindex-ts-89f41a1f24ab"
        },
        {
            "date": "Nov 20, 2023",
            "author": "Ankush k Singal",
            "title": "Becoming Proficient in Document Extraction",
            "tags": [
                "technology",
                "software-development",
                "software-engineering",
                "tech",
                "artificial-intelligence"
            ],
            "content": "\nIntroduction\n\n\n  In the domain of document handling, accurately extracting crucial information\n  from images has posed an enduring obstacle. Despite Optical Character\n  Recognition (OCR) advancements in converting images to editable text, it faces\n  numerous intricacies with diverse document formats and quality. Here enters\n  Zephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\n  addresses these hurdles, heralding a transformative era in image-based\n  document extraction.\n\n\n\n\n    Source:\n    Zephyr-llama-index\n\n\n\nThe OCR Dilemma: Obstacles and Constraints Optical Character\n\nRecognition (OCR), though potent, faces impediments such as:\n\n\nDiverse Document Formats: Documents exhibit intricate\n    layouts, fonts, and structures, posing challenges for traditional OCR to\n    precisely interpret and extract information.\n  \n\nQuality and Clarity: Images with low resolution,\n    blurriness, or skewed angles hinder OCR\u2019s accuracy in deciphering text.\n  \n\nHandwritten and Cursive Content: OCR often struggles with\n    handwritten text or cursive fonts, resulting in errors or incomplete\n    extraction.\n  \n\nMultilingual Complexity: Processing documents in multiple\n    languages poses a challenge for OCR systems lacking proficiency in\n    recognizing and extracting varied linguistic content.\n  \n\n\n\nSource: Created by Author using MidJourney\n\n\nZephyr 7b LLM: Narrowing the Divide\n\n\n  Zephyr 7b LLM revolutionizes the landscape by tackling these inherent\n  constraints of OCR technology:\n\n\n\nAdvanced Machine Learning Algorithms:\n\n\n\n  Employing state-of-the-art machine learning algorithms, Zephyr 7b LLM\n  undergoes extensive training with diverse document formats and languages. This\n  equips it to adapt and learn from various document structures, resulting in\n  heightened accuracy and robust extraction capabilities.\n\n\n2. Contextual Comprehension:\n\n\n  Diverging from conventional OCR, Zephyr 7b LLM doesn\u2019t merely identify\n  individual characters; it comprehends the context in which these characters\n  exist. This contextual understanding significantly reduces errors, ensuring\n  precise extraction even from intricate document layouts.\n\n\n3. Adaptive Image Processing:\n\n\n  The fusion with LlamaIndex amplifies Zephyr 7b LLM\u2019s ability to handle images\n  of varying resolutions or qualities. Leveraging adaptive image processing\n  techniques, it rectifies distortions, enhances clarity, and optimizes images\n  for meticulous OCR analysis.\n\n\n4. Multilingual Proficiency:\n\n\n  Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency\n  facilitates seamless content extraction from documents in various languages,\n  extending global accessibility for businesses dealing with multilingual\n  documentation.\n\n\n\nSource: Created by Author using MidJourney\n\n\nImplementation of Code\n\n\n  The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\n  transformation in document extraction. By merging Zephyr\u2019s advanced OCR\n  capabilities with LlamaIndex\u2019s image enhancement and data organization\n  features, this integration presents a comprehensive solution:\n\n\n\nAugmented Precision: The fusion of Zephyr\u2019s machine\n    learning expertise and LlamaIndex\u2019s image enhancement markedly heightens the\n    accuracy of extracted data, diminishing errors and enhancing overall\n    efficiency.\n  \n\nEfficient Workflow: Users experience an optimized workflow,\n    enabling swift extraction and conversion of image-based documents into\n    structured, actionable data, facilitating expedited decision-making\n    processes.\n  \n\nAdaptability Across Document Varieties: This integration\n    empowers users to handle diverse document formats and languages\n    effortlessly, granting access to previously challenging document types for\n    extraction and analysis.\n  \n\n\n\nSource: Image created by Author using MidJourney\n\n\nStep 1: Install and Import Libraries\n\n!pip install llama-index transformers accelerate sentencepiece bitsandbytes -q\n\nStep 2: Load the Model\n\nimport torchfrom transformers import BitsAndBytesConfigfrom llama_index.prompts import PromptTemplatefrom llama_index.llms import HuggingFaceLLMquantization_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_compute_dtype=torch.float16,    bnb_4bit_quant_type=\"nf4\",    bnb_4bit_use_double_quant=True,)def messages_to_prompt(messages):  prompt = \"\"  for message in messages:    if message.role == 'system':      prompt += f\"<|system|>\\n{message.content}</s>\\n\"    elif message.role == 'user':      prompt += f\"<|user|>\\n{message.content}</s>\\n\"    elif message.role == 'assistant':      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"  # ensure we start with a system prompt, insert blank if needed  if not prompt.startswith(\"<|system|>\\n\"):    prompt = \"<|system|>\\n</s>\\n\" + prompt  # add final assistant prompt  prompt = prompt + \"<|assistant|>\\n\"  return promptllm = HuggingFaceLLM(    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),    context_window=3900,    max_new_tokens=2000,    model_kwargs={\"quantization_config\": quantization_config},    # tokenizer_kwargs={},    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},    messages_to_prompt=messages_to_prompt,    device_map=\"auto\",)\nfrom llama_index import ServiceContext, set_global_service_contextservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\nset_global_service_context(service_context)\n\nStep 3: Storing your index\n\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndexfrom llama_index.readers.file.base import (    DEFAULT_FILE_READER_CLS,    ImageReader,)from llama_index.response.notebook_utils import (    display_response,    display_image,)from llama_index.indices.query.query_transform.base import (    ImageOutputQueryTransform,)filename_fn = lambda filename: {\"file_name\": filename}llama_reader = SimpleDirectoryReader(    input_dir=\"/content/llama\",    file_metadata=filename_fn,)llama_documents = llama_reader.load_data()llama_index = VectorStoreIndex.from_documents(llama_documents)\n\nStep 4: Query Transformations\n\nfrom llama_index.query_engine import TransformQueryEnginequery_engine = llama_index.as_query_engine(similarity_top_k=2)query_engine = TransformQueryEngine(    query_engine, query_transform=ImageOutputQueryTransform(width=400))llama_response = query_engine.query(    \"Show an image to illustrate how tree index works and explain briefly\",)display_response(llama_response)#OutputFinal Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works.tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance.when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter.for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent.the following image illustrates how a tree index works:! Tree Index Examplein this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\".i hope this helps clarify how tree index works!\n\nStep 5: Lets read the receipts\n\nfrom llama_index.readers.file.base import DEFAULT_FILE_READER_CLSfrom llama_index.readers.file.image_reader import ImageReaderimage_parser =ImageReader(    keep_image=True,    parse_text=True    )file_extractor = DEFAULT_FILE_READER_CLSfile_extractor.update({    \".jpg\": image_parser,    \".png\": image_parser,    \".jpeg\": image_parser,    })receipt_reader = SimpleDirectoryReader(    input_dir=\"/content/data\",    file_metadata=filename_fn,    file_extractor=file_extractor,)receipt_documents = receipt_reader.load_data()print(len(receipt_documents))#Output3\nreceipts_index = VectorStoreIndex.from_documents(receipt_documents)from llama_index.query_engine import TransformQueryEnginequery_engine = receipts_index.as_query_engine()receipts_response = query_engine.query(    \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\",)display_response(receipts_response)# Output Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.\nConclusion\n\n  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\n  in image-based document extraction. Beyond addressing OCR\u2019s inherent\n  challenges, it enhances the precision and efficiency of data extraction from\n  images, fostering improved productivity and decision-making in\n  document-focused workflows.\n\n\u201cStay connected and support my work through various platforms:\n\n\nGitHub: For\n    all my open-source projects and Notebooks, you can visit my GitHub profile\n    at\n    https://github.com/andysingal. If you find my content valuable, don\u2019t hesitate to leave a star.\n  \n\n    Patreon: If you\u2019d like to provide additional support, you can consider\n    becoming a patron on my Patreon page at\n    https://www.patreon.com/AndyShanu.\n  \n\nMedium: You\n    can read my latest articles and insights on Medium at\n    https://medium.com/@andysingal.\n  \n\nThe Kaggle:\n    Check out my Kaggle profile for data science and machine learning projects\n    at\n    https://www.kaggle.com/alphasingal.\n  \n\nHugging Face:\n    For natural language processing and AI-related projects, you can explore my\n    Huggingface profile at\n    https://huggingface.co/Andyrasika.\n  \n\n    YouTube: To watch my video content, visit my YouTube channel at\n    https://www.youtube.com/@andy111007.\n  \n\n    LinkedIn: To stay updated on my latest projects and posts, you can follow me\n    on LinkedIn. Here is the link to my profile:\n    https://www.linkedin.com/in/ankushsingal/.\"\n\n\n\n  Requests and questions: If you have a project in mind that you\u2019d like me to\n  work on or if you have any questions about the concepts I\u2019ve explained, don\u2019t\n  hesitate to let me know. I\u2019m always looking for new ideas for future Notebooks\n  and I love helping to resolve any doubts you might have.\n\n\n  Remember, each \u201cLike\u201d, \u201cShare\u201d, and \u201cStar\u201d greatly contributes to my work and\n  motivates me to continue producing more quality content. Thank you for your\n  support!\n\n\n  If you enjoyed this story, feel free\n  to subscribe\n  to Medium, and you will get notifications when my new articles will be\n  published, as well as full access to thousands of stories from other authors.\n\nResource:\n\n\nData used for above code\n\n\nllama-index\n\n\n",
            "related_posts": [
                "Agentic RAG With LlamaIndex",
                "How to train a custom GPT on your data with EmbedAI + LlamaIndex",
                "Becoming Proficient in Document Extraction",
                "LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases"
            ],
            "link": "https://www.llamaindex.ai/blog//becoming-proficient-in-document-extraction-32aa13046ed5"
        },
        {
            "date": "Jun 6, 2023",
            "author": "Jerry Liu",
            "title": "Building the data framework for LLMs",
            "tags": [
                "ai",
                "fundraising",
                "seed-investment",
                "machine-learning",
                "nlp"
            ],
            "content": "Today is an exciting day for LlamaIndex, and a big milestone in my personal journey with generative AI. I\u2019ve followed generative models for most of my academic/professional career \u2014 from my research on GANs/sensor compression to following Transformers/GPT developments. It became increasingly clear that as these models got bigger/better, they were evolving from knowledge generators to intelligent engines that could reason/act over new information.I formalized some of these key intuitions more concretely:LLMs are fantastic reasoning engines, capable of question-answering, summarization, planning, and more. They had the promise of becoming the \u201cneural\u201d compute unit at the core of a new age of AI-enabled software.Yet, LLMs inherently have no awareness of your own data.No one really knew the best practices for feeding your data into the LLM. Models had limited context windows and were expensive to finetune.If we could offer a toolkit to help set up the data architecture for LLM apps, then we could enable anyone to build LLM-powered knowledge workers and transform the way that software is written over private data. LLM-enabled software requires new infrastructure tooling over your data and has significant implications for the modern software data stack.Determined to tackle this challenge, I built GPT Index (which we later rebranded to LlamaIndex), an initial exploratory effort to organize and retrieve information using LLMs. (first Tweet is here!)It happened at the perfect time. Since last November, there has been an explosion in developer interest in building applications on top of LLMs. Most developers were figuring out ways to leverage the reasoning capabilities of LLMs on top of their own private data. Just two months in, I joined forces with Simon Suo, a brilliant AI technologist and my former colleague, and we evolved LlamaIndex from an exploratory project into a comprehensive framework designed to connect a user\u2019s private data with LLMs. It gained recognition within the AI community, captivating the attention of hackers, developers, and industry experts alike. In just six months, the project garnered an impressive following, with 16K Github Stars, 20K Twitter followers, 200K monthly downloads, and 6K active Discord users. Companies like Instabase, Front, and Uber started experimenting with LlamaIndex on top of their data.Some initial stacks started to emerge \u2014 for instance a common paradigm for building QA systems and chatbots was using a simple retrieval mechanism (top-k lookup from a vector database) with an LLM. LlamaIndex became viewed as a critical data orchestration component of the emerging LLM software landscape.Yet, it became clear that there were still significant technical challenges in the space of LLMs and data, and no one had the right answers. Even with the capable toolkit that we\u2019ve developed, we were just starting to scratch the surface on unlocking value from data.We are thrilled to share that LlamaIndex has secured $8.5 million in seed funding, led by Greylock, to help propel these efforts further. We\u2019re excited to work with Jerry Chen, Saam Motamedi, and Jason Risch on the Greylock team. Joining us in this exciting journey are Jack Altman (CEO of Lattice), Lenny Rachitsky (Lenny\u2019s Newsletter), Mathilde Collin (CEO of Front), Raquel Urtasun (CEO of Waabi), Joey Gonzalez (Berkeley), and many others. Their belief in our vision and the impact of LlamaIndex on the future of AI fuels our passion in solving these data + AI problems.Why LlamaIndex?Calling an LLM API is easy. Setting up a software system that can extract insights from your private data is harder.LlamaIndex is the advanced data framework for your LLM applications. It encompasses essential features allowing you to both manage and query your data.Data Management: Data ingestion, data parsing/slicing, data storage/indexing.Data Querying: Data retrieval, response synthesis, multi-step interactions over your data.LlamaIndex allows you to seamlessly integrate individual or enterprise data, including files, workplace apps, and databases, with LLM applications. We also offer an extensive array of integrations with other storage providers and downstream applications.100+ data loaders13+ vector database providersIntegrations with observability and experimentation frameworks (e.g. prompt tracking and system tracing)Integrations as a ChatGPT Retrieval Plugin or with PoeThe end result is that you can build a variety of amazing knowledge-intensive LLM applications. This ranges from a search engine over your data, to chatbot-style interfaces, to structured analytics helpers, to autonomous knowledge agents.What\u2019s next?There are so many things that we want to do to more fully realize our vision of unlocking LLM capabilities on top of your data. We\u2019ll broadly break this down into two categories: 1) our continued commitment to the open-source developer community, and 2) solving the data problem at scale for enterprises.Build the best open source data framework and developer communityAt a high-level, we want to continue iterating on our core feature capabilities, improving reliability, and satisfy both the needs of beginner and advanced users.Handle complex queries: We want to continue advancing the idea of \u201cquerying your data\u201d, whether it\u2019s through leveraging agent-style interactions for data retrieval and synthesis or program synthesis/DSL.Multi-modal data management: The future of foundation models is multimodal, not just contained to LLMs. There are many types of semi-structured data (e.g. semi-structured data like JSONs, yaml files) as well as \u201ccomplex\u201d unstructured data (audio, images, video) that we\u2019d love to have native support for.Better evaluation of LLM data systems: Properly evaluating LLM calls is already tricky (how do you best evaluate the quality of a generated output? Some libraries for handling this). This becomes even more tricky when you chain LLM calls within an overall data system. We want to invest efforts into this area to provide greater transparency to our users.Optimization of Latency/Cost: Users are faced with a plethora of choices when it comes to building a data-driven LLM app: the choice of LLM model, embedding model, vector database, etc. They must choose in accordance to a variety of factors, from latency and cost to privacy.Ease of use for both beginner users and advanced users: Our goal is to make the utilization of LLM capabilities accessible and user-friendly for individuals at all skill levels. We will develop clear tutorials, examples, and tools to simplify the learning curve and convey the value of all of our features.Solving the data problem at scale for EnterprisesAs we\u2019re iterating on the open-source project, we also want to identify the surrounding pain points in being able to build and deploy data-powered LLM apps to production. Our solution to this will build upon the success of our open-source project and be a natural evolution to the enterprise setting.Production-ready data ingestion and management: We want to handle data updates, data consistency, and scalability to larger volumes of data parsing. We also want to continue expanding on the right storage abstractions for multi-modal data.Scale to Large Data Volumes: Enterprises will typically have orders of magnitude more data than an individual. We want to invest in hosted infrastructure/deployment solutions around our core package so that you don\u2019t have to.Domain-specific LLM solutions: We want to offer packaged solutions to enable users to easily build LLM apps in different domains, from healthcare to finance to legal.If you\u2019re building LLM apps in the enterprise setting, we\u2019d love to chat and learn more about pain points + desired features! Check out our form here.Join the Llama Gang! \ud83e\udd99Join the Llama(Index) gang as we embark on this journey to solve problems at the intersection of LLMs and data. We are not just building tools for ML practitioners/researchers; the emerging LLM + data architecture stacks have implications for all of software development. As a result, we are operating at the intersection of incredibly fun and challenging problems from a variety of different fields:Foundation Model DevelopmentInformation Retrieval + Recommendation SystemsData SystemsMLOpsDevOpsInterested in checking out the project?Find our project on Github and check out our DocsCheck out our brand new landing page: https://llamaindex.aiJoin our Discord or Follow our TwitterAlso, we\u2019re hiring!We\u2019re looking for founding engineers \u2014 experience in one or more of AI, data systems, and full-stack/front-end is nice to have but not a requirement.If you\u2019re interested, fill out our form here.",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//building-the-data-framework-for-llms-bca068e89e0e"
        },
        {
            "date": "May 22, 2024",
            "author": "MyMagic AI",
            "title": "Batch inference with MyMagic AI and LlamaIndex",
            "tags": [
                "mymagic-ai",
                "batch-inference"
            ],
            "content": "This is a guest post from MyMagic AI.MyMagic AI allows processing and analyzing large datasets with AI. MyMagic AI offers a powerful API for batch inference (also known as offline or delayed inference) that brings various open-source Large Language Models (LLMs) such as Llama 70B, Mistral 7B, Mixtral 8x7B, CodeLlama70b, and advanced Embedding models to its users. Our framework is designed to perform data extraction, summarization, categorization, sentiment analysis, training data generation, and embedding, to name a few. And now it's integrated directly into LlamaIndex!Part 1: batch inferenceHow It Works:1. Setup:Organize Your Data in an AWS S3 or GCS Bucket:Create a folder using your user ID assigned to you upon registration.Inside that folder, create another folder (called a \"session\") to store all the files you need for your tasks.Purpose of the 'Session' Folder:This \"Session\" folder keeps your files separate from others, making sure that your tasks run on the right set of files. You can name your session subfolder anything you like.Granting Access to MyMagic AI:To allow MyMagic AI to securely access your files in the cloud, follow the setup instructions provided in the MyMagic AI documentation.2. Install: Install both MyMagic AI\u2019s API integration and LlamaIndex library:pip install llama-index\npip install llama-index-llms-mymagic3. API Request: The llamaIndex library is a wrapper around MyMagic AI\u2019s API. What it does under the hood is simple: it sends a POST request to the MyMagic AI API while specifying the model, storage provider, bucket name, session name, and other necessary details.import asyncio\nfrom llama_index.llms.mymagic import MyMagicAI\n\nllm = MyMagicAI(\n    api_key=\"user_...\", # provided by MyMagic AI upon sign-up\n    storage_provider=\"s3\",\n    bucket_name=\"batch-bucket\", # you may name anything\n    session=\"my-session\",\n    role_arn=\"arn:aws:iam::<your account id>:role/mymagic-role\",\n    system_prompt=\"You are an AI assistant that helps to summarize the documents without essential loss of information\", # default prompt at https://docs.mymagic.ai/api-reference/endpoint/create\n    region=\"eu-west-2\",\n)We have designed the integration to allow the user to set up the bucket and data together with the system prompt when instantiating the llm object. Other inputs, e.g. question (i.e. your prompt), model and max_tokens are dynamic requirements when submitting complete and acomplete requests.resp = llm.complete(\n    question=\"Summarise this in one sentence.\",\n    model=\"mixtral8x7\", \n    max_tokens=20,  # default is 10\n)\nprint(resp)\nasync def main():\n    aresp = await llm.acomplete(\n        question=\"Summarize this in one sentence.\",\n        model=\"llama7b\",\n        max_tokens=20,\n    )\n    print(aresp)\n\nasyncio.run(main())This dynamic entry allows developers to experiment with different prompts and models in their workflow while also controlling for model output to cap their spending limit. MyMagic AI\u2019s backend supports both synchronous requests (complete) and asynchronous requests (acomplete). It is advisable, however, to use our async endpoints as much as possible as batch jobs are inherently asynchronous with potentially long processing times (depending on the size of your data).Currently, we do not support chat or achat methods as our API is not designed for real-time interactive experience. However, we are planning to add those methods in the future that will function in a \u201cbatch way\u201d. The user queries will be aggregated and appended as one prompt (to give the chat context) and sent to all files at once.Use CasesWhile there are myriads of use cases, here we provide a few to help motivate our users. Feel free to embed our API in your workflows that are good fit for batch processing.1. ExtractionImagine needing to extract specific information from millions of files stored in a bucket. Information from all files will be extracted with one API call instead of a million sequential ones.2. ClassificationFor businesses looking to classify customer reviews such as positive, neutral, and negative. With one request you can start processing the requests over the weekend and get them ready by Monday morning.3. EmbeddingEmbedding text files for further machine learning applications is another powerful use case of MyMagic AI's API. You will be ready for your vector db in a matter of days not weeks.4. Training (Fine-tuning) Data GenerationImagine generating thousands of synthetic data for your fine-tuning tasks. With MyMagic AI\u2019s API, you can reduce the generation time by a factor of 5-10x compared to GPT-3.5.5. TranscriptionMyMagic AI\u2019s API supports different types of files, so it is also easy to batch transcribe many mp3 or mp4 files in your bucket.Part 2: Integration with LlamaIndex\u2019s RAG PipelineThe output from batch inference processes, often voluminous, can seamlessly integrate into LlamaIndex's RAG pipeline for effective data storage and retrieval.This section demonstrates how to use the Llama3 model from the Ollama library coupled with BGE embedding to manage information storage and execute queries. Please ensure the following prerequisites are installed and Llama3 model is pulled:pip install llama-index-embeddings-huggingface\ncurl -fsSL https://ollama.com/install.sh | sh\nollama pull llama3For this demo, we have run a batch summarization job on 5 Amazon reviews (but this might be millions in some real scenarios) and saved the results as reviews_1_5.json:{\n  \"id_review1\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes a family with a young boy who believes there is a zombie in his closet, while his parents are constantly fighting. The movie is criticized for its inconsistent genre, described as a slow-paced drama with occasional thriller elements. The review praises the well-playing parents and the decent dialogs but criticizes the lack of a boogeyman-like horror element. The overall rating is 3 out of 10.\"\n  },\n  \"id_review2\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document is a positive review of a light-hearted Woody Allen comedy. The reviewer praises the witty dialogue, likable characters, and Woody Allen's control over his signature style. The film is noted for making the reviewer laugh more than any recent Woody Allen comedy and praises Scarlett Johansson's performance. It concludes by calling the film a great comedy to watch with friends.\"\n  },\n  \"id_review3\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes a well-made film about one of the great masters of comedy, filmed in an old-time BBC fashion that adds realism. The actors, including Michael Sheen, are well-chosen and convincing. The production is masterful, showcasing realistic details like the fantasy of the guard and the meticulously crafted sets of Orton and Halliwell's flat. Overall, it is a terrific and well-written piece.\"\n  },\n  \"id_review4\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"Petter Mattei's 'Love in the Time of Money' is a visually appealing film set in New York, exploring human relations in the context of money, power, and success. The characters, played by a talented cast including Steve Buscemi and Rosario Dawson, are connected in various ways but often unaware of their shared links. The film showcases the different stages of loneliness experienced by individuals in a big city. Mattei successfully portrays the world of these characters, creating a luxurious and sophisticated look. The film is a modern adaptation of Arthur Schnitzler's play on the same theme. Mattei's work is appreciated, and viewers look forward to his future projects.\"\n  },\n  \"id_review5\": {\n    \"query\": \"Summarize the document!\",\n    \"output\": \"The document describes the TV show 'Oz', set in the Oswald Maximum Security State Penitentiary. Known for its brutality, violence, and lack of privacy, it features an experimental section of the prison called Em City, where all the cells have glass fronts and face inwards. The show goes where others wouldn't dare, featuring graphic violence, injustice, and the harsh realities of prison life. The viewer may become comfortable with uncomfortable viewing if they can embrace their darker side.\"\n  },\n  \"token_count\": 3391\n}\nNow let\u2019s embed and store this document and ask questions using LlamaIndex\u2019s query engine. Bring in our dependencies:import os\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom llama_index.llms.ollama import OllamaConfigure the embedding model and Llama3 modelembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\nllm = Ollama(model=\"llama3\", request_timeout=300.0)Update settings for the indexing pipeline:Settings.llm = llm\nSettings.embed_model = embed_model\nSettings.chunk_size = 512 # This parameter defines the size of text chunks for embedding\n\ndocuments = SimpleDirectoryReader(\"reviews_1_5.json\").load_data() #Modify path for your caseNow create our index, our query engine and run a query:index = VectorStoreIndex.from_documents(documents, show_progress=True)\n\nquery_engine = index.as_query_engine(similarity_top_k=3)\n\nresponse = query_engine.query(\"What is the least favourite movie?\")\nprint(response)Output:Based on query results, the least favourite movie is: review 1 with a rating of 3 out of 10.Now we know that the review 1 is the least favorite movie among these reviews.Next StepsThis shows how batch inference combined with real-time inference can be a powerful tool for analyzing, storing and retrieving information from massive amounts of data. Get started with MyMagic AI\u2019s API today!",
            "related_posts": [
                "Batch inference with MyMagic AI and LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//batch-inference-with-mymagic-ai-and-llamaindex"
        },
        {
            "date": "Sep 12, 2023",
            "author": "Jerry Liu",
            "title": "LlamaIndex + Vectara",
            "tags": [
                "llamaindex",
                "nlp",
                "ai",
                "llm",
                "search"
            ],
            "content": "(co-authored by Ofer Mendelevitch, head of Developer Relations at Vectara, and Logan Markewich, founding engineer at LlamaIndex)IntroductionVectara is a trusted GenAI platform. Exposing a set of easy to use APIs, Vectara\u2019s platform reduces the complexity involved in developing Grounded Generation (aka retrieval-augmented-generation) applications, and managing the LLM infrastructure that\u2019s required to deploy them at scale in production.Today we\u2019re happy to announce Vectara\u2019s integration with LlamaIndex via a new type of Index: the Managed Index. In this blog post, we\u2019ll dig deeper into how a ManagedIndex works, and show examples of using Vectara as a Managed Index.What is Vectara?Vectara is an end-to-end platform that offers powerful generative AI capabilities for developers, including:Data processing. Vectara supports various file types for ingestion including markdown, PDF, PPT, DOC, HTML and many others. At ingestion time, the text is automatically extracted from the files, and chunked into sentences. Then a vector embedding is computed for each chunk, so you don\u2019t need to call any additional service for that.Vector and text storage. Vectara hosts and manages the vector store (where the document embeddings are stored) as well as the associated text. Developers don\u2019t need to go through a long and expensive process of evaluation and choice of vector databases. Nor do they have to worry about setting up that Vector database, managing it in their production environment, re-indexing, and many other DevOps considerations that become important when you scale your application beyond a simple prototype.Query flow. When issuing a query, calculating the embedding vector for that query and retrieving the resulting text segments (based on similarity match) is fully managed by Vectara. Vectara also provides a robust implementation of hybrid search and re-ranking out of the box, which together with a state of the art embedding model ensures the most relevant text segments are returned in the retrieval step.Security and Privacy. Vectara\u2019s API is fully encrypted in transit and at rest, and supports customer-managed-keys (CMK). We never train on your data, so you can be sure your data is safe from privacy leaks.Figure 1: Vectara\u2019s API platform for \u201cGrounded Generation\u201dThe nice thing is that all this complexity is fully managed by Vectara, taking a lot of the heavy lifting off of the developer\u2019s shoulders, so that they don\u2019t have to specialize in the constantly evolving skills of large language models, embedding models, vector stores and MLOps.From VectorStoreIndex to ManagedIndexLlamaIndex is a data framework for building LLM applications. It provides a set of composable modules for users to define a data pipeline for their application. This consists of data loaders, text splitters, metadata extractors, and vector store integrations.A popular abstraction that users use is the VectorStoreIndex, providing integrations with different vector databases.However, a challenge here is that users still need to carefully define how to load data, parse it, as well as choose an embedding model and a vector DB to use. Since Vectara abstracts away this complexity, the Vectara and LlamaIndex teams jointly came up with a new abstraction: The ManagedIndex.As shown in figure 2, when ingesting data into a VectorStoreIndex, data is processed locally taking advantage of multiple components like Data Connectors and Node parsers.Figure 2: typical flow of document processing in LlamaIndex for a VectorStoreIndexWith Vectara (figure 3), this whole flow is replaced by a single \u201cindexing\u201d API call , and all this processing is instead performed in the backend by the Vectara platform.Figure 3: pre-processing with the VectaraIndex simplifies the complex ingest flow to a single step.How does the VectaraIndex work?Let\u2019s take a look at a simple question-answering example using VectaraIndex, in this case asking questions from one of Paul Graham\u2019s Essays.Step 1: Setup your Vectara account and IndexTo get started, follow our quickstart guide: signup for a free Vectara account, create a corpus (index), and generate your API key.Then setup your Vectara customer_id, corpus_id and api_key as environment variables, so that the VectaraIndex can access those easily, for example:VECTARA_CUSTOMER_ID=<YOUR_CUSTOMER_ID>\nVECTARA_CORPUS_ID=<YOUR_CORPUS_ID>\nVECTARA_API_KEY=\"zwt_RbZfGT\u2026\"Step 2: Create a VectaraIndex instance with LlamaIndexBuilding the Vectara Index is extremely simple:from llama_index import SimpleDirectoryReader\nfrom llama_index.indices import VectaraIndex\nFrom pprint Import pprint\n\ndocuments = SimpleDirectoryReader(\"paul_graham\").load_data()\nindex = VectaraIndex.from_documents(documents)Here we load Paul Graham\u2019s Essay using LlamaIndex\u2019s SimpleDirectoryReader into a single document. The from_documents() constructor is then used to generate the VectaraIndex instance.Unlike the common flow that uses LlamaIndex tools like data connectors, parsers and embedding models to process the input data, with VectaraIndex the documents are sent directly to Vectara via the Indexing API. Vectara\u2019s platform then processes, chunks, encodes and stores the text and embeddings into a Vectara corpus, making it available instantly for querying.Step 3: QueryAfter the data is fully ingested, you can take advantage of the rich set of query constructs built into LlamaIndex. For example let\u2019s use the index to retrieve the top-k most relevant nodes:retriever = index.as_retriever(similarity_top_k=7)\n# docs should contain the 7 most relevant documents for the query\ndocs = retriever.retrieve(\u201cWhat is the IBM 1401?\u201d)\npprint(docs[0].node.text)(\u2018My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \u201cdata processing.\u201d This was in 9th grade, so I was 13 or 14. The school district\u2019s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it.\u2019)Here we printed out the top matching Node given the query \u201cwhat is the IBM 1401?\u201d This in turn results in a call to Vectara\u2019s Search API that returns the top-k matching document segments.Those are transformed into NodeWithScore objects and thus can be used as usual with the rest of the LlamaIndex querying tools. For example we can use LlamaIndex\u2019s query_engine() to convert the retrieved matching document segments (nodes) into a comprehensive response to our question:# Get an answer to the query based on the content of the essay\nresponse = index.as_query_engine().query(\"What can the 1401 do?\")\nprint(response)The 1401 was used for \u201cdata processing\u201d and could load programs into memory and run them. It had a card reader, printer, CPU, disk drives, and used an early version of Fortran as the programming language. The only form of input to programs was data stored on punched cards.Why Use VectaraIndex with LlamaIndex?By adding the concept of a \u201cManaged Index\u201d and the VectaraIndex to LlamaIndex, users can continue to take advantage of the tools and capabilities offered by the LlamaIndex library while integrating with a generative AI platform like Vectara.Retrievers and Query Engines are just the tip of the iceberg. Using a managed index with Vectara, developers have full access to advanced utilities like routers, advanced query engines, data agents, chat engines, and more! Being able to retrieve context using Vectara empowers developers to build these complex applications using LlamaIndex components.For example, in the following code we use the chat engine in LlamaIndex to quickly create a chat interaction using our VectaraIndex:chat = index.as_chat_engine(chat_mode='context')\nres = chat.chat(\"When did the author learn Lisp?\")\nprint(res.response)\u201cThe author learned Lisp in college.\u201dA follow up question retains the chat history for context, as you might expect:chat.chat(\"and was it helpful for projects?\").response\u201cYes, learning Lisp was helpful for the author\u2019s projects. They used Lisp in both Viaweb and Y Combinator, indicating its usefulness in their work.\u201dchat.chat(\"what was a distinctive characteristic of that programming language?\").response\u201cA distinctive characteristic of Lisp is that its core is a language defined by writing an interpreter in itself. It was originally designed as a formal model of computation and an alternative to the Turing machine. This self-referential nature of Lisp sets it apart from other programming languages.\u201dFor more information on how to use chat-engines, check out the documentation, and for more information on other query capabilities with LlamaIndex, check out the full documentation here.SummaryLlamaIndex makes it super easy to populate VectaraIndex with content from any document or data source, while utilizing the Vectara service for managing the document processing, chunking, embedding and making all of this data available for advanced retrieval in query time using the LlamaIndex library.VectaraIndex is based on the new LlamaIndex Managed Index abstraction, which better supports GenAI platforms like Vectara, and enables additional vendors who also provide end-to-end platforms to join in.To get started with Vectara and LlamaIndex you can follow the Vectara quickstart guide to setup your account, and the examples above with your own data.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-vectara-7a3889cd34cb"
        },
        {
            "date": "Jan 2, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201301\u201302",
            "tags": [
                "llamaindex",
                "llm",
                "ai",
                "newsletter",
                "rag"
            ],
            "content": "Hello, Llama Lovers \ud83e\udd99,Happy New Year! As we step into 2024, we\u2019re thrilled to bring you a special edition of our newsletter, packed with updates from the last two weeks of 2023. This edition is brimming with the latest features, community demos, courses, insightful tutorials, guides, and webinars that we\u2019ve curated for you.Have you been working on an interesting project, written an engaging article, or created a video? We can\u2019t wait to hear about it! Please share your work with us at news@llamaindex.ai. Don\u2019t forget to subscribe to our newsletter via our website to receive all these exciting updates directly in your inbox.\ud83e\udd29 First, the highlights:LLMCompiler Implementation: A SOTA agent implementation for faster, efficient handling of complex queries. Notebook, Tweet.MultiDocAutoRetrieverPack: A RAG template for structured retrieval and dynamic responses to large documents and metadata. Tweet, LlamaPack.Structured Hierarchical RAG: New RAG technique for optimized retrieval over multiple documents, ensuring precise, relevant responses. Docs, Tweet.Custom Agents: A simple abstraction for custom agent reasoning loops, enabling easy integration with RAG, SQL, and other systems, and enhancing response refinement for complex queries. Docs, Tweet.New lower-level agent API: For enhanced transparency, debuggability, and control, supporting step-wise execution and task modification. Docs, Tweet.\u2728 Feature Releases and Enhancements:We have introduced a simple abstraction for building custom agent reasoning loops, surpassing prepackaged frameworks like ReAct. This tool allows for easy integration with RAG, SQL, or other systems, and we demonstrated how to build an agent with retry logic for routers, enhancing its ability to manage complex, multi-part questions and refine query responses. Docs, Tweet.We have implemented the LLMCompiler project, a SOTA agent framework enabling DAG-based planning and parallel function execution. This surpasses traditional sequential methods in speed, allowing for quicker and more efficient handling of complex queries in any LLM and data pipeline. Notebook, Tweet.We have introduced MultiDocAutoRetrieverPack, a RAG template for efficiently handling large documents and metadata, offering structured retrieval and dynamic responses tailored to specific queries. Tweet, LlamaPack.We have introduced a Structured Hierarchical RAG technique, optimizing RAG over multiple documents. It involves modeling documents as structured metadata for auto-retrieval, indexed in a vector database. This method dynamically selects documents based on inferred properties and performs recursive retrieval within each document for precise, relevant responses in your RAG pipeline. Docs, Tweet.We have launched a new feature for advanced RAG that allows step-wise feedback for complex query executions, improving interpretability and control. This is particularly beneficial for weaker models that struggle with multi-part tasks. We also introduced a step-by-step chat interface for enhanced user interaction and control. Notebook, Tweet.We have integrated with OpenRouterAI, offering a unified API for easy LLM access, cost efficiency, and reliable fallback options. OpenRouterAI allows users to compare costs, latency, and throughput for various models, like mixtral-8x7b, directly on their platform. Notebook, Tweet.We have introduced a new lower-level agent API that enhances transparency, debuggability, and control. This API allows for granular control over agents, decouples task creation from execution, and supports step-wise execution. It also enables viewing each step, upcoming steps, and soon, modifying intermediate steps with human feedback. Docs, Tweet.\ud83d\udc40 Community Demos:Automated LeetCode Crash Course: The Project integrates advanced ML with traditional algorithms to streamline LeetCode study for technical interviews. It involves extracting and summarizing LeetCode problems using an LLM, organizing these summaries in a vector store, and employing scikit-learn for clustering. Blog, Code.RAG Assisted Auto Developer: A project by Ocean Li for building a devbot that understands and writes code. It integrates various tools: LlamaIndex for indexing codebases, Autogen / OpenAI Code Interpreter for code writing and testing, and lionagi.ai for orchestration. Notebook.\ud83d\udcda Courses:We\u2019ve partnered with ActiveLoop AI to provide a free course on retrieval-augmented generation for production, featuring 33 lessons, 7 hands-on assignments, and a certification upon completion.Beginner-friendly course from IBM Skills Network on using LlamaIndex with IBM Watsonx to create effective product recommendations.\ud83d\uddfa\ufe0f Guides:Guide to Semi-Structured Image QA with Gemini: Learn to extract data from unlabeled images and query it, using multi-modal models and advanced retrieval techniques, as demonstrated with the SROIE v2 dataset which contains images of receipts/invoices.Guide to Advanced RAG Concepts: A comprehensive survey by Ivan Ilin, covering twelve core concepts including chunking, hierarchical indexing, query rewriting, and more. Each section provides resources and guides from our system for deeper understanding and practical application.Guide to Building Hybrid Search: Learn to create a hybrid search for RAG from scratch. The process involves generating sparse vectors, fusing sparse and dense queries, and implementing this in a Qdrant engine database for effective RAG integration.Guide to Building Structured Retrieval with LLMs: Set up auto-retrieval in Pinecone vector database, monitor prompts with Arize AI Phoenix, and tailor prompts for specific queries to enhance your document handling and structured data analysis.Guide on Evaluating LLM Evaluators: our new evaluation method and dataset bundle, are designed to benchmark LLMs as evaluators against human annotations. This involves comparing LLM judge predictions (1\u20135 score) with ground-truth judgments, using metrics like Correlation, Hamming Distance, and Agreement Rate.\u270d\ufe0f Tutorials:Ryan Nguyen tutorial on Processing Tables in RAG Pipelines with LlamaIndex and UnstructuredIO.Wenqi Glantz tutorial on Safeguarding RAG Pipelines: A Step-by-Step Guide to Implementing Llama Guard with LlamaIndex.Wenqi Glantz tutorial on 10+ Ways to Run Open-Source Models with LlamaIndex.Jina AI tutorial on enhancing RAG applications by integrating Jina v2 embeddings with LlamaIndex and Mixtral LLM via Hugging Face.Ankush Singal tutorial on Benchmarking RAG Pipelines With A Evaluation Pack in Forward-Looking Active Retrieval Augmented Generation (FLARE).Laurie\u2019s tutorial on Effortlessly Running Mistral AI\u2019s Mixtral 8x7b: Learn to use OLLAMA with LlamaIndex for a one-line setup of a local, open-source retrieval-augmented generation app with API, featuring Qdrant engine integration for vector storage.Tomaz Bratanic tutorial on Multimodal RAG pipeline with LlamaIndex and Neo4j.Sudarshan Koirala video tutorial on using Mistral API with LlamaIndex.Chia Jeng Yang tutorial on Technical Considerations for Complex RAG.\ud83c\udfa5 Webinars:Webinar with Google Developers on advanced RAG applications and multi-modal settings with Google Gemini.Webinar of Jerry Liu with Louis-Fran\u00e7ois on the Future of AI: LlamaIndex, LLMs, RAG, Prompting, and more.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-01-02-f349db8c1842"
        },
        {
            "date": "Feb 13, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201302\u201313",
            "tags": [
                "newsletter",
                "llamaindex",
                "ai",
                "rag",
                "llm"
            ],
            "content": "Greetings, LlamaIndex Adventurers \ud83e\udd99,Welcome to an exhilarating week of discoveries at LlamaIndex! Our community\u2019s lively input and the abundance of learning tools await to supercharge your journey through LlamaIndex.Before we dive into the updates, we have two major announcements:LlamaIndex v0.10: Our latest open-source release marks a monumental step towards production readiness. With a new core package and hundreds of integrations and LlamaPacks now available as separate PyPi packages, we\u2019ve massively improved organization and version tracking. Major updates include the refactoring of LlamaHub into a central hub for all integrations and the deprecation of ServiceContext for an enhanced development experience. Blog, Migration Guide.Introducing Short Courses on Advanced RAG Development: Master complex RAG systems with our series, covering everything from unstructured data to agent integration. Learn through LlamaIndex Query Pipelines, from basic text-to-SQL to advanced query techniques, and build scalable RAG applications with hands-on guidance. Video1, Video2.Your creativity fuels our inspiration! We\u2019re excited to see any projects, articles, or videos you\u2019re passionate about. Share your incredible creations with us at news@llamaindex.ai. Haven\u2019t joined our newsletter yet? Make sure to subscribe on our website for the latest LlamaIndex updates delivered directly to your inbox.\ud83e\udd29 The highlights:Self-RAG: Introducing Self-RAG, now part of LlamaIndex as a LlamaPack. Boosts LLM training and RAG workflows with dynamic capabilities. Notebook, Tweet.LlamaIndex + FlowiseAI Integration: Seamlessly merge LlamaIndex with FlowiseAI for effortless, no-code RAG app development. Docs, Tweet.RAG Guide with MistralAI: MistralAI\u2019s new doc includes a RAG guide with LlamaIndex. Utilize Mistral-medium for enhanced RAG functions. Docs.\u2728 Feature Releases and Enhancements:We have introduced a seamless integration between LlamaIndex and FlowiseAI, enabling easy, no-code development of advanced RAG applications with a drag-and-drop interface for quick chatbot or agent integration. Docs, Tweet.We have introduced Self-RAG, a dynamic retrieval tool by Akari Asai\u2019s team, now available as a LlamaPack for easy integration, enhancing LLM training and RAG workflows with dynamic, iterative capabilities. Notebook, Tweet.We have introduced the RAG CLI tool that allows you to search any file on your filesystem using on-device language model embeddings, featuring the power of Mistral-7B and bge-m3 for an advanced, customizable experience. Docs, Tweet.We have launched full-stack agent servers with a single CLI command using create-llama from LlamaIndex, offering instant access to 50+ tools for any agent project. Tweet.We have introduced agents in LlamaIndex.TS, enabling advanced AI software development in TypeScript with features like function calling and multi-document handling. Blog, Docs, Tweet.DeepEval is integrated with LlamaIndex, significantly enhancing RAG evaluation capabilities and introducing unit testing for LlamaIndex apps in CI/CD environments. Docs.\ud83d\uddfa\ufe0f Guides:Guide to RAG with LlamaIndex in MistralAI\u2019s new documentation with Mistral-medium and Mistral embedding models.Guide to Building Agentic RAG to incorporate user feedback in real-time enhancing complex searches with a human-in-the-loop approach.Guide to Integrating Huggingface\u2019s New Messages API with OpenAI compatibility, simplifying the integration process for Inference Endpoints and Text Generation Inference.\u270d\ufe0f Tutorials:Plaban Nayak tutorial on Setting up Query Pipeline For Advanced RAG Workflow using LlamaIndex.Krish Naik tutorial on Step-by-Step Guide to Building a RAG LLM App with Llama2 and LlamaIndex.HelixML tutorial to Knowledge Memorization by fine-tuning Mistral-7B for enhanced knowledge memorization, offering a new way to reason across contexts without RAG\u2019s limitations.Wenqi Glantz tutorial on NeMo Guardrails, the Ultimate Open-Source LLM Security Toolkit.\ud83c\udfa5 Webinar:Webinar of Laurie with Ankit Khare(Rockset) delves into the essentials of RAG \u2014 its purpose, methodology, how LlamaIndex facilitates it, and exciting developments for 2024.Webinar with Zilong Wang, and Tianyang Liu on Advanced Tabular Data Understanding with LLMs.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex, even more, Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-02-13-26fa79601ba5"
        },
        {
            "date": "Jul 17, 2023",
            "author": "Adam Hofmann",
            "title": "Building Better Tools for LLM Agents",
            "tags": [
                "ai",
                "llamaindex",
                "llm",
                "agents",
                "nlp"
            ],
            "content": "Over the past month I\u2019ve been diving into the world of Large Language Model (LLM) Agents and building out LlamaIndex\u2019s library of tools for use with agents. I helped to lead the LlamaHub Tools effort as part of broader Data Agents launch last week.In the process of building out LlamaHub Tools I\u2019ve collected some techniques for creating effective and easy to use tools, and want to share some of my thoughts.Context on LlamaHub ToolsLlamaHub Tools allow LLMs like ChatGPT to connect to APIs and act on a user\u2019s behalf to create, read, update and delete data. Examples of tools that we\u2019ve put together include drafting and sending emails, reading and creating Google Calendar invites, searching Wikipedia, and that\u2019s just a few of the 15 tools we are releasing on launch.Overview of tool abstractionsSo how exactly do LlamaHub Tools work? The LlamaHub tool abstractions allow you to easily write Python functions that can be understood and called by Agents. Instead of trying to make an Agent do complicated mathematics for example, we can provide the Agent with a Tool that calls Wolfram Alpha and provides the result to the Agent:from llama_index.tools.base import BaseToolSpec\n\nQUERY_URL_TMPL = \"http://api.wolframalpha.com/v1/result?appid={app_id}&amp;i={query}\"\n\n# Inherit from the LlamaIndex BaseToolSpec abstraction\nclass WolframAlphaToolSpec(BaseToolSpec):\n\n  # Define the functions that we export to the LLM\n    spec_functions = [\"wolfram_alpha_query\"]\n\n  # Initialize with our wolfram alpha API key\n    def __init__(self, app_id: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize with parameters.\"\"\"\n        self.token = app_id\n  \n  # Our function to be called by the Agent\n  def wolfram_alpha_query(self, query: str):\n          \"\"\"\n          Make a query to wolfram alpha about a mathematical or scientific problem.\n  \n          Example inputs:\n              \"(7 * 12 ^ 10) / 321\"\n              \"How many calories are there in a pound of strawberries\"\n  \n          Args:\n              query (str): The query to be passed to wolfram alpha.\n  \n          \"\"\"\n          response = requests.get(QUERY_URL_TMPL.format(app_id=self.token, query=urllib.parse.quote_plus(query)))\n          return response.textThe above code is enough to define a LlamaIndex Tool that allows the Agent to query to Wolfram Alpha. No more incorrect guesses at math problems! We can initialize an instance of the Tool Spec like this:# Initialize an instance of the Tool\nwolfram_spec = WolframAlphaToolSpec(app_id=\"your-key\")\n# Convert the Tool Spec to a list of tools. In this case we just have one tool.\ntools = wolfram_spec.to_tool_list()\n# Convert the tool to an OpenAI function and inspect\nprint(tools[0].metadata.to_openai_function())Here\u2019s the cleaned up output of the print statement:{\n  'description': '\n    Make a query to wolfram alpha about a mathematical or scientific problem.\n  \n          Example inputs:\n              \"(7 * 12 ^ 10) / 321\"\n              \"How many calories are there in a pound of strawberries\"\n  \n          Args:\n              query (str): The query to be passed to wolfram alpha.',\n  'name': 'wolfram_alpha_query',\n  'parameters': {\n    'properties': {'query': {'title': 'Query', 'type': 'string'}},\n    'title': 'wolfram_alpha_query',\n    'type': 'object'\n  }\n}We can see that the docstring describing how to use the Tool get passed to the Agent. Additionally, the parameters, type info and function name are passed along to give the Agent a strong idea on how it can use this function. All of this information is essentially acting as the prompt for how the agent understands the tool.Inheriting from the BaseToolSpec class means it\u2019s very simple to write Tools for Agents to use. In fact, the above tool definition is only 9 lines of code, ignoring white space, imports and comments. We can easily get the function ready for Agents to use without any heavy boilerplate or modifications. Let\u2019s look at loading the Tool into an OpenAI Agent:agent = OpenAIAgent.from_tools(tools, verbose=True)\nagent.chat('What is (7 * 12 ^ 10) / 321')\n\"\"\" OUTPUT:\n=== Calling Function ===\nCalling function: wolfram_alpha_query with args: {\n  \"query\": \"(7 * 12 ^ 10) / 14\"\n}\nGot output: 30958682112\n========================\nResponse(response='The result of the expression (7 * 12 ^ 10) / 14 is 30,958,682,112.', source_nodes=[], metadata=None)\n\"\"\"And we can test out passing this query to ChatGPT without the tools:&gt; 'What is (7 * 12 ^ 10) / 321'\n\"\"\"\nTo calculate the expression (7 * 12^10) / 14, you need to follow the order of operations, which is parentheses, exponents, multiplication, and division (from left to right).\n\nStep 1: Calculate the exponent 12^10.\n12^10 = 619,173,642,24.\n\nStep 2: Multiply 7 by the result from Step 1.\n7 * 619,173,642,24 = 4,333,215,496,68.\n\nStep 3: Divide the result from Step 2 by 14.\n4,333,215,496,68 / 14 = 309,515,392,62.\n\nTherefore, the result of the expression (7 * 12^10) / 14 is 309,515,392,62.\n\"\"\"This example should show how easily you can write new Tools for use with Agents. For the rest of the blog post I\u2019ll be talking about tips and tricks I\u2019ve found to write more functional and effective tools. Hopefully by the end of the blog post you are excited to write and contribute some Tools of your own!Techniques for building better toolsBelow are a variety of tactics for writing more usable and functional tools to minimize friction when interfacing with the Agent. Not all of the tactics apply to every tool, but usually at least a few of the techniques below will prove valuable.Writing useful tool promptsHere\u2019s an example of the function signature and docstring for a tool that an Agent can call to create a draft email.def create_draft(\n        self,\n        to: List[str],\n        subject: str,\n        message: str\n    ) -&gt; str:\n        \"\"\"Create and insert a draft email.\n           Print the returned draft's message and id.\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            to (List[str]): The email addresses to send the message to, eg ['adam@example.com']\n            subject (str): The subject for the event\n            message (str): The message for the event\n        \"\"\"This prompt takes advantage of a few different patterns to ensure that the agent can use the tool effectively:Give a concise description of the function and its purposeInform the Agent on what data will be returned from this functionList the arguments that the function accepts, with descriptions and type informationGive example values for arguments with a specific format, eg adam@example.comTool prompts should be concise as to not take up too much length in context, but also informative enough that the agent can use the tool without making mistakes.Making tools tolerant of partial inputsOne way to help Agents make fewer mistakes is to write tools that are more tolerant of their inputs, for example by making inputs optional when the value can be inferred from somewhere else. Take the example of drafting an email, but this time let\u2019s consider a tool that updates a draft email:def update_draft(\n        self,\n        draft_id: str,\n        to: Optional[List[str]] = None,\n        subject: Optional[str] = None,\n        message: Optional[str] = None,\n    ) -&gt; str:\n        \"\"\"Update a draft email.\n           Print the returned draft's message and id.\n           This function is required to be passed a draft_id that is obtained when creating messages\n           Returns: Draft object, including draft id and message meta data.\n\n        Args:\n            draft_id (str): the id of the draft to be updated\n            to (Optional[str]): The email addresses to send the message to\n            subject (Optional[str]): The subject for the event\n            message (Optional[str]): The message for the event\n        \"\"\"The Gmail API requires all of the above values when updating a draft, however using just the draft_id we can fetch the current content of the draft and use the existing values as defaults if the Agent did not provide the values when updating the draft:def update_draft(...):\n  ...\n  draft = self.get_draft(draft_id)\n  headers = draft['message']['payload']['headers']\n  for header in headers:\n      if header['name'] == 'To' and not to:\n          to = header['value']\n      elif header['name'] == 'Subject' and not subject:\n          subject = header['value']\n    elif header['name'] == 'Message' and not message:\n      message = header['values']\n  ...By providing the above logic in the update_draft function, the Agent can invoke update_draft with only one of the fields (and the draft_id), and we can update the draft as the user expects. This means that in more circumstances the Agent can complete the task successfully, instead of returning an error or needing to ask for more information.Validating input and Agent error handlingDespite best efforts at prompting and tolerance, we can end up in circumstances where the Agent invokes a tool in a way that it can\u2019t complete the task at hand. However, we can detect this and prompt the Agent to recover the error on its own.For example, in the update_draft example above, what do we do if the agent calls the function without a draft_id? We could simply pass along the null value and return an error from the Gmail API library, but we could also detect that a null draft_id will invariably cause an error, and return a prompt for the agent instead:def update_draft(...):\n  if draft_id == None:\n    return \"You did not provide a draft id when calling this function. If you previously created or retrieved the draft, the id is available in context\"Now, if the Agent invokes update_draft without a draft_id , it is made aware of the exact mistake it made and given instructions on how it can correct the issue.In my experience working with this tool, the Agent will often immediately call the update_draft function in the correct way when receiving this prompt, or if there is no draft_id available, it will inform the user of the issue and ask the user for a draft_id. Either scenario is much better than crashing or returning an opaque error from a library to the user.Providing simple functions related to the toolAgents can struggle at what would otherwise be simple functions for a computer to calculate. For example, when building a tool for creating events in Google Calendar, a user may prompt the Agent with something like this:Create an event on my Calendar to discuss the Tools PR with adam@example.com tomorrow at 4pmCan you see the problem? If we try asking ChatGPT what day it is:agent.chat('what day is it?')\n# > I apologize for the confusion. As an AI language model, I don't have real-time data or access to the current date. My responses are based on the information I was last trained on, which is up until September 2021. To find out the current day, I recommend checking your device's clock, referring to a calendar, or checking an online source for the current date.Agents won\u2019t know what the current date is, and so the Agent would either call the function incorrectly, providing a string like tomorrow for the date, hallucinate a date sometime in the past based on when it was trained, or put the burden on the user to tell it the date. All of the above actions cause friction and frustration for the user.Instead, in the Google Calendar Tool Spec we provide a simple deterministic function for the agent to call if it needs to fetch the date:def get_date(self):\n        \"\"\"\n        A function to return todays date.\n        Call this before any other functions if you are unaware of the current date\n        \"\"\"\n        return datetime.date.today()Now, when the Agent tries to handle the prompt above, it can first call the function to get the date and then create the event as the user requested, inferring the date for \u201ctomorrow\u201d or \u201ca week from now\u201d. No errors, no guesses and no need for further user interaction!Returning prompts from functions that perform mutationsSome functions perform mutations to data in a way that it isn\u2019t clear what useful data can be returned from the function, back to the agent. For example, in the Google Calendar tool if an event is successfully created it doesn\u2019t make sense to return the content of the event back to the Agent, as the agent just passed in all of the information and thus has it in context.Generally with functions that are focused on mutations (create, update, delete) we can help the Agent understand its actions better by using the return value of these functions to further prompt the agent. For example, from the Google Calendar create_event tool we could do the following:def create_event(...):\n  ...\n  return 'Event created succesfully! You can move onto the next step.'  This helps the agent register that the action succeeded and encourages it to complete the action it was prompted for, especially if creating the google calendar event is only a single step in a multiple step instruction. We can still return ids as part of these prompts as well:def create_event(...):\n  ...\n  event = service.events().insert(...).execute()\n  return 'Event created with id {event.id}! You can move onto the next step.'Storing large responses in indices for the Agent to readOne consideration when building tools that has been mentioned already is the size of the context window the Agent has. Currently, LLMs tend to have context windows from 4k-16k tokens, however it can certainly be larger or smaller. If the size of the data that a tool would return is larger than the context window, the Agent will be unable to process the data and error out.One consideration when building tools that has been mentioned already is the size of the context window the Agent has. Currently, LLMs tend to have context windows from 4k-16k tokens, however it can certainly be larger or smaller. If the size of the data that a tool would return is larger than the context window, the Agent will be unable to process the data and error out.The only consideration that needs to be made when creating tools that might need to be wrapped by the LoadAndSearchTool, is they need to return a list of LlamaIndex documents. For a tool that returns a string, the only modification you need to make to have it be compatible with the LoadAndSearchTool is wrapping it in a document and an array:from llama_index.readers.schema.base import Document\n\n# Not compatible\ndef large_text_response_function():\n  ...\n  return result\n\n# LoadAndSearch compatible\ndef large_text_response_function():\n  ...\n  return [Document(text=result)]Verify how the Agent understands the toolA useful technique for debugging tools in development is to ask the Agent about its own tools: the tools it has available, what arguments the tools accept, what those arguments represent, and what the tool is used for. The responses of the Agent are useful in determining where your prompts might be lacking or helping pinpoint why an Agent is failing to successfully use a tool that you are developing.An example conversation debugging the Google Calendar Tool Spec, assuming you have setup an Agent similar to the example notebook:agent.chat('what tools do you have available')\n# I have the following tools available\n#   1. `load_data`: This tool allows me to load data from your calendar. It can retrieve a specified number of events starting from a given date.\n#   2. `create_event`: This tool allows me to create an event on your calendar. You can provide details such as the title, description, location, start datetime, end datetime, and attendees.\n#   3. `get_date`: This tool returns today's date. It can be used to ensure that the correct date is used when working with other tools.\n# Please let me know if you would like more information about any of these tools or if you have any specific requests.\nagent.chat('what arguments does the load_data tool take')\n# 'The `load_data` tool takes the following arguments:\n#   1. `number_of_results` (optional, default: 100): Specifies the number of events to return from your calendar.\n#   2. `start_date` (optional): Specifies the start date from which to retrieve events. It can be provided as a string in ISO format (e.g., \"2023-07-05\") or as a `datetime.date` object.\n# You can use these arguments to customize the number of events and the start date for retrieving data from your calendar.Concluding thoughtsBuilding tools for Agents requires you to think critically about how users will interact with the Agent, and to try and anticipate any possible usage patterns. Building well thought out tools that can be resilient to errors and issues requires careful consideration and testing of possible use cases. I hope that these reflections and techniques assist you in creating new tools for Agents, and don\u2019t forget to share your tools on LlamaHub.",
            "related_posts": [
                "Building a multi-agent concierge system",
                "Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems",
                "Automate online tasks with MultiOn and LlamaIndex",
                "LlamaIndex Newsletter 2024-04-02"
            ],
            "link": "https://www.llamaindex.ai/blog//building-better-tools-for-llm-agents-f8c5a6714f11"
        },
        {
            "date": "Jun 11, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-06-11",
            "tags": [],
            "content": "Hello Llama Fans\ud83e\udd99Step into this week's edition of the LlamaIndex newsletter, where we bring you a slew of exciting updates, in-depth guides, demos, enriching educational tutorials, and webinars designed to enhance your experience and understanding of our platforms and tools.\ud83e\udd29\u00a0The highlights:Enhanced Memory Modules: New memory modules in LlamaIndex boost agentic RAG capabilities with Vector Memory for message storage and retrieval, and Simple Composable Memory for integrating multiple memory sources. Notebook1, Notebook2, Tweet.Create-llama and E2B Integration: Launched integration turns agents into advanced data analysts, enabling Python coding for data analysis and generating detailed files like graph images. Tweet.LlamaParse and Knowledge Graphs: Guide on integrating LlamaParse with Knowledge Graphs to develop RAG pipelines and agents for complex query handling.Prometheus-2 RAG Evaluation: Guide on using Prometheus-2, an affordable, transparent LLM based on Mistral models for effective RAG application evaluation with customized criteria.Agentic RAG : Video tutorial on Agentic RAG covering memory, planning, and reasoning, enhancing knowledge retrieval and agent capabilities.\u2728 Feature Releases and Enhancements:We have introduced new memory modules in LlamaIndex for enhancing agentic RAG capabilities. The Vector Memory module enables storage and retrieval of user messages using vector search, while the Simple Composable Memory module allows for integration of multiple memory sources. Notebook1, Notebook2, Tweet.We have launched an integration between Create-llama and E2B\u2019s sandbox, transforming agents into powerful data analysts. This new feature allows agents to write Python code for data analysis and return comprehensive files, like graph images, enhancing the scope of what agents can accomplish. Tweet.We have launched an integration with Nomic-Embed-Vision that transforms Nomic-Embed-Text into a multimodal embedding that excels in handling image, text, and combined tasks, outperforming OpenAI CLIP with open access for all. Notebook.\ud83d\uddfa\ufe0f Guides:Guide to Integrating LlamaParse with Knowledge Graphs to develop a RAG pipeline for sophisticated query retrieval, and create an agent capable of answering complex queries effectively.Guide to Using Prometheus-2 for RAG Evaluation for assessing RAG applications, built on Mistral base models, it offers an affordable and transparent solution for evaluation, capable of direct assessments, pairwise rankings, and tailored criteria, ensuring alignment with human judgments.Guide to Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively.\ud83d\udda5\ufe0f\u00a0Demos:Laurie Voss\u2019s LLM-powered file organizer project that categorizes files into folders based on LLM-decided categories without renaming them, ensuring important filenames remain intact. It organizes your files in multiple passes to balance folder sizes, resulting in descriptive yet practical folder names to help you find files easily.\u270d\ufe0f Tutorials:Laurie Voss\u2019s video tutorial on transitioning from basic RAG to fully agentic knowledge retrieval, featuring real-world code examples that cover routing, memory, planning, tool use, and advanced agentic reasoning methods like Chain of Thought and Tree of Thought, along with insights into observability, controllability, and customizability.Prince krampah's tutorials on Agentic RAG Systems, offering comprehensive insights into advanced system building with detailed explanations on router query engines, function calling, and multi-step reasoning across complex documents.kingzzm\u2019s tutorial on Three Forms of Query Rewriting for RAG to enhance RAG pipelines with techniques like sub-question decomposition, HyDE for aligning questions with embedding semantics, and step-back prompting for tackling complex queries more effectively.Rajdeep Borgohain's tutorial to build a customer-support voicebot with advanced features like speech-to-text and text-to-speech, integrated into a RAG pipeline for efficient handling of customer support exchanges using Inferless, LlamaIndex, faster-whisper, Piper, and Pinecone.Pavan Mantha's tutorial on securing RAG apps using Azure for application security, including identity management, secure key storage, and managed Qdrant.\ud83d\udcf9\u00a0Webinar:Join us for our webinar with Tomaz Bratanic from Neo4j on LlamaIndex property graph for insights into high-level and low-level graph construction, retrieval, and knowledge graph agents.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-06-11"
        },
        {
            "date": "Feb 27, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024\u201302\u201327",
            "tags": [
                "llm",
                "ai",
                "rag",
                "newsletter",
                "llamaindex"
            ],
            "content": "Yo, LlamaIndex Fans \ud83e\udd99,Dive into a week brimming with thrilling developments at LlamaIndex! The dynamic input from our community and our rich selection of learning materials are all set to enhance your journey with LlamaIndex.Last week, the LlamaIndex ecosystem took a significant leap forward with the launch of LlamaCloud, a suite of advanced services designed for production-level\u00a0context enhancement in LLM and RAG applications:LlamaParse: Offers sophisticated parsing for complex documents, making it possible to answer detailed queries.Managed Ingestion and Retrieval API: Facilitates easier data management, connecting with over 150 sources and 40+ storage solutions.LlamaParse is now available for a public preview, primarily focusing on PDFs with a user cap, while the API is in a private preview for select enterprise partners. If you haven\u2019t explored these new features yet, we invite you to check them out for more details or to discuss commercial terms.Your innovation inspires us! We\u2019re eager to see the projects, articles, or videos that inspire you. Share your remarkable works with us at news@llamaindex.ai. And if you haven\u2019t already, subscribe to our newsletter on our website to receive the latest LlamaIndex updates straight to your inbox.\ud83e\udd29 The highlights:Enhanced RAG Retrieval with Sub-Document Summaries: Introducing a novel chunking method that improves RAG performance by incorporating hierarchical metadata into chunks, ensuring precise and context-aware information retrieval. Notebook, Tweet.MistralAI Cookbook: A comprehensive guide to leveraging the Mistral-Large model from MistralAI, featuring near-GPT-4 reasoning, function calling, and JSON output for cutting-edge applications. Docs, Tweet.Gemma Cookbook: A comprehensive guide to using Gemma, GoogleDeepMind\u2019s latest LLM offering, with options for 2B and 7B parameters, facilitating the development of local RAG systems on your laptop. Notebook, Tweet.ColBERT Integration: Document reranking with ColBERT via LlamaIndex, delivering a solution that is about 100x faster than BERT-based models for more efficient data processing. Notebook, Tweet.Counselor Copilot \u2014 Social Impact Through RAG: Spotlight on Counselor Copilot, an innovative RAG project supporting the Trevor Project\u2019s crisis counselors, providing real-time assistance with context, suggestions, and actions to aid LGBTQ+ youth effectively. BlogPost, Tweet.\u2728 Feature Releases and Enhancements:We have launched a new chunking strategy to enhance RAG retrieval: Sub-Document Summaries. This approach overcomes the limitations of naive chunking by injecting hierarchical metadata, offering a nuanced balance of global context awareness and precision through subdocument summaries for improved performance. Notebook, Tweet.We have launched a cookbook for the latest mistral-large model from MistralAI offering advanced features like near GPT-4 level reasoning, Function calling, JSON Output, and more. Docs, Tweet.We have launched a cookbook on Gemma, a new family of state-of-the-art LLMs by GoogleDeepMind, with 2B and 7B parameter options using Ollama to build local RAG on your laptop. Notebook, Tweet.We have introduced ColBERT through LlamaIndex, offering a one-line integration for a reranking model that\u2019s ~100x faster than traditional BERT-based models, ensuring efficient document handling with superior performance. Notebook, Tweet.We have introduced a way to integrate advanced RAG into full-stack web apps with create-llama, using LlamaPacks, in just two lines of code. create-llama, Tweet.\ud83c\udfa5 Demos:Counselor Copilot: An interesting RAG project by Riya Jagetia and team, designed to assist crisis counselors at the Trevor Project in supporting LGBTQ+ youth. This tool acts as a real-time copilot, offering context, suggested replies, and various actions to enhance counselor effectiveness, showcasing a unique and socially impactful application of advanced RAG techniques. BlogPost, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to simplifying advanced RAG development: Our latest insights pinpoint solutions for key challenges, including our innovative LlamaParse for complex PDF QA, shared in our AI in Production presentation.\u270d\ufe0f Tutorials:Marco Bertelli\u00a0tutorial on Building an Interactive Chatbot with React.Wenqi Glantz\u00a0tutorial on The Journey of RAG Development: From Notebook to Microservices.Wenqi Glantz\u00a0tutorial video on 12 RAG Pain Points and Solutions in the RAG pipeline.\ud83c\udfa5 Webinar:Webinar with Sisil from JasperAI on Practical Tips and Tricks for Productionizing RAG.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-02-27-4b9102a0f824"
        },
        {
            "date": "Nov 13, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex turns 1!",
            "tags": [
                "llamaindex",
                "ai",
                "llm",
                "rag"
            ],
            "content": "It\u2019s our birthday! One year ago, Jerry pushed his first commit to GPT Index, the project that would become LlamaIndex. It worked with GPT-3, the state of the art model available at the time. That initial version was very simple, but the problem statement \u2014 and the solution \u2014 remain the same:one fundamental limitation of GPT-3 is the context size [\u2026] the ability to feed \u201cknowledge\u201d to GPT-3 is mostly limited to this limited prompt size [\u2026] But what if GPT-3 can have access to potentially a much larger database of knowledge[\u2026]?Twelve months have passed and there\u2019s been a tsunami of new developments in the world of generative AI and LLMs, but the reason LlamaIndex was invented remains: even the most sophisticated model isn\u2019t trained on your data, which can be locked behind an API or in a SQL database, and even the latest GPT-4-Turbo context size of 128,000 tokens isn\u2019t enough to hold even a relatively modest dataset. Retrieval-Augmented Generation (RAG) is here to stay.Big numbersAt just 1 year old, LlamaIndex has gotten very big. How big? Here\u2019s some numbers:Over 450 contributors to our open-source library!Nearly 3,000 open-source projects depend on LlamaIndex!Nearly 4,000 members in our Discord (come join us!)47,000 lines of Python in the library! (Don\u2019t worry, it\u2019s still just 0.5MB to download)Nearly 900,000 downloads every month!RAG deployed among popular open-source projects, as well as in production in enterprise settings.Big thanksBut big numbers aside, the thing we\u2019re proudest of is our community: we have users in (nearly) every country in the world, from single hobby developers to Fortune 500 companies and everyone in between. LlamaIndex\u2019s founder, Jerry Liu, says:Our community is everything at LlamaIndex. We love seeing the amazing things people are building every day! It\u2019s what gets us up in the morning and keeps us motivated to keep pushing the boundaries of what developers can do with GenAI. And we\u2019re especially grateful to the developers who give back by pushing PRs, issues and bug reports. They\u2019re what makes the open source world go round.Big milestonesWhat\u2019s happened in a year? Well, everything! But here\u2019s some highlights:November 2022: Launched GPT Tree Index, a way of organizing information into a tree. Based on the initial interest/traction, we expanded this into a List Index and Keyword Index. Then ChatGPT launched in NovemberDecember 2022: Some big feature releases: support for indexing embeddings + vector stores, and initial data loaders for Notion, Slack, and Google DriveJanuary 2023: LlamaIndex hits Github trending for the first time!February 2023: We launched LlamaHub with Jesse Zhang, containing an initial repository of data loaders for users to access. We ran a sweepstakes with OctoML and got 50+ data loader submissions!March 2023: ChatGPT API launched and then Plugins. We scrambled to support the new API + Plugin integrations.April 2023: We incorporated!May 2023: At the end of April, we launched 0.6.0, where we completely rewrote the entire framework from the ground-up for greater modularity and composability for different levels of abstraction.June 2023: We announced that we raised $8.5M in funding!July 2023: We launched Data Agents + Agent Tools on LlamaHub. We also launched a Typescript packageAugust 2023: We integrated with OpenAI fine-tuning and launched a variety of LLM and embedding fine-tuning abstractions.September 2023: We launched secinsights.ai \u2014 a production-ready full-stack applicationOctober 2023: We launched LlamaIndex Chat \u2014 a full-stack Typescript template.November 2023: Went fully multi-modal with the release of GPT-4-vision!Big plansWith all that growth and all those features, what\u2019s next for us? Stay tuned!",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-turns-1-f69dcdd45fe3"
        },
        {
            "date": "May 7, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-05-07",
            "tags": [
                "newsletter"
            ],
            "content": "Hello LlamaIndex fam! \ud83e\udd99If you\u2019re in SF, join us for the first-ever Llama 3 Hackathon (invitation here)! Shack15 is an amazing venue and it\u2019s sure to be a fun time. If you can\u2019t make it, stay tuned for the rundown on the cool projects that come out of the event. Now, on to the highlights:\ud83e\udd29\u00a0The highlights:LlamaIndex.TS hits v0.3! Loads of new features inside!LlamaIndex Python hits v0.10.34! A bumper release!That\u2019s a lot!\u2728 Feature Releases and Enhancements:Two big releases this week!LlamaIndex.TS hit version 0.3! Tweet, Blog postFeatures: Agent support including ReAct, Anthropic and OpenAI agents, as well as a generic AgentRunner classStandardized Web Streams compatible with React 19, Deno, and Node 22More comprehensive type systemEnhanced support for deployment on Next.js, Deno, Cloudflare Workers and WakuLlamaIndex Python hit version 0.10.34! TweetFeatures:   Introspective agents that work through reflectionSupport for huggingface text-generation-inference APIStructured planning agentA chat summary memory bufferRust-based LLM support with Mistral.rsMilvus sparse hybrid searchGoogle Firestore vector store supportA new LlamaPack for the Reflection Agentic Pattern. Tweet\ud83c\udfa5\u00a0Demos:Filter AirBnB listings using natural language with this open-source demo! It uses Mistral AI\u2019s Mixtral 8x7b and Qdrant engine, plus Streamlit to build UI. Tweet, Blog postFully local RAG with Llama 3, Ollama and LlamaIndex! A short, sweet guide. Tweet, Blog postFine-tune your embedding model using labels from a reranker. Tweet, Blog post\ud83d\uddfa\ufe0f Guides:Hanane Dupouy walks us through building an agent that can perform complex financial calculations. Tweet, SlidesPlaban Nayak sets up a local, open-source RAG pipeline that uses Llama 3 and Qdrant to demonstrate how to improve the accuracy of your RAG with reranking. Tweet, Blog postJason Zhou talks about the components needed for agentic RAG. TweetDivyanshu Dixit walks us through agents dedicated to workflow automation. Tweet, Blog post\u270d\ufe0f Tutorials:Tyler Hutcherson of Redis and our own Laurie Voss walk you through building agentic RAG with semantic caching and other production-ready techniques. Video, NotebookCleanlab has a tutorial on getting trustworthiness scores from your RAG pipeline to allow you to avoid hallucinations and course-correct. Tweet, Notebook\ud83c\udfa5\u00a0Webinars:On May 8 we\u2019ll be co-hosting a webinar with Pulumi on deploying AI applications to AWS. TweetOur own Andrei and friends walk you from basic RAG through handling long-context RAG all the way to evaluating your RAG pipeline. Tweet, Video, Notebook",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-12"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-05-07"
        },
        {
            "date": "May 17, 2023",
            "author": "Jerry Liu",
            "title": "Using LLM\u2019s for Retrieval and Reranking",
            "tags": [
                "artificial-intelligence",
                "machine-learning",
                "large-language-models",
                "llamaindex",
                "nlp"
            ],
            "content": "SummaryThis blog post outlines some of the core abstractions we have created in LlamaIndex around LLM-powered retrieval and reranking, which helps to create enhancements to document retrieval beyond naive top-k embedding-based lookup.LLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost. We show how using embedding-based retrieval as a first-stage pass, and second-stage retrieval as a reranking step can help provide a happy medium. We provide results over the Great Gatsby and the Lyft SEC 10-k.Two-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based rerankingIntroduction and BackgroundThere has been a wave of \u201cBuild a chatbot over your data\u201d applications in the past few months, made possible with frameworks like LlamaIndex and LangChain. A lot of these applications use a standard stack for retrieval augmented generation (RAG):Use a vector store to store unstructured documents (knowledge corpus)Given a query, use a retrieval model to retrieve relevant documents from the corpus, and a synthesis model to generate a response.The retrieval model fetches the top-k documents by embedding similarity to the query.In this stack, the retrieval model is not a novel idea; the concept of top-k embedding-based semantic search has been around for at least a decade, and doesn\u2019t involve the LLM at all.There are a lot of benefits to embedding-based retrieval:It\u2019s very fast to compute dot products. Doesn\u2019t require any model calls during query-time.Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There\u2019s a class of queries where embedding-based retrieval returns very relevant results.Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant context to the query, which in turn degrades the quality of the overall RAG system, regardless of the quality of the LLM.This is also not a new problem: one approach to resolve this in existing IR and recommendation systems is to create a two stage process. The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision. Then the second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to \u201crerank\u201d the existing retrieved candidates.Covering the downsides of embedding-based retrieval is worth an entire series of blog posts. This blog post is an initial exploration of an alternative retrieval method and how it can (potentially) augment embedding-based retrieval methods.LLM Retrieval and RerankingOver the past week, we\u2019ve developed a variety of initial abstractions around the concept of \u201cLLM-based\u201d retrieval and reranking. At a high-level, this approach uses the LLM to decide which document(s) / text chunk(s) are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric.Simple diagram of how LLM-based retrieval worksAn example prompt would look like the following:\nA list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.\n  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n  as the relevance score. The relevance score is a number from 1\u201310 based on how relevant you think the document is to the question.\n  Do not include any documents that are not relevant to the question.\n  Example format:\n  Document 1:\n  <summary of document 1>\n  Document 2:\n  <summary of document 2>\n  \u2026\n  Document 10:\n  <summary of document 10>\n  Question: <question>\n  Answer:\n  Doc: 9, Relevance: 7\n  Doc: 3, Relevance: 4\n  Doc: 7, Relevance: 3\n  Let's try this now:\n  {context_str}\n  Question: {query_str}\n  Answer:\nThe prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document:You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk.You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new document summary index, but you can also choose to do it yourself.Given a collection of documents, we can then create document \u201cbatches\u201d and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches.You can use our abstractions in two forms: as a standalone retriever module (ListIndexLLMRetriever) or a reranker module (LLMRerank). The remainder of this blog primarily focuses on the reranker module given the speed/cost.LLM Retriever(ListIndexLLMRetriever)This module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index.from llama_index import GPTListIndex\nfrom llama_index.indices.list.retrievers import ListIndexLLMRetriever\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\n# high - level API\nquery_str = \"What did the author do during his time in college?\"\nretriever = index.as_retriever(retriever_mode=\"llm\")\nnodes = retriever.retrieve(query_str)\n# lower-level API\nretriever = ListIndexLLMRetriever()\nresponse_synthesizer = ResponseSynthesizer.from_args()\nquery_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\nresponse = query_engine.query(query_str)Use Case: This could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes.LLM Reranker (LLMRerank)This module is defined as part of our NodePostprocessor abstraction, which is defined for second-stage processing after an initial retrieval pass.The postprocessor can be used on its own or as part of a RetrieverQueryEngine call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index.from llama_index.indices.query.schema import QueryBundle\nquery_bundle = QueryBundle(query_str)\n# configure retriever\nretriever = VectorIndexRetriever(\nindex=index,\nsimilarity_top_k=vector_top_k,\n)\nretrieved_nodes = retriever.retrieve(query_bundle)\n# configure reranker\nreranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\nretrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)Limitations/CaveatsThere are certain limitations and caveats to LLM-based retrieval, especially with this initial version.LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete.Using third-party LLM API\u2019s costs money.The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents.Using the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful.Initial Experimental ResultsLet\u2019s take a look at how well LLM reranking works!We show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don\u2019t showcase as many results given that it tends to run a lot slower than either of the first two approaches).We analyze results over two very different sources of data: the Great Gatsby and the 2021 Lyft SEC 10-k. We only analyze results over the \u201cretrieval\u201d portion and not synthesis to better isolate the performance of different retrieval methods.The results are presented in a qualitative fashion. A next step would definitely be more comprehensive evaluation over an entire dataset!The Great GatsbyIn our first example, we load in the Great Gatsby as a Document object, and build a vector index over it (with chunk size set to 512).# LLM Predictor (gpt-3.5-turbo) + service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n# load documents\ndocuments = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)We then define a get_retrieved_nodes function \u2014 this function can either do just embedding-based retrieval over the index, or embedding-based retrieval + reranking.def get_retrieved_nodes(\n    query_str, vector_top_k=10, reranker_top_n=3, with_reranker=False\n):\n  query_bundle = QueryBundle(query_str)\n  # configure retriever\n  retriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=vector_top_k,\n  )\n  retrieved_nodes = retriever.retrieve(query_bundle)\n  if with_reranker:\n    # configure reranker\n    reranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\n    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n  return retrieved_nodesWe then ask some questions. With embedding-based retrieval we set k=3. With two-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based reranking.Question: \u201dWho was driving the car that hit Myrtle?\u201d(For those of you who are not familiar with the Great Gatsby, the narrator finds out later on from Gatsby that Daisy was actually the one driving the car, but Gatsby takes the blame for her).The top retrieved contexts are shown in the images below. We see that in embedding-based retrieval, the top two texts contain semantics of the car crash but give no details as to who was actually responsible. Only the third text contains the proper answer.Retrieved context using top-k embedding lookup (baseline)In contrast, the two-stage approach returns just one relevant context, and it contains the correct answer.Retrieved context using two-stage pipeline (embedding lookup then rerank)2021 Lyft SEC 10-KWe want to ask some questions over the 2021 Lyft SEC 10-K, specifically about the COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a ctrl-f for \u201cCOVID-19\u201d returns 127 matches.We use a similar setup as the Gatsby example above. The main differences are that we set the chunk size to 128 instead of 512, we set k=5 for the embedding retrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage approach.We then ask the following questions and analyze the results.Question: \u201cWhat initiatives are the company focusing on independently of COVID-19?\u201dResults for the baseline are shown in the image above. We see that results corresponding to indices 0, 1, 3, 4, are about measures directly in response to Covid-19, even though the question was specifically about company initiatives that were independent of the COVID-19 pandemic.Retrieved context using top-k embedding lookup (baseline)We get more relevant results in approach 2, by widening the top-k to 40 and then using an LLM to filter for the top-5 contexts. The independent company initiatives include \u201cexpansion of Light Vehicles\u201d (1), \u201cincremental investments in brand/marketing\u201d (2), international expansion (3), and accounting for misc. risks such as natural disasters and operational risks in terms of financial performance (4).Retrieved context using two-stage pipeline (embedding lookup then rerank)ConclusionThat\u2019s it for now! We\u2019ve added some initial functionality to help support LLM-augmented retrieval pipelines, but of course there\u2019s a ton of future steps that we couldn\u2019t quite get to. Some questions we\u2019d love to explore:How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.)What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance.Exploring different prompts and text summarization methods to help determine document relevanceExploring if there\u2019s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?)ResourcesYou can play around with the notebooks yourself!Great Gatsby Notebook2021 Lyft 10-K Notebook",
            "related_posts": [
                "Querying a network of knowledge with llama-index-networks",
                "LlamaIndex Newsletter 2024\u201302\u201327",
                "Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot",
                "Introducing LlamaCloud and LlamaParse"
            ],
            "link": "https://www.llamaindex.ai/blog//using-llms-for-retrieval-and-reranking-23cf2d3a14b6"
        },
        {
            "date": "Dec 12, 2023",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2023\u201312\u201312",
            "tags": [
                "llamaindex",
                "llm",
                "newsletter",
                "rag",
                "ai"
            ],
            "content": "Howdy, Llama Enthusiasts \ud83e\udd99,We are thrilled to announce another exciting week filled with full of the latest updates, features, insightful tutorials, guides, webinars, and so much more. Have a groundbreaking project, compelling article, or captivating video? We\u2019re all ears! Reach out to us at news@llamaindex.ai.Don\u2019t forget to subscribe to our newsletter via our website to have all these exciting developments delivered directly to your inbox.\ud83e\udd29 First, the highlights:Llama Datasets: A diverse collection of community-contributed datasets for benchmarking RAG pipelines. Blog, Tweet.RAGs v5: Enables multi-modal data handling with natural language for both text and image sources. Tweet.Production RAG Pipeline: New features and a guide for efficient RAG while handling updates to your data, including incremental re-indexing for Google Docs and enhanced transformation and caching processes. Notebook, Tweet.Revamped LlamaHub: A community-driven hub with universal data loaders, a new user interface, and a range of tools, templates, and datasets. Tweet.AutoTranslateDoc: An open-source project for translating GitHub repository documentation into over 15 languages. Blog, Repo, Tweet\u2728 Feature Releases and Enhancements:We launched Llama Datasets \ud83e\udd99\ud83d\udcdd, a collection of community-contributed datasets tailored for benchmarking RAG pipelines in various use cases. These datasets offer flexibility in selecting the most appropriate one for specific LLM applications. The initial release includes a diverse range, such as Code Help Desk, FinanceBench, Mini TruthfulQA, Mini Squad V2, Blockchain Solana, Uber 10K, Llama 2 Paper, Paul Graham Essay, Origin of COVID-19, CovidQADataset, MiniCovidQADataset and LLM Survey Paper. Each dataset, designed as a QA set, integrates smoothly with Llama Index abstractions, providing a platform for comprehensive benchmarking across multiple metrics. All datasets are available on LlamaHub for easy download and evaluation. Blog, Tweet.We launched RAGs v5, enabling multi-modal data handling with natural language for both text and image sources. Key features include enhanced multi-modal indexing, the capability to view sources in any RAG agent, and support for loading entire directories, not just single files. Tweet.We have launched new features and a guide for building a production RAG pipeline, enabling efficient question-answering with LLMs on production data even while it is continuously updated. This includes incremental re-indexing for Google Docs changes and enhanced transformation and caching processes in our updated IngestionPipeline. Notebook, Tweet.We launched a one-click, full-stack LlamaIndex template now available on Replit! This template features a full-stack Next.js app in TypeScript, capable of reading any files you provide, and includes a chat interface for querying those documents. It\u2019s completely customizable and based on our popular create-llama generator. Replit Template, Tweet.We have introduced RAGEvaluatorPack to easily benchmark your RAG pipeline on any dataset with a single line of code, offering metrics like correctness, relevancy, and context similarity. Docs, Tweet.We released community templates for create-llama, offering a selection of community-contributed starter templates during setup. Current examples include embedded-tables for analyzing complex tables in large PDFs, and multi-document-agent for comparing multiple documents. Tweet.We launched multi-modal support in create-llama, our user-friendly command-line tool for generating full-stack LlamaIndex apps. Now, easily integrate GPT-4-vision in your app, allowing you to upload images to the web interface and receive answers about them in just seconds. Tweet.We launched the Ollama LlamaPack, a new offering that integrates local LLMs and embeddings into a fully local RAG pipeline, enhancing language model accessibility and capabilities. Docs, Tweet.We launched the revamped LlamaHub, a hub for community-driven modules to enhance LLM app development, featuring universal data loaders, a new user interface, and a range of tools, templates, and datasets. Tweet.We introduced AutoTranslateDoc, an open-source project for translating GitHub repository documentation into over 15 languages, including Chinese, Spanish, and French. This tool, successfully implemented in our own LlamaIndex.TS docs, simplifies the internationalization process for open-source projects. Blog, Repo, TweetWe released support for exact match and range queries in 4 vector databases including Weaviate, Chroma, Qdrant and Pinecone, allowing auto-retrieval via metadata filters, elevating the functionality of structured and unstructured data querying. Tweet.\ud83d\uddfa\ufe0f Guides:Guide on building LLM apps for financial data which is presented at MindsDB event. Learn to query diverse financial data using advanced RAG with techniques for multi-document comparisons, embedded tables, and converting text queries into domain-specific languages.Guide on advanced RAG Cheat Sheet, a concise guide offering solutions for different RAG-related pain points and techniques. It\u2019s part of our Snowflake BUILD talk and PyData Global talk.\u270d\ufe0f Tutorials:Blog by Waii.ai on creating an agent that queries both enterprise databases and PDF data, combining advanced text-to-SQL techniques and a Llama Index RAG pipeline, for effective analysis of structured and unstructured data like retail sales trends.Wenqi Glantz\u2019s tutorial on using LLMs for querying knowledge graphs introduces seven strategies, now easily accessible through our LlamaPacks and featured in our Neo4j query engine.An hour comprehensive workshop tutorial by AIMakerspace on RAG strategies over complex documents through recursive retrieval.Laurie\u2019s video on using LlamaIndex for multi-modal retrieval-augmented generation apps teaches you to build indexes and retrieve data from text and images, for enhanced query responses.Ravi Theja\u2019s video on Understanding LlamaIndex 0.9v abstractions and features.\ud83e\udd1d Integrations:We integrated AssemblyAI with Llama Index TS, enhancing the capabilities and offering new, innovative solutions. Blog.We integrated Panel, a powerful framework for building interactive data apps as a LlamaPack. This provides you with a robust chat interface for talking to your data with full streaming support in a single line of code. Docs, Tweet.We integrated FlagEmbeddingReranker to further boost your RAG pipeline. Notebook, Tweet.\ud83c\udfa5 Webinars:Webinar featuring Haotian Liu, the author of LLaVa which includes a deep dive into the open-source multi-modal models of LLaVa, which are competitive with GPT-4V, and a presentation on multi-modal use cases with LLaVa + LlamaIndex by Haotian Zhang from the LlamaIndex team.\ud83c\udfe2 Calling all enterprises:Are you building with LlamaIndex? We are working hard to make LlamaIndex even more Enterprise-ready and have sneak peeks at our upcoming products available for partners. Interested? Get in touch.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-05-07",
                "LlamaIndex Newsletter 2024-04-30",
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2023-12-12-4a5d542fbb1e"
        },
        {
            "date": "May 21, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-05-21",
            "tags": [],
            "content": "Hello LlamaIndex Community! \ud83e\udd99Welcome to another exciting weekly update from LlamaIndex! Last week was a standout in the AI world with significant updates on GPT-4o and Gemini models. We're thrilled to bring you a host of exceptional integration updates, guides, tutorials, webinars, and events.\ud83e\udd29\u00a0The highlights:LlamaIndex on Vertex AI (Google Cloud): Introducing the new RAG API powered by advanced LlamaIndex modules on Vertex AI (Google Cloud). Docs, Tweet.Enhanced Document Parsing with GPT-4o: Integrated GPT-4o in LlamaParse for superior document parsing. Notebook, Tweet.Cookbook on Structured Image Extraction with GPT-4o: Check out our new cookbook on using GPT-4o for Structured Image Extraction. Notebook, Tweet.\u2728 Feature Releases and Enhancements:We have launched LlamaIndex on Vertex AI (Google Cloud) to introduce the new RAG API, enhanced by LlamaIndex's advanced modules. This integration simplifies setup and enhances user access for the developers with the flexibility to connect a variety of data sources and file types. It fully supports the latest LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini 1.0 models. Docs, Tweet.We have introduced GPT-4o with LlamaParse, offering enhanced document parsing into markdown for complex files, ensuring higher data quality for your RAG pipeline. Note the increased cost of $0.60 USD per page. Note the increased cost of $0.60 USD per page compared to the standard $0.003 per page. Notebook, Tweet.We have released a cookbook on using GPT-4o for Structured Image Extraction, showing how to convert images into structured JSONs with a 0% failure rate and higher quality than GPT-4V. Notebook, Tweet.LlamaParse integration with Quivr to enhance document parsing capabilities. Now, you can easily process complex documents like PDFs, PPTX, and Markdown files, ensuring clean data storage and accurate retrieval in your personalized AI assistants. Docs, Tweet.\ud83d\uddfa\ufe0f Guides:Guide to Enhanced QA with LlamaParse on complex tables like train schedules. This approach uses spatial text layout and GPT-4o to preserve essential information, ensuring accurate and error-free data interpretation.Guide to Speeding Up Vector Search with Minimal Accuracy Loss using Jina Embeddings to achieve 32x faster vector search performance at just a 4% cost in accuracy. It involves encoding your data as binary digits, significantly reducing storage and compute requirements.\u270d\ufe0f Tutorials:Kate Silverstein tutorial on building local research assistant**,** set up a local, private research assistant on your laptop effortlessly with Mozilla's llamafile. Tutorial covers everything from downloading and activating the model, to connecting via LlamaIndex and managing your data.Plaban Nayak tutorial on Multi-document Agentic RAG using Llama-Index and Mistral.Diptiman Raichaudhuri tutorial on fully local Text-to-SQL using DuckDB as the database, Ollama + Mixtral-8x7B as the model, and LlamaIndex for text-to-SQL orchestration.Mandar Karhade tutorial on showing an end-to-end experimentation pipeline for iterating on chunk sizes, generating a synthetic dataset, and measuring how it affects evaluation metrics.\ud83d\udcf9\u00a0Webinar:Join us for a webinar on \"Open-Source Longterm Memory for Autonomous Agents\" this Thursday at 9am PT, where we'll explore the memary architecture with Julian Saks, Kevin Li, Seyeong Han and rest of memary team, diving deep into the challenges and future of long-term memory for autonomous systems.\ud83d\udcc5\u00a0Events:We are having our first-ever meetup at our new office in San Francisco! Join us to connect with our team and friends from Activeloop and Tryolabs, as we discuss the latest developments in generative AI.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-05-21"
        },
        {
            "date": "Feb 27, 2024",
            "author": "Andrei",
            "title": "Querying a network of knowledge with llama-index-networks",
            "tags": [
                "llm",
                "retrieval-augmented",
                "ai",
                "distributed",
                "machine-learning"
            ],
            "content": "The main premise behind RAG is the injection of context (or knowledge) to the LLM in order to yield more accurate responses from it. As such, a crucial component to a RAG system is the data source from which it gets its knowledge. It\u2019s intuitive then to reason that the more knowledge that the RAG system can tap into, the better it would ultimately become (in terms of answering queries of potentially both depth and breadth). The spirit of this concept is not so different from that found in basically every other data-driven discipline \u2014 access to more (good) data, that is subsequently and effectively used, usually leads to better outcomes.It is with that backdrop, that we\u2019re thrilled to announce the release of our newest latest llama-index library extension, called llama-index-networks. This library extension makes it possible to build a network of RAGs built on top of external data sources and supplied by external actors. This new network paradigm allows for a new way for data suppliers to provide their data to those who want it in order to build more knowledgeable systems!In this blog post, we\u2019ll introduce the main classes of the new extension library and show you how in just a couple lines of code, you can make your QueryEngine ready to contribute as part of a network of RAGs. We\u2019ll also share ideas of what this can mean for how data suppliers actually supply their data to consumers within this new era of LLMs.A note on terminology: in this post, we use llama-index-networks to refer to the actual extension, whereas llama-index[networks] refers to an installation of llama-index that comes with the llama-index-networks extension.The story of Alex, Beth, and BobAn illustrative example of actors in a network and their problem statements.To illustrate how the llama-index-networks package can be used, we consider three made-up characters, Alex, Bob, and Beth, and the following scenario:Both Bob and Beth each have their own set of documents and both have already built quite the outstanding RAG systems over them (using llama-index of course!)Alex has heard about these insightful documents that both Bob and Beth have and would like to be able to query the individual RAGs built over the both of them.Bob and Beth, being as kind as they are (or, perhaps they were paid some undisclosed dollar amount), agree to give access to Alex to their RAG systems.To facilitate this new fashion of knowledge exchange, they agree to setup a network of RAGs that Alex can query.Bob and Beth build a web service over their RAGsContributorService is built around a QueryEngine.With the llama-index-networks package, Bob and Beth can make their respective QueryEngine\u2019s ready to participate in the network with only a few lines of code.\"\"\"Beth's contributor service file.\n\nBeth builds her QueryEngine and exposes it behind the standard\nLlamaIndex Network Contributor Service. \n\nNOTE: Bob would probably make use of Docker and cloud \ncompute services to make this production grade.\n\"\"\"\n\nfrom llama_index.networks.contributor import ContributorService\nimport uvicorn\n\nbeth_query_engine = ...\nbeth_contributor_service = ContributorService.from_config_file(\n    \".env.contributor\",  # settings/secrets for the service\n    beth_query_engine\n)\n\n\nif __name__ == \"__main__:\n    uvicorn.run(beth_contributor_service.app, port=8000)Bob would use similar lines of code to make his QueryEngine ready to contribute to any LlamaIndex network. Note, that the dotenv file .env.contributor contains settings for the service as well as any necessary api keys (e.g., OPENAI_API_KEY or ANTHROPIC_API_KEY), which under the hood is implemented as REST service using FastAPI.Alex builds a NetworkQueryEngineAlex builds a NetworkQueryEngine that connects to Beth and Bob\u2019s individual ContributorService\u2019s.For Alex\u2019s part, he uses the NetworkQueryEngine class of the llama-index-networks extension to be able to connect to both Beth and Bob\u2019s ContributorService\u2019s.\"\"\"Alex's network query engine.\n\nAlex builds a NetworkQueryEngine to connect to a \nlist of ContributorService\u2019s.\n\"\"\"\n\nfrom llama_index.networks.contributor import ContributorClient\nfrom llama_index.networks.query_engine import NetworkQueryEngine\nfrom llama_index.llms.groq import Groq\n\n# Use ContributorClient to connect to a ContributorService\nbeth_client = ContributorClient.from_config_file(\n    env_file=\".env.beth_contributor.client\"\n)\nbob_client = ContributorClient.from_config_file(\n    env_file=\".env.bob_contributor.client\"\n)\ncontributors = [beth_client, bob_client]\n\n# NetworkQueryEngine\nllm = Groq()\nnetwork_query_engine = NetworkQueryEngine.from_args(\n    contributors=contributors,\n    llm=llm\n)\n\n# Can query it like any other query engine\nnetwork_query_engine.query(\"Why is the sky blue?\")Here the dotenv files store the service parameters such as an api_url required to connect to the ContributorService.Before moving on to the next section of this blog post, we\u2019ll take the next few lines to explain a bit of what\u2019s involved under the hood when Alex query\u2019s his NetworkQueryEngine. When the query() method is invoked (Async is also supported!), the query is sent to all contributors. Their responses are stored as new Nodes and a response is synthesized (with the associated ResponseSynthesizer). After reading that, some may notice that this is the usual pattern when working with our QueryEngine abstractions; and, that indeed was the point. Using a NetworkQueryEngine should be very similar to how you would use any other QueryEngine in our library!This marks the end of our little story about Alex, Bob and Beth. Before wrapping up this blog post, we first provide a few potential opportunities that may arise through the usage of llama-index-networks.A new world of data supply and consumptionRAG marketplaces is a use case that can be made possible with llama-index[networks].One possible world that could easily be powered by llama-index[networks] are marketplaces for RAG. A place where data suppliers package their data in the form of RAGs to data consumers look to expand their own query system\u2019s knowledge. Potential RAG (data) suppliers could be your local newspaper, book publishing companies, etc.In this new data supply and consumption model, there is more onus on the data supplier to prepare the data in a fashion that makes it easier to consume \u2014 specifically, the suppliers own the building of the query engine. This should benefit the data consumer greatly since, with a standard interface that is provided by the likes of a ContributorService (that encapsulates a QueryEngine), they can get access to the knowledge they seek from the data easier than ever before (i.e., in relation to traditional data marketplaces that exchange raw data).It is with this kind of vision that we\u2019ve built llama-index[networks] to make it: (i) easier for data suppliers to supply the knowledge contained in their data in new and arguably more effective ways, and (ii) easier for data consumers to connect to these new forms of external knowledge.Internal networks: another potential use caseIn addition to powering RAG marketplaces, we foresee the need of connecting RAGs that an overarching company may own, but not necessarily manage. More concretely, a franchise may have the rights to the data across all of its operations. And, while they could build a \u201ccentral\u201d, monolithic RAG over the entire database, it may still be more efficient and effective to build RAGs over the individual operators and query these instead.The idea of exchanging information to build better, more knowledgeable systems is not new. However, the idea of using RAGs to facilitate that exchange may be (to our knowledge, it is), and we believe that both existing and new use cases requiring such collaboration can benefit from concept.A quick note on privacyAn important consideration in the collaboration of data is privacy and security. It bears mentioning that the examples above assume that the data that is being shared across the network is in compliance with data privacy and security laws and regulations. It is our belief that as this technology grows, that the necessary features and capabilities will be developed and incorporated to facilitate in-compliance RAG networks.Check out the demo to learn more!To see an actual demo of a network connecting to a set of contributors, check out the Github repository code for llama-index-networks and navigate to the examples/demo subfolder.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f"
        },
        {
            "date": "Jun 25, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-06-25",
            "tags": [],
            "content": "Hello to All Llama Lovers!\ud83e\udd99Welcome to this week\u2019s issue of the LlamaIndex newsletter! This edition is packed with thrilling updates, comprehensive guides, and detailed tutorials to help you gain a deeper understanding of our tools.\ud83e\udd29\u00a0The highlights:CrewAI Multi-Agent Integration: Integrated with CrewAI to enhance task-solving with specialized agent crews and LlamaIndex integrations. Notebook, Tweet.MistralAI Fine-Tuning API Integration: Enhance model training and performance monitoring with our new integration of MistralAI\u2019s Fine-Tuning API. Notebook, Tweet.\u2728 Feature Releases and Enhancements:We have launched a Multi-Agent integration with CrewAI to build a crew of specialized agents that collaboratively solve tasks. Enhance these agents with external knowledge and third-party tools through easy integrations with LlamaIndex, including advanced RAG query engines and tools from LlamaHub. Notebook, Tweet.We have integrated MistralAI\u2019s Fine-Tuning API to create and synthesize training and evaluation datasets, assess model after fine-tuning, and monitor performance metrics with RAGAS and Weights & Biases. Notebook, Tweet.\ud83d\udca1\u00a0Demos:RAGapp by **Marcus Schiesser** simplifies Agentic RAG in enterprise settings with functionalities akin to using GPTs by OpenAI. The latest version includes a code interpreter and a tool to call any OpenAPI, all built using LlamaIndex.\ud83d\uddfa\ufe0f Guides:Guide to Multi-Document Agentic RAG Using LightningAI: Jay Shah\u2019s template that enables you to set up a multi-document agent for search and summarization across research notebooks. This out-of-the-box solution, integrated with Streamlit, allows for full visualization and is part of LightningAI\u2019s suite of tools for developing and sharing ML and genAI native apps.Guide to Making a Serverless RAG Chatbot: Azure\u2019s quick start repository for creating a serverless RAG chatbot using LlamaIndex and AzureOpenAI.Guide to Building an Agent in LlamaIndex: Our comprehensive guide which covers building a basic agent, using local models, adding RAG, enhancing retrieval with LlamaParse, and developing custom tools.\u270d\ufe0f Tutorials:JinoRohit\u2019s tutorial on using a LlamaIndex pipeline with MLflow for systematic tracking and tuning of RAG parameters, enhancing answer accuracy through precise evaluation metrics and datasets.Hanane Dupouy\u2019s tutorial demonstrates how to apply CRAG (Corrective RAG) for financial analysis using LlamaIndex\u2019s CRAG LlamaPack. This technique assesses retrieval quality and supplements the knowledge base with web searches to ensure contextual accuracy and relevance.Soham\u2019s tutorial to create an agent that automates GitHub commits using Composio and LlamaIndex Tools.Aruna Withanage\u2019s tutorial on creating custom text-to-SQL pipelines using LlamaIndex\u2019s DAG capabilities.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-06-25"
        },
        {
            "date": "Mar 18, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM",
            "tags": [
                "nvidia"
            ],
            "content": "Generative AI is rapidly transforming the global economy. Enterprises are increasingly looking to adopt generative AI to drive business transformation, but face challenges around protecting IP, ensuring security and compliance, and moving smoothly from proof of concept to production.That\u2019s why LlamaIndex is excited to announce that it is integrated with NVIDIA NIM inference microservices to help enterprises seamlessly deploy generative AI at scale. NVIDIA NIM, part of the NVIDIA AI Enterprise software platform, optimizes inference on more than two dozen popular AI models from NVIDIA and its partner ecosystem.LlamaIndex is an open-source tool for connecting your data to LLMs and extracting valuable insights. By integrating NVIDIA NIM runtimes with LlamaIndex\u2019s data connection capabilities, enterprises will be able to:Connect generative AI models hosted using NVIDIA NIM to their own proprietary data sources, allowing them to generate accurate model outputs while keeping sensitive data secureSearch across and extract insights from both structured and unstructured enterprise data to enhance the knowledge and accuracy of AI modelsBuild data-enriched generative AI applications for use cases like enterprise search, question answering, analytics, and moreFor developers, using NVIDIA NIM with LlamaIndex provides a seamless path from experimentation to production. They can access the NIM microservices from the newly launched NVIDIA API catalog to quickly build AI applications using industry-standard protocols, and then easily transition those applications to work on their self-hosted NVIDIA NIM instance for enhanced security, customization, and cost effectiveness at scale.The combination of NVIDIA NIM for optimized model inference and LlamaIndex\u2019s data connection helps unlock the full potential of enterprise-level generative AI. We look forward to seeing the innovative applications that emerge from this integration as more organizations embrace AI to transform their business.You can expect an in-depth technical blog post about how to use NVIDIA NIM and LlamaIndex very soon!",
            "related_posts": [
                "LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM",
                "NVIDIA Research: RAG with Long Context LLMs"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-accelerates-enterprise-generative-ai-with-nvidia-nim"
        },
        {
            "date": "Sep 1, 2023",
            "author": "Yi Ding",
            "title": "ChatGPT\u2019s Knowledge is Two Years Old: What to do if you\u2019re building applications?",
            "tags": [
                "llamaindex",
                "chatgpt",
                "ai",
                "fine-tuning",
                "nlp"
            ],
            "content": "It\u2019s official: as of today, ChatGPT\u2019s knowledge cutoff is 2 years old. Why doesn\u2019t OpenAI just update it?There are some fundamental reasons for this: training new LLMs is an expensive \u2014 at least tens of millions of dollars \u2014 and not guaranteed process. Cleaning new data sets for training is also expensive.What should I do if I\u2019m building an application that needs more recent data?You may be tempted to just send ChatGPT the entire wikipedia pages for 2022 and 2023: https://en.wikipedia.org/wiki/2022 You\u2019ll soon run into two limits: 1. there is a limit on the number of words you can send to a large language model (LLM). This is called the \u201ccontext window.\u201d 2. LLM APIs charge you by the word, so the more you send it, the more expensive your API calls become.The standard technique is one called \u201cRetrieval Augmented Generation\u201d or RAG. What it is, boiled down very simply, is a process of searching for the right context, giving that context to the LLM, and then getting better results back. At LlamaIndex we are the RAG experts, but there is a whole community of open source projects that are tackling this problem. We have integrated with over 20 open source vector databases and there are other open source tools like LangChain, Semantic Kernel, DSPy, Axilla and others (put your favorites in the comments!) that are attacking the problem in different ways.Another technique is called fine tuning. Here, you essentially create a new custom model on top of an existing LLM. While LlamaIndex does support fine tuning, it often requires much more work and data: What if I don\u2019t need more recent data?That\u2019s totally OK! Not every application needs data that\u2019s more recent than 2021. Before LlamaIndex, I worked on an open source reading education tool, and phonics have definitely not changed in the last two years. If you\u2019re building something to write bedtime stories (\u2764\ufe0f Kidgeni https://kidgeni.com/) or raps (check out TextFX! https://textfx.withgoogle.com/) your applicationWhat if I just want to use ChatGPT with more recent information?There are a lot of chatbots that use Retrieval Augmented Generation currently. A few of the ones I\u2019ve personally tried are Metaphor https://metaphor.systems/, Perplexity https://www.perplexity.ai/ and Medisearch https://medisearch.io/, and of course Google Bard and BingGPT.",
            "related_posts": [
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19",
                "LlamaIndex Newsletter 2024-03-05",
                "Querying a network of knowledge with llama-index-networks"
            ],
            "link": "https://www.llamaindex.ai/blog//chatgpts-knowledge-is-two-year-s-old-what-to-do-if-you-re-building-applications-72ceacde135c"
        },
        {
            "date": "Nov 6, 2023",
            "author": "Huiqiang Jiang",
            "title": "LongLLMLingua: Bye-bye to Middle Loss and Save on Your RAG Costs via Prompt Compression",
            "tags": [
                "large-language-models",
                "llm",
                "retrieval",
                "nlp",
                "chatgpt"
            ],
            "content": "In the RAG, after the retrieval phase, it\u2019s necessary to perform Re-ranking + Fine-Grained Prompt Compression + Subsequence Recovery to enhance LLM\u2019s perception of key information, which is LongLLMLingua.TL;DR: While Retrieval-Augmented Generation (RAG) is highly effective in various scenarios, it still has drawbacks such as 1) Performance drop, like the \u201cLost in the middle\u201d issue, 2) High costs, both financially and in terms of latency, and 3) Context windows limitation. LongLLMLingua offers a solution to these problems in RAG or Long Context scenarios via prompt compression. It can boost accuracy by as much as 21.4% while only using \u00bc of the tokens. In long context situations, it can save $28 for every 1000 examples.See real-world cases on the project page.We previously wrote a blog post introducing the design of LLMLingua, which started from the perspective of designing a special language for LLMs. This time, our focus will be on the scenarios involving RAG.Retrieval-Augmented Generation is currently the most reliable and proven technique for creating AI-agents that are grounded on any specific collection of text. Frameworks like LlamaIndex provide comprehensive RAG solutions to help users utilize specialized data in LLMs more conveniently.A common misunderstanding is that retrieving as many relevant documents as possible during the RAG process and stitching them together to form a long retrieved prompt is beneficial, especially as more and more LLMs support longer context windows. However, this method can introduce more noise into the prompt and weaken the LLM\u2019s perception of key information, leading to issues such as \u2018lost in the middle\u2019[1].These issues become more apparent in real-world scenarios involving RAG. Better retrieval mechanisms can introduce higher quality noise documents, which can more easily lead to a drop in performance.Re-ranking is an intuitive concept.One intuitive idea is to reposition the most relevant information to the sides of the prompt through re-ranking. This concept of re-ranking has already been implemented in frameworks such as LlamaIndex and LangChain.However, according to our experiments, it\u2019s difficult for an embedding model to serve as a \u2018good\u2019 re-ranker. The underlying reason is the lack of an interaction process between the query and the document. The dual-tower structure of embeddings is not suitable for re-ranking in general scenarios, although it may be effective after fine-tuning.Using LLMs directly as a re-ranker may also lead to misjudgments due to hallucinations. Recently, some re-ranking models have been extended from embedding models, such as bge-rerank. However, such re-ranking models generally have context window limitations.To address the above issues, we propose a Question-aware Coarse-Grained prompt compression method. This method evaluates the relevance between the context and the question based on the perplexity corresponding to the question.To mitigate the hallucination problem in smaller LLMs, we append a restrictive statement, specifically \u201cWe can get the answer to this question in the given documents\u201d, after the question to limit the latent space caused by related hallucinations.Figure 1. The accuracy of different methods for ranking documents from Multi-documemnt QA dataset, which increases from top to bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods. You can find the script in this link.Results show that this approach significantly outperforms both embedding models and re-ranking models. We\u2019ve added some recently released embedding and reranking models. As you can see, the performance of bge-rerank-large is very close to that of LongLLMLingua. Reranking models generally perform better than embedding models. Currently, Jina is the best performing method among the embedding models.Compress unrelated and unimportant informationBesides recalling as many relevant documents as possible, another approach is to compress irrelevant or unimportant contexts as much as possible.Previous work on long context has focused on how to extend LLMs to support longer context windows. However, almost no work has explored whether this can actually improve the performance of downstream tasks. Some previous studies have shown that the presence of more noise in the prompt, as well as the position of key information in the prompt, can affect the performance of LLMs.From the perspective of prompt compression, Selective Context[2] and LLMLingua[3] estimate the importance of elements by using a small language model to calculate the mutual information or perplexity of the prompt. However, in scenarios like RAG or long context scenarios, this method can easily lose key information because it cannot perceive the question information.In recent submissions to ICLR\u201924, there have been some similar practices. For example, Recomp[4] reduces the use of tokens in RAG scenarios by jointly training compressors of two different granularities. RAG in Long Context[5] decomposes the long context into a series of chunks and uses retrieval methods for compression, which is actually the retrieval-based method implemented in the LongLLMLingua paper. In addition, Walking Down the Memory Maze[6] also designed a hierarchical summarization tree to enhance the LLM\u2019s perception of key information.Question-aware Fine-grained Prompt CompressionIn order to make token-level prompt compression also perceive the information of the question, we propose a contrastive perplexity, which compares the difference between the perplexity distribution corresponding to the document and the perplexity distribution corresponding to the document with the question.An intuitive feeling is that when the question serves as context, the perplexity corresponding to the relevant tokens in the document will decrease. This decrease in magnitude represents the importance of the tokens in the document relative to the question.Figure 3. Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt QA dataset. The document with the ground truth is located on the left side of the dashed line.Figure 3 shows the distribution difference in extracting key tokens between perplexity and contrastive perplexity.How to reduce the loss in the middleSince Coarse-grained Prompt compression far exceeds other retrieval methods in terms of accuracy, it is a very natural idea to use this ranking information to rearrange the documents that are more related to the question to the beginning and end of the prompt. However, through our testing, we found that rearranging to the beginning of the prompt is more effective than evenly distributing at both ends. So, we choose to reorder the most related document to the beginning of the prompt.How to achieve adaptive granular control during compression?In order to better use the information from the two grained compressions, in the fine-grained prompt compression, we dynamically allocate different compression ratios to different documents based on the rank information obtained from the coarse-grained compression, thereby preserving more important information from important documents.How to improve the integrity of key information?Since LongLLMLingua is a token-level prompt compression, it will inevitably delete some tokens of the word, which may result in some retrieval-related tasks not getting complete results. But this can actually be recovered through a simple subsequence matching method. Specifically, there is a subsequence relationship between the original prompt, compressed prompt and response. By establishing the mapping relationship between the response subsequence that appears in the compressed prompt and the subsequence of the original prompt, the original prompt content can be effectively recovered.ExperimentsTo evaluate the effectiveness of LongLLMLingua, we conducted detailed tests in Multi-document QA (RAG) and two long Context benchmarks. Particularly, the dataset chosen for Multi-document QA is very close to the actual RAG scenario (e.g. Bing Chat), where Contriever (one of the state-of-the-art retrieval systems) is used to recall 20 relevant documents including one ground-truth. The original documents have a high semantic relevance with the question.As can be seen, compared to Retrieval-based methods and compression-based methods, LongLLMLingua improves performance more in the RAG scenario, and can increase up to 21.4 points at a 4x compression rate, avoiding the original \u201clost in the middle\u201d situation.The results of the two benchmarks, LongBench and ZeroScrolls, also reached similar conclusions. LongLLMLingua is better at retaining key information related to the question in long context scenarios.Besides, LongLLMLingua is very efficient and can speed up the end-to-end inference process.Used in LlamaIndexThank Jerry Liu for your help with the LongLLMLingua project. Now you can use LongLLMLingua as a NodePostprocessor in this widely used RAG framework. For specific usage, you can refer to the example 1, example 2 and the following code.from llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.response_synthesizers import CompactAndRefine\nfrom llama_index.indices.postprocessor import LongLLMLinguaPostprocessor\n\nnode_postprocessor = LongLLMLinguaPostprocessor(\n    instruction_str=\"Given the context, please answer the final question\",\n    target_token=300,\n    rank_method=\"longllmlingua\",\n    additional_compress_kwargs={\n        \"condition_compare\": True,\n        \"condition_in_question\": \"after\",\n        \"context_budget\": \"+100\",\n        \"reorder_context\": \"sort\",  # enable document reorder\n        \"dynamic_context_compression_ratio\": 0.4, # enable dynamic compression ratio\n    },\n)References[1] Lost in the Middle: How Language Models Use Long Contexts. Nelson F. Liu etc.\n[2] Compressing Context to Enhance Inference Efficiency of Large Language Models. Yucheng Li etc.\n[3] LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. Huiqiang Jiang, Qianhui Wu etc.\n[4] RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. Fangyuan Xu etc.\n[5] Retrieval meets Long Context Large Language Models. Peng Xu etc.\n[6] Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. Howard Chen etc.",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "LlamaIndex Newsletter 2024-03-19",
                "One-click Open Source RAG Observability with Langfuse"
            ],
            "link": "https://www.llamaindex.ai/blog//longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7"
        },
        {
            "date": "Jul 9, 2024",
            "author": "LlamaIndex",
            "title": "LlamaIndex Newsletter 2024-07-09",
            "tags": [],
            "content": "Hello, Llama Lovers! \ud83e\udd99Welcome to this week\u2019s edition of the LlamaIndex newsletter! We\u2019re thrilled to share some exciting updates about llama-agents, along with demos, extensive guides, and in-depth tutorials to enhance your understanding of our tools.Before we dive into our newsletter, we\u2019re excited to announce the return of Community Office Hours. If you have use-cases, in-depth questions, or feedback for the team at LlamaIndex, join us during our community office hours! We\u2019ll set up a 15-30 minute Zoom call to discuss it.Sign up here to participate.\ud83e\udd29\u00a0The highlights:Multi-Agent Kubernetes Kit Launched: Deploy multi-agent systems easily with our new Kubernetes Starter Kit featuring ready-to-use tools and configurations. Notebook, Tweet.Enhanced Communication with RabbitMQ: Boost multi-agent system reliability and scalability in production with our new RabbitMQ integration. Notebook, Tweet.Reflection as a Service Guide: Improve agent reliability with our guide on building Reflection as a Service, perfect for output validation and correction. Notebook, Tweet.Corrective RAG as a Service Guide: Create a self-correcting RAG that ensures context relevance and integrates search fallbacks before generation. Notebook, Tweet.Tutorial series on Property Graphs: 6-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j, and Ollama. Videos, Tweet.\u2728 Feature Releases and Enhancements:We have launched a Multi-Agent on Kubernetes Starter Kit to build and deploy a multi-agent system using Docker Compose and Kubernetes using llama-agents. This kit includes prebuilt agent loops and tools, as well as Dockerfiles and Kubernetes manifests for easy production deployment. Notebook, Tweet.We have integrated RabbitMQ with llama-agents to enhance multi-agent communication, offering scalability and reliability for handling large request volumes in production. Notebook, Tweet.[Yi-01.AI](http://Yi-01.AIhttps://x.com/01AI_Yi) is integrated with LlamaIndex for enhanced retrieval and indexing, streamlining the development of smarter, faster RAG applications. Docs.We have launched a 6-part video series on Property Graphs in LlamaIndex using MistralAI, Neo4j and Ollama. Tweet.\ud83d\udca1\u00a0Demos:OpenContracts by John Scrudato: A fully open-source, AI-powered Document Analytics Tool, integrates genAI capabilities and LlamaIndex for robust query handling and data extraction across documents. This tool is particularly valuable for legal analysis, enabling users to manage, process, and query vast arrays of contracts and legal documents. Docs.\ud83d\uddfa\ufe0f Guides:Guide to build Reflection as a Service to enhance agent reliability with our new standalone service, ideal for validating and correcting outputs across multiple agents. Notebook, Tweet.Guide to build Corrective RAG as a Service, a self-correcting RAG that dynamically validates context relevance, seamlessly integrating web search fallbacks before generation. Notebook, Tweet.\u270d\ufe0f Tutorials:Pavan Kumar\u2019s tutorial to build a RAG pipeline that lives on a Raspberry Pi device with docker, Ollama, Qdrant, and using LlamaIndex as the orchestration layer.Trade Mamba\u2019s video tutorial to build an AI-enabled trading assistant using LlamaIndex\u2019s agent/tool/RAG abstractions for tasks like tracking portfolio values, managing stock orders, and conducting vector searches for semantic information.Giskard\u2019s toolkit enables diverse question generation featuring question types like simple, complex, distracting, situational, double, and conversational for RAG evaluation, as demonstrated in the tutorial on using a LlamaIndex pipeline with an IPCC Climate Report.Pavan Kumar\u2019s tutorial demonstrates building a Multi-Document Financial Analyst Agent using LlamaIndex RAG and ReAct tools, analyzing categorized SEC documents with SnowflakeDB embeddings and MistralAI via Ollama.Ross A.\u2019s tutorial on retrieval evaluations for RAG delves into essential metrics like precision@K and NDCG, and demonstrates how to convert datasets to BEIR format for assessing LlamaIndex retrievers.\ud83c\udfa5\u00a0Webinar:Join us for a webinar on July 10th, featuring Jerry Liu (LlamaIndex) and Ayush Thakur (Weights & Biases) on A Principled Approach to RAG Experimentation + Evaluation to learn how to build, evaluate, and refine RAG pipelines.",
            "related_posts": [],
            "link": "https://www.llamaindex.ai/blog//llamaindex-newsletter-2024-07-09"
        },
        {
            "date": "Jan 31, 2024",
            "author": "Ravi Theja",
            "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG",
            "tags": [
                "llamaindex",
                "llm",
                "openai",
                "retrieval-augmented",
                "ai"
            ],
            "content": "IntroductionRetrieving the appropriate chunks, nodes, or context is a critical aspect of building an efficient Retrieval-Augmented Generation (RAG) application. However, a vector or embedding-based search may not be effective for all types of user queries.To address this, Hybrid search combines both keyword-based methods (BM25) and vector (embedding) search techniques. Hybrid search has a specific parameter, Alpha to balance the weightage between keyword (BM25) and vector search in retrieving the right context for your RAG application. (alpha=0.0 - keyword search (BM25) and alpha=1.0 - vector search)But here\u2019s where it gets interesting: fine-tuning Alpha isn\u2019t just a task; it\u2019s an art form. Achieving the ideal balance is crucial for unlocking the full potential of hybrid search. This involves adjusting different Alpha values for various types of user queries in your RAG system.In this blog post, we will look into tuning Alpha within the Weaviate vector database using the Retrieval Evaluation module of LlamaIndex with and without rerankers with the help of Hit Rate and MRR metrics.Before diving into the implementation, let\u2019s first understand the different query types and metrics we will be using in this article.Different User Query Types:User queries in an RAG application vary based on individual intent. For these diverse query types, it\u2019s essential to fine-tune the Alpha parameter. This process involves routing each user query to a specific Alpha value for effective retrieval and response synthesis. Microsoft has identified various user query categories, and we have selected a few for tuning our hybrid search. Below are the different user query types we considered:Web Search Queries: Brief queries similar to those typically inputted into search engines.Concept Seeking Queries: Abstract questions that necessitate detailed, multi-sentence answers.Fact Seeking Queries: Queries that have a single, definitive answer.Keyword Queries: Concise queries composed solely of crucial identifier words.Queries With Misspellings: Queries containing typos, transpositions, and common misspellings.Exact Sub-string Searches: Queries that exactly match sub-strings from the original context.Let\u2019s look at sample examples in each of these different user query types:Web Search QueriesTransfer capabilities of LLaMA language model to non-English languages2. Concept Seeking QueriesWhat is the dual-encoder architecture used in recent works on dense retrievers?3. Fact Seeking QueriesWhat is the total number of propositions the English Wikipedia dump is segmented into in FACTOID WIKI?4. Keyword QueriesGTR retriever recall rate5. Queries With MisspellingsWhat is the advntage of prposition retrieval over sentnce or passage retrieval?6. Exact Sub-string Searchesfirst kwords for the GTR retriever. Finer-grainedRetrieval Evaluation Metrics:We will utilize Hit Rate and MRR metrics for retrieval evaluation. Let\u2019s get into understanding these metrics.Hit Rate:Hit Rate measures the proportion of queries for which the correct chunk/ context appears within the top-k results chunks/ contexts. Put simply, it evaluates how frequently our system correctly identifies the chunk within its top-k chunks.Mean Reciprocal Rank (MRR):MRR assesses a system\u2019s accuracy by taking into account the position of the highest-ranking relevant chunk/ context for each query. It calculates the average of the inverse of these positions across all queries. For instance, if the first relevant chunk/ context is at the top of the list, its reciprocal rank is 1. If it\u2019s the second item, the reciprocal rank becomes 1/2, and this pattern continues accordingly.The remainder of this blog post is divided into two main sections:Implementing Alpha Tuning in Hybrid Search for Various Query Types.Analyzing the results of two different document datasets:Indexing a Single Document: The LLM Compiler Paper.Indexing Three Documents: The LLM Compiler, Llama Beyond English, and Dense X Retrieval Papers.You can also continue following along in the Google Colab Notebook from this point forward.ImplementationWe will adopt a systematic approach to implement the experimental workflow, which involves the following steps:Data Download.Data Loading.Weaviate Client Setup.Index Creation and Node Insertion.Define LLM (GPT-4)Define CohereAI Reranker.Generation of Synthetic Queries for Various Query Types.Define CustomRetriever.Functions for Retrieval Evaluation and Metrics Calculation.Conducting Retrieval Evaluation for Different Query Types and Alpha Values.Let\u2019s begin by defining some essential functions for our implementation.get_weaviate_client - sets up weaviate client.load_documents - load the documents from the file path.create_nodes - create nodes by chunking the documents using a text splitter.connect_index - connect to weaviate index.insert_nodes_index - insert nodes into the index.def get_weaviate_client(api_key, url):\n  auth_config = weaviate.AuthApiKey(api_key=api_key)\n\n  client = weaviate.Client(\n    url=url,\n    auth_client_secret=auth_config\n  )\n  return client\n\ndef load_documents(file_path, num_pages=None):\n  if num_pages:\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()[:num_pages]\n  else:\n    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n  return documents\n\ndef create_nodes(documents, chunk_size=512, chunk_overlap=0):\n  node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n  nodes = node_parser.get_nodes_from_documents(documents)\n  return nodes\n\ndef connect_index(weaviate_client):\n  vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)\n  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n  index = VectorStoreIndex([], storage_context=storage_context)\n  return index\n\ndef insert_nodes_index(index, nodes):\n  index.insert_nodes(nodes)Download Data!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.04511.pdf\" -O \"llm_compiler.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2401.01055.pdf\" -O \"llama_beyond_english.pdf\"\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2312.06648.pdf\" -O \"dense_x_retrieval.pdf\"2. Load Data# load documents, we will skip references and appendices from the papers.\ndocuments1 = load_documents(\"llm_compiler.pdf\", 12)\ndocuments2 = load_documents(\"dense_x_retrieval.pdf\", 9)\ndocuments3 = load_documents(\"llama_beyond_english.pdf\", 7)\n\n# create nodes\nnodes1 = create_nodes(documents1)\nnodes2 = create_nodes(documents2)\nnodes3 = create_nodes(documents3)3. Setup Weaviate Clienturl = 'cluster URL'\napi_key = 'your api key'\n\nclient = get_weaviate_client(api_key, url)4. Create an Index and Insert Nodes.index = connect_index(client)\n\ninsert_nodes_index(index, nodes1)5. Define LLM# Deing LLM for query generation\nllm = OpenAI(model='gpt-4', temperature=0.1)6. Create Synthetic QueriesWe will create queries as discussed earlier, check prompts for each of the query types in the notebook, and code for each type of query. Showing code snippet for reference.queries = generate_question_context_pairs(\n    nodes, \n  llm=llm, \n  num_questions_per_chunk=2, \n  qa_generate_prompt_tmpl = qa_template\n)7. Define rerankerreranker = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=4)8. Define CustomRetrieverWe will define CustomRetriever class to perform retrieval operations with and without a reranker.class CustomRetriever(BaseRetriever):\n    \"\"\"Custom retriever that performs hybrid search with and without reranker\"\"\"\n\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n        reranker: CohereRerank\n    ) -&gt; None:\n        \"\"\"Init params.\"\"\"\n\n        self._vector_retriever = vector_retriever\n        self._reranker = reranker\n\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\"\"\"\n\n        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n        if self._reranker != None:\n            retrieved_nodes = self._reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n        else:\n            retrieved_nodes = retrieved_nodes[:4]\n\n        return retrieved_nodes\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        \"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"\n        return self._retrieve(query_bundle)\n\n    async def aretrieve(self, str_or_query_bundle: QueryType) -&gt; List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return await self._aretrieve(str_or_query_bundle)9. Define functions for retriever evaluation and metrics computationWe will look into retriever performance for different alpha values with and without reranker.# Alpha values and datasets to test\nalpha_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n\n# Function to evaluate retriever and return results\nasync def evaluate_retriever(alpha, dataset, reranker=None):\n    retriever = VectorIndexRetriever(index,\n                                     vector_store_query_mode=\"hybrid\",\n                                     similarity_top_k=10,\n                                     alpha=alpha)\n    custom_retriever = CustomRetriever(retriever,\n                                       reranker)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names([\"mrr\", \"hit_rate\"], retriever=custom_retriever)\n    eval_results = await retriever_evaluator.aevaluate_dataset(dataset)\n    return eval_results\n\n# Function to calculate and store metrics\ndef calculate_metrics(eval_results):\n    metric_dicts = []\n    for eval_result in eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[\"hit_rate\"].mean()\n    mrr = full_df[\"mrr\"].mean()\n    return hit_rate, mrr10. Retrieval EvaluationHere we do retrieval evaluation on different query types (datasets) and alpha values to understand which alpha will be suitable for which query type. You need to plug in the reranker accordingly to compute the retrieval evaluation with and without the reranker.# Asynchronous function to loop over datasets and alpha values and evaluate\nasync def main():\n    results_df = pd.DataFrame(columns=['Dataset', 'Alpha', 'Hit Rate', 'MRR'])\n\n    for dataset in datasets_single_document.keys():\n        for alpha in alpha_values:\n            eval_results = await evaluate_retriever(alpha, datasets_single_document[dataset])\n            hit_rate, mrr = calculate_metrics(eval_results)\n            new_row = pd.DataFrame({'Dataset': [dataset], 'Alpha': [alpha], 'Hit Rate': [hit_rate], 'MRR': [mrr]})\n            results_df = pd.concat([results_df, new_row], ignore_index=True)\n\n    # Determine the grid size for subplots\n    num_rows = len(datasets_single_document) // 2 + len(datasets_single_document) % 2\n    num_cols = 2\n\n    # Plotting the results in a grid\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 4), squeeze=False)  # Ensure axes is always 2D\n\n    for i, dataset in enumerate(datasets_single_document):\n        ax = axes[i // num_cols, i % num_cols]\n        dataset_df = results_df[results_df['Dataset'] == dataset]\n        ax.plot(dataset_df['Alpha'], dataset_df['Hit Rate'], marker='o', label='Hit Rate')\n        ax.plot(dataset_df['Alpha'], dataset_df['MRR'], marker='o', linestyle='--', label='MRR')\n        ax.set_xlabel('Alpha')\n        ax.set_ylabel('Metric Value')\n        ax.set_title(f'{dataset}')\n        ax.legend()\n        ax.grid(True)\n\n    # If the number of datasets is odd, remove the last (empty) subplot\n    if len(datasets_single_document) % num_cols != 0:\n        fig.delaxes(axes[-1, -1])  # Remove the last subplot if not needed\n\n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    plt.show()\n\n# Run the main function\nasyncio.run(main())Analyze the results:Having completed the implementation phase, we now turn our attention to analyzing the outcomes. We conducted two sets of experiments: one on a single document and another on multiple documents. These experiments varied in alpha values, types of user queries, and the inclusion or exclusion of a reranker. The accompanying graphs display the results, focusing on the Hit Rate and MRR (Mean Reciprocal Rank) as retrieval evaluation metrics.Please keep in mind that following observations are specific to the datasets used in our study. We encourage you to conduct the experiment with your own documents and draw your relevant observations and conclusions.With Single Document:Without Reranker:With Reranker:With Multiple Documents:Without Reranker:With Reranker:Observations:There is a boost in Hit Rate and MRR in single and multiple documents indexing with the help of a reranker. Time and again it proves using reranker is pretty useful in your RAG application.Though most of the time hybrid search wins over keyword/ vector search, it should be carefully evaluated for different query types based on user queries in the RAG application.The behavior is different when you index a single document and multiple documents, which suggests it\u2019s always better to tune alpha as you add documents into the index.Let\u2019s look at a deeper analysis of different query types:Web Search Queries:\u2014 MRR is higher with hybrid search with alpha=0.2/0.6 based on with/ without rerankers irrespective of single/ multiple documents indexing.\u2014 The Hit rate is higher with alpha=1.0 for both single/ multiple documents indexing and with/ without rerankers.Concept Seeking Queries:\u2014 MRR and Hit Rate are higher with hybrid search (with different alpha values) in Multiple documents indexing.\u2014 MRR and Hit Rate are higher at Alpha=0.0 indicating keyword search works better in Single document indexing. Should be noted that MRR has different behavior with and without reranking.Fact Seeking Queries\u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in Multiple documents indexing.\u2014 MRR and Hit Rate are higher with hybrid search with reranker and keyword search (alpha=0.0) is better without reranker in single documents indexing.Keyword Queries\u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in Multiple documents indexing.\u2014 MRR and Hit Rate are higher with hybrid search with reranker and keyword search is better without reranker in single documents indexing. (though MRR is slightly higher with alpha=0.2)Queries With Misspellings\u2014 MRR and Hit Rate are higher with Hybrid search with/ without reranker in single and multiple documents indexing. (Though in some cases hybrid search with alpha=1.0 wins).\u2014 This also demonstrates that vector search performs better with misspelled queries, as keyword searches lose effectiveness in such cases.Exact Sub-string Searches\u2014 MRR and Hit Rate are higher with Keyword search with/ without reranker in Single documents indexing and without reranker in multiple documents indexing.\u2014 MRR and Hit Rate are higher with Hybrid search (alpha=0.4) with reranker in multiple documents indexing.What\u2019s Next?In this blog post, we looked into the tuning of Alpha in a hybrid search system for a range of query types. It was interesting to see how the results varied when indexing either a single document or multiple documents. Going forward, you might consider experimenting with documents from diverse domains, employing different query lengths for various query types. Should you come across any noteworthy observations, we encourage you to share them with us in the comments. It would certainly be interesting to discuss these findings with the wider community.References:Hybrid Search ExplainedAzure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities",
            "related_posts": [
                "LlamaIndex Newsletter 2024-04-02",
                "LlamaIndex Newsletter 2024-03-26",
                "Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations",
                "LlamaIndex Newsletter 2024-03-19"
            ],
            "link": "https://www.llamaindex.ai/blog//llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00"
        },
        {
            "date": "May 14, 2024",
            "author": "Kate Silverstein",
            "title": "Using LlamaIndex and llamafile to build a local, private research assistant",
            "tags": [
                "llamafile"
            ],
            "content": "This is a guest post from our friends at Mozilla about Llamafilellamafile, an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from HuggingFace then run the file. That's it. On most computers, you won't need to install anything.There are a few reasons why you might want to run an LLM on your laptop, including:1. Privacy: Running locally means you won't have to share your data with third parties.2. High availability: Run your LLM-based app without an internet connection.3. Bring your own model: You can easily test many different open-source LLMs (anything available on HuggingFace) and see which one works best for your task.4. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-based system without paying for API calls.In this blog post, we'll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we'll show how to use LlamaIndex with your llamafile as the LLM & embedding backend for a local RAG-based research assistant. You won't have to sign up for any cloud service or send your data to any third party--everything will just run on your laptop.Note: You can also get all of the example code below as a Jupyter notebook from our GitHub repo.Download and run a llamafileFirst, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There's nothing to install or configure (with a few caveats, discussed here).Each llamafile bundles 1) model weights & metadata in gguf format + 2) a copy of `llama.cpp` specially compiled using [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT-like browser interface, a CLI, and an OpenAI-compatible REST API for chat models.There are only 2 steps to setting up a llamafile:1. Download a llamafile2. Make the llamafile executableWe'll go through each step in detail below.Step 1: Download a llamafileThere are many llamafiles available on the HuggingFace model hub (just search for 'llamafile') but for the purpose of this walkthrough, we'll use TinyLlama-1.1B (0.67 GB, model info). To download the model, you can either click this download link: TinyLlama-1.1B or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection.wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile This model is small and won't be very good at actually answering questions but, since it's a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it's good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like Mistral-7B-Instruct (5.15 GB, model info).Step 2: Make the llamafile executableIf you didn't download the llamafile from the command line, figure out where your browser stored your downloaded llamafile.Now, open your computer's terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile`If you're using macOS, Linux, or BSD, you'll need to grant permission for your computer to execute this new file. (You only need to do this once.):If you're on Windows, instead just rename the file by adding \".exe\" on the end e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafileKick the tiresNow, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded:./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n\nllamafile v0.7.0This post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don't work as expected, please post an issue on the llamafile issue tracker.The easiest way to use your llamafile is via its built-in chat interface. In a terminal, run./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafileYour browser should open automatically and display a chat interface. (If it doesn't, just open your browser and point it at http://localhost:8080). When you're done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you're running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile.In the rest of this walkthrough, we'll be using the llamafile's built-in inference server instead of the browser interface. The llamafile's server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available here. To start the llamafile in server mode, run:./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embeddingSummary: Download and run a llamafile# 1. Download the llamafile-ized model\nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n\n# 2. Make it executable (you only need to do this once)\nchmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n# 3. Run in server mode\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embeddingBuild a research assistant using LlamaIndex and llamafileNow, we'll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose homing pigeons. We'll show how to prepare your data, index into a vector store, then query it.One of the nice things about running an LLM locally is privacy. You can mix both \"public data\" like Wikipedia pages and \"private data\" without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won't have to worry about leaking data. Below, we'll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic.To get started, download our example data:mkdir data\n\n# Download 'The Homing Pigeon' manual from Project Gutenberg\nwget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n\n# Download some notes on homing pigeons\nwget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.mdNext, we'll need to install LlamaIndex and a few of its integrations:# Install llama-index\npip install llama-index-core\n# Install llamafile integrations and SimpleWebPageReader\npip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-webStart your llamafile server and configure LlamaIndexIn this example, we'll use the same llamafile to both produce the embeddings that will get indexed in our vector store and as the LLM that will answer queries later on. (However, there is no reason you can't use one llamafile for the embeddings and separate llamafile for the LLM functionality--you would just need to start the llamafile servers on different ports.)To start the llamafile server, open a terminal and run:./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080Now, we'll configure LlamaIndex to use this llamafile:# Configure LlamaIndex\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.llamafile import LlamafileEmbedding\nfrom llama_index.llms.llamafile import Llamafile\nfrom llama_index.core.node_parser import SentenceSplitter\n\nSettings.embed_model = LlamafileEmbedding(base_url=\"http://localhost:8080\")\n\nSettings.llm = Llamafile(\n\tbase_url=\"http://localhost:8080\",\n\ttemperature=0,\n\tseed=0\n)\n\n# Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don't take up the model's entire\n# context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both\n# the system prompt and the user's actual question.\nSettings.transformations = [\n\tSentenceSplitter(\n    \tchunk_size=256,\n    \tchunk_overlap=5\n\t)\n]Prepare your data and build a vector storeNow, we'll load our data and index it.# Load local data\nfrom llama_index.core import SimpleDirectoryReader\nlocal_doc_reader = SimpleDirectoryReader(input_dir='./data')\ndocs = local_doc_reader.load_data(show_progress=True)\n\n# We'll load some Wikipedia pages as well\nfrom llama_index.readers.web import SimpleWebPageReader\nurls = [\n\t'https://en.wikipedia.org/wiki/Homing_pigeon',\n\t'https://en.wikipedia.org/wiki/Magnetoreception',\n]\nweb_reader = SimpleWebPageReader(html_to_text=True)\ndocs.extend(web_reader.load_data(urls))\n\n# Build the index\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(\n\tdocs,\n\tshow_progress=True,\n)\n\n# Save the index\nindex.storage_context.persist(persist_dir=\"./storage\")Query your research assistantFinally, we're ready to ask some questions about homing pigeons.query_engine = index.as_query_engine()\nprint(query_engine.query(\"What were homing pigeons used for?\"))\tHoming pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection.print(query_engine.query(\"When were homing pigeons first used?\"))The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries.ConclusionIn this post, we've shown how to download and set up an LLM running locally via llamafile. Then, we showed how to use this LLM with LlamaIndex to build a simple RAG-based research assistant for learning about homing pigeons. Your assistant ran 100% locally: you didn't have to pay for API calls or send data to a third party.As a next step, you could try running the examples above with a better model like Mistral-7B-Instruct. You could also try building a research assistant for different topic like \"semiconductors\" or \"how to bake bread\".To find out more about llamafile, check out the project on GitHub, read this blog post on bash one-liners using LLMs, or say hi to the community on Discord.",
            "related_posts": [
                "Using LlamaIndex and llamafile to build a local, private research assistant"
            ],
            "link": "https://www.llamaindex.ai/blog//using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"
        },
        {
            "date": "May 28, 2023",
            "author": "Jerry Liu",
            "title": "Combining Text-to-SQL with Semantic Search for Retrieval Augmented Generation",
            "tags": [
                "artificial-intelligence",
                "machine-learning",
                "nlp",
                "data-science",
                "database"
            ],
            "content": "SummaryIn this article, we showcase a powerful new query engine ( SQLAutoVectorQueryEngine ) in LlamaIndex that can leverage both a SQL database as well as a vector store to fulfill complex natural language queries over a combination of structured and unstructured data. This query engine can leverage the expressivity of SQL over structured data, and join it with unstructured context from a vector database. We showcase this query engine on a few examples and show that it can handle queries that make use of both structured/unstructured data, or either.Check out the full guide here: https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html.ContextData lakes in enterprises typically encompass both structured and unstructured data. Structured data is typically stored in a tabular format in SQL databases, organized into tables with predefined schemas and relationships between entities. On the other hand, unstructured data found in data lakes lacks a predefined structure and does not fit neatly into traditional databases. This type of data includes text documents, but also other multimodal formats such as audio recordings, videos, and more.Large Language Models (LLMs) have the ability to extract insights from both structured and unstructured data. There have been some initial tooling and stacks that have emerged for tackling both types of data:Text-to-SQL (Structured data): Given a collection of tabular schemas, we convert natural language into a SQL statement which can then be executed against the database.Semantic Search with a Vector Database (Unstructured Data): Store unstructured documents along with their embeddings in a vector database (e.g. Pinecone, Chroma, Milvus, Weaviate, etc.). During query-time, fetch the relevant documents by embedding similarity, and then put into the LLM input prompt to synthesize a response.Each of these stacks solves particular use cases.Text-to-SQL Over Structured DataIn the structured setting, SQL is an extremely expressive language for operating over tabular data \u2014 in the case of analytics, you can get aggregations, join information across multiple tables, sort by timestamp, and much more. Using the LLM to convert natural language to SQL can be thought as a program synthesis \u201ccheat code\u201d \u2014 just let the LLM compile to the right SQL query, and let the SQL engine on the database handle the rest!Use Case: Text-to-SQL queries are well-suited for analytics use cases where the answer can be found by executing a SQL statement. They are not suited for cases where you\u2019d need more detail than what is found in a structured table, or if you\u2019d need more sophisticated ways of determining relevance to the query beyond simple constructs like WHERE conditions.Example queries suited for Text-to-SQL:\u201cWhat is the average population of cities in North America\u201d?\u201cWhat are the largest cities and populations in each respective continent?\u201dSemantic Search over Unstructured DataIn the unstructured setting, the behavior for retrieval-augmented generation systems is to first perform retrieval and then synthesis. During retrieval, we first look up the most relevant documents to the query by embedding similarity. Some vector stores support being able to handle additional metadata filters for retrieval. We can choose to manually specify the set of required filters, or have the LLM \u201cinfer\u201d what the query string and metadata filters should be (see our auto-retrieval modules in LlamaIndex or LangChain\u2019s self-query module).Use Case: Retrieval Augmented Generation is well suited for queries where the answer can be obtained within some sections of unstructured text data. Most existing vector stores (e.g. Pinecone, Chroma) do not offer a SQL-like interface; hence they are less suited for queries that involve aggregations, joins, sums, etc.Example queries suited for Retrieval Augmented Generation\u201cTell me about the historical museums in Berlin\u201d\u201cWhat does Jordan ask from Nick on behalf of Gatsby?\u201dCombining These Two SystemsFor some queries, we may want to make use of knowledge in both structured tables as well as vector databases/document stores in order to give the best answer to the query. Ideally this can give us the best of both worlds: the analytics capabilities over structured data, and semantic understanding over unstructured data.Here\u2019s an example use case:You have access to a collection of articles about different cities, stored in a vector databaseYou also have access to a structured table containing statistics for each city.Given this data collection, let\u2019s take an example query: \u201cTell me about the arts and culture of the city with the highest population.\u201dThe \u201cproper\u201d way to answer this question is roughly as follows:Query the structured table for the city with the highest population.SELECT city, population FROM city_stats ORDER BY population DESC LIMIT 1Convert the original question into a more detailed question: \u201cTell me about the arts and culture of Tokyo.\u201dAsk the new question over your vector database.Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer.Let\u2019s think about some of the high-level implications of such a sequence:Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first \u201cretrieval\u201d step.We want to make sure that we can somehow \u201cjoin\u201d the results from the SQL query with the context stored in the vector database. There is no existing language to \u201cjoin\u201d information between a SQL and vector database. We will have to implement this behavior ourselves.Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population.A Query Engine to Combine Structured Analytics and Semantic SearchWe have created a brand-new query engine ( SQLAutoVectorQueryEngine ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer.The SQLAutoVectorQueryEngine is initialized through passing in a SQL query engine ( GPTNLStructStoreQueryEngine ) as well as a query engine that uses our vector store auto-retriever module ( VectorIndexAutoRetriever ). Both the SQL query engine and vector query engines are wrapped as \u201cTool\u201d objects containing a name and description field.Reminder: the VectorIndexAutoRetriever takes in a natural language query as input. Given some knowledge of the metadata schema of the vector database, the auto retriever first infers the other necessary query parameters to pass in (e.g. top-k value, and metadata filters), and executes a query against the vector database with all the query parameters.Diagram of the flow for SQLAutoVectorQueryEngineDuring query-time, we run the following steps:A selector prompt (similarly used in our RouterQueryEngine , see guide) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the RetrieverQueryEngine with VectorIndexAutoRetriever .If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output.A query transformation is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is \u201cTell me about the arts and culture of the city with the highest population.\u201d, and the SQL query returns Tokyo as the city with the highest population, then the new query is \u201cTell me about the arts and culture of Tokyo.\u201d The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response.The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a VectorIndexAutoRetriever module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like query_str=\"arts and culture\" and filters={\"title\": \"Tokyo\"} .The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer.Taking a step back, here are some general comments about this approach:Using our auto-retrieval module is our way of simulating a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with.This also implies that there doesn\u2019t need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and document store metadata though; that way we don\u2019t need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters.ExperimentsSo how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection.SetupOur experiment setup is very simple. We have a SQL table called city_stats which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin.We also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate \u201cNode\u201d object; each chunk also contains a title metadata attribute containing the city name.We then derive the VectorIndexAutoRetriever and RetrieverQueryEngine from the Pinecone vector index.from llama_index.indices.vector_store.retrievers import VectorIndexAutoRetriever\nfrom llama_index.vector_stores.types import MetadataInfo, VectorStoreInfo\nfrom llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n\n\nvector_store_info = VectorStoreInfo(\n    content_info='articles about different cities',\n    metadata_info=[\n        MetadataInfo(\n            name='city', \n            type='str', \n            description='The name of the city'),\n    ]\n)\nvector_auto_retriever = VectorIndexAutoRetriever(vector_index, vector_store_info=vector_store_info)\n\nretriever_query_engine = RetrieverQueryEngine.from_args(\n    vector_auto_retriever, service_context=service_context\n)You can also get the SQL query engine as followssql_query_engine = sql_index.as_query_engine()Both the SQL query engine and vector query engine can be wrapped as QueryEngineTool objects.sql_tool = QueryEngineTool.from_defaults(\n    query_engine=sql_query_engine,\n    description=(\n        'Useful for translating a natural language query into a SQL query over a table containing: '\n        'city_stats, containing the population/country of each city'\n    )\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=query_engine,\n    description=f'Useful for answering semantic questions about different cities',\n)Finally, we can define our SQLAutoVectorQueryEnginequery_engine = SQLAutoVectorQueryEngine(\n    sql_tool,\n    vector_tool,\n    service_context=service_context\n)ResultsWe run some example queries.Query 1query_engine.query(\n  'Tell me about the arts and culture of the city with the highest population'\n)Intermediate steps:Final Response:Tokyo is the city with the highest population, with 13.96 million people. It is a vibrant city with a rich culture and a wide variety of art forms. From traditional Japanese art such as calligraphy and woodblock prints to modern art galleries and museums, Tokyo has something for everyone. There are also many festivals and events throughout the year that celebrate the city's culture and art. Some popular festivals and events in Tokyo include the Sann\u014d Festival at Hie Shrine, the Sanja Festival at Asakusa Shrine, and the biennial Kanda Festivals. These events often feature parades with elaborately decorated floats and thousands of people. Additionally, an enormous fireworks display over the Sumida River takes place annually on the last Saturday of July, attracting over a million viewers. During spring, when cherry blossoms bloom, many residents gather in Ueno Park, Inokashira Park, and the Shinjuku Gyoen National Garden for picnics under the blossoms.This query runs through the full flow of the SQLAutoVectorQueryEngine . It first queries the SQL database for the city with the highest population (\u201cTokyo\u201d), and then queries the vector database with the new query. The results are combined into a final response.Query 2query_engine.query(\"Tell me about the history of Berlin\")Intermediate Steps:Final Response:Berlin's history dates back to the early 13th century when it was founded as a small settlement. In 1618, the Margraviate of Brandenburg entered into a personal union with the Duchy of Prussia, and in 1701, they formed the Kingdom of Prussia with Berlin as its capital. The city grew and merged with neighboring cities, becoming a center of the Enlightenment under the rule of Frederick the Great in the 18th century.\n\nThe Industrial Revolution in the 19th century transformed Berlin, expanding its economy, population, and infrastructure. In 1871, it became the capital of the newly founded German Empire. The early 20th century saw Berlin as a hub for the German Expressionist movement and a major world capital known for its contributions to science, technology, arts, and other fields.\n\nIn 1933, Adolf Hitler and the Nazi Party came to power, leading to a decline in Berlin's Jewish community and the city's involvement in World War II. After the war, Berlin was divided into East and West Berlin, with the former under Soviet control and the latter under the control of the United States, United Kingdom, and France. The Berlin Wall was built in 1961, physically and ideologically dividing the city until its fall in 1989. Following the reunification of Germany in 1990, Berlin once again became the capital of a unified Germany and has since continued to grow and develop as a major global city.This query only requires the vector database and not the SQL database. The initial selector correctly identifies that we should just query the vector database and return the result.Query 3query_engine.query('Can you give me the country corresponding to each city?')Intermediate StepsFinal Response Toronto is in Canada, Tokyo is in Japan, and Berlin is in Germany.This query can be answered by just querying the SQL database, it does not need additional information from the vector database. The query transform step correctly identifies \u201cNone\u201d as the followup question, indicating that the original question has been answered.ConclusionSo far, the stacks around LLMs + unstructured data and LLMs + structured data have largely been separate. We\u2019re excited about how combining LLMs on top of both structured and unstructured data can unlock new retrieval/query capabilities in novel and interesting ways!We\u2019d love for you to try out the SQLAutoVectorQueryEngine and let us know what you think.The full notebook walkthrough can be found in this guide (associated notebook).",
            "related_posts": [
                "Querying a network of knowledge with llama-index-networks",
                "How to build LLM Agents in TypeScript with LlamaIndex.TS",
                "Agentic RAG With LlamaIndex",
                "How to train a custom GPT on your data with EmbedAI + LlamaIndex"
            ],
            "link": "https://www.llamaindex.ai/blog//combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b"
        }
    ]
}